<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Into the Unknown – models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../about.html">Stephen Barrie</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#model-summaries" id="toc-model-summaries" class="nav-link active" data-scroll-target="#model-summaries">Model Summaries</a>
  <ul class="collapse">
  <li><a href="#big-transfer-resnetv2-bit-resnetv2.py" id="toc-big-transfer-resnetv2-bit-resnetv2.py" class="nav-link" data-scroll-target="#big-transfer-resnetv2-bit-resnetv2.py">Big Transfer ResNetV2 (BiT) [resnetv2.py]</a></li>
  <li><a href="#cross-stage-partial-networks-cspnet.py" id="toc-cross-stage-partial-networks-cspnet.py" class="nav-link" data-scroll-target="#cross-stage-partial-networks-cspnet.py">Cross-Stage Partial Networks [cspnet.py]</a></li>
  <li><a href="#densenet-densenet.py" id="toc-densenet-densenet.py" class="nav-link" data-scroll-target="#densenet-densenet.py">DenseNet [densenet.py]</a></li>
  <li><a href="#dla-dla.py" id="toc-dla-dla.py" class="nav-link" data-scroll-target="#dla-dla.py">DLA [dla.py]</a></li>
  <li><a href="#dual-path-networks-dpn.py" id="toc-dual-path-networks-dpn.py" class="nav-link" data-scroll-target="#dual-path-networks-dpn.py">Dual-Path Networks [dpn.py]</a></li>
  <li><a href="#gpu-efficient-networks-byobnet.py" id="toc-gpu-efficient-networks-byobnet.py" class="nav-link" data-scroll-target="#gpu-efficient-networks-byobnet.py">GPU-Efficient Networks [byobnet.py]</a></li>
  <li><a href="#hrnet-hrnet.py" id="toc-hrnet-hrnet.py" class="nav-link" data-scroll-target="#hrnet-hrnet.py">HRNet [hrnet.py]</a></li>
  <li><a href="#inception-v3-inception_v3.py" id="toc-inception-v3-inception_v3.py" class="nav-link" data-scroll-target="#inception-v3-inception_v3.py">Inception-V3 [inception_v3.py]</a></li>
  <li><a href="#inception-v4-inception_v4.py" id="toc-inception-v4-inception_v4.py" class="nav-link" data-scroll-target="#inception-v4-inception_v4.py">Inception-V4 [inception_v4.py]</a></li>
  <li><a href="#inception-resnet-v2-inception_resnet_v2.py" id="toc-inception-resnet-v2-inception_resnet_v2.py" class="nav-link" data-scroll-target="#inception-resnet-v2-inception_resnet_v2.py">Inception-ResNet-V2 [inception_resnet_v2.py]</a></li>
  <li><a href="#nasnet-a-nasnet.py" id="toc-nasnet-a-nasnet.py" class="nav-link" data-scroll-target="#nasnet-a-nasnet.py">NASNet-A [nasnet.py]</a></li>
  <li><a href="#pnasnet-5-pnasnet.py" id="toc-pnasnet-5-pnasnet.py" class="nav-link" data-scroll-target="#pnasnet-5-pnasnet.py">PNasNet-5 [pnasnet.py]</a></li>
  <li><a href="#efficientnet-efficientnet.py" id="toc-efficientnet-efficientnet.py" class="nav-link" data-scroll-target="#efficientnet-efficientnet.py">EfficientNet [efficientnet.py]</a></li>
  <li><a href="#mobilenet-v3-mobilenetv3.py" id="toc-mobilenet-v3-mobilenetv3.py" class="nav-link" data-scroll-target="#mobilenet-v3-mobilenetv3.py">MobileNet-V3 [mobilenetv3.py]</a></li>
  <li><a href="#regnet-regnet.py" id="toc-regnet-regnet.py" class="nav-link" data-scroll-target="#regnet-regnet.py">RegNet [regnet.py]</a></li>
  <li><a href="#repvgg-byobnet.py" id="toc-repvgg-byobnet.py" class="nav-link" data-scroll-target="#repvgg-byobnet.py">RepVGG [byobnet.py]</a></li>
  <li><a href="#resnet-resnext-resnet.py" id="toc-resnet-resnext-resnet.py" class="nav-link" data-scroll-target="#resnet-resnext-resnet.py">ResNet, ResNeXt [resnet.py]</a></li>
  <li><a href="#res2net-res2net.py" id="toc-res2net-res2net.py" class="nav-link" data-scroll-target="#res2net-res2net.py">Res2Net [res2net.py]</a></li>
  <li><a href="#resnest-resnest.py" id="toc-resnest-resnest.py" class="nav-link" data-scroll-target="#resnest-resnest.py">ResNeSt [resnest.py]</a></li>
  <li><a href="#rexnet-rexnet.py" id="toc-rexnet-rexnet.py" class="nav-link" data-scroll-target="#rexnet-rexnet.py">ReXNet [rexnet.py]</a></li>
  <li><a href="#selective-kernel-networks-sknet.py" id="toc-selective-kernel-networks-sknet.py" class="nav-link" data-scroll-target="#selective-kernel-networks-sknet.py">Selective-Kernel Networks [sknet.py]</a></li>
  <li><a href="#selecsls-selecsls.py" id="toc-selecsls-selecsls.py" class="nav-link" data-scroll-target="#selecsls-selecsls.py">SelecSLS [selecsls.py]</a></li>
  <li><a href="#squeeze-and-excitation-networks-senet.py" id="toc-squeeze-and-excitation-networks-senet.py" class="nav-link" data-scroll-target="#squeeze-and-excitation-networks-senet.py">Squeeze-and-Excitation Networks [senet.py]</a></li>
  <li><a href="#tresnet-tresnet.py" id="toc-tresnet-tresnet.py" class="nav-link" data-scroll-target="#tresnet-tresnet.py">TResNet [tresnet.py]</a></li>
  <li><a href="#vgg-vgg.py" id="toc-vgg-vgg.py" class="nav-link" data-scroll-target="#vgg-vgg.py">VGG [vgg.py]</a></li>
  <li><a href="#vision-transformer-vision_transformer.py" id="toc-vision-transformer-vision_transformer.py" class="nav-link" data-scroll-target="#vision-transformer-vision_transformer.py">Vision Transformer [vision_transformer.py]</a></li>
  <li><a href="#vovnet-v2-and-v1-vovnet.py" id="toc-vovnet-v2-and-v1-vovnet.py" class="nav-link" data-scroll-target="#vovnet-v2-and-v1-vovnet.py">VovNet V2 and V1 [vovnet.py]</a></li>
  <li><a href="#xception-xception.py" id="toc-xception-xception.py" class="nav-link" data-scroll-target="#xception-xception.py">Xception [xception.py]</a></li>
  <li><a href="#xception-modified-aligned-gluon-gluon_xception.py" id="toc-xception-modified-aligned-gluon-gluon_xception.py" class="nav-link" data-scroll-target="#xception-modified-aligned-gluon-gluon_xception.py">Xception (Modified Aligned, Gluon) [gluon_xception.py]</a></li>
  <li><a href="#xception-modified-aligned-tf-aligned_xception.py" id="toc-xception-modified-aligned-tf-aligned_xception.py" class="nav-link" data-scroll-target="#xception-modified-aligned-tf-aligned_xception.py">Xception (Modified Aligned, TF) [aligned_xception.py]</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">



<section id="model-summaries" class="level1">
<h1>Model Summaries</h1>
<p>The model architectures included come from a wide variety of sources. Sources, including papers, original impl (“reference code”) that I rewrote / adapted, and PyTorch impl that I leveraged directly (“code”) are listed below.</p>
<p>Most included models have pretrained weights. The weights are either:</p>
<ol type="1">
<li>from their original sources</li>
<li>ported by myself from their original impl in a different framework (e.g.&nbsp;Tensorflow models)</li>
<li>trained from scratch using the included training script</li>
</ol>
<p>The validation results for the pretrained weights are <a href="../../../../posts/Kaggle_Comp/pytorch-image-models/docs/results.html">here</a></p>
<p>A more exciting view (with pretty pictures) of the models within <code>timm</code> can be found at <a href="https://paperswithcode.com/lib/timm">paperswithcode</a>.</p>
<section id="big-transfer-resnetv2-bit-resnetv2.py" class="level2">
<h2 class="anchored" data-anchor-id="big-transfer-resnetv2-bit-resnetv2.py">Big Transfer ResNetV2 (BiT) [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnetv2.py">resnetv2.py</a>]</h2>
<ul>
<li>Paper: <code>Big Transfer (BiT): General Visual Representation Learning</code> - https://arxiv.org/abs/1912.11370</li>
<li>Reference code: https://github.com/google-research/big_transfer</li>
</ul>
</section>
<section id="cross-stage-partial-networks-cspnet.py" class="level2">
<h2 class="anchored" data-anchor-id="cross-stage-partial-networks-cspnet.py">Cross-Stage Partial Networks [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/cspnet.py">cspnet.py</a>]</h2>
<ul>
<li>Paper: <code>CSPNet: A New Backbone that can Enhance Learning Capability of CNN</code> - https://arxiv.org/abs/1911.11929</li>
<li>Reference impl: https://github.com/WongKinYiu/CrossStagePartialNetworks</li>
</ul>
</section>
<section id="densenet-densenet.py" class="level2">
<h2 class="anchored" data-anchor-id="densenet-densenet.py">DenseNet [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/densenet.py">densenet.py</a>]</h2>
<ul>
<li>Paper: <code>Densely Connected Convolutional Networks</code> - https://arxiv.org/abs/1608.06993</li>
<li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li>
</ul>
</section>
<section id="dla-dla.py" class="level2">
<h2 class="anchored" data-anchor-id="dla-dla.py">DLA [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dla.py">dla.py</a>]</h2>
<ul>
<li>Paper: https://arxiv.org/abs/1707.06484</li>
<li>Code: https://github.com/ucbdrive/dla</li>
</ul>
</section>
<section id="dual-path-networks-dpn.py" class="level2">
<h2 class="anchored" data-anchor-id="dual-path-networks-dpn.py">Dual-Path Networks [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/dpn.py">dpn.py</a>]</h2>
<ul>
<li>Paper: <code>Dual Path Networks</code> - https://arxiv.org/abs/1707.01629</li>
<li>My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained</li>
<li>Reference code: https://github.com/cypw/DPNs</li>
</ul>
</section>
<section id="gpu-efficient-networks-byobnet.py" class="level2">
<h2 class="anchored" data-anchor-id="gpu-efficient-networks-byobnet.py">GPU-Efficient Networks [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/byobnet.py">byobnet.py</a>]</h2>
<ul>
<li>Paper: <code>Neural Architecture Design for GPU-Efficient Networks</code> - https://arxiv.org/abs/2006.14090</li>
<li>Reference code: https://github.com/idstcv/GPU-Efficient-Networks</li>
</ul>
</section>
<section id="hrnet-hrnet.py" class="level2">
<h2 class="anchored" data-anchor-id="hrnet-hrnet.py">HRNet [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/hrnet.py">hrnet.py</a>]</h2>
<ul>
<li>Paper: <code>Deep High-Resolution Representation Learning for Visual Recognition</code> - https://arxiv.org/abs/1908.07919</li>
<li>Code: https://github.com/HRNet/HRNet-Image-Classification</li>
</ul>
</section>
<section id="inception-v3-inception_v3.py" class="level2">
<h2 class="anchored" data-anchor-id="inception-v3-inception_v3.py">Inception-V3 [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_v3.py">inception_v3.py</a>]</h2>
<ul>
<li>Paper: <code>Rethinking the Inception Architecture for Computer Vision</code> - https://arxiv.org/abs/1512.00567</li>
<li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li>
</ul>
</section>
<section id="inception-v4-inception_v4.py" class="level2">
<h2 class="anchored" data-anchor-id="inception-v4-inception_v4.py">Inception-V4 [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_v4.py">inception_v4.py</a>]</h2>
<ul>
<li>Paper: <code>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</code> - https://arxiv.org/abs/1602.07261</li>
<li>Code: https://github.com/Cadene/pretrained-models.pytorch</li>
<li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets</li>
</ul>
</section>
<section id="inception-resnet-v2-inception_resnet_v2.py" class="level2">
<h2 class="anchored" data-anchor-id="inception-resnet-v2-inception_resnet_v2.py">Inception-ResNet-V2 [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/inception_resnet_v2.py">inception_resnet_v2.py</a>]</h2>
<ul>
<li>Paper: <code>Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</code> - https://arxiv.org/abs/1602.07261</li>
<li>Code: https://github.com/Cadene/pretrained-models.pytorch</li>
<li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets</li>
</ul>
</section>
<section id="nasnet-a-nasnet.py" class="level2">
<h2 class="anchored" data-anchor-id="nasnet-a-nasnet.py">NASNet-A [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/nasnet.py">nasnet.py</a>]</h2>
<ul>
<li>Papers: <code>Learning Transferable Architectures for Scalable Image Recognition</code> - https://arxiv.org/abs/1707.07012</li>
<li>Code: https://github.com/Cadene/pretrained-models.pytorch</li>
<li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</li>
</ul>
</section>
<section id="pnasnet-5-pnasnet.py" class="level2">
<h2 class="anchored" data-anchor-id="pnasnet-5-pnasnet.py">PNasNet-5 [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/pnasnet.py">pnasnet.py</a>]</h2>
<ul>
<li>Papers: <code>Progressive Neural Architecture Search</code> - https://arxiv.org/abs/1712.00559</li>
<li>Code: https://github.com/Cadene/pretrained-models.pytorch</li>
<li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet</li>
</ul>
</section>
<section id="efficientnet-efficientnet.py" class="level2">
<h2 class="anchored" data-anchor-id="efficientnet-efficientnet.py">EfficientNet [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/efficientnet.py">efficientnet.py</a>]</h2>
<ul>
<li>Papers:
<ul>
<li>EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252</li>
<li>EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665</li>
<li>EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946</li>
<li>EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html</li>
<li>MixNet - https://arxiv.org/abs/1907.09595</li>
<li>MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626</li>
<li>MobileNet-V2 - https://arxiv.org/abs/1801.04381</li>
<li>FBNet-C - https://arxiv.org/abs/1812.03443</li>
<li>Single-Path NAS - https://arxiv.org/abs/1904.02877</li>
</ul></li>
<li>My PyTorch code: https://github.com/rwightman/gen-efficientnet-pytorch</li>
<li>Reference code: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet</li>
</ul>
</section>
<section id="mobilenet-v3-mobilenetv3.py" class="level2">
<h2 class="anchored" data-anchor-id="mobilenet-v3-mobilenetv3.py">MobileNet-V3 [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/mobilenetv3.py">mobilenetv3.py</a>]</h2>
<ul>
<li>Paper: <code>Searching for MobileNetV3</code> - https://arxiv.org/abs/1905.02244</li>
<li>Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</li>
</ul>
</section>
<section id="regnet-regnet.py" class="level2">
<h2 class="anchored" data-anchor-id="regnet-regnet.py">RegNet [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/regnet.py">regnet.py</a>]</h2>
<ul>
<li>Paper: <code>Designing Network Design Spaces</code> - https://arxiv.org/abs/2003.13678</li>
<li>Reference code: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py</li>
</ul>
</section>
<section id="repvgg-byobnet.py" class="level2">
<h2 class="anchored" data-anchor-id="repvgg-byobnet.py">RepVGG [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/byobnet.py">byobnet.py</a>]</h2>
<ul>
<li>Paper: <code>Making VGG-style ConvNets Great Again</code> - https://arxiv.org/abs/2101.03697</li>
<li>Reference code: https://github.com/DingXiaoH/RepVGG</li>
</ul>
</section>
<section id="resnet-resnext-resnet.py" class="level2">
<h2 class="anchored" data-anchor-id="resnet-resnext-resnet.py">ResNet, ResNeXt [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnet.py">resnet.py</a>]</h2>
<ul>
<li>ResNet (V1B)
<ul>
<li>Paper: <code>Deep Residual Learning for Image Recognition</code> - https://arxiv.org/abs/1512.03385</li>
<li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li>
</ul></li>
<li>ResNeXt
<ul>
<li>Paper: <code>Aggregated Residual Transformations for Deep Neural Networks</code> - https://arxiv.org/abs/1611.05431</li>
<li>Code: https://github.com/pytorch/vision/tree/master/torchvision/models</li>
</ul></li>
<li>‘Bag of Tricks’ / Gluon C, D, E, S ResNet variants
<ul>
<li>Paper: <code>Bag of Tricks for Image Classification with CNNs</code> - https://arxiv.org/abs/1812.01187</li>
<li>Code: https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py</li>
</ul></li>
<li>Instagram pretrained / ImageNet tuned ResNeXt101
<ul>
<li>Paper: <code>Exploring the Limits of Weakly Supervised Pretraining</code> - https://arxiv.org/abs/1805.00932</li>
<li>Weights: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext (NOTE: CC BY-NC 4.0 License, NOT commercial friendly)</li>
</ul></li>
<li>Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts
<ul>
<li>Paper: <code>Billion-scale semi-supervised learning for image classification</code> - https://arxiv.org/abs/1905.00546</li>
<li>Weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models (NOTE: CC BY-NC 4.0 License, NOT commercial friendly)</li>
</ul></li>
<li>Squeeze-and-Excitation Networks
<ul>
<li>Paper: <code>Squeeze-and-Excitation Networks</code> - https://arxiv.org/abs/1709.01507</li>
<li>Code: Added to ResNet base, this is current version going forward, old <code>senet.py</code> is being deprecated</li>
</ul></li>
<li>ECAResNet (ECA-Net)
<ul>
<li>Paper: <code>ECA-Net: Efficient Channel Attention for Deep CNN</code> - https://arxiv.org/abs/1910.03151v4</li>
<li>Code: Added to ResNet base, ECA module contributed by <span class="citation" data-cites="VRandme">@VRandme</span>, reference https://github.com/BangguWu/ECANet</li>
</ul></li>
</ul>
</section>
<section id="res2net-res2net.py" class="level2">
<h2 class="anchored" data-anchor-id="res2net-res2net.py">Res2Net [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/res2net.py">res2net.py</a>]</h2>
<ul>
<li>Paper: <code>Res2Net: A New Multi-scale Backbone Architecture</code> - https://arxiv.org/abs/1904.01169</li>
<li>Code: https://github.com/gasvn/Res2Net</li>
</ul>
</section>
<section id="resnest-resnest.py" class="level2">
<h2 class="anchored" data-anchor-id="resnest-resnest.py">ResNeSt [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnest.py">resnest.py</a>]</h2>
<ul>
<li>Paper: <code>ResNeSt: Split-Attention Networks</code> - https://arxiv.org/abs/2004.08955</li>
<li>Code: https://github.com/zhanghang1989/ResNeSt</li>
</ul>
</section>
<section id="rexnet-rexnet.py" class="level2">
<h2 class="anchored" data-anchor-id="rexnet-rexnet.py">ReXNet [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/rexnet.py">rexnet.py</a>]</h2>
<ul>
<li>Paper: <code>ReXNet: Diminishing Representational Bottleneck on CNN</code> - https://arxiv.org/abs/2007.00992</li>
<li>Code: https://github.com/clovaai/rexnet</li>
</ul>
</section>
<section id="selective-kernel-networks-sknet.py" class="level2">
<h2 class="anchored" data-anchor-id="selective-kernel-networks-sknet.py">Selective-Kernel Networks [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/sknet.py">sknet.py</a>]</h2>
<ul>
<li>Paper: <code>Selective-Kernel Networks</code> - https://arxiv.org/abs/1903.06586</li>
<li>Code: https://github.com/implus/SKNet, https://github.com/clovaai/assembled-cnn</li>
</ul>
</section>
<section id="selecsls-selecsls.py" class="level2">
<h2 class="anchored" data-anchor-id="selecsls-selecsls.py">SelecSLS [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/selecsls.py">selecsls.py</a>]</h2>
<ul>
<li>Paper: <code>XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera</code> - https://arxiv.org/abs/1907.00837</li>
<li>Code: https://github.com/mehtadushy/SelecSLS-Pytorch</li>
</ul>
</section>
<section id="squeeze-and-excitation-networks-senet.py" class="level2">
<h2 class="anchored" data-anchor-id="squeeze-and-excitation-networks-senet.py">Squeeze-and-Excitation Networks [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/senet.py">senet.py</a>]</h2>
<p>NOTE: I am deprecating this version of the networks, the new ones are part of <code>resnet.py</code></p>
<ul>
<li>Paper: <code>Squeeze-and-Excitation Networks</code> - https://arxiv.org/abs/1709.01507</li>
<li>Code: https://github.com/Cadene/pretrained-models.pytorch</li>
</ul>
</section>
<section id="tresnet-tresnet.py" class="level2">
<h2 class="anchored" data-anchor-id="tresnet-tresnet.py">TResNet [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tresnet.py">tresnet.py</a>]</h2>
<ul>
<li>Paper: <code>TResNet: High Performance GPU-Dedicated Architecture</code> - https://arxiv.org/abs/2003.13630</li>
<li>Code: https://github.com/mrT23/TResNet</li>
</ul>
</section>
<section id="vgg-vgg.py" class="level2">
<h2 class="anchored" data-anchor-id="vgg-vgg.py">VGG [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vgg.py">vgg.py</a>]</h2>
<ul>
<li>Paper: <code>Very Deep Convolutional Networks For Large-Scale Image Recognition</code> - https://arxiv.org/pdf/1409.1556.pdf</li>
<li>Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py</li>
</ul>
</section>
<section id="vision-transformer-vision_transformer.py" class="level2">
<h2 class="anchored" data-anchor-id="vision-transformer-vision_transformer.py">Vision Transformer [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py">vision_transformer.py</a>]</h2>
<ul>
<li>Paper: <code>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</code> - https://arxiv.org/abs/2010.11929</li>
<li>Reference code and pretrained weights: https://github.com/google-research/vision_transformer</li>
</ul>
</section>
<section id="vovnet-v2-and-v1-vovnet.py" class="level2">
<h2 class="anchored" data-anchor-id="vovnet-v2-and-v1-vovnet.py">VovNet V2 and V1 [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vovnet.py">vovnet.py</a>]</h2>
<ul>
<li>Paper: <code>CenterMask : Real-Time Anchor-Free Instance Segmentation</code> - https://arxiv.org/abs/1911.06667</li>
<li>Reference code: https://github.com/youngwanLEE/vovnet-detectron2</li>
</ul>
</section>
<section id="xception-xception.py" class="level2">
<h2 class="anchored" data-anchor-id="xception-xception.py">Xception [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/xception.py">xception.py</a>]</h2>
<ul>
<li>Paper: <code>Xception: Deep Learning with Depthwise Separable Convolutions</code> - https://arxiv.org/abs/1610.02357</li>
<li>Code: https://github.com/Cadene/pretrained-models.pytorch</li>
</ul>
</section>
<section id="xception-modified-aligned-gluon-gluon_xception.py" class="level2">
<h2 class="anchored" data-anchor-id="xception-modified-aligned-gluon-gluon_xception.py">Xception (Modified Aligned, Gluon) [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/gluon_xception.py">gluon_xception.py</a>]</h2>
<ul>
<li>Paper: <code>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</code> - https://arxiv.org/abs/1802.02611</li>
<li>Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo, https://github.com/jfzhang95/pytorch-deeplab-xception/</li>
</ul>
</section>
<section id="xception-modified-aligned-tf-aligned_xception.py" class="level2">
<h2 class="anchored" data-anchor-id="xception-modified-aligned-tf-aligned_xception.py">Xception (Modified Aligned, TF) [<a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/aligned_xception.py">aligned_xception.py</a>]</h2>
<ul>
<li>Paper: <code>Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</code> - https://arxiv.org/abs/1802.02611</li>
<li>Reference code: https://github.com/tensorflow/models/tree/master/research/deeplab</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>