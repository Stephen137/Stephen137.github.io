<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Into the Unknown â€“ changes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../about.html">Stephen Barrie</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#recent-changes" id="toc-recent-changes" class="nav-link active" data-scroll-target="#recent-changes">Recent Changes</a>
  <ul class="collapse">
  <li><a href="#jan-5-2023" id="toc-jan-5-2023" class="nav-link" data-scroll-target="#jan-5-2023">Jan 5, 2023</a></li>
  <li><a href="#dec-23-2022" id="toc-dec-23-2022" class="nav-link" data-scroll-target="#dec-23-2022">Dec 23, 2022 ðŸŽ„â˜ƒ</a></li>
  <li><a href="#dec-8-2022" id="toc-dec-8-2022" class="nav-link" data-scroll-target="#dec-8-2022">Dec 8, 2022</a></li>
  <li><a href="#dec-6-2022" id="toc-dec-6-2022" class="nav-link" data-scroll-target="#dec-6-2022">Dec 6, 2022</a></li>
  <li><a href="#dec-5-2022" id="toc-dec-5-2022" class="nav-link" data-scroll-target="#dec-5-2022">Dec 5, 2022</a></li>
  <li><a href="#oct-15-2022" id="toc-oct-15-2022" class="nav-link" data-scroll-target="#oct-15-2022">Oct 15, 2022</a></li>
  <li><a href="#oct-10-2022" id="toc-oct-10-2022" class="nav-link" data-scroll-target="#oct-10-2022">Oct 10, 2022</a></li>
  <li><a href="#sept-23-2022" id="toc-sept-23-2022" class="nav-link" data-scroll-target="#sept-23-2022">Sept 23, 2022</a></li>
  <li><a href="#sept-7-2022" id="toc-sept-7-2022" class="nav-link" data-scroll-target="#sept-7-2022">Sept 7, 2022</a></li>
  <li><a href="#aug-29-2022" id="toc-aug-29-2022" class="nav-link" data-scroll-target="#aug-29-2022">Aug 29, 2022</a></li>
  <li><a href="#aug-26-2022" id="toc-aug-26-2022" class="nav-link" data-scroll-target="#aug-26-2022">Aug 26, 2022</a></li>
  <li><a href="#aug-15-2022" id="toc-aug-15-2022" class="nav-link" data-scroll-target="#aug-15-2022">Aug 15, 2022</a></li>
  <li><a href="#aug-5-2022" id="toc-aug-5-2022" class="nav-link" data-scroll-target="#aug-5-2022">Aug 5, 2022</a></li>
  <li><a href="#july-28-2022" id="toc-july-28-2022" class="nav-link" data-scroll-target="#july-28-2022">July 28, 2022</a></li>
  <li><a href="#july-27-2022" id="toc-july-27-2022" class="nav-link" data-scroll-target="#july-27-2022">July 27, 2022</a></li>
  <li><a href="#july-8-2022" id="toc-july-8-2022" class="nav-link" data-scroll-target="#july-8-2022">July 8, 2022</a></li>
  <li><a href="#may-13-2022" id="toc-may-13-2022" class="nav-link" data-scroll-target="#may-13-2022">May 13, 2022</a></li>
  <li><a href="#may-2-2022" id="toc-may-2-2022" class="nav-link" data-scroll-target="#may-2-2022">May 2, 2022</a></li>
  <li><a href="#april-22-2022" id="toc-april-22-2022" class="nav-link" data-scroll-target="#april-22-2022">April 22, 2022</a></li>
  <li><a href="#march-23-2022" id="toc-march-23-2022" class="nav-link" data-scroll-target="#march-23-2022">March 23, 2022</a></li>
  <li><a href="#march-21-2022" id="toc-march-21-2022" class="nav-link" data-scroll-target="#march-21-2022">March 21, 2022</a></li>
  <li><a href="#feb-2-2022" id="toc-feb-2-2022" class="nav-link" data-scroll-target="#feb-2-2022">Feb 2, 2022</a></li>
  <li><a href="#jan-14-2022" id="toc-jan-14-2022" class="nav-link" data-scroll-target="#jan-14-2022">Jan 14, 2022</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">



<section id="recent-changes" class="level1">
<h1>Recent Changes</h1>
<section id="jan-5-2023" class="level3">
<h3 class="anchored" data-anchor-id="jan-5-2023">Jan 5, 2023</h3>
<ul>
<li>ConvNeXt-V2 models and weights added to existing <code>convnext.py</code>
<ul>
<li>Paper: <a href="http://arxiv.org/abs/2301.00808">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a></li>
<li>Reference impl: https://github.com/facebookresearch/ConvNeXt-V2 (NOTE: weights currently CC-BY-NC)</li>
</ul></li>
</ul>
</section>
<section id="dec-23-2022" class="level3">
<h3 class="anchored" data-anchor-id="dec-23-2022">Dec 23, 2022 ðŸŽ„â˜ƒ</h3>
<ul>
<li>Add FlexiViT models and weights from https://github.com/google-research/big_vision (check out paper at https://arxiv.org/abs/2212.08013)
<ul>
<li>NOTE currently resizing is static on model creation, on-the-fly dynamic / train patch size sampling is a WIP</li>
</ul></li>
<li>Many more models updated to multi-weight and downloadable via HF hub now (convnext, efficientnet, mobilenet, vision_transformer*, beit)</li>
<li>More model pretrained tag and adjustments, some model names changed (working on deprecation translations, consider main branch DEV branch right now, use 0.6.x for stable use)</li>
<li>More ImageNet-12k (subset of 22k) pretrain models popping up:
<ul>
<li><code>efficientnet_b5.in12k_ft_in1k</code> - 85.9 @ 448x448</li>
<li><code>vit_medium_patch16_gap_384.in12k_ft_in1k</code> - 85.5 @ 384x384</li>
<li><code>vit_medium_patch16_gap_256.in12k_ft_in1k</code> - 84.5 @ 256x256</li>
<li><code>convnext_nano.in12k_ft_in1k</code> - 82.9 @ 288x288</li>
</ul></li>
</ul>
</section>
<section id="dec-8-2022" class="level3">
<h3 class="anchored" data-anchor-id="dec-8-2022">Dec 8, 2022</h3>
<ul>
<li>Add â€˜EVA lâ€™ to <code>vision_transformer.py</code>, MAE style ViT-L/14 MIM pretrain w/ EVA-CLIP targets, FT on ImageNet-1k (w/ ImageNet-22k intermediate for some)
<ul>
<li>original source: https://github.com/baaivision/EVA</li>
</ul></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">model</th>
<th style="text-align: right;">top1</th>
<th style="text-align: right;">param_count</th>
<th style="text-align: right;">gmac</th>
<th style="text-align: right;">macts</th>
<th style="text-align: left;">hub</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">eva_large_patch14_336.in22k_ft_in22k_in1k</td>
<td style="text-align: right;">89.2</td>
<td style="text-align: right;">304.5</td>
<td style="text-align: right;">191.1</td>
<td style="text-align: right;">270.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">eva_large_patch14_336.in22k_ft_in1k</td>
<td style="text-align: right;">88.7</td>
<td style="text-align: right;">304.5</td>
<td style="text-align: right;">191.1</td>
<td style="text-align: right;">270.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">eva_large_patch14_196.in22k_ft_in22k_in1k</td>
<td style="text-align: right;">88.6</td>
<td style="text-align: right;">304.1</td>
<td style="text-align: right;">61.6</td>
<td style="text-align: right;">63.5</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">eva_large_patch14_196.in22k_ft_in1k</td>
<td style="text-align: right;">87.9</td>
<td style="text-align: right;">304.1</td>
<td style="text-align: right;">61.6</td>
<td style="text-align: right;">63.5</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
</tbody>
</table>
</section>
<section id="dec-6-2022" class="level3">
<h3 class="anchored" data-anchor-id="dec-6-2022">Dec 6, 2022</h3>
<ul>
<li>Add â€˜EVA gâ€™, BEiT style ViT-g/14 model weights w/ both MIM pretrain and CLIP pretrain to <code>beit.py</code>.
<ul>
<li>original source: https://github.com/baaivision/EVA</li>
<li>paper: https://arxiv.org/abs/2211.07636</li>
</ul></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">model</th>
<th style="text-align: right;">top1</th>
<th style="text-align: right;">param_count</th>
<th style="text-align: right;">gmac</th>
<th style="text-align: right;">macts</th>
<th style="text-align: left;">hub</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">eva_giant_patch14_560.m30m_ft_in22k_in1k</td>
<td style="text-align: right;">89.8</td>
<td style="text-align: right;">1014.4</td>
<td style="text-align: right;">1906.8</td>
<td style="text-align: right;">2577.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">eva_giant_patch14_336.m30m_ft_in22k_in1k</td>
<td style="text-align: right;">89.6</td>
<td style="text-align: right;">1013</td>
<td style="text-align: right;">620.6</td>
<td style="text-align: right;">550.7</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">eva_giant_patch14_336.clip_ft_in1k</td>
<td style="text-align: right;">89.4</td>
<td style="text-align: right;">1013</td>
<td style="text-align: right;">620.6</td>
<td style="text-align: right;">550.7</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">eva_giant_patch14_224.clip_ft_in1k</td>
<td style="text-align: right;">89.1</td>
<td style="text-align: right;">1012.6</td>
<td style="text-align: right;">267.2</td>
<td style="text-align: right;">192.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/BAAI/EVA">link</a></td>
</tr>
</tbody>
</table>
</section>
<section id="dec-5-2022" class="level3">
<h3 class="anchored" data-anchor-id="dec-5-2022">Dec 5, 2022</h3>
<ul>
<li>Pre-release (<code>0.8.0dev0</code>) of multi-weight support (<code>model_arch.pretrained_tag</code>). Install with <code>pip install --pre timm</code>
<ul>
<li>vision_transformer, maxvit, convnext are the first three model impl w/ support</li>
<li>model names are changing with this (previous _21k, etc. fn will merge), still sorting out deprecation handling</li>
<li>bugs are likely, but I need feedback so please try it out</li>
<li>if stability is needed, please use 0.6.x pypi releases or clone from <a href="https://github.com/rwightman/pytorch-image-models/tree/0.6.x">0.6.x branch</a></li>
</ul></li>
<li>Support for PyTorch 2.0 compile is added in train/validate/inference/benchmark, use <code>--torchcompile</code> argument</li>
<li>Inference script allows more control over output, select k for top-class index + prob json, csv or parquet output</li>
<li>Add a full set of fine-tuned CLIP image tower weights from both LAION-2B and original OpenAI CLIP models</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">model</th>
<th style="text-align: right;">top1</th>
<th style="text-align: right;">param_count</th>
<th style="text-align: right;">gmac</th>
<th style="text-align: right;">macts</th>
<th style="text-align: left;">hub</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">88.6</td>
<td style="text-align: right;">632.5</td>
<td style="text-align: right;">391</td>
<td style="text-align: right;">407.5</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_large_patch14_clip_336.openai_ft_in12k_in1k</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">304.5</td>
<td style="text-align: right;">191.1</td>
<td style="text-align: right;">270.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_336.openai_ft_in12k_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">632</td>
<td style="text-align: right;">167.4</td>
<td style="text-align: right;">139.4</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_large_patch14_clip_336.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">304.5</td>
<td style="text-align: right;">191.1</td>
<td style="text-align: right;">270.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_large_patch14_clip_224.openai_ft_in12k_in1k</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">304.2</td>
<td style="text-align: right;">81.1</td>
<td style="text-align: right;">88.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_large_patch14_clip_224.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">87.9</td>
<td style="text-align: right;">304.2</td>
<td style="text-align: right;">81.1</td>
<td style="text-align: right;">88.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_large_patch14_clip_224.openai_ft_in1k</td>
<td style="text-align: right;">87.9</td>
<td style="text-align: right;">304.2</td>
<td style="text-align: right;">81.1</td>
<td style="text-align: right;">88.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_224.openai_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_large_patch14_clip_336.laion2b_ft_in1k</td>
<td style="text-align: right;">87.9</td>
<td style="text-align: right;">304.5</td>
<td style="text-align: right;">191.1</td>
<td style="text-align: right;">270.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_336.laion2b_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_huge_patch14_clip_224.laion2b_ft_in1k</td>
<td style="text-align: right;">87.6</td>
<td style="text-align: right;">632</td>
<td style="text-align: right;">167.4</td>
<td style="text-align: right;">139.4</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_huge_patch14_clip_224.laion2b_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_large_patch14_clip_224.laion2b_ft_in1k</td>
<td style="text-align: right;">87.3</td>
<td style="text-align: right;">304.2</td>
<td style="text-align: right;">81.1</td>
<td style="text-align: right;">88.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_large_patch14_clip_224.laion2b_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch16_clip_384.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">87.2</td>
<td style="text-align: right;">86.9</td>
<td style="text-align: right;">55.5</td>
<td style="text-align: right;">101.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch16_clip_384.openai_ft_in12k_in1k</td>
<td style="text-align: right;">87</td>
<td style="text-align: right;">86.9</td>
<td style="text-align: right;">55.5</td>
<td style="text-align: right;">101.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in12k_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch16_clip_384.laion2b_ft_in1k</td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">86.9</td>
<td style="text-align: right;">55.5</td>
<td style="text-align: right;">101.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_384.laion2b_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch16_clip_384.openai_ft_in1k</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">86.9</td>
<td style="text-align: right;">55.5</td>
<td style="text-align: right;">101.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_384.openai_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch16_clip_224.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">17.6</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch16_clip_224.openai_ft_in12k_in1k</td>
<td style="text-align: right;">85.9</td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">17.6</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in12k_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch32_clip_448.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">85.8</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">17.9</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch32_clip_448.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch16_clip_224.laion2b_ft_in1k</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">17.6</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_224.laion2b_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch32_clip_384.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">85.4</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">13.1</td>
<td style="text-align: right;">16.5</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch32_clip_384.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch16_clip_224.openai_ft_in1k</td>
<td style="text-align: right;">85.3</td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">17.6</td>
<td style="text-align: right;">23.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch16_clip_224.openai_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch32_clip_384.openai_ft_in12k_in1k</td>
<td style="text-align: right;">85.2</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">13.1</td>
<td style="text-align: right;">16.5</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch32_clip_384.openai_ft_in12k_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch32_clip_224.laion2b_ft_in12k_in1k</td>
<td style="text-align: right;">83.3</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">4.4</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vit_base_patch32_clip_224.laion2b_ft_in1k</td>
<td style="text-align: right;">82.6</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">4.4</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch32_clip_224.laion2b_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">vit_base_patch32_clip_224.openai_ft_in1k</td>
<td style="text-align: right;">81.9</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">4.4</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/vit_base_patch32_clip_224.openai_ft_in1k">link</a></td>
</tr>
</tbody>
</table>
<ul>
<li>Port of MaxViT Tensorflow Weights from official impl at https://github.com/google-research/maxvit
<ul>
<li>There was larger than expected drops for the upscaled 384/512 in21k fine-tune weights, possible detail missing, but the 21k FT did seem sensitive to small preprocessing</li>
</ul></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">model</th>
<th style="text-align: right;">top1</th>
<th style="text-align: right;">param_count</th>
<th style="text-align: right;">gmac</th>
<th style="text-align: right;">macts</th>
<th style="text-align: left;">hub</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">maxvit_xlarge_tf_512.in21k_ft_in1k</td>
<td style="text-align: right;">88.5</td>
<td style="text-align: right;">475.8</td>
<td style="text-align: right;">534.1</td>
<td style="text-align: right;">1413.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_xlarge_tf_512.in21k_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_xlarge_tf_384.in21k_ft_in1k</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">475.3</td>
<td style="text-align: right;">292.8</td>
<td style="text-align: right;">668.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_xlarge_tf_384.in21k_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_base_tf_512.in21k_ft_in1k</td>
<td style="text-align: right;">88.2</td>
<td style="text-align: right;">119.9</td>
<td style="text-align: right;">138</td>
<td style="text-align: right;">704</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_base_tf_512.in21k_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_large_tf_512.in21k_ft_in1k</td>
<td style="text-align: right;">88</td>
<td style="text-align: right;">212.3</td>
<td style="text-align: right;">244.8</td>
<td style="text-align: right;">942.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_large_tf_512.in21k_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_large_tf_384.in21k_ft_in1k</td>
<td style="text-align: right;">88</td>
<td style="text-align: right;">212</td>
<td style="text-align: right;">132.6</td>
<td style="text-align: right;">445.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_large_tf_384.in21k_ft_in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_base_tf_384.in21k_ft_in1k</td>
<td style="text-align: right;">87.9</td>
<td style="text-align: right;">119.6</td>
<td style="text-align: right;">73.8</td>
<td style="text-align: right;">332.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_base_tf_384.in21k_ft_in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_base_tf_512.in1k</td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">119.9</td>
<td style="text-align: right;">138</td>
<td style="text-align: right;">704</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_base_tf_512.in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_large_tf_512.in1k</td>
<td style="text-align: right;">86.5</td>
<td style="text-align: right;">212.3</td>
<td style="text-align: right;">244.8</td>
<td style="text-align: right;">942.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_large_tf_512.in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_base_tf_384.in1k</td>
<td style="text-align: right;">86.3</td>
<td style="text-align: right;">119.6</td>
<td style="text-align: right;">73.8</td>
<td style="text-align: right;">332.9</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_base_tf_384.in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_large_tf_384.in1k</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">212</td>
<td style="text-align: right;">132.6</td>
<td style="text-align: right;">445.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_large_tf_384.in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_small_tf_512.in1k</td>
<td style="text-align: right;">86.1</td>
<td style="text-align: right;">69.1</td>
<td style="text-align: right;">67.3</td>
<td style="text-align: right;">383.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_small_tf_512.in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_tiny_tf_512.in1k</td>
<td style="text-align: right;">85.7</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">33.5</td>
<td style="text-align: right;">257.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_tiny_tf_512.in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_small_tf_384.in1k</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">69</td>
<td style="text-align: right;">35.9</td>
<td style="text-align: right;">183.6</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_small_tf_384.in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_tiny_tf_384.in1k</td>
<td style="text-align: right;">85.1</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">17.5</td>
<td style="text-align: right;">123.4</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_tiny_tf_384.in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_large_tf_224.in1k</td>
<td style="text-align: right;">84.9</td>
<td style="text-align: right;">211.8</td>
<td style="text-align: right;">43.7</td>
<td style="text-align: right;">127.4</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_large_tf_224.in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_base_tf_224.in1k</td>
<td style="text-align: right;">84.9</td>
<td style="text-align: right;">119.5</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">95</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_base_tf_224.in1k">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">maxvit_small_tf_224.in1k</td>
<td style="text-align: right;">84.4</td>
<td style="text-align: right;">68.9</td>
<td style="text-align: right;">11.7</td>
<td style="text-align: right;">53.2</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_small_tf_224.in1k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">maxvit_tiny_tf_224.in1k</td>
<td style="text-align: right;">83.4</td>
<td style="text-align: right;">30.9</td>
<td style="text-align: right;">5.6</td>
<td style="text-align: right;">35.8</td>
<td style="text-align: left;"><a href="https://huggingface.co/timm/maxvit_tiny_tf_224.in1k">link</a></td>
</tr>
</tbody>
</table>
</section>
<section id="oct-15-2022" class="level3">
<h3 class="anchored" data-anchor-id="oct-15-2022">Oct 15, 2022</h3>
<ul>
<li>Train and validation script enhancements</li>
<li>Non-GPU (ie CPU) device support</li>
<li>SLURM compatibility for train script</li>
<li>HF datasets support (via ReaderHfds)</li>
<li>TFDS/WDS dataloading improvements (sample padding/wrap for distributed use fixed wrt sample count estimate)</li>
<li>in_chans !=3 support for scripts / loader</li>
<li>Adan optimizer</li>
<li>Can enable per-step LR scheduling via args</li>
<li>Dataset â€˜parsersâ€™ renamed to â€˜readersâ€™, more descriptive of purpose</li>
<li>AMP args changed, APEX via <code>--amp-impl apex</code>, bfloat16 supportedf via <code>--amp-dtype bfloat16</code></li>
<li>main branch switched to 0.7.x version, 0.6x forked for stable release of weight only adds</li>
<li>master -&gt; main branch rename</li>
</ul>
</section>
<section id="oct-10-2022" class="level3">
<h3 class="anchored" data-anchor-id="oct-10-2022">Oct 10, 2022</h3>
<ul>
<li>More weights in <code>maxxvit</code> series, incl first ConvNeXt block based <code>coatnext</code> and <code>maxxvit</code> experiments:
<ul>
<li><code>coatnext_nano_rw_224</code> - 82.0 @ 224 (G) â€“ (uses ConvNeXt conv block, no BatchNorm)</li>
<li><code>maxxvit_rmlp_nano_rw_256</code> - 83.0 @ 256, 83.7 @ 320 (G) (uses ConvNeXt conv block, no BN)</li>
<li><code>maxvit_rmlp_small_rw_224</code> - 84.5 @ 224, 85.1 @ 320 (G)</li>
<li><code>maxxvit_rmlp_small_rw_256</code> - 84.6 @ 256, 84.9 @ 288 (G) â€“ could be trained better, hparams need tuning (uses ConvNeXt block, no BN)</li>
<li><code>coatnet_rmlp_2_rw_224</code> - 84.6 @ 224, 85 @ 320 (T)</li>
<li>NOTE: official MaxVit weights (in1k) have been released at https://github.com/google-research/maxvit â€“ some extra work is needed to port and adapt since my impl was created independently of theirs and has a few small differences + the whole TF same padding fun.</li>
</ul></li>
</ul>
</section>
<section id="sept-23-2022" class="level3">
<h3 class="anchored" data-anchor-id="sept-23-2022">Sept 23, 2022</h3>
<ul>
<li>LAION-2B CLIP image towers supported as pretrained backbones for fine-tune or features (no classifier)
<ul>
<li>vit_base_patch32_224_clip_laion2b</li>
<li>vit_large_patch14_224_clip_laion2b</li>
<li>vit_huge_patch14_224_clip_laion2b</li>
<li>vit_giant_patch14_224_clip_laion2b</li>
</ul></li>
</ul>
</section>
<section id="sept-7-2022" class="level3">
<h3 class="anchored" data-anchor-id="sept-7-2022">Sept 7, 2022</h3>
<ul>
<li>Hugging Face <a href="https://huggingface.co/docs/hub/timm"><code>timm</code> docs</a> home now exists, look for more here in the future</li>
<li>Add BEiT-v2 weights for base and large 224x224 models from https://github.com/microsoft/unilm/tree/master/beit2</li>
<li>Add more weights in <code>maxxvit</code> series incl a <code>pico</code> (7.5M params, 1.9 GMACs), two <code>tiny</code> variants:
<ul>
<li><code>maxvit_rmlp_pico_rw_256</code> - 80.5 @ 256, 81.3 @ 320 (T)</li>
<li><code>maxvit_tiny_rw_224</code> - 83.5 @ 224 (G)</li>
<li><code>maxvit_rmlp_tiny_rw_256</code> - 84.2 @ 256, 84.8 @ 320 (T)</li>
</ul></li>
</ul>
</section>
<section id="aug-29-2022" class="level3">
<h3 class="anchored" data-anchor-id="aug-29-2022">Aug 29, 2022</h3>
<ul>
<li>MaxVit window size scales with img_size by default. Add new RelPosMlp MaxViT weight that leverages this:
<ul>
<li><code>maxvit_rmlp_nano_rw_256</code> - 83.0 @ 256, 83.6 @ 320 (T)</li>
</ul></li>
</ul>
</section>
<section id="aug-26-2022" class="level3">
<h3 class="anchored" data-anchor-id="aug-26-2022">Aug 26, 2022</h3>
<ul>
<li>CoAtNet (https://arxiv.org/abs/2106.04803) and MaxVit (https://arxiv.org/abs/2204.01697) <code>timm</code> original models
<ul>
<li>both found in <a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/maxxvit.py"><code>maxxvit.py</code></a> model def, contains numerous experiments outside scope of original papers</li>
<li>an unfinished Tensorflow version from MaxVit authors can be found https://github.com/google-research/maxvit</li>
</ul></li>
<li>Initial CoAtNet and MaxVit timm pretrained weights (working on more):
<ul>
<li><code>coatnet_nano_rw_224</code> - 81.7 @ 224 (T)</li>
<li><code>coatnet_rmlp_nano_rw_224</code> - 82.0 @ 224, 82.8 @ 320 (T)</li>
<li><code>coatnet_0_rw_224</code> - 82.4 (T) â€“ NOTE timm â€˜0â€™ coatnets have 2 more 3rd stage blocks</li>
<li><code>coatnet_bn_0_rw_224</code> - 82.4 (T)</li>
<li><code>maxvit_nano_rw_256</code> - 82.9 @ 256 (T)</li>
<li><code>coatnet_rmlp_1_rw_224</code> - 83.4 @ 224, 84 @ 320 (T)</li>
<li><code>coatnet_1_rw_224</code> - 83.6 @ 224 (G)</li>
<li><ol start="20" type="A">
<li>= TPU trained with <code>bits_and_tpu</code> branch training code, (G) = GPU trained</li>
</ol></li>
</ul></li>
<li>GCVit (weights adapted from https://github.com/NVlabs/GCVit, code 100% <code>timm</code> re-write for license purposes)</li>
<li>MViT-V2 (multi-scale vit, adapted from https://github.com/facebookresearch/mvit)</li>
<li>EfficientFormer (adapted from https://github.com/snap-research/EfficientFormer)</li>
<li>PyramidVisionTransformer-V2 (adapted from https://github.com/whai362/PVT)</li>
<li>â€˜Fast Normâ€™ support for LayerNorm and GroupNorm that avoids float32 upcast w/ AMP (uses APEX LN if available for further boost)</li>
</ul>
</section>
<section id="aug-15-2022" class="level3">
<h3 class="anchored" data-anchor-id="aug-15-2022">Aug 15, 2022</h3>
<ul>
<li>ConvNeXt atto weights added
<ul>
<li><code>convnext_atto</code> - 75.7 @ 224, 77.0 @ 288</li>
<li><code>convnext_atto_ols</code> - 75.9 @ 224, 77.2 @ 288</li>
</ul></li>
</ul>
</section>
<section id="aug-5-2022" class="level3">
<h3 class="anchored" data-anchor-id="aug-5-2022">Aug 5, 2022</h3>
<ul>
<li>More custom ConvNeXt smaller model defs with weights
<ul>
<li><code>convnext_femto</code> - 77.5 @ 224, 78.7 @ 288</li>
<li><code>convnext_femto_ols</code> - 77.9 @ 224, 78.9 @ 288</li>
<li><code>convnext_pico</code> - 79.5 @ 224, 80.4 @ 288</li>
<li><code>convnext_pico_ols</code> - 79.5 @ 224, 80.5 @ 288</li>
<li><code>convnext_nano_ols</code> - 80.9 @ 224, 81.6 @ 288</li>
</ul></li>
<li>Updated EdgeNeXt to improve ONNX export, add new base variant and weights from original (https://github.com/mmaaz60/EdgeNeXt)</li>
</ul>
</section>
<section id="july-28-2022" class="level3">
<h3 class="anchored" data-anchor-id="july-28-2022">July 28, 2022</h3>
<ul>
<li>Add freshly minted DeiT-III Medium (width=512, depth=12, num_heads=8) model weights. Thanks <a href="https://github.com/TouvronHugo">Hugo Touvron</a>!</li>
</ul>
</section>
<section id="july-27-2022" class="level3">
<h3 class="anchored" data-anchor-id="july-27-2022">July 27, 2022</h3>
<ul>
<li>All runtime benchmark and validation result csv files are up-to-date!</li>
<li>A few more weights &amp; model defs added:
<ul>
<li><code>darknetaa53</code> - 79.8 @ 256, 80.5 @ 288</li>
<li><code>convnext_nano</code> - 80.8 @ 224, 81.5 @ 288</li>
<li><code>cs3sedarknet_l</code> - 81.2 @ 256, 81.8 @ 288</li>
<li><code>cs3darknet_x</code> - 81.8 @ 256, 82.2 @ 288</li>
<li><code>cs3sedarknet_x</code> - 82.2 @ 256, 82.7 @ 288</li>
<li><code>cs3edgenet_x</code> - 82.2 @ 256, 82.7 @ 288</li>
<li><code>cs3se_edgenet_x</code> - 82.8 @ 256, 83.5 @ 320</li>
</ul></li>
<li><code>cs3*</code> weights above all trained on TPU w/ <code>bits_and_tpu</code> branch. Thanks to TRC program!</li>
<li>Add output_stride=8 and 16 support to ConvNeXt (dilation)</li>
<li>deit3 models not being able to resize pos_emb fixed</li>
<li>Version 0.6.7 PyPi release (/w above bug fixes and new weighs since 0.6.5)</li>
</ul>
</section>
<section id="july-8-2022" class="level3">
<h3 class="anchored" data-anchor-id="july-8-2022">July 8, 2022</h3>
<p>More models, more fixes * Official research models (w/ weights) added: * EdgeNeXt from (https://github.com/mmaaz60/EdgeNeXt) * MobileViT-V2 from (https://github.com/apple/ml-cvnets) * DeiT III (Revenge of the ViT) from (https://github.com/facebookresearch/deit) * My own models: * Small <code>ResNet</code> defs added by request with 1 block repeats for both basic and bottleneck (resnet10 and resnet14) * <code>CspNet</code> refactored with dataclass config, simplified CrossStage3 (<code>cs3</code>) option. These are closer to YOLO-v5+ backbone defs. * More relative position vit fiddling. Two <code>srelpos</code> (shared relative position) models trained, and a medium w/ class token. * Add an alternate downsample mode to EdgeNeXt and train a <code>small</code> model. Better than original small, but not their new USI trained weights. * My own model weight results (all ImageNet-1k training) * <code>resnet10t</code> - 66.5 @ 176, 68.3 @ 224 * <code>resnet14t</code> - 71.3 @ 176, 72.3 @ 224 * <code>resnetaa50</code> - 80.6 @ 224 , 81.6 @ 288 * <code>darknet53</code> - 80.0 @ 256, 80.5 @ 288 * <code>cs3darknet_m</code> - 77.0 @ 256, 77.6 @ 288 * <code>cs3darknet_focus_m</code> - 76.7 @ 256, 77.3 @ 288 * <code>cs3darknet_l</code> - 80.4 @ 256, 80.9 @ 288 * <code>cs3darknet_focus_l</code> - 80.3 @ 256, 80.9 @ 288 * <code>vit_srelpos_small_patch16_224</code> - 81.1 @ 224, 82.1 @ 320 * <code>vit_srelpos_medium_patch16_224</code> - 82.3 @ 224, 83.1 @ 320 * <code>vit_relpos_small_patch16_cls_224</code> - 82.6 @ 224, 83.6 @ 320 * <code>edgnext_small_rw</code> - 79.6 @ 224, 80.4 @ 320 * <code>cs3</code>, <code>darknet</code>, and <code>vit_*relpos</code> weights above all trained on TPU thanks to TRC program! Rest trained on overheating GPUs. * Hugging Face Hub support fixes verified, demo notebook TBA * Pretrained weights / configs can be loaded externally (ie from local disk) w/ support for head adaptation. * Add support to change image extensions scanned by <code>timm</code> datasets/parsers. See (https://github.com/rwightman/pytorch-image-models/pull/1274#issuecomment-1178303103) * Default ConvNeXt LayerNorm impl to use <code>F.layer_norm(x.permute(0, 2, 3, 1), ...).permute(0, 3, 1, 2)</code> via <code>LayerNorm2d</code> in all cases. * a bit slower than previous custom impl on some hardware (ie Ampere w/ CL), but overall fewer regressions across wider HW / PyTorch version ranges. * previous impl exists as <code>LayerNormExp2d</code> in <code>models/layers/norm.py</code> * Numerous bug fixes * Currently testing for imminent PyPi 0.6.x release * LeViT pretraining of larger models still a WIP, they donâ€™t train well / easily without distillation. Time to add distill support (finally)? * ImageNet-22k weight training + finetune ongoing, work on multi-weight support (slowly) chugging along (there are a LOT of weights, sigh) â€¦</p>
</section>
<section id="may-13-2022" class="level3">
<h3 class="anchored" data-anchor-id="may-13-2022">May 13, 2022</h3>
<ul>
<li>Official Swin-V2 models and weights added from (https://github.com/microsoft/Swin-Transformer). Cleaned up to support torchscript.</li>
<li>Some refactoring for existing <code>timm</code> Swin-V2-CR impl, will likely do a bit more to bring parts closer to official and decide whether to merge some aspects.</li>
<li>More Vision Transformer relative position / residual post-norm experiments (all trained on TPU thanks to TRC program)
<ul>
<li><code>vit_relpos_small_patch16_224</code> - 81.5 @ 224, 82.5 @ 320 â€“ rel pos, layer scale, no class token, avg pool</li>
<li><code>vit_relpos_medium_patch16_rpn_224</code> - 82.3 @ 224, 83.1 @ 320 â€“ rel pos + res-post-norm, no class token, avg pool</li>
<li><code>vit_relpos_medium_patch16_224</code> - 82.5 @ 224, 83.3 @ 320 â€“ rel pos, layer scale, no class token, avg pool</li>
<li><code>vit_relpos_base_patch16_gapcls_224</code> - 82.8 @ 224, 83.9 @ 320 â€“ rel pos, layer scale, class token, avg pool (by mistake)</li>
</ul></li>
<li>Bring 512 dim, 8-head â€˜mediumâ€™ ViT model variant back to life (after using in a pre DeiT â€˜smallâ€™ model for first ViT impl back in 2020)</li>
<li>Add ViT relative position support for switching btw existing impl and some additions in official Swin-V2 impl for future trials</li>
<li>Sequencer2D impl (https://arxiv.org/abs/2205.01972), added via PR from author (https://github.com/okojoalg)</li>
</ul>
</section>
<section id="may-2-2022" class="level3">
<h3 class="anchored" data-anchor-id="may-2-2022">May 2, 2022</h3>
<ul>
<li>Vision Transformer experiments adding Relative Position (Swin-V2 log-coord) (<code>vision_transformer_relpos.py</code>) and Residual Post-Norm branches (from Swin-V2) (<code>vision_transformer*.py</code>)
<ul>
<li><code>vit_relpos_base_patch32_plus_rpn_256</code> - 79.5 @ 256, 80.6 @ 320 â€“ rel pos + extended width + res-post-norm, no class token, avg pool</li>
<li><code>vit_relpos_base_patch16_224</code> - 82.5 @ 224, 83.6 @ 320 â€“ rel pos, layer scale, no class token, avg pool</li>
<li><code>vit_base_patch16_rpn_224</code> - 82.3 @ 224 â€“ rel pos + res-post-norm, no class token, avg pool</li>
</ul></li>
<li>Vision Transformer refactor to remove representation layer that was only used in initial vit and rarely used since with newer pretrain (ie <code>How to Train Your ViT</code>)</li>
<li><code>vit_*</code> models support removal of class token, use of global average pool, use of fc_norm (ala beit, mae).</li>
</ul>
</section>
<section id="april-22-2022" class="level3">
<h3 class="anchored" data-anchor-id="april-22-2022">April 22, 2022</h3>
<ul>
<li><code>timm</code> models are now officially supported in <a href="https://www.fast.ai/">fast.ai</a>! Just in time for the new Practical Deep Learning course. <code>timmdocs</code> documentation link updated to <a href="http://timm.fast.ai/">timm.fast.ai</a>.</li>
<li>Two more model weights added in the TPU trained <a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights">series</a>. Some In22k pretrain still in progress.
<ul>
<li><code>seresnext101d_32x8d</code> - 83.69 @ 224, 84.35 @ 288</li>
<li><code>seresnextaa101d_32x8d</code> (anti-aliased w/ AvgPool2d) - 83.85 @ 224, 84.57 @ 288</li>
</ul></li>
</ul>
</section>
<section id="march-23-2022" class="level3">
<h3 class="anchored" data-anchor-id="march-23-2022">March 23, 2022</h3>
<ul>
<li>Add <code>ParallelBlock</code> and <code>LayerScale</code> option to base vit models to support model configs in <a href="https://arxiv.org/abs/2203.09795">Three things everyone should know about ViT</a></li>
<li><code>convnext_tiny_hnf</code> (head norm first) weights trained with (close to) A2 recipe, 82.2% top-1, could do better with more epochs.</li>
</ul>
</section>
<section id="march-21-2022" class="level3">
<h3 class="anchored" data-anchor-id="march-21-2022">March 21, 2022</h3>
<ul>
<li>Merge <code>norm_norm_norm</code>. <strong>IMPORTANT</strong> this update for a coming 0.6.x release will likely de-stabilize the master branch for a while. Branch <a href="https://github.com/rwightman/pytorch-image-models/tree/0.5.x"><code>0.5.x</code></a> or a previous 0.5.x release can be used if stability is required.</li>
<li>Significant weights update (all TPU trained) as described in this <a href="https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-tpu-weights">release</a>
<ul>
<li><code>regnety_040</code> - 82.3 @ 224, 82.96 @ 288</li>
<li><code>regnety_064</code> - 83.0 @ 224, 83.65 @ 288</li>
<li><code>regnety_080</code> - 83.17 @ 224, 83.86 @ 288</li>
<li><code>regnetv_040</code> - 82.44 @ 224, 83.18 @ 288 (timm pre-act)</li>
<li><code>regnetv_064</code> - 83.1 @ 224, 83.71 @ 288 (timm pre-act)</li>
<li><code>regnetz_040</code> - 83.67 @ 256, 84.25 @ 320</li>
<li><code>regnetz_040h</code> - 83.77 @ 256, 84.5 @ 320 (w/ extra fc in head)</li>
<li><code>resnetv2_50d_gn</code> - 80.8 @ 224, 81.96 @ 288 (pre-act GroupNorm)</li>
<li><code>resnetv2_50d_evos</code> 80.77 @ 224, 82.04 @ 288 (pre-act EvoNormS)</li>
<li><code>regnetz_c16_evos</code> - 81.9 @ 256, 82.64 @ 320 (EvoNormS)</li>
<li><code>regnetz_d8_evos</code> - 83.42 @ 256, 84.04 @ 320 (EvoNormS)</li>
<li><code>xception41p</code> - 82 @ 299 (timm pre-act)</li>
<li><code>xception65</code> - 83.17 @ 299</li>
<li><code>xception65p</code> - 83.14 @ 299 (timm pre-act)</li>
<li><code>resnext101_64x4d</code> - 82.46 @ 224, 83.16 @ 288</li>
<li><code>seresnext101_32x8d</code> - 83.57 @ 224, 84.270 @ 288</li>
<li><code>resnetrs200</code> - 83.85 @ 256, 84.44 @ 320</li>
</ul></li>
<li>HuggingFace hub support fixed w/ initial groundwork for allowing alternative â€˜config sourcesâ€™ for pretrained model definitions and weights (generic local file / remote url support soon)</li>
<li>SwinTransformer-V2 implementation added. Submitted by <a href="https://github.com/ChristophReich1996">Christoph Reich</a>. Training experiments and model changes by myself are ongoing so expect compat breaks.</li>
<li>Swin-S3 (AutoFormerV2) models / weights added from https://github.com/microsoft/Cream/tree/main/AutoFormerV2</li>
<li>MobileViT models w/ weights adapted from https://github.com/apple/ml-cvnets</li>
<li>PoolFormer models w/ weights adapted from https://github.com/sail-sg/poolformer</li>
<li>VOLO models w/ weights adapted from https://github.com/sail-sg/volo</li>
<li>Significant work experimenting with non-BatchNorm norm layers such as EvoNorm, FilterResponseNorm, GroupNorm, etc</li>
<li>Enhance support for alternate norm + act (â€˜NormActâ€™) layers added to a number of models, esp EfficientNet/MobileNetV3, RegNet, and aligned Xception</li>
<li>Grouped conv support added to EfficientNet family</li>
<li>Add â€˜group matchingâ€™ API to all models to allow grouping model parameters for application of â€˜layer-wiseâ€™ LR decay, lr scale added to LR scheduler</li>
<li>Gradient checkpointing support added to many models</li>
<li><code>forward_head(x, pre_logits=False)</code> fn added to all models to allow separate calls of <code>forward_features</code> + <code>forward_head</code></li>
<li>All vision transformer and vision MLP models update to return non-pooled / non-token selected features from <code>foward_features</code>, for consistency with CNN models, token selection or pooling now applied in <code>forward_head</code></li>
</ul>
</section>
<section id="feb-2-2022" class="level3">
<h3 class="anchored" data-anchor-id="feb-2-2022">Feb 2, 2022</h3>
<ul>
<li><a href="https://github.com/Chris-hughes10">Chris Hughes</a> posted an exhaustive run through of <code>timm</code> on his blog yesterday. Well worth a read. <a href="https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055">Getting Started with PyTorch Image Models (timm): A Practitionerâ€™s Guide</a></li>
<li>Iâ€™m currently prepping to merge the <code>norm_norm_norm</code> branch back to master (ver 0.6.x) in next week or so.
<ul>
<li>The changes are more extensive than usual and may destabilize and break some model API use (aiming for full backwards compat). So, beware <code>pip install git+https://github.com/rwightman/pytorch-image-models</code> installs!</li>
<li><code>0.5.x</code> releases and a <code>0.5.x</code> branch will remain stable with a cherry pick or two until dust clears. Recommend sticking to pypi install for a bit if you want stable.</li>
</ul></li>
</ul>
</section>
<section id="jan-14-2022" class="level3">
<h3 class="anchored" data-anchor-id="jan-14-2022">Jan 14, 2022</h3>
<ul>
<li>Version 0.5.4 w/ release to be pushed to pypi. Itâ€™s been a while since last pypi update and riskier changes will be merged to main branch soonâ€¦.</li>
<li>Add ConvNeXT models /w weights from official impl (https://github.com/facebookresearch/ConvNeXt), a few perf tweaks, compatible with timm features</li>
<li>Tried training a few small (~1.8-3M param) / mobile optimized models, a few are good so far, more on the wayâ€¦
<ul>
<li><code>mnasnet_small</code> - 65.6 top-1</li>
<li><code>mobilenetv2_050</code> - 65.9</li>
<li><code>lcnet_100/075/050</code> - 72.1 / 68.8 / 63.1</li>
<li><code>semnasnet_075</code> - 73</li>
<li><code>fbnetv3_b/d/g</code> - 79.1 / 79.7 / 82.0</li>
</ul></li>
<li>TinyNet models added by <a href="https://github.com/rsomani95">rsomani95</a></li>
<li>LCNet added via MobileNetV3 architecture</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>