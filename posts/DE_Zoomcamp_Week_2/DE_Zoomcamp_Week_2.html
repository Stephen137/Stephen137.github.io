<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stephen Barrie">
<meta name="dcterms.date" content="2023-03-16">

<title>Into the Unknown - Data Engineering Zoomcamp - Week 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Stephen Barrie</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Data Engineering Zoomcamp - Week 2</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">GCP</div>
                <div class="quarto-category">BigQuery</div>
                <div class="quarto-category">Data Lakes</div>
                <div class="quarto-category">ETL</div>
                <div class="quarto-category">Prefect</div>
                <div class="quarto-category">DataTalksClub</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stephen Barrie </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 16, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#week-2---workflow-orchestration" id="toc-week-2---workflow-orchestration" class="nav-link active" data-scroll-target="#week-2---workflow-orchestration">Week 2 - Workflow Orchestration</a>
  <ul class="collapse">
  <li><a href="#data-lakes" id="toc-data-lakes" class="nav-link" data-scroll-target="#data-lakes">2.1.1 Data Lakes</a></li>
  <li><a href="#introduction-to-workflow-orchestration-data-flow-logistics" id="toc-introduction-to-workflow-orchestration-data-flow-logistics" class="nav-link" data-scroll-target="#introduction-to-workflow-orchestration-data-flow-logistics">2.2.1 Introduction to Workflow Orchestration (data flow logistics)</a></li>
  <li><a href="#introduction-to-prefect" id="toc-introduction-to-prefect" class="nav-link" data-scroll-target="#introduction-to-prefect">2.2.2 Introduction to Prefect</a></li>
  <li><a href="#extract-transform-and-load-etl-with-google-cloud-platform-gcp-and-prefect" id="toc-extract-transform-and-load-etl-with-google-cloud-platform-gcp-and-prefect" class="nav-link" data-scroll-target="#extract-transform-and-load-etl-with-google-cloud-platform-gcp-and-prefect">2.2.3 Extract, Transform and Load (ETL) with Google Cloud Platform (GCP) and Prefect</a></li>
  <li><a href="#from-google-cloud-storage-to-biq-query" id="toc-from-google-cloud-storage-to-biq-query" class="nav-link" data-scroll-target="#from-google-cloud-storage-to-biq-query">2.2.4 From Google Cloud Storage to Biq Query</a></li>
  <li><a href="#parametrizing-flow-deployments-with-etl-into-gcs-flow" id="toc-parametrizing-flow-deployments-with-etl-into-gcs-flow" class="nav-link" data-scroll-target="#parametrizing-flow-deployments-with-etl-into-gcs-flow">2.2.5 Parametrizing Flow &amp; Deployments with ETL into GCS flow</a></li>
  <li><a href="#schedules-and-docker-storage-with-infrastructure" id="toc-schedules-and-docker-storage-with-infrastructure" class="nav-link" data-scroll-target="#schedules-and-docker-storage-with-infrastructure">2.2.6 Schedules and Docker Storage with Infrastructure</a></li>
  <li><a href="#prefect-cloud---additional-resources" id="toc-prefect-cloud---additional-resources" class="nav-link" data-scroll-target="#prefect-cloud---additional-resources">2.2.7 Prefect Cloud - Additional Resources</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="week-2---workflow-orchestration" class="level2">
<h2 class="anchored" data-anchor-id="week-2---workflow-orchestration">Week 2 - Workflow Orchestration</h2>
<p>Just like a physical transport logistics system, it is important to have a smooth data logistics system. This process is also known as <code>Workflow Orchestration</code>. Workflow orchestration allows us to turn any code into a workflow that we can schedule, run and observe.</p>
<p>Core features:</p>
<ul>
<li>remote execution</li>
<li>scheduling</li>
<li>retries</li>
<li>caching</li>
<li>integration with external systems (APIs, databases)</li>
<li>ad-hoc runs</li>
<li>parametrization</li>
<li>alert when something fails</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/2c7716b2-c0a1-49e9-8aff-76f8ad72aa0f-1-4e3e08f6-0273-4434-b889-b6921482a0d6.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">week_2.JPG</figcaption>
</figure>
</div>
<section id="data-lakes" class="level3">
<h3 class="anchored" data-anchor-id="data-lakes">2.1.1 Data Lakes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/9b51c01f-62ce-4a12-9d35-111d3215c9dc-1-63571a21-e79a-4984-9ae2-064605e666d9.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data_lake.JPG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/6c4ba914-18f6-4bf3-9082-645ee3d266dd-1-99ac080c-c55c-4967-9b59-6ee28c178207.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data_lake_origin.JPG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/0f3fcb0a-a074-4bd2-b3a2-e0b287b81d08-1-8d8ec4af-26c4-4148-9113-958b9cec5bf8.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">lake_vs_warehouse.JPG</figcaption>
</figure>
</div>
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/df1c005c-83ac-4a37-94cb-25c767d2c460-1-32a74388-8691-46cb-af3c-ca2f3e11c4c2.JPG" class="img-fluid" alt="ELT.JPG"> <img src="DE_Zoomcamp_Week_2_files/figure-html/df1c005c-83ac-4a37-94cb-25c767d2c460-2-e8510545-0ce2-41c5-911b-b155778c8a6b.JPG" class="img-fluid" alt="ETL.JPG"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/c540bb8e-2372-4b4d-9f68-81eab391f86f-1-6ae5b11c-ef1d-4bda-bb11-92e54d61126b.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data_lake_cloud.JPG</figcaption>
</figure>
</div>
</section>
<section id="introduction-to-workflow-orchestration-data-flow-logistics" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-workflow-orchestration-data-flow-logistics">2.2.1 Introduction to Workflow Orchestration (data flow logistics)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/179e3b76-5524-45b7-ba72-f6b0a9e3d31c-1-8e933809-32f3-4068-b99c-4af53575eac5.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data_engineering_map.JPG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/1cb5c1fb-eee9-4955-86b6-8b32b654540a-1-7bb92351-add5-415c-a679-d0ca038ddd66.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">workflow_orchestration.JPG</figcaption>
</figure>
</div>
</section>
<section id="introduction-to-prefect" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-prefect">2.2.2 Introduction to Prefect</h3>
<p><a href="https://docs.prefect.io/">Prefect</a> is air traffic control for the modern data stack. Monitor, coordinate, and orchestrate dataflows between and across your applications. Build pipelines, deploy them anywhere, and configure them remotely. You might just love your workflows again.</p>
<p>In this session, we are going to take a look at a basic python script that pulls the yellow taxi data into a postgres db and then transforms that script to be orchestrated with Prefect.</p>
<p>Prefect is the modern open source dataflow automation platform that will allow us to add observability and orchestration by utilizing python to write code as workflows to build,run and monitor pipelines at scale.</p>
<p>First let’s clone the <a href="https://github.com/discdiver/prefect-zoomcamp">Prefect repo</a> from the command line:</p>
<pre><code>git clone https://github.com/discdiver/prefect-zoomcamp.git</code></pre>
<p>Next, create a python environment :</p>
<pre><code>conda create -n zoomcamp python=3.9   </code></pre>
<p>Once created we need to activate it:</p>
<pre><code>conda activate zoomcamp</code></pre>
<p>To deactivate an environment use:</p>
<pre><code>conda deactivate  </code></pre>
<p>Note from the terminal that we are no longer running in <code>base</code> but our newly created <code>zoomcamp</code> environment:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/a2132ee7-9331-4903-9e9b-ee6ea6ca5be6-1-b25f0936-402d-48f8-9a9c-e62ccca914ad.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">zoomcamp_environ.JPG</figcaption>
</figure>
</div>
<p>Then install all package dependencies with:</p>
<pre><code>pip install -r requirements.txt</code></pre>
<p>Once that’s done we can check that has installed successfully and which version we have from the command line:</p>
<pre><code>prefect version</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/fcae600d-2050-4b29-ab9a-844f17926db5-1-d77efb41-a9d9-41ac-b90d-16651906a4b2.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_version.JPG</figcaption>
</figure>
</div>
<p>I started Docker Desktop and executed these commands :</p>
<p>docker run -d<br>
-e POSTGRES_USER=“root”<br>
-e POSTGRES_PASSWORD=“root”<br>
-e POSTGRES_DB=“ny_taxi”<br>
-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data<br>
-p 5432:5432<br>
postgres:13</p>
<p>Then, I executed the <code>ingest_data.py</code> file and ran with</p>
<pre><code>python ingest_data.py

#!/usr/bin/env python
# coding: utf-8
import os
import argparse
from time import time
import pandas as pd
from sqlalchemy import create_engine


def ingest_data(user, password, host, port, db, table_name, url):

# the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {url} -O {csv_name}")
postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'
engine = create_engine(postgres_url)

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')

df.to_sql(name=table_name, con=engine, if_exists='append')


while True: 

    try:
        t_start = time()
        
        df = next(df_iter)

        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

        df.to_sql(name=table_name, con=engine, if_exists='append')

        t_end = time()

        print('inserted another chunk, took %.3f second' % (t_end - t_start))

    except StopIteration:
        print("Finished ingesting data into the postgres database")
        break

if __name__ == '__main__':
user = "root"
password = "root"
host = "localhost"
port = "5432"
db = "ny_taxi"
table_name = "yellow_taxi_trips"
csv_url = "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"

ingest_data(user, password, host, port, db, table_name, csv_url)</code></pre>
<p>I then opened up <code>pgcli</code></p>
<pre><code>pgcli -h localhost -p 5432 -u root -d ny_taxi</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/a5f9a912-b054-411a-8ec2-fcfee8bf8074-1-d6190ac2-543f-455c-8e42-d68a7bab06d6.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data_ingest.JPG</figcaption>
</figure>
</div>
<p>So we can see that the data ingested into the postgres db. This is great but we had to manually trigger this python script. Using a workflow orchestration tool will allow us to add a scheduler so that we won’t have to trigger this script manually anymore. Additionally, we’ll get all the functionality that comes with workflow orchestation such as visibility, and resilience to the dataflow with automatic retries or caching and more.</p>
<p>Let’s transform this into a Prefect flow. A flow is the most basic Prefect object that is a container for workflow logic and allows you to interact and understand the state of the workflow. Flows are like functions, they take inputs, preform work, and return an output. We can start by using the <code>@flow decorator</code>to a <code>main_flow</code> function.</p>
<ul>
<li>import prefect with <code>from prefect import flow, task</code></li>
<li>move everything that was in our <code>'if __name__ == '__main__'</code> function to a new <code>def main():</code> function (replace with a reference to <code>main()</code></li>
<li>add <code>@flow(name="Ingest Flow")</code> above a new <code>def main()</code> function</li>
<li>remove the <code>while True</code> part of our original script</li>
</ul>
<p>I started Docker Desktop and executed these commands:</p>
<p>docker run -d<br>
-e POSTGRES_USER=“root”<br>
-e POSTGRES_PASSWORD=“root”<br>
-e POSTGRES_DB=“ny_taxi”<br>
-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data<br>
-p 5432:5432<br>
postgres:13</p>
<p>Then start the Prefect Orion orchestration engine using:</p>
<pre><code>prefect orion start</code></pre>
<p>Open another terminal window and run the following command:</p>
<pre><code>prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api


#!/usr/bin/env python
# coding: utf-8
import os
import argparse
from time import time
import pandas as pd
from sqlalchemy import create_engine
from prefect import flow, task                 # Added

def ingest_data(user, password, host, port, db, table_name, url):

# the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {url} -O {csv_name}")
postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'
engine = create_engine(postgres_url)

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')

df.to_sql(name=table_name, con=engine, if_exists='append')


@flow(name="Ingest Flow")                       # Added
def main_flow():
user = "root"
password = "root"
host = "localhost"
port = "5432"
db = "ny_taxi"
table_name = "yellow_taxi_trips"
csv_url = "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"

ingest_data(user, password, host, port, db, table_name, csv_url)


if __name__ == '__main__':
main_flow()                                      # everything that was here moved into the new def main(): function </code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/ef3a2b5b-5737-48d6-9baf-16af79062218-1-e81304af-7105-4e54-9c72-ba74c0ee69e2.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">flow_run1.JPG</figcaption>
</figure>
</div>
<p>We successfully completed a <code>flow run</code>.</p>
<p>Flows contain tasks so let’s transform ingest_data into a task by adding the <span class="citation" data-cites="task">@task</span> decorator. Tasks are not required for flows but tasks are special because they receive metadata about upstream dependencies and the state of those dependencies before the function is run, which gives you the opportunity to have a task wait on the completion of another task before executing.</p>
<p>We can simplify this script and transform it into an extract and transform before we load the data into the postgres db. We start by breaking apart the large <code>ingest_data</code> function into multiple functions so that we can get more <code>visibility</code> into the <code>tasks</code> that are running or potentially causing failures.</p>
<p>Let’s create a new task called <code>extract data</code> that will take the url for the csv and the task will actually return the results. Since this is pulling data from external my system (something we may not control) we want to add automatic <code>retries</code> and also add a <code>caching</code> so that if this task has already been run, it will not need to run again.</p>
<pre><code>import from prefect.tasks import task_input_hash</code></pre>
<p>If we look at the data in PotsgreSQL we can see that on row 4, there is a passenger count of 0. So let’s do a transformation step to cleanse the data before we load the data to postgres. We can create a new task called <code>transform_data</code> for this.</p>
<p>Lastly, let’s actually simplify the original <code>ingest_data()</code> function and rename this to <code>load_data()</code></p>
<pre><code>#!/usr/bin/env python
# coding: utf-8
import os
import argparse
from time import time
import pandas as pd
from sqlalchemy import create_engine
from prefect import flow, task 
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(log_prints=True, tags=["extract"], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))
def extract_data(url: str):
# the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {url} -O {csv_name}")

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

return df

@task(log_prints=True)
def transform_data(df):
    print(f"pre: missing passenger count: {df['passenger_count'].isin([0]).sum()}")
    df = df[df['passenger_count'] != 0]
    print(f"post: missing passenger count: {df['passenger_count'].isin([0]).sum()}")
    return df



@task(log_prints=True, retries=3)
def load_data(user, password, host, port, db, table_name, df):
  postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'
  engine = create_engine(postgres_url)
  df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')
  df.to_sql(name=table_name, con=engine, if_exists='append')



@flow(name="Ingest Flow")
def main_flow():
    user = "root"
    password = "root"
    host = "localhost"
    port = "5432"
    db = "ny_taxi"
    table_name = "yellow_taxi_trips"
    csv_url = "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"

    raw_data=extract_data(csv_url)
    data = transform_data(raw_data)
    load_data(user, password, host, port, db, table_name, data)

if __name__ == '__main__':
    main_flow()</code></pre>
<p>There’s a lot more we can add by sprinkling in Prefect to our flow. We could parameterize the flow to take a table name so that we could change the table name loaded each time the flow was run.</p>
<p>Flows can also contain other flows - and so we can create a <code>sub-flow</code> :</p>
<pre><code>#!/usr/bin/env python
# coding: utf-8
import os
import argparse
from time import time
import pandas as pd
from sqlalchemy import create_engine
from prefect import flow, task 
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(log_prints=True, tags=["extract"], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))
def extract_data(url: str):
# the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {url} -O {csv_name}")

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

return df

@task(log_prints=True)
def transform_data(df):
    print(f"pre: missing passenger count: {df['passenger_count'].isin([0]).sum()}")
    df = df[df['passenger_count'] != 0]
    print(f"post: missing passenger count: {df['passenger_count'].isin([0]).sum()}")
    return df

@task(log_prints=True, retries=3)
def load_data(user, password, host, port, db, table_name, df):
  postgres_url = f"postgresql://{user}:{password}@{host}:{port}/{db}"
  engine = create_engine(postgres_url)
  df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')
  df.to_sql(name=table_name, con=engine, if_exists='append')

  @flow(name="Subflow", log_prints=True)
  def log_subflow(table_name:str):
      print("Logging Subflow for: {table_name}")

@flow(name="Ingest Flow")
def main_flow(table_name: str):
    user = "root"
    password = "root"
    host = "localhost"
    port = "5432"
    db = "ny_taxi"
    csv_url = "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
    log_subflow = (table_name)
    raw_data = extract_data(csv_url)
    data = transform_data(raw_data)
    load_data(user, password, host, port, db, table_name, data)

if __name__ == '__main__':
    main_flow("yellow_taxi_trips")</code></pre>
<p>That has run successfully:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/f11c4519-5759-4535-8ba7-e2e29c7ba9d1-1-ad18c860-f545-43e8-b325-1dd463a19f8d.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">sub_flow.JPG</figcaption>
</figure>
</div>
<p>Let’s now open the open source UI to visualise our flow runs :</p>
<pre><code>prefect orion start</code></pre>
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/e8101dbc-2bbb-4f1f-8f5a-9cd69b18b396-1-25cc8011-10b0-403c-9a51-91ce959b7c26.JPG" class="img-fluid" alt="prefect_orion.JPG"><img src="DE_Zoomcamp_Week_2_files/figure-html/e8101dbc-2bbb-4f1f-8f5a-9cd69b18b396-2-964d7165-c296-4d3e-9dd3-7a4525ab9915.jpg" class="img-fluid" alt="download.jpg"></p>
<p>This should default but if you are having problems or just want to make sure you set the prefect config to point to the api URL:</p>
<pre><code>prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/e1fc85f2-1bff-4204-bd21-f08e2677be11-1-ac55d25f-10b8-4a61-b207-a2d8633faff9.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_api_url.JPG</figcaption>
</figure>
</div>
<p>This is especially important if you are going to host the Url somewhere else and need to change the url for the api that your flows are communicating with.</p>
<p>Opening up the localhost we can see the Prefect UI, which gives us a nice dashboard to see all of our flow run history.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/87e11462-e644-494c-a0a1-ae6c4344c0e1-1-818033db-c98d-4b52-8a51-e4fdb40fa5ca.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_UI.JPG</figcaption>
</figure>
</div>
<p>We can then drill down into the runs to obtain more details :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/e4695d23-a4e4-4a2e-b187-e8070490e177-1-a1f37a03-bb12-4a0f-97df-563cdfdbd6b1.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">optimistic_flounder.JPG</figcaption>
</figure>
</div>
<p>A quick navigation lets us dive into the logs of that flow run, navigate around. You’ll notice over on the side we have <code>Deployments</code>, <code>Work Queues</code>, <code>Blocks</code>, <code>Notifications</code>, and <code>Task Run Concurrency</code>.</p>
<p>Blocks are a primitive within Prefect that enable the storage of configuration and provide an interface with interacting with external systems. There are several different types of blocks you can build, and you can even create your own. Block names are immutable so they can be reused across multiple flows. Blocks can also build upon blocks or be installed as part of Integration <a href="https://docs.prefect.io/collections/catalog/">collection</a> which is prebuilt tasks and blocks that are pip installable. For example, a lot of users use the SqlAlchemy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/0c78ce1d-4fe6-49a5-9f4a-5ecdddda4d5d-1-6d20c728-7a32-4b11-b1ff-1f6bb02de81f.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_blocks.JPG</figcaption>
</figure>
</div>
<p>Let’s add the sqlalchemy connector :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/601b22a3-430d-4803-85da-ddf4a3235aea-1-4c09f604-44a7-43b4-9f1c-33c8da259000.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">sqlalchemy_connector.PNG</figcaption>
</figure>
</div>
<p>As you can see we have now created the block and can now update our ingestion script to include the necessary code to make the connection work :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/33face34-248f-478b-8557-0525beec192a-1-43264408-8954-4456-93a8-73dc3fab9213.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">sqlalchemy_connector_block.PNG</figcaption>
</figure>
</div>
<pre><code>#!/usr/bin/env python
# coding: utf-8

import os
import argparse
from time import time
import pandas as pd
from sqlalchemy import create_engine
from prefect import flow, task
from prefect.tasks import task_input_hash
from datetime import timedelta
from prefect_sqlalchemy import SqlAlchemyConnector

@task(log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))
def extract_data(csv_url):
# the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if csv_url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {csv_url} -O {csv_name}")

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)
return df 

@task(log_prints=True)
def transform_data(df):
    print(f"pre: missing passenger count: {df['passenger_count'].isin([0]).sum()}")
    df = df[df['passenger_count'] != 0]
    print(f"post: missing passenger count: {df['passenger_count'].isin([0]).sum()}")
    return df 

@task(log_prints=True, retries=3)
def ingest_data(table_name, df):
    connection_block = SqlAlchemyConnector.load("postgres-connector")

with connection_block.get_connection(begin=False) as engine:    
    df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')
    df.to_sql(name=table_name, con=engine, if_exists='append')

@flow(name="Subflow", log_prints=True)
def log_subflow(table_name: str):
    print(f"Logging Subflow for: {table_name}")

@flow(name="Ingest Flow")
def main_flow(table_name: str):
    csv_url = "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
    log_subflow(table_name)
    raw_data = extract_data(csv_url)
    data = transform_data(raw_data)
    ingest_data(table_name, data)

if __name__ == '__main__':
    main_flow("yellow_taxi_trips")</code></pre>
<p>And run our flow:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/3b25417b-6c63-4a55-832c-0e66803cceaf-1-1015e5a8-ebb3-4fb9-bc2f-bc4f4323ff95.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">ingest_data_flow.PNG</figcaption>
</figure>
</div>
</section>
<section id="extract-transform-and-load-etl-with-google-cloud-platform-gcp-and-prefect" class="level3">
<h3 class="anchored" data-anchor-id="extract-transform-and-load-etl-with-google-cloud-platform-gcp-and-prefect">2.2.3 Extract, Transform and Load (ETL) with Google Cloud Platform (GCP) and Prefect</h3>
<p>OK, so next up we are going to create an <code>ETL</code> flow using <code>Prefect</code> to grab a csv file from the web, clean it up, write it out locally and finally upload the data to a <code>data lake</code> within <code>Google Cloud Storage(GCS)</code>.</p>
<section id="create-prefect-gcp-blocks" class="level4">
<h4 class="anchored" data-anchor-id="create-prefect-gcp-blocks">Create Prefect GCP blocks</h4>
<p>First of all let’s register our GCP blocks from the command line:</p>
<pre><code>prefect block register -m prefect_gcp</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/771b5b0f-1b0d-489d-acc3-1a892f84de15-1-0468a656-27f0-44a2-bf41-068f70b8fd9e.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_gcp_block.png</figcaption>
</figure>
</div>
<p>And now let’s create a <code>GCS Bucket</code> block from the Prefect GUI :</p>
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/bc99ba80-fb9d-42c0-9988-51039192297d-1-b51d593c-ff4e-451f-a363-d6ca870f95e2.PNG" class="img-fluid" alt="gcs_bucket.PNG">+</p>
<p>and add credentials :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/bc99ba80-fb9d-42c0-9988-51039192297d-2-c963fa84-beeb-4b86-9ba3-849218f4c792.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">gcs_bucket_plus_credentials.PNG</figcaption>
</figure>
</div>
<p>Not that I created a service account previously within GCP and generated a key which I have saved on my machine in JSON format. I then copied and pasted the contents of the file (a dictionary) into the Service Account Info field.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/bc99ba80-fb9d-42c0-9988-51039192297d-3-d50abbf9-4d3f-4114-a3d8-bd99bea5f121.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">gcp_blocks.PNG</figcaption>
</figure>
</div>
<p>We can now grab the block code and incorporate this into our workflow:</p>
<pre><code>from pathlib import Path
import pandas as pd
from prefect import flow, task
from prefect_gcp.cloud_storage import GcsBucket # this code provided on creation of our block
from random import randint


@task(retries=3)
def fetch(dataset_url: str) -&gt; pd.DataFrame:
"""Read taxi data from web into pandas DataFrame"""
# if randint(0, 1) &gt; 0:
#     raise Exception

    df = pd.read_csv(dataset_url)
    return df


@task(log_prints=True)
def clean(df: pd.DataFrame) -&gt; pd.DataFrame:
"""Fix dtype issues"""
    df["tpep_pickup_datetime"] = pd.to_datetime(df["tpep_pickup_datetime"])
    df["tpep_dropoff_datetime"] = pd.to_datetime(df["tpep_dropoff_datetime"])
    print(df.head(2))
    print(f"columns: {df.dtypes}")
    print(f"rows: {len(df)}")
    return df


@task()
def write_local(df: pd.DataFrame, color: str, dataset_file: str) -&gt; Path:
"""Write DataFrame out locally as parquet file"""
    path = Path(f"data/{color}/{dataset_file}.parquet")
    df.to_parquet(path, compression="gzip")
    return path


@task()
def write_gcs(path: Path) -&gt; None:
"""Upload local parquet file to GCS"""
    gcs_block = GcsBucket.load("de-zoomcamp") # the name we gave our block on creation
    gcs_block.upload_from_path(from_path=path, to_path=path)
    return


@flow()
def etl_web_to_gcs() -&gt; None:
"""The main ETL function"""
    color = "yellow"
    year = 2021
    month = 1
    dataset_file = f"{color}_tripdata_{year}-{month:02}"
    dataset_url = f"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{color}/{dataset_file}.csv.gz"

    df = fetch(dataset_url)
    df_clean = clean(df)
    path = write_local(df_clean, color, dataset_file)
    write_gcs(path)


if __name__ == "__main__":
    etl_web_to_gcs()</code></pre>
<p>Note the above flow has been hardcoded in places, but we will be refining / <code>parametrizing</code> this later.</p>
<p>First let’s create directories to store our data locally :</p>
<pre><code>mkdir data
mkdir yellow</code></pre>
<p>and then run the script :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/69e0a5f9-a852-49de-a97c-d8024b06fdce-1-4bc25313-67cb-4d5f-af78-b5624716d2f0.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">etl_web_to_gcs.PNG</figcaption>
</figure>
</div>
<p>We can visualise the run on Prefect:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/dcfd864f-8bc7-4eb1-ba4e-0fc9a5c98108-1-e0d8cbad-7684-4181-a56d-bd57ddfd866b.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_etl_web_to_gcs.PNG</figcaption>
</figure>
</div>
<p>And confirm that the data has been successfully uploaded to our <code>data lake</code> bucket on GCS :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/3cbfae00-d842-495a-8c1a-045473a7b388-1-b40351cc-9f28-4e90-bfcc-b4dfad6b4502.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">data_lake_bucket_yellow_taxi.PNG</figcaption>
</figure>
</div>
</section>
</section>
<section id="from-google-cloud-storage-to-biq-query" class="level3">
<h3 class="anchored" data-anchor-id="from-google-cloud-storage-to-biq-query">2.2.4 From Google Cloud Storage to Biq Query</h3>
<p>Now that we have successfully uploaded our data to a <code>data lake</code> within Google Cloud Storage let’s now move it to <code>Big Query</code> which is a <code>data warehouse</code>. We can do this by creating a flow which extracts our data from Google Cloud Storage, creates a DataFrame, does some very basic cleaning (fill missing passenger count with zero) and then writes our dataframe to Big Query.</p>
<p>Before we go ahead and create our flow let’s first head over to GCP and configure Big Query. First let’s add our data which we previously uploaded to our data bucket:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/b48f0e85-16fd-4dc2-9480-952a53861e2c-1-41ac58ea-ffc4-40a6-bcbd-de483143a85b.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">big_query_add_data.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/b48f0e85-16fd-4dc2-9480-952a53861e2c-4-ec86b762-af21-40ee-a116-ca0e0fd84f96.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">yellow_parquet.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/b48f0e85-16fd-4dc2-9480-952a53861e2c-2-93f65716-0774-4001-be11-966a57f0dd6a.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">create_table.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/b48f0e85-16fd-4dc2-9480-952a53861e2c-3-ebd8bc4f-b184-48b9-afba-992b077d5e32.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">table.PNG</figcaption>
</figure>
</div>
<p>Let’s delete the data from the table prior to creating a flow which will automate this process for us:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/0a0d0dd5-1f6c-41ff-9af7-cfe91a6fc6d5-1-3b401569-bd11-459f-9955-e4a42c555643.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">delete_data_biq_query.PNG</figcaption>
</figure>
</div>
<p>Let’s now create our flow. Note that we can make use of our Prefect blocks created previously, namely our <code>GcsBucket</code>, and <code>GcpCredentials</code>.</p>
<pre><code>from pathlib import Path
import pandas as pd
from prefect import flow, task
from prefect_gcp.cloud_storage import GcsBucket
from prefect_gcp import GcpCredentials


@task(retries=3)
def extract_from_gcs(color: str, year: int, month: int) -&gt; Path:
"""Download trip data from GCS"""
    gcs_path = f"data/{color}/{color}_tripdata_{year}-{month:02}.parquet"
    gcs_block = GcsBucket.load("de-zoomcamp") # this is the name we gave our Prefect block on creation
    gcs_block.get_directory(from_path=gcs_path, local_path=f"../data/") # .. saves the file into a directory up a level
    return Path(f"../data/{gcs_path}")


@task()
def transform(path: Path) -&gt; pd.DataFrame:
"""Data cleaning example"""
    df = pd.read_parquet(path)
    print(f"pre: missing passenger count: {df['passenger_count'].isna().sum()}")
    df["passenger_count"].fillna(0, inplace=True)
    print(f"post: missing passenger count: {df['passenger_count'].isna().sum()}")
    return df


@task()
def write_bq(df: pd.DataFrame) -&gt; None:
"""Write DataFrame to BiqQuery"""

    gcp_credentials_block = GcpCredentials.load("de-gcp-creds") # 

    df.to_gbq(
    destination_table="de_zoomcamp.ny_taxi_rides", # this is the name of our table created in Big Query
    project_id="taxi-rides-ny-137", # this is our project ID on Google Cloud Platform
    credentials=gcp_credentials_block.get_credentials_from_service_account(),
    chunksize=500_000,
    if_exists="append",
)


@flow()
def etl_gcs_to_bq():
"""Main ETL flow to load data into Big Query"""
    color = "yellow"
    year = 2021
    month = 1

    path = extract_from_gcs(color, year, month)
    df = transform(path)
    write_bq(df)


if __name__ == "__main__":
    etl_gcs_to_bq()</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/f1887ff3-c0c6-4de6-bf0a-bd8113372277-1-ea1311fa-1f05-4a71-8215-28e8bf99d74c.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">etl_gcs_to_BQ.PNG</figcaption>
</figure>
</div>
<p>Our flow appears to have run successfully - let’s check Big Query:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/14fa6aa8-c165-4cf3-b773-e9c018aa2372-1-724bac99-ebd7-448a-9711-b2f0eecdbe1e.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">gcs_to_BQ_confirm.PNG</figcaption>
</figure>
</div>
<p>Yes, all <code>1,369,765</code> rows of our data are now available for query within BigQuery.</p>
</section>
<section id="parametrizing-flow-deployments-with-etl-into-gcs-flow" class="level3">
<h3 class="anchored" data-anchor-id="parametrizing-flow-deployments-with-etl-into-gcs-flow">2.2.5 Parametrizing Flow &amp; Deployments with ETL into GCS flow</h3>
<p>Let’s now build upon the existing flow and blocks that we configured previously and learn how to add <code>Parameterization</code> to our flows and create deployments. This removes the inflexibility of hard coded flows, by allowing our flow to take <code>parameters</code>, to be defined at run time. So to start, let’s allow our flow to take parameters of <code>year</code>, <code>month</code>, and <code>color</code>:</p>
<pre><code>from pathlib import Path
import pandas as pd
from prefect import flow, task
from prefect_gcp.cloud_storage import GcsBucket
from random import randint
from prefect.tasks import task_input_hash
from datetime import timedelta


@task(retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1)) # make use of cache so not having to re-read dataset
def fetch(dataset_url: str) -&gt; pd.DataFrame:
"""Read taxi data from web into pandas DataFrame"""
# if randint(0, 1) &gt; 0:
#     raise Exception

    df = pd.read_csv(dataset_url)
    return df


@task(log_prints=True)
def clean(df: pd.DataFrame) -&gt; pd.DataFrame:
"""Fix dtype issues"""
    df["tpep_pickup_datetime"] = pd.to_datetime(df["tpep_pickup_datetime"])
    df["tpep_dropoff_datetime"] = pd.to_datetime(df["tpep_dropoff_datetime"])
    print(df.head(2))
    print(f"columns: {df.dtypes}")
    print(f"rows: {len(df)}")
    return df


@task()
def write_local(df: pd.DataFrame, color: str, dataset_file: str) -&gt; Path:
"""Write DataFrame out locally as parquet file"""
    path = Path(f"data/{color}/{dataset_file}.parquet")
    df.to_parquet(path, compression="gzip")
    return path


@task()
def write_gcs(path: Path) -&gt; None:
"""Upload local parquet file to GCS"""
    gcs_block = GcsBucket.load("de-zoomcamp")  # the name we gave our block on creation
    gcs_block.upload_from_path(from_path=path, to_path=path)
    return


@flow()
def etl_web_to_gcs(year: int, month: int, color: str) -&gt; None:
"""The main ETL function"""
    dataset_file = f"{color}_tripdata_{year}-{month:02}"
    dataset_url = f"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{color}/{dataset_file}.csv.gz"

    df = fetch(dataset_url)
    df_clean = clean(df)
    path = write_local(df_clean, color, dataset_file)
    write_gcs(path)


@flow()
def etl_parent_flow(
    months: list[int] = [1, 2], year: int = 2021, color: str = "yellow" # pass in months as a list
):
for month in months:
    etl_web_to_gcs(year, month, color)


if __name__ == "__main__":
    color = "yellow"
    months = [1, 2, 3]
    year = 2021
    etl_parent_flow(months, year, color)</code></pre>
<p>Once again create the required directories to save locally and run the flow:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/54d90aaf-1fa6-4ff4-ab7f-c03a96e308a5-2-43443cf8-b787-4fda-ab7c-32b938f2b077.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">parametrize1.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/54d90aaf-1fa6-4ff4-ab7f-c03a96e308a5-3-dc9ea213-5e67-413e-baad-2536dfe39213.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">parametrize2.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/54d90aaf-1fa6-4ff4-ab7f-c03a96e308a5-1-3d16b68d-8cd1-4064-85fb-db1c8dece8e6.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">parametrize3.PNG</figcaption>
</figure>
</div>
<p>Excellent. Our data has been picked up, cleansed and flow runs commpleted - we can see the logs from our Orion terminal :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/fb7ff412-821d-4f2e-af3d-f39cab803d22-2-7ca7c38a-4b37-4162-a9e5-d4bfe672aa22.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_parametrize.PNG</figcaption>
</figure>
</div>
<p>And we can see our parent run, and the three subflow runs :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/fb7ff412-821d-4f2e-af3d-f39cab803d22-1-5a433fa7-2ba2-4380-b5f5-4f36bba3de85.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_parent_subflows.PNG</figcaption>
</figure>
</div>
<p><code>Deployment</code> using Prefect</p>
<p>This is all very good and works well but we had to execute the flows from the terminal manually. Let’s look at how we can use <code>Prefect</code> to configure and deploy runs from the API. A <code>deployment</code> in Prefect is a server-side concept that encapsulates a flow, allowing it to be scheduled and triggered via the API.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/875b2d2a-cf08-4431-bb9c-625b08c97d87-1-3d9ee78c-b596-4715-93e0-9ae7b845fe7b.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">deployments.PNG</figcaption>
</figure>
</div>
<p>A flow can have multiple deployments and you can think of it as the container of metadata needed for the flow to be scheduled. This might be what type of infrastructure the flow will run on, or where the flow code is stored, maybe it’s scheduled or has certain parameters.</p>
<p>There are two ways to create a deployment:</p>
<ul>
<li>using the CLI command</li>
<li>with python.</li>
</ul>
<p>We will see how to set up the deployment with Python in the next section. So for now we are going to create one using the CLI.</p>
<p>Inside our terminal we can type :</p>
<pre><code>prefect deployment build ./parameterized_flow.py:etl_parent_flow -n "Parameterized ETL"</code></pre>
<p>The file name is <code>./parametrized_flow.py</code> and <code>:etl_parent_flow</code> specifies the entry point of our flow and the <code>-n</code> refers to the name that we want to give our deployment.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/4b08c794-111e-4239-bd6c-045dd0d9a18c-1-088d9bff-e12e-48ea-b58b-705099ace341.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">deployment_yaml.PNG</figcaption>
</figure>
</div>
<p>Now we can see it created a yaml file with all our details. This is the metadata. The default file did not include any parameters but I have amended the file to include :</p>
<pre><code>{ "color": "yellow", "months" :[1, 2, 3], "year": 2021}

###
### A complete description of a Prefect Deployment for flow 'etl-parent-flow'
###
name: Parameterized ETL
description: null
version: 13fb131389eead1a3008d65d3170ceb0
# The work queue that will handle this deployment's runs
work_queue_name: default
work_pool_name: null
tags: []
parameters: { "color": "yellow", "months" :[1, 2, 3], "year": 2021} # default file did not include any parameters
schedule: null
is_schedule_active: null
infra_overrides: {}
infrastructure:
  type: process
  env: {}
  labels: {}
  name: null
  command: null
  stream_output: true
  working_dir: null
  block_type_slug: process
  _block_type_slug: process

###
### DO NOT EDIT BELOW THIS LINE
###
flow_name: etl-parent-flow
manifest_path: null
storage: null
path: /home/stephen137/data-engineering-zoomcamp/prefect-zoomcamp/flows/03_deployments
entrypoint: parameterized_flow.py:etl_parent_flow
parameter_openapi_schema:
  title: Parameters
  type: object
  properties:
months:
  title: months
  default:
  - 1
  - 2
  position: 0
  type: array
  items:
    type: integer
year:
  title: year
  default: 2021
  position: 1
  type: integer
color:
  title: color
  default: yellow
  position: 2
  type: string
  required: null
  definitions: null
timestamp: '2023-03-14T09:33:56.112462+00:00'</code></pre>
<p>Now we need to apply the deployment by running the yaml file:</p>
<pre><code>prefect deployment apply etl_parent_flow-deployment.yaml</code></pre>
<p>This sends all the metadata over to the Prefect API.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/0ef005ed-411d-460d-8b04-3dbbbc40f0e0-1-626ea313-4c24-4704-9471-372419066515.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">deployment_apply.PNG</figcaption>
</figure>
</div>
<p>From the Prefect UI we can see the deployment, trigger a flow run, change parameters etc. Let’s go ahead and trigger a quick run :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-1-3bac0f28-73cc-40f0-ab3b-c086727ead77.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_deployment.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-3-85c9b17b-baa2-483c-a8b4-77b9f0e9abfd.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_scheduled_run.PNG</figcaption>
</figure>
</div>
<p>So, our run is scheduled but is <code>queued up</code>. If we go now to the Prefect Orion UI <code>Work Queues</code> we can see we have <code>1 late run</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-4-8f3ff4a7-2a7d-46dd-920f-44709db90711.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">work_queue_late_run.PNG</figcaption>
</figure>
</div>
<p>We need to have an agent living in our execution environment (local) to trigger it. When we deployed from the terminal the following message was generated :</p>
<pre><code>To execute flow runs from this deployment, start an agent that pulls work from the 'default' work queue:
$ prefect agent start -q 'default'</code></pre>
<p>So let’s now go ahead and start our agent:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-5-b32b94a0-57ff-450b-ae27-e413f3f329a5.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">agent_1.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-6-bf60ecdc-fe58-4eb2-8e58-8cdb2e9565d3.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">agent_2.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-2-66a8d183-21d1-4e02-af87-5aedcf88e8af.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">agent_3.PNG</figcaption>
</figure>
</div>
<p>Our run has completed successfully and this is confirmed within the Prefect Orion UI:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/88c2c8ea-94dd-4d1e-9972-9c978eed79b5-7-c2fa7890-1076-4252-a77d-7a530174fece.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">deployment_complete.PNG</figcaption>
</figure>
</div>
<p>So this completed successfully however it is good practice to make use of <code>Notifications</code>. We can customise these from the UI or build these into our flows using Prefect blocks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/293536ef-7a58-4eb9-b5be-a96b3bf5bd13-1-fccc31cf-4035-431c-bfa9-b1f9869adb6d.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">notifications.PNG</figcaption>
</figure>
</div>
</section>
<section id="schedules-and-docker-storage-with-infrastructure" class="level3">
<h3 class="anchored" data-anchor-id="schedules-and-docker-storage-with-infrastructure">2.2.6 Schedules and Docker Storage with Infrastructure</h3>
<p>Let’s now have a look at <code>scheduling</code> our flows and running our flows in containers in Docker. First let’s look at scheduling. We have a deployment that we ran in the previous section - we have various scheduling options which we can configure via the Prefect UI :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/8ad4c488-b10f-47f9-b798-fe5e9bb47eba-1-fe1359b9-5b40-4870-b1b1-f5a4f89cd4e3.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_scheduler.PNG</figcaption>
</figure>
</div>
<p>The <code>Interval</code> scheduler is fairly self-explanatory, for example every five minutes. Another option is something called <a href="https://en.wikipedia.org/wiki/Cron"><code>Cron</code></a> which allows us to configure period runs on a given schedule based on the following syntax :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/90a5c5e7-1921-41ba-b90c-7c39bea95f42-3-54b67c5f-defe-4315-bccf-7d519b7a4526.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">crontab.PNG</figcaption>
</figure>
</div>
<p>For example, using our Prefect deployment, we can schedule for every minute on day 3 of the month:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/90a5c5e7-1921-41ba-b90c-7c39bea95f42-2-4c6d1619-19f8-4e3b-94f5-3710bdddd05e.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_cron.PNG</figcaption>
</figure>
</div>
<p>The <code>RRule (recurring rule)</code> feature is more complex and is not currently available from within the Prefect UI, but we can configure these from the command line at the deployment stage :</p>
<pre><code>prefect deployment build ./parameterized_flow.py:etl_parent_flow -n "ETL2" --cron "0 0 * * *" -a</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/90a5c5e7-1921-41ba-b90c-7c39bea95f42-1-3982faf3-e3f8-4b7f-aebe-ef3082cfccfd.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">schedule_ETL2.PNG</figcaption>
</figure>
</div>
<p>We can see this has been scheduled within the Prefect UI to run every day at 12:00am UTC :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/90a5c5e7-1921-41ba-b90c-7c39bea95f42-4-605d5aa2-89d4-4f69-aa37-f12f6a96be7e.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_schedule_ETL2.PNG</figcaption>
</figure>
</div>
<p>We can find out more using :</p>
<pre><code>prefect deployment build --help</code></pre>
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/5283c2ba-f183-4f4b-a3f6-ca02333eb0e1-1-b013f32d-acc5-489f-a9ca-5a011b199afd.PNG" class="img-fluid" alt="prefect_build_deploy_help_1.PNG"> <img src="DE_Zoomcamp_Week_2_files/figure-html/5283c2ba-f183-4f4b-a3f6-ca02333eb0e1-2-d1fbb68e-f149-4f77-99ee-41893ee5e31b.PNG" class="img-fluid" alt="prefect_build_deploy_help_2.PNG"></p>
<p>So far we have had our flow code sitting locally on our machine, but if we want to make things a little more production ready and enable other people to access our flow code, we could put our code on <a href="https://github.com/">GitHub</a>, <a href="https://bitbucket.org/product">BitBucket</a>, <a href="https://about.gitlab.com/">GitLab</a>, <a href="https://aws.amazon.com/pm/serv-s3/?trk=518a7bef-5b4f-4462-ad55-80e5c177f12b&amp;sc_channel=ps&amp;s_kwcid=AL!4422!3!645186213301!e!!g!!aws%20s3%20cloud%20storage&amp;ef_id=%7Bgclid%7D:G:s&amp;s_kwcid=AL!4422!3!645186213301!e!!g!!aws%20s3%20cloud%20storage">AWS S3</a>, <a href="https://azure.microsoft.com/en-us/free/search/?&amp;ef_id=%7Bgclid%7D:G:s&amp;OCID=AIDcmm4rphvbww_SEM_%7Bgclid%7D:G:s">MS Azure</a> - any of those version control systems.</p>
<p>Let’s now look at how to store our flow in a <code>Docker</code> image on <a href="https://hub.docker.com/">DockerHub</a> and then when we run a docker container our code will be right there - we will be baking it into the image.</p>
<p>The first thing we need to do is create a Dockerfile :</p>
<pre><code>FROM prefecthq/prefect:2.7.7-python3.9

COPY docker-requirements.txt .

RUN pip install -r docker-requirements.txt --trusted-host pypi.python.org --no-cache-dir

COPY flows /opt/prefect/flows  
RUN mkdir -p /opt/prefect/data/yellow</code></pre>
<p>And we also need to create a requirements file :</p>
<pre><code>pandas==1.5.2
prefect-gcp[cloud_storage]==0.2.4
protobuf==4.21.11
pyarrow==10.0.1
pandas-gbq==0.18.1</code></pre>
<p>Let’s now build our docker image from the command line :</p>
<pre><code>docker image build -t stephen137/prefect:zoomcamp .</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/5263050e-5478-4dce-aba3-435fadae9f9f-1-2a5e85f0-cd8f-4295-85cd-06eb73e53797.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_schedule.PNG</figcaption>
</figure>
</div>
<p>Now we want to push the image to dockerhub :</p>
<pre><code>docker image push stephen137/prefect:zoomcamp</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/94b945b0-34cf-4414-bc42-f2d4845108fa-1-a3d56968-b4f2-4b2e-85f3-572a02216787.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_schedule_image_push.PNG</figcaption>
</figure>
</div>
<p>That was successful and we can see our image within the dockerhub UI :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/94b945b0-34cf-4414-bc42-f2d4845108fa-2-c5207dbe-97c9-473d-b574-cd055cf8ebcc.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">dockerhub.PNG</figcaption>
</figure>
</div>
<p>It is possible to create a docker block using code - but let’s for now just use:</p>
<pre><code>pip install prefect-docker</code></pre>
<p>and then register the block :</p>
<pre><code>prefect block register -m prefect_docker</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/cf85e52f-114f-41db-bbf3-0b46a1614262-2-af742231-b2c9-4a11-b080-46f755eb43a1.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_block.PNG</figcaption>
</figure>
</div>
<p>And then we can do some basic block config (image name being most important - as previously specified at docker build stage) within the Prefect UI :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/cf85e52f-114f-41db-bbf3-0b46a1614262-3-b489ecb0-d609-4bb8-b692-d5d679fb8115.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_docker_container_block.PNG</figcaption>
</figure>
</div>
<p>And the block code that we can add to our flow is :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/cf85e52f-114f-41db-bbf3-0b46a1614262-1-a32a2015-fbc8-4cfe-95e0-fc045dc5c8a5.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_container_block_code.PNG</figcaption>
</figure>
</div>
<p>Let’s now create our deployment from a python file (we did this previously from the command line) and include the block code above :</p>
<pre><code>from prefect.deployments import Deployment
from parameterized_flow import etl_parent_flow
from prefect.infrastructure.docker import DockerContainer

docker_block = DockerContainer.load("de-zoomcamp") # as specified on creation of block

docker_dep = Deployment.build_from_flow(
    flow=etl_parent_flow,
    name="docker-flow",
    infrastructure=docker_block,
)


if __name__ == "__main__":
    docker_dep.apply()</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/5a62de08-f53e-4e1c-a47d-38bb9e399970-2-d9574725-fa69-4154-a9d6-54eb0f10498c.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_deploy.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/5a62de08-f53e-4e1c-a47d-38bb9e399970-1-69270363-4c13-4504-abb7-25266b50c69f.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_flow_deployment.PNG</figcaption>
</figure>
</div>
<p>Before we run this, let’s just take a look at something :</p>
<pre><code>prefect profile ls</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/23e3a8ed-d2b4-405f-b194-e0de8e4b86ff-1-47ef7084-0df8-4f37-9156-ac16525a99cd.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_profile.PNG</figcaption>
</figure>
</div>
<p>So we are using default at the moment, but let’s configure to the <a href="https://docs.prefect.io/concepts/settings/">Prefect API url</a> :</p>
<pre><code>prefect config set PREFECT_API_URL="http://127.0.0.1:4200/api"</code></pre>
<p>This will ensure our docker container is able to interface with the Prefect Orion server.</p>
<p>OK, so we are almost ready to start our flow. Let’s fire up an agent :</p>
<pre><code>prefect agent start -q default</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/aa27e615-f5b4-4adf-9d27-d4c78b1b1f43-2-4ebdde0f-772f-4988-8dbb-86af83de73f3.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">prefect_agent.PNG</figcaption>
</figure>
</div>
<p>And as confirmed from the Orion UI :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/aa27e615-f5b4-4adf-9d27-d4c78b1b1f43-1-19d12390-3143-454d-b590-5c38faddd7ce.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">orion_agent_start.PNG</figcaption>
</figure>
</div>
<p>And let’s now run our flow from the command line, and override the default parameter :</p>
<pre><code>prefect deployment run etl-parent-flow/docker-flow -p "months=[1,2]"</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/55504c5c-8618-426f-9003-0b5a713c4bf7-1-63b0adbf-1fcc-4010-a602-05bab48f9236.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_prefect_flow.PNG</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/22dff54f-12d9-47c0-9a8a-40546da201d3-1-5f4f9748-d28e-4697-84ab-48e035750bd5.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_flow_crash.PNG</figcaption>
</figure>
</div>
<p>According to the course <code>FAQs</code> this error occurs because we are running Prefect locally on our machine at localhost:4200, when we run docker without specifying their network, Docker calls the localhost:4200 inside the container but not the localhost:4200 on our machine.</p>
<p>One suggested solution is to set the network Mode to <code>bridge</code> although that didn’t work for me.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="DE_Zoomcamp_Week_2_files/figure-html/0825ba4d-28ec-4c0e-b2ae-7e171294b250-1-e2306166-18aa-41d4-b069-5601cfa5a88c.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">docker_container_bridge.PNG</figcaption>
</figure>
</div>
<p>Hopefully I can find a solution to this as I progress with the course, but let’s push on.</p>
</section>
<section id="prefect-cloud---additional-resources" class="level3">
<h3 class="anchored" data-anchor-id="prefect-cloud---additional-resources">2.2.7 Prefect Cloud - Additional Resources</h3>
<p>Further resource in support of the workflow orchestrations we have covered is included in the <a href="https://docs.prefect.io/">Prefect docs</a> which cover the core concepts for <code>flows</code>, <code>tasks</code> and <code>task runners</code>. It also includes getting started tutorials as well as a <code>recipes</code> which are common, extensible examples for setting up Prefect in your execution environment with ready-made ingredients such as Dockerfiles, Terraform files, and GitHub Actions.</p>
<p>Prefect also offers a <a href="https://docs.prefect.io/ui/overview/">hosted cloud</a> solution, instead of having to host the Orion cloud UI yourself, and the cloud version includes some additional features such as <code>Automations</code> and workspaces which can be used collaboratively.</p>
<p>For technical discussions you can visit the <a href="https://discourse.prefect.io/">Prefect Discourse</a> and the <a href="https://github.com/PrefectHQ/prefect">Prefect Github</a>. For blogs and guides it is well worth checking out <a href="https://github.com/anna-geller">Anna Geller’s Github</a>.</p>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<p>We covered a lot this week! We started off by creating a simple python script that ingested a csv file into a database (<code>postgreSQL</code>).</p>
<p>We then showed how to streamline our data pipeline by leveraging the <code>Prefect</code> workflow orchestration tool. We created an ETL flow which:</p>
<ul>
<li>grabbed a csv file from the web (1.4 million rows)</li>
<li>carried out some preprocessing, converted to a DataFrame</li>
<li>uploaded to a data lake [Google Cloud Storage]</li>
<li>moved it to a data warehouse [BigQuery]</li>
</ul>
<p>In week 3 we will be digging deeper into Biq Query, and taking a look at partitioning, clustering and best practice. We also learn about Machine Learning techniques using Big Query.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>