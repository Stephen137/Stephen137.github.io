<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stephen Barrie">
<meta name="dcterms.date" content="2024-01-31">

<title>Into the Unknown - Fraud Detection using Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Stephen Barrie</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fraud Detection using Python</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Imbalanced data</div>
                <div class="quarto-category">SMOTE</div>
                <div class="quarto-category">K-means</div>
                <div class="quarto-category">Text mining</div>
                <div class="quarto-category">LDA</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stephen Barrie </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 31, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#fraud-detection-in-python" id="toc-fraud-detection-in-python" class="nav-link active" data-scroll-target="#fraud-detection-in-python">Fraud Detection in Python</a>
  <ul class="collapse">
  <li><a href="#introduction-and-preparing-your-data" id="toc-introduction-and-preparing-your-data" class="nav-link" data-scroll-target="#introduction-and-preparing-your-data">1. Introduction and preparing your data</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">1.1 Dataset</a></li>
  <li><a href="#libraries-packages" id="toc-libraries-packages" class="nav-link" data-scroll-target="#libraries-packages">Libraries &amp; packages</a></li>
  <li><a href="#checking-the-fraud-to-non-fraud-ratio" id="toc-checking-the-fraud-to-non-fraud-ratio" class="nav-link" data-scroll-target="#checking-the-fraud-to-non-fraud-ratio">1.1.1 Checking the fraud to non-fraud ratio</a></li>
  <li><a href="#plotting-our-data" id="toc-plotting-our-data" class="nav-link" data-scroll-target="#plotting-our-data">1.1.2 Plotting our data</a></li>
  <li><a href="#increasing-successful-detections-using-data-resampling" id="toc-increasing-successful-detections-using-data-resampling" class="nav-link" data-scroll-target="#increasing-successful-detections-using-data-resampling">1.2 Increasing successful detections using data resampling</a></li>
  <li><a href="#applying-smote" id="toc-applying-smote" class="nav-link" data-scroll-target="#applying-smote">1.2.1 Applying SMOTE</a></li>
  <li><a href="#compare-smote-to-original-data" id="toc-compare-smote-to-original-data" class="nav-link" data-scroll-target="#compare-smote-to-original-data">1.2.2 Compare SMOTE to original data</a></li>
  <li><a href="#fraud-detection-algorithms-in-action" id="toc-fraud-detection-algorithms-in-action" class="nav-link" data-scroll-target="#fraud-detection-algorithms-in-action">1.3 Fraud detection algorithms in action</a></li>
  <li><a href="#exploring-the-traditional-way-to-catch-fraud" id="toc-exploring-the-traditional-way-to-catch-fraud" class="nav-link" data-scroll-target="#exploring-the-traditional-way-to-catch-fraud">1.3.1 Exploring the traditional way to catch fraud</a></li>
  <li><a href="#using-ml-classification-to-catch-fraud" id="toc-using-ml-classification-to-catch-fraud" class="nav-link" data-scroll-target="#using-ml-classification-to-catch-fraud">1.3.2 Using ML classification to catch fraud</a></li>
  <li><a href="#logistic-regression-combined-with-smote" id="toc-logistic-regression-combined-with-smote" class="nav-link" data-scroll-target="#logistic-regression-combined-with-smote">1.3.3 Logistic regression combined with SMOTE</a></li>
  <li><a href="#using-a-pipeline" id="toc-using-a-pipeline" class="nav-link" data-scroll-target="#using-a-pipeline">1.4 Using a pipeline</a></li>
  </ul></li>
  <li><a href="#fraud-detection-using-labeled-data" id="toc-fraud-detection-using-labeled-data" class="nav-link" data-scroll-target="#fraud-detection-using-labeled-data">2 Fraud detection using labeled data</a>
  <ul class="collapse">
  <li><a href="#review-of-classification-methods" id="toc-review-of-classification-methods" class="nav-link" data-scroll-target="#review-of-classification-methods">2.1 Review of classification methods</a></li>
  <li><a href="#natural-hit-rate" id="toc-natural-hit-rate" class="nav-link" data-scroll-target="#natural-hit-rate">2.1.1 Natural hit rate</a></li>
  <li><a href="#random-forest-classifier" id="toc-random-forest-classifier" class="nav-link" data-scroll-target="#random-forest-classifier">2.1.2 Random Forest Classifier</a></li>
  <li><a href="#performance-evaluation" id="toc-performance-evaluation" class="nav-link" data-scroll-target="#performance-evaluation">2.2 Performance Evaluation</a></li>
  <li><a href="#performance-metrics-for-the-rf-model" id="toc-performance-metrics-for-the-rf-model" class="nav-link" data-scroll-target="#performance-metrics-for-the-rf-model">2.2.1 Performance metrics for the RF model</a></li>
  <li><a href="#plotting-the-precision-recall-curve" id="toc-plotting-the-precision-recall-curve" class="nav-link" data-scroll-target="#plotting-the-precision-recall-curve">2.2.2 Plotting the Precision Recall Curve</a></li>
  <li><a href="#adjusting-your-algorithm-weights" id="toc-adjusting-your-algorithm-weights" class="nav-link" data-scroll-target="#adjusting-your-algorithm-weights">2.3 Adjusting your algorithm weights</a></li>
  <li><a href="#model-adjustments" id="toc-model-adjustments" class="nav-link" data-scroll-target="#model-adjustments">2.3.1 Model adjustments</a></li>
  <li><a href="#adjusting-our-random-forest-to-fraud-detection" id="toc-adjusting-our-random-forest-to-fraud-detection" class="nav-link" data-scroll-target="#adjusting-our-random-forest-to-fraud-detection">2.3.2 Adjusting our Random Forest to fraud detection</a></li>
  <li><a href="#gridsearchcv-to-find-optimal-parameters" id="toc-gridsearchcv-to-find-optimal-parameters" class="nav-link" data-scroll-target="#gridsearchcv-to-find-optimal-parameters">2.3.3 GridSearchCV to find optimal parameters</a></li>
  <li><a href="#model-results-using-gridsearchcv" id="toc-model-results-using-gridsearchcv" class="nav-link" data-scroll-target="#model-results-using-gridsearchcv">2.3.4 Model results using GridSearchCV</a></li>
  <li><a href="#ensemble-methods" id="toc-ensemble-methods" class="nav-link" data-scroll-target="#ensemble-methods">2.4 Ensemble methods</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">2.4.1 Logistic Regression</a></li>
  <li><a href="#voting-classifier-1" id="toc-voting-classifier-1" class="nav-link" data-scroll-target="#voting-classifier-1">2.4.2 Voting Classifier</a></li>
  <li><a href="#adjust-weights-within-the-voting-classifier" id="toc-adjust-weights-within-the-voting-classifier" class="nav-link" data-scroll-target="#adjust-weights-within-the-voting-classifier">2.4.3 Adjust weights within the Voting Classifier</a></li>
  </ul></li>
  <li><a href="#fraud-detection-using-unlabeled-data" id="toc-fraud-detection-using-unlabeled-data" class="nav-link" data-scroll-target="#fraud-detection-using-unlabeled-data">3. Fraud detection using unlabeled data</a>
  <ul class="collapse">
  <li><a href="#normal-versus-abnormal-behaviour" id="toc-normal-versus-abnormal-behaviour" class="nav-link" data-scroll-target="#normal-versus-abnormal-behaviour">3.1 Normal versus abnormal behaviour</a></li>
  <li><a href="#exploring-your-data" id="toc-exploring-your-data" class="nav-link" data-scroll-target="#exploring-your-data">3.1.1 Exploring your data</a></li>
  <li><a href="#customer-segmentation" id="toc-customer-segmentation" class="nav-link" data-scroll-target="#customer-segmentation">3.1.2 Customer segmentation</a></li>
  <li><a href="#using-statistics-to-define-normal-behavior" id="toc-using-statistics-to-define-normal-behavior" class="nav-link" data-scroll-target="#using-statistics-to-define-normal-behavior">3.1.3 Using statistics to define normal behavior</a></li>
  <li><a href="#clustering-methods-to-detect-fraud" id="toc-clustering-methods-to-detect-fraud" class="nav-link" data-scroll-target="#clustering-methods-to-detect-fraud">3.2 Clustering methods to detect fraud</a></li>
  <li><a href="#scaling-the-data" id="toc-scaling-the-data" class="nav-link" data-scroll-target="#scaling-the-data">3.2.1 Scaling the data</a></li>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering">3.2.2 K-means clustering</a></li>
  <li><a href="#elbow-method" id="toc-elbow-method" class="nav-link" data-scroll-target="#elbow-method">3.2.3 Elbow method</a></li>
  <li><a href="#assigning-fraud-versus-non-fraud" id="toc-assigning-fraud-versus-non-fraud" class="nav-link" data-scroll-target="#assigning-fraud-versus-non-fraud">3.3 Assigning fraud versus non-fraud</a></li>
  <li><a href="#detecting-outliers" id="toc-detecting-outliers" class="nav-link" data-scroll-target="#detecting-outliers">3.3.1 Detecting outliers</a></li>
  <li><a href="#other-clustering-fraud-detection-methods" id="toc-other-clustering-fraud-detection-methods" class="nav-link" data-scroll-target="#other-clustering-fraud-detection-methods">3.4 Other clustering fraud detection methods</a></li>
  <li><a href="#dbscan" id="toc-dbscan" class="nav-link" data-scroll-target="#dbscan">3.4.1 DBSCAN</a></li>
  <li><a href="#assessing-smallest-clusters" id="toc-assessing-smallest-clusters" class="nav-link" data-scroll-target="#assessing-smallest-clusters">3.4.2 Assessing smallest clusters</a></li>
  <li><a href="#checking-results" id="toc-checking-results" class="nav-link" data-scroll-target="#checking-results">3.4.3 Checking results</a></li>
  </ul></li>
  <li><a href="#fraud-detection-using-text" id="toc-fraud-detection-using-text" class="nav-link" data-scroll-target="#fraud-detection-using-text">4. Fraud detection using text</a>
  <ul class="collapse">
  <li><a href="#using-text-data" id="toc-using-text-data" class="nav-link" data-scroll-target="#using-text-data">4.1 Using text data</a></li>
  <li><a href="#word-search-with-dataframes" id="toc-word-search-with-dataframes" class="nav-link" data-scroll-target="#word-search-with-dataframes">4.1.1 Word search with dataframes</a></li>
  <li><a href="#using-list-of-terms" id="toc-using-list-of-terms" class="nav-link" data-scroll-target="#using-list-of-terms">4.1.2 Using list of terms</a></li>
  <li><a href="#creating-a-flag" id="toc-creating-a-flag" class="nav-link" data-scroll-target="#creating-a-flag">4.1.3 Creating a flag</a></li>
  <li><a href="#text-mining-to-detect-fraud" id="toc-text-mining-to-detect-fraud" class="nav-link" data-scroll-target="#text-mining-to-detect-fraud">4.2 Text mining to detect fraud</a></li>
  <li><a href="#removing-stopwords" id="toc-removing-stopwords" class="nav-link" data-scroll-target="#removing-stopwords">4.2.1 Removing stopwords</a></li>
  <li><a href="#topic-modeling-on-fraud" id="toc-topic-modeling-on-fraud" class="nav-link" data-scroll-target="#topic-modeling-on-fraud">4.3 Topic modeling on fraud</a></li>
  <li><a href="#create-dictionary-and-corpus" id="toc-create-dictionary-and-corpus" class="nav-link" data-scroll-target="#create-dictionary-and-corpus">4.3.1 Create dictionary and corpus</a></li>
  <li><a href="#lda-model" id="toc-lda-model" class="nav-link" data-scroll-target="#lda-model">4.3.2 LDA model</a></li>
  <li><a href="#flagging-fraud-based-on-topics" id="toc-flagging-fraud-based-on-topics" class="nav-link" data-scroll-target="#flagging-fraud-based-on-topics">4.4 Flagging fraud based on topics</a></li>
  <li><a href="#finding-fraudsters-based-on-topic" id="toc-finding-fraudsters-based-on-topic" class="nav-link" data-scroll-target="#finding-fraudsters-based-on-topic">4.4.1 Finding fraudsters based on topic</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a>
  <ul class="collapse">
  <li><a href="#working-with-imbalanced-data" id="toc-working-with-imbalanced-data" class="nav-link" data-scroll-target="#working-with-imbalanced-data">Working with imbalanced data</a></li>
  <li><a href="#fraud-detection-with-labeled-data" id="toc-fraud-detection-with-labeled-data" class="nav-link" data-scroll-target="#fraud-detection-with-labeled-data">Fraud detection with labeled data</a></li>
  <li><a href="#fraud-detection-without-labels-1" id="toc-fraud-detection-without-labels-1" class="nav-link" data-scroll-target="#fraud-detection-without-labels-1">Fraud detection without labels</a></li>
  <li><a href="#text-mining-for-fraud-detection" id="toc-text-mining-for-fraud-detection" class="nav-link" data-scroll-target="#text-mining-for-fraud-detection">Text mining for fraud detection</a></li>
  <li><a href="#further-learning-for-fraud-detection" id="toc-further-learning-for-fraud-detection" class="nav-link" data-scroll-target="#further-learning-for-fraud-detection">Further learning for fraud detection</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="fraud-detection-in-python" class="level1">
<h1>Fraud Detection in Python</h1>
<section id="introduction-and-preparing-your-data" class="level2">
<h2 class="anchored" data-anchor-id="introduction-and-preparing-your-data">1. Introduction and preparing your data</h2>
<p>In this section, we’ll learn about the typical challenges associated with fraud detection, and will learn how to resample our data in a smart way, to tackle problems with imbalanced data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/d5d496f9-392c-485f-8f16-32065b7e73bb-1-f26c1703-fe66-4c9b-8a64-3a4ed77f856d.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">how_deal_fraud.JPG</figcaption>
</figure>
</div>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">1.1 Dataset</h3>
<p>The original <a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud">dataset</a> contains transactions made by credit cards over two days in September 2013 by European cardholders. The dataset is highly unbalanced.</p>
<p>Machine Learning algorithms usually work best when the different classes contained in the dataset are more or less equally present. If there are few cases of fraud, then there’s little data to learn how to identify them. This is known as class imbalance, and it’s one of the main challenges of fraud detection.</p>
<p>Let’s explore this dataset, and observe this class imbalance problem.</p>
</section>
<section id="libraries-packages" class="level3">
<h3 class="anchored" data-anchor-id="libraries-packages">Libraries &amp; packages</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># data wrangling / viz</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># stats / machine learning</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.pipeline <span class="im">import</span> Pipeline </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> average_precision_score</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> VotingClassifier</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.cluster <span class="im">import</span> homogeneity_score</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.cluster <span class="im">import</span> silhouette_score</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># text mining</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install nltk</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co">#!pip install pyLDAvis</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="checking-the-fraud-to-non-fraud-ratio" class="level3">
<h3 class="anchored" data-anchor-id="checking-the-fraud-to-non-fraud-ratio">1.1.1 Checking the fraud to non-fraud ratio</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/creditcard.csv'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Time</th>
<th data-quarto-table-cell-role="th">V1</th>
<th data-quarto-table-cell-role="th">V2</th>
<th data-quarto-table-cell-role="th">V3</th>
<th data-quarto-table-cell-role="th">V4</th>
<th data-quarto-table-cell-role="th">V5</th>
<th data-quarto-table-cell-role="th">V6</th>
<th data-quarto-table-cell-role="th">V7</th>
<th data-quarto-table-cell-role="th">V8</th>
<th data-quarto-table-cell-role="th">V9</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">V21</th>
<th data-quarto-table-cell-role="th">V22</th>
<th data-quarto-table-cell-role="th">V23</th>
<th data-quarto-table-cell-role="th">V24</th>
<th data-quarto-table-cell-role="th">V25</th>
<th data-quarto-table-cell-role="th">V26</th>
<th data-quarto-table-cell-role="th">V27</th>
<th data-quarto-table-cell-role="th">V28</th>
<th data-quarto-table-cell-role="th">Amount</th>
<th data-quarto-table-cell-role="th">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.0</td>
<td>-1.359807</td>
<td>-0.072781</td>
<td>2.536347</td>
<td>1.378155</td>
<td>-0.338321</td>
<td>0.462388</td>
<td>0.239599</td>
<td>0.098698</td>
<td>0.363787</td>
<td>...</td>
<td>-0.018307</td>
<td>0.277838</td>
<td>-0.110474</td>
<td>0.066928</td>
<td>0.128539</td>
<td>-0.189115</td>
<td>0.133558</td>
<td>-0.021053</td>
<td>149.62</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.0</td>
<td>1.191857</td>
<td>0.266151</td>
<td>0.166480</td>
<td>0.448154</td>
<td>0.060018</td>
<td>-0.082361</td>
<td>-0.078803</td>
<td>0.085102</td>
<td>-0.255425</td>
<td>...</td>
<td>-0.225775</td>
<td>-0.638672</td>
<td>0.101288</td>
<td>-0.339846</td>
<td>0.167170</td>
<td>0.125895</td>
<td>-0.008983</td>
<td>0.014724</td>
<td>2.69</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1.0</td>
<td>-1.358354</td>
<td>-1.340163</td>
<td>1.773209</td>
<td>0.379780</td>
<td>-0.503198</td>
<td>1.800499</td>
<td>0.791461</td>
<td>0.247676</td>
<td>-1.514654</td>
<td>...</td>
<td>0.247998</td>
<td>0.771679</td>
<td>0.909412</td>
<td>-0.689281</td>
<td>-0.327642</td>
<td>-0.139097</td>
<td>-0.055353</td>
<td>-0.059752</td>
<td>378.66</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.0</td>
<td>-0.966272</td>
<td>-0.185226</td>
<td>1.792993</td>
<td>-0.863291</td>
<td>-0.010309</td>
<td>1.247203</td>
<td>0.237609</td>
<td>0.377436</td>
<td>-1.387024</td>
<td>...</td>
<td>-0.108300</td>
<td>0.005274</td>
<td>-0.190321</td>
<td>-1.175575</td>
<td>0.647376</td>
<td>-0.221929</td>
<td>0.062723</td>
<td>0.061458</td>
<td>123.50</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2.0</td>
<td>-1.158233</td>
<td>0.877737</td>
<td>1.548718</td>
<td>0.403034</td>
<td>-0.407193</td>
<td>0.095921</td>
<td>0.592941</td>
<td>-0.270533</td>
<td>0.817739</td>
<td>...</td>
<td>-0.009431</td>
<td>0.798278</td>
<td>-0.137458</td>
<td>0.141267</td>
<td>-0.206010</td>
<td>0.502292</td>
<td>0.219422</td>
<td>0.215153</td>
<td>69.99</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">284802</td>
<td>172786.0</td>
<td>-11.881118</td>
<td>10.071785</td>
<td>-9.834783</td>
<td>-2.066656</td>
<td>-5.364473</td>
<td>-2.606837</td>
<td>-4.918215</td>
<td>7.305334</td>
<td>1.914428</td>
<td>...</td>
<td>0.213454</td>
<td>0.111864</td>
<td>1.014480</td>
<td>-0.509348</td>
<td>1.436807</td>
<td>0.250034</td>
<td>0.943651</td>
<td>0.823731</td>
<td>0.77</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">284803</td>
<td>172787.0</td>
<td>-0.732789</td>
<td>-0.055080</td>
<td>2.035030</td>
<td>-0.738589</td>
<td>0.868229</td>
<td>1.058415</td>
<td>0.024330</td>
<td>0.294869</td>
<td>0.584800</td>
<td>...</td>
<td>0.214205</td>
<td>0.924384</td>
<td>0.012463</td>
<td>-1.016226</td>
<td>-0.606624</td>
<td>-0.395255</td>
<td>0.068472</td>
<td>-0.053527</td>
<td>24.79</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">284804</td>
<td>172788.0</td>
<td>1.919565</td>
<td>-0.301254</td>
<td>-3.249640</td>
<td>-0.557828</td>
<td>2.630515</td>
<td>3.031260</td>
<td>-0.296827</td>
<td>0.708417</td>
<td>0.432454</td>
<td>...</td>
<td>0.232045</td>
<td>0.578229</td>
<td>-0.037501</td>
<td>0.640134</td>
<td>0.265745</td>
<td>-0.087371</td>
<td>0.004455</td>
<td>-0.026561</td>
<td>67.88</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">284805</td>
<td>172788.0</td>
<td>-0.240440</td>
<td>0.530483</td>
<td>0.702510</td>
<td>0.689799</td>
<td>-0.377961</td>
<td>0.623708</td>
<td>-0.686180</td>
<td>0.679145</td>
<td>0.392087</td>
<td>...</td>
<td>0.265245</td>
<td>0.800049</td>
<td>-0.163298</td>
<td>0.123205</td>
<td>-0.569159</td>
<td>0.546668</td>
<td>0.108821</td>
<td>0.104533</td>
<td>10.00</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">284806</td>
<td>172792.0</td>
<td>-0.533413</td>
<td>-0.189733</td>
<td>0.703337</td>
<td>-0.506271</td>
<td>-0.012546</td>
<td>-0.649617</td>
<td>1.577006</td>
<td>-0.414650</td>
<td>0.486180</td>
<td>...</td>
<td>0.261057</td>
<td>0.643078</td>
<td>0.376777</td>
<td>0.008797</td>
<td>-0.473649</td>
<td>-0.818267</td>
<td>-0.002415</td>
<td>0.013649</td>
<td>217.00</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>284807 rows × 31 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Explore the features available in your dataframe</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.info())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 284807 entries, 0 to 284806
Data columns (total 31 columns):
 #   Column  Non-Null Count   Dtype  
---  ------  --------------   -----  
 0   Time    284807 non-null  float64
 1   V1      284807 non-null  float64
 2   V2      284807 non-null  float64
 3   V3      284807 non-null  float64
 4   V4      284807 non-null  float64
 5   V5      284807 non-null  float64
 6   V6      284807 non-null  float64
 7   V7      284807 non-null  float64
 8   V8      284807 non-null  float64
 9   V9      284807 non-null  float64
 10  V10     284807 non-null  float64
 11  V11     284807 non-null  float64
 12  V12     284807 non-null  float64
 13  V13     284807 non-null  float64
 14  V14     284807 non-null  float64
 15  V15     284807 non-null  float64
 16  V16     284807 non-null  float64
 17  V17     284807 non-null  float64
 18  V18     284807 non-null  float64
 19  V19     284807 non-null  float64
 20  V20     284807 non-null  float64
 21  V21     284807 non-null  float64
 22  V22     284807 non-null  float64
 23  V23     284807 non-null  float64
 24  V24     284807 non-null  float64
 25  V25     284807 non-null  float64
 26  V26     284807 non-null  float64
 27  V27     284807 non-null  float64
 28  V28     284807 non-null  float64
 29  Amount  284807 non-null  float64
 30  Class   284807 non-null  int64  
dtypes: float64(30), int64(1)
memory usage: 67.4 MB
None</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the occurrences of fraud and no fraud and print them</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>occ <span class="op">=</span> df[<span class="st">'Class'</span>].value_counts()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(occ)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Class
0    284315
1       492
Name: count, dtype: int64</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the ratio of fraud cases</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(occ <span class="op">/</span> <span class="bu">len</span>(df))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Class
0    0.998273
1    0.001727
Name: count, dtype: float64</code></pre>
</div>
</div>
<p>As we can see, the proportion of fraudulent transactions (denoted by 1) is very low. We will learn how to deal with this in the next section.</p>
</section>
<section id="plotting-our-data" class="level3">
<h3 class="anchored" data-anchor-id="plotting-our-data">1.1.2 Plotting our data</h3>
<p>Let’s look at the data and visualize the fraud to non-fraud ratio. It is always a good starting point in your fraud analysis, to look at your data first, before you make any changes to it. Moreover, when talking to your colleagues, a picture often makes it very clear that we’re dealing with heavily imbalanced data. Let’s create a plot to visualize the ratio fraud to non-fraud data points on the dataset.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prep_data(df):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df.iloc[:, <span class="dv">1</span>:<span class="dv">29</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array(X).astype(np.float64)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> df.iloc[:, <span class="dv">29</span>]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>np.array(y).astype(np.float64)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X,y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to create a scatter plot of our data and labels</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_data(X, y):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Class #0"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.15</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Class #1"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, linewidth<span class="op">=</span><span class="fl">0.15</span>, c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X and y from the prep_data function </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>X_original, y_original <span class="op">=</span> prep_data(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot our data by running our plot data function on X and y</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plot_data(X_original, y_original)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>By visualizing our data you can immediately see how scattered our fraud cases are, and how few cases we have. A picture often makes the imbalance problem very clear. In the next sections we’ll visually explore how to improve our fraud to non-fraud balance.</p>
</section>
<section id="increasing-successful-detections-using-data-resampling" class="level3">
<h3 class="anchored" data-anchor-id="increasing-successful-detections-using-data-resampling">1.2 Increasing successful detections using data resampling</h3>
<section id="undersampling" class="level4">
<h4 class="anchored" data-anchor-id="undersampling">Undersampling</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/1755411d-0899-49e5-8df4-c3c7a8b05bcc-2-5c3edb13-8e52-4bd1-9f5b-6d3211393078.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">undersampling.JPG</figcaption>
</figure>
</div>
<p>The most straightforward way to adjust the imbalance of your data, is to undersample the majority class, aka non-fraud cases, or oversample the minority class, aka the fraud cases. With undersampling, you take random draws from your non-fraud observations, to match the amount of fraud observations as seen on the picture.</p>
</section>
<section id="oversampling" class="level4">
<h4 class="anchored" data-anchor-id="oversampling">Oversampling</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/1755411d-0899-49e5-8df4-c3c7a8b05bcc-1-1ddbdc40-c6a6-4126-9cbc-890ecb141221.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">oversampling.JPG</figcaption>
</figure>
</div>
<p>You can implement resampling methods using Python’s imbalanced learn module. It is compatible with scikit-learn and allows you to implement these methods in just two lines of code. As you can see here, I import the package and take the Random Oversampler and assign it to method. I simply fit the method onto my original feature set X, and labels y, to obtained a resampled feature set X, and resampled y. I plot the datasets here side by side, such that you can see the effect of my resampling method. The darker blue color of the data points reflect that there are more identical data points now.</p>
</section>
<section id="synthetic-minority-oversampling-technique-smote" class="level4">
<h4 class="anchored" data-anchor-id="synthetic-minority-oversampling-technique-smote">Synthetic Minority Oversampling Technique (SMOTE)</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/1755411d-0899-49e5-8df4-c3c7a8b05bcc-3-7c8b1c71-f24c-4e3a-9e25-a7ce891aaac0.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">SMOTE.JPG</figcaption>
</figure>
</div>
<p>The Synthetic Minority Oversampling Technique, or SMOTE, is another way of adjusting the imbalance by oversampling your minority observations, aka your fraud cases. But with SMOTE, we’re not just copying the minority class. Instead, as you see in this picture, SMOTE uses characteristics of nearest neighbors of fraud cases to create new synthetic fraud cases, and thereby avoids duplicating observations.</p>
</section>
<section id="which-resampling-method-to-use" class="level4">
<h4 class="anchored" data-anchor-id="which-resampling-method-to-use">Which resampling method to use?</h4>
<p>You might wonder which one of these methods is the best? Well, it depends very much on the situation. If you have very large amounts of data, and also many fraud cases, you might find it computationally easier to undersample, rather than to increase data even more. But in most cases, throwing away data is not desirable. When it comes to oversampling, SMOTE is more sophisticated as it does not duplicate data. But this only works well if your fraud cases are quite similar to each other. If fraud is spread out over your data and not very distinct, using nearest neighbors to create more fraud cases introduces a bit of noise in the data, as the nearest neighbors might not necessarily be fraud cases.</p>
</section>
<section id="when-to-use-resampling-methods" class="level4">
<h4 class="anchored" data-anchor-id="when-to-use-resampling-methods">When to use resampling methods</h4>
<p>One thing to keep in mind when using resampling methods, is to only resample on your training set. Your goal is to better train your model by giving it balanced amounts of data. Your goal is not to predict your synthetic samples. Always make sure your test data is free of duplicates or synthetic data, such that you can test your model on real data only. The way to do this, is to first split the data into a train and test set, as you can see here. I then resample the training set only. I fit my model into the resampled training data, and lastly, I obtain my performance metrics by looking at my original, not resampled, test data. These steps should look familiar to you.</p>
</section>
</section>
<section id="applying-smote" class="level3">
<h3 class="anchored" data-anchor-id="applying-smote">1.2.1 Applying SMOTE</h3>
<p>In the following example , we are going to re-balance our data using the <code>Synthetic Minority Over-sampling Technique (SMOTE)</code>. Unlike ROS, SMOTE does not create exact copies of observations, but creates new, synthetic, samples that are quite similar to the existing observations in the minority class.</p>
<p>SMOTE is therefore slightly more sophisticated than just copying observations, so let’s apply SMOTE to our credit card data.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the Time column</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop(<span class="st">'Time'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the prep_data function</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> prep_data(df)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the resampling method</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>method <span class="op">=</span> SMOTE()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the resampled feature set</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># apply SMOTE to the original X, y </span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>X_resampled, y_resampled <span class="op">=</span> method.fit_resample(X, y)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the resampled data</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plot_data(X_resampled, y_resampled)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As you can see, our minority class is now much more prominently visible in our data. To see the results of SMOTE even better, we’ll compare it to the original data in the next section.</p>
</section>
<section id="compare-smote-to-original-data" class="level3">
<h3 class="anchored" data-anchor-id="compare-smote-to-original-data">1.2.2 Compare SMOTE to original data</h3>
<p>In the last exercise, you saw that using SMOTE suddenly gives us more observations of the minority class. Let’s compare those results to our original data, to get a good feeling for what has actually happened. Let’s have a look at the value counts again of our old and new data, and let’s plot the two scatter plots of the data side by side</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_plot(X_original,y_original,X_resampled,y_resampled, method):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start a plot figure</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    f, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sub-plot number 1, this is our normal data</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    c0 <span class="op">=</span> ax1.scatter(X_original[y_original <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X_original[y_original <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Class #0"</span>,alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    c1 <span class="op">=</span> ax1.scatter(X_original[y_original <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X_original[y_original <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Class #1"</span>,alpha<span class="op">=</span><span class="fl">0.5</span>, c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">'Original set'</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sub-plot number 2, this is our oversampled data</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    ax2.scatter(X_resampled[y_resampled <span class="op">==</span> <span class="dv">0</span>, <span class="dv">0</span>], X_resampled[y_resampled <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Class #0"</span>, alpha<span class="op">=</span><span class="fl">.5</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    ax2.scatter(X_resampled[y_resampled <span class="op">==</span> <span class="dv">1</span>, <span class="dv">0</span>], X_resampled[y_resampled <span class="op">==</span> <span class="dv">1</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Class #1"</span>, alpha<span class="op">=</span><span class="fl">.5</span>,c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(method)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># some settings and ready to go</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    plt.figlegend((c0, c1), (<span class="st">'Class #0'</span>, <span class="st">'Class #1'</span>), loc<span class="op">=</span><span class="st">'lower center'</span>,</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>                  ncol<span class="op">=</span><span class="dv">2</span>, labelspacing<span class="op">=</span><span class="fl">0.</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#plt.tight_layout(pad=3)</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the value_counts on the original labels y</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.value_counts(pd.Series(y)))</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the value_counts</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.value_counts(pd.Series(y_resampled)))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run compare_plot</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>compare_plot(X_original, y_original, X_resampled, y_resampled, method<span class="op">=</span><span class="st">'SMOTE'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_3288/1066387134.py:2: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.
  print(pd.value_counts(pd.Series(y)))
/tmp/ipykernel_3288/1066387134.py:5: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.
  print(pd.value_counts(pd.Series(y_resampled)))</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.0    284315
1.0       492
Name: count, dtype: int64
0.0    284315
1.0    284315
Name: count, dtype: int64</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-13-output-3.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Groupby categories and take the mean</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.groupby(<span class="st">'category'</span>).mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>TypeError: agg function failed [how-&gt;mean,dtype-&gt;object]</code></pre>
</div>
</div>
<p>It should by now be clear that our SMOTE has balanced our data completely, and that the minority class is now equal in size to the majority class. Visualizing the data shows the effect on your data very clearly. In the next section, we’ll demonstrate that there are multiple ways to implement SMOTE and that each method will have a slightly different effect.</p>
</section>
<section id="fraud-detection-algorithms-in-action" class="level3">
<h3 class="anchored" data-anchor-id="fraud-detection-algorithms-in-action">1.3 Fraud detection algorithms in action</h3>
<p>As a data scientist, you’ll often be asked to defend your method of choice, so it is important to understand the intricacies of both methods.</p>
<section id="traditional-fraud-detection-with-rules-based-systems" class="level4">
<h4 class="anchored" data-anchor-id="traditional-fraud-detection-with-rules-based-systems">Traditional fraud detection with rules based systems</h4>
<p>Traditionally, fraud analysts use rules based systems for detection of fraud. For example in the case of credit cards, the analysts might create rules based on a location and block transactions from risky zip codes. They might also create rules to block transactions from cards used too frequently, for example in the last 30 minutes. Some of these rules can be highly efficient at catching fraud, whilst others are not and results in false alarm too often.</p>
</section>
<section id="drawbacks-of-using-rules-based-systems" class="level4">
<h4 class="anchored" data-anchor-id="drawbacks-of-using-rules-based-systems">Drawbacks of using rules based systems</h4>
<p>A major limitation of rules based systems, is that the thresholds per rule are fixed, and those do not adapt as fraudulent behavior changes over time. Also, it’s very difficult to determine what the right threshold should be. Second, with a rule, you’ll get a yes or no outcome, unlike with machine learning where you can get a probability value. With probabilities, you can much better fine tune the outcomes to the amount of cases you want to inspect as a fraud team. Effectively, with a machine learning model, you can easily determine how many false positives and false negatives are acceptable, and with rules that’s much harder. Rules based system also cannot capture the interaction of features like machine learning models can. So, for example, suppose the size of a transaction only matters in combination with the frequency, for determining fraudulent transactions. A rules based systems cannot really deal with that.</p>
</section>
<section id="why-use-machine-learning-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="why-use-machine-learning-for-fraud-detection">Why use machine learning for fraud detection?</h4>
<p>Machine learning models don’t have these limitations. They will adapt to new data, and therefore can capture new fraudulent behavior. You are able to capture interactions between features, and can work with probabilities rather than yes/no answers. Machine learning models therefore typically have a better performance in fraud detection. However, machine learning models are not always the holy grail. Some simple rules might prove to be quite capable of catching fraud. You therefore want to explore whether you can combine models with rules, to improve overall performance.</p>
</section>
</section>
<section id="exploring-the-traditional-way-to-catch-fraud" class="level3">
<h3 class="anchored" data-anchor-id="exploring-the-traditional-way-to-catch-fraud">1.3.1 Exploring the traditional way to catch fraud</h3>
<p>In this example we are going to try finding fraud cases in our credit card dataset the “old way”. First we’ll define threshold values using common statistics, to split <code>fraud</code> and <code>non-fraud</code>. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams. Statistical thresholds are often determined by looking at the mean values of observations.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the mean for each group</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>df.groupby(<span class="st">'Class'</span>).mean()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Implement a rule for stating which cases are flagged as fraud</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'flag_as_fraud'</span>] <span class="op">=</span> np.where(np.logical_and(df.V1 <span class="op">&lt;</span> <span class="op">-</span><span class="dv">3</span>, df.V3 <span class="op">&lt;</span> <span class="op">-</span><span class="dv">5</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a crosstab of flagged fraud cases versus the actual fraud cases</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.crosstab(df.Class, df.flag_as_fraud, rownames<span class="op">=</span>[<span class="st">'Actual Fraud'</span>], colnames<span class="op">=</span>[<span class="st">'Flagged Fraud'</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Flagged Fraud       0     1
Actual Fraud               
0              283089  1226
1                 322   170</code></pre>
</div>
</div>
<p>Not bad, with this rule, we detect 170 out of 492 fraud cases (34.5%), but can’t detect the other 322, and get 1,226 false positives (that is we flagged the transaction as fraud but it wasn’t). In the next section, we’ll see how this measures up to a machine learning model.</p>
</section>
<section id="using-ml-classification-to-catch-fraud" class="level3">
<h3 class="anchored" data-anchor-id="using-ml-classification-to-catch-fraud">1.3.2 Using ML classification to catch fraud</h3>
<p>In this exercise we’ll see what happens when we use a simple machine learning model on our credit card data instead. Let’s implement a Logistic Regression model.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the training and testing sets</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model to our data</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain model predictions</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> model.predict(X_test)</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the classifcation report and confusion matrix</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classification report:</span><span class="ch">\n</span><span class="st">'</span>, classification_report(y_test, predicted))</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>conf_mat <span class="op">=</span> confusion_matrix(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>predicted)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion matrix:</span><span class="ch">\n</span><span class="st">'</span>, conf_mat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classification report:
               precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.85      0.60      0.70       147

    accuracy                           1.00     85443
   macro avg       0.93      0.80      0.85     85443
weighted avg       1.00      1.00      1.00     85443

Confusion matrix:
 [[85281    15]
 [   59    88]]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
</div>
</div>
<p>We are getting far fewer false positives (15 compared with 1,226), so that’s an improvement. Also, we’re catching a higher percentage (59.9%) of fraud cases (88 out of 147) compared with 34.5% that is also better than before.</p>
<p>Do you understand why we have fewer observations to look at in the confusion matrix? Remember we are using only our test data to calculate the model results on. We’re comparing the crosstab on the full dataset from the last exercise, with a confusion matrix of only 30% of the total dataset, so that’s where that difference comes from.</p>
<p>In the next section, we’ll dive deeper into understanding these model performance metrics. Let’s now explore whether we can improve the prediction results even further with resampling methods.</p>
</section>
<section id="logistic-regression-combined-with-smote" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-combined-with-smote">1.3.3 Logistic regression combined with SMOTE</h3>
<p>In this example, we’re going to take the Logistic Regression model from the previous exercise, and combine that with a SMOTE resampling method. We’ll show how to do that efficiently by using a pipeline that combines the resampling method with the model in one go. First, we need to define the pipeline that we’re going to use.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the pipeline module we need for this from imblearn</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.pipeline <span class="im">import</span> Pipeline </span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define which resampling method and which ML model to use in the pipeline</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>resampling <span class="op">=</span> SMOTE()</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the pipeline, tell it to combine SMOTE with the Logistic Regression model</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline([(<span class="st">'SMOTE'</span>, resampling), (<span class="st">'Logistic Regression'</span>, model)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-a-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="using-a-pipeline">1.4 Using a pipeline</h3>
<p>Now that we have our pipeline defined, aka combining a logistic regression with a SMOTE method, let’s run it on the data. We can treat the pipeline as if it were a single machine learning model.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split your data X and y, into a training and a test set and fit the pipeline onto the training data</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data </span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>pipeline.fit(X_train, y_train) </span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> pipeline.predict(X_test)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the results from the classification report and confusion matrix </span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Classifcation report:</span><span class="ch">\n</span><span class="st">'</span>, classification_report(y_test, predicted))</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>conf_mat <span class="op">=</span> confusion_matrix(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>predicted)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Confusion matrix:</span><span class="ch">\n</span><span class="st">'</span>, conf_mat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classifcation report:
               precision    recall  f1-score   support

         0.0       1.00      0.98      0.99     85296
         1.0       0.09      0.90      0.16       147

    accuracy                           0.98     85443
   macro avg       0.55      0.94      0.58     85443
weighted avg       1.00      0.98      0.99     85443

Confusion matrix:
 [[83970  1326]
 [   15   132]]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
</div>
</div>
<p>As we can see, the SMOTE slightly improves our results. We now manage to find 132 out of 147 cases of fraud (89.8%), but we have a slightly higher number of false positives, 1,326 cases.</p>
<p>Remember, resampling does not necessarily lead to better results. When the fraud cases are very spread and scattered over the data, using SMOTE can introduce a bit of bias. Nearest neighbors aren’t necessarily also fraud cases, so the synthetic samples might ‘confuse’ the model slightly.</p>
<p>In the next sections, we’ll learn how to also adjust our machine learning models to better detect the minority fraud cases.</p>
</section>
</section>
<section id="fraud-detection-using-labeled-data" class="level2">
<h2 class="anchored" data-anchor-id="fraud-detection-using-labeled-data">2 Fraud detection using labeled data</h2>
<p>In this section we will learn how to flag fraudulent transactions with <code>supervised</code> learning. You will use classifiers, adjust them, and compare them to find the most efficient fraud detection model.</p>
<section id="review-of-classification-methods" class="level3">
<h3 class="anchored" data-anchor-id="review-of-classification-methods">2.1 Review of classification methods</h3>
<section id="what-is-classification" class="level4">
<h4 class="anchored" data-anchor-id="what-is-classification">What is classification?</h4>
<p>Classification is the problem of identifying to which class a new observation belongs, on the basis of a training set of data containing observations whose class is known. Classes are sometimes called targets, labels or categories. For example, spam detection in email service providers can be identified as a classification problem. This is a binary classification since there are only two classes as spam and not spam. Fraud detection is a classification problem, as we try to predict whether observations are fraudulent, yes or no. Lastly, assigning a diagnosis to a patient based on characteristics of a tumor, malignant or benign, is a classification problem. Classification problems normally have a categorical output like a yes or no, 1 or 0, True or False. In the case of fraud detection, the negative non-fraud class is the majority class, whereas the fraud cases are the minority class.</p>
</section>
<section id="classification-methods-commonly-used-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="classification-methods-commonly-used-for-fraud-detection">Classification methods commonly used for fraud detection</h4>
<p><code>Neural networks</code><br>
They are capable of fitting highly non-linear models to our data. They tend to be slightly more complex to implement than most of the other classifiers.</p>
<p><code>Decision trees</code></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/298d5148-3903-4c27-86a2-7fec672bb3fc-2-bccafe94-c6ca-438d-9f35-2276b39960bc.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">decision_trees.JPG</figcaption>
</figure>
</div>
<p>As you can see in the picture, decision trees give very transparent results, that are easily interpreted by fraud analysts. Nonetheless, they are prone to <strong>overfit</strong> to your data.</p>
<p><code>Random Forest</code></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/298d5148-3903-4c27-86a2-7fec672bb3fc-1-6ec34631-2fc3-4fdb-bacc-37f4461e3c03.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">random_forest.JPG</figcaption>
</figure>
</div>
<p>Random forests are a more robust option to use, as they construct a multitude of decision trees when training your model and outputting the class that is the mode or mean predicted class of all the individual trees. To be more precise, a random forest consists of a collection of trees on a random subset of features. Final predictions are the combined results of those trees. Random forests can handle complex data and are not prone to overfit. They are interpretable by looking at feature importance, and can be adjusted to work well on highly imbalanced data. The only drawback is that they can be computationally quite heavy to run. Nonetheless, random forests are very popular for fraud detection.</p>
</section>
</section>
<section id="natural-hit-rate" class="level3">
<h3 class="anchored" data-anchor-id="natural-hit-rate">2.1.1 Natural hit rate</h3>
<p>First let’s explore how prevalent fraud is in the dataset, to understand what the “natural accuracy” is, if we were to predict everything as non-fraud. It is important to understand which level of “accuracy” you need to “beat” in order to get a better prediction than by doing nothing. In the following exercises, we’ll create a random forest classifier for fraud detection. That will serve as the “baseline” model that we’re going to try to improve.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the total number of observations from the length of y</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>total_obs <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the total number of non-fraudulent observations </span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>non_fraud <span class="op">=</span> [obs <span class="cf">for</span> obs <span class="kw">in</span> y <span class="cf">if</span> obs <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>count_non_fraud <span class="op">=</span> non_fraud.count(<span class="dv">0</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the percentage of non fraud observations in the dataset</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>percentage <span class="op">=</span> (<span class="bu">float</span>(count_non_fraud)<span class="op">/</span><span class="bu">float</span>(total_obs)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the percentage: this is our "natural accuracy" by doing nothing</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(percentage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>99.82725143693798</code></pre>
</div>
</div>
<p>This tells us that by doing nothing, we would be correct in 99.8% of the cases. So now you understand, that if we get an accuracy of less than this number, our model does not actually add any value in predicting how many cases are correct. Let’s see how a random forest does in predicting fraud in our data.</p>
</section>
<section id="random-forest-classifier" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-classifier">2.1.2 Random Forest Classifier</h3>
<p>Let’s now create a random forest classifier for fraud detection. Hopefully we can do better than the baseline accuracy we just calculated, which was roughly 99.8% This model will serve as the “baseline” model that you’re going to try to improve.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the random forest model from sklearn</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Split your data into training and test set</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model as the random forest</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see how our Random Forest model performs without doing anything special to it :</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to our training set</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain predictions from the test data </span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> model.predict(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the accuracy performance metric</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(accuracy_score(y_test, predicted))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9995201479348805</code></pre>
</div>
</div>
<p>The Random Forest model achieves an accuracy score of 99.95%.</p>
</section>
<section id="performance-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="performance-evaluation">2.2 Performance Evaluation</h3>
<section id="accuracy-isnt-everything" class="level4">
<h4 class="anchored" data-anchor-id="accuracy-isnt-everything">Accuracy isn’t everything</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/fe9b8f3a-a174-4ae8-8b4e-9c91762db4f1-2-27ad14bf-1607-4f10-905a-63f6112b356f.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">accuracy.JPG</figcaption>
</figure>
</div>
<p>As you can see on these two images, accuracy is not a reliable performance metric when working with highly imbalanced data, as is the case in fraud detection. By doing nothing, aka predicting everything is the majority class, in the picture on the right, you often obtain a higher accuracy than by actually trying to build a predictive model, in the picture on the left. So let’s discuss other performance metrics that are actually informative and reliable.</p>
</section>
<section id="false-positives-false-negatives-and-actual-fraud" class="level4">
<h4 class="anchored" data-anchor-id="false-positives-false-negatives-and-actual-fraud">False positives, false negatives, and actual fraud</h4>
<p>First of all, we need to understand the concepts of false positives, false negatives etc really well for fraud detection. Let’s refresh that for a moment.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/fe9b8f3a-a174-4ae8-8b4e-9c91762db4f1-3-c2bfcf22-f037-4396-896a-368e35ed3594.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">falsepos_falseneg.JPG</figcaption>
</figure>
</div>
<p>The true positives and true negatives are the cases you are predicting correct, in our case, fraud and non-fraud. The images on the top left and bottom right are true negatives and true positives, respectively. The false negatives as seen in the bottom left, is predicting the person is not pregnant, but actually is. So these are the cases of fraud you are not catching with your model. The false positives in the top right are the cases that we predict to be pregnant, but aren’t actually. These are “false alarm” cases, and can result in a burden of work whilst there actually is nothing going on.</p>
<p>Depending on the business case, one might care more about false negatives than false positives, or vice versa. A credit card company might want to catch as much fraud as possible and reduce false negatives, as fraudulent transactions can be incredibly costly, whereas a false alarm just means someone’s transaction is blocked.</p>
<p>On the other hand, an insurance company can not handle many false alarms, as it means getting a team of investigators involved for each positive prediction.</p>
</section>
<section id="precision-recall-tradeoff" class="level4">
<h4 class="anchored" data-anchor-id="precision-recall-tradeoff">Precision-recall tradeoff</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/fe9b8f3a-a174-4ae8-8b4e-9c91762db4f1-1-15c57515-fca2-4c10-9691-4d7262a6d757.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">precision_recall.JPG</figcaption>
</figure>
</div>
<p>The credit card company, therefore, wants to optimize for recall, whereas the insurance company cares more for precision.</p>
<p><code>Precision</code> is the fraction of actual fraud cases out of all predicted fraud cases, ie, the true positives relative to the true positives plus false positives</p>
<p><code>Recall</code> is the fraction of predicted fraud cases out of all the actual fraud cases, ie, the true positives relative to the true positives plus false negatives. Typically, precision and recall are inversely related.</p>
<p>Basically as precision increases, recall falls and vice versa. You can plot the tradeoff between the two in the precision-recall curve, as seen here on the left.</p>
<p>The <code>F-score</code> weighs both precision and recall into one measure, so if you want to use a performance metric that takes into account a balance between precision and recall, F-score is the one to use.</p>
</section>
<section id="obtaining-performance-metrics" class="level4">
<h4 class="anchored" data-anchor-id="obtaining-performance-metrics">Obtaining performance metrics</h4>
<p>Obtaining precision and recall from scikit-learn is relatively straightforward.</p>
<pre><code>from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score

average precision = average_precision_score(y_test, predicted)</code></pre>
<p>The average precision is calculated with the <code>average_precision_score</code>, which you need to run on the actual labels y_test and your predictions. The curve is obtained in a similar way, which you can then plot to look at the trade-off between the two.</p>
</section>
<section id="precision-recall-curve" class="level4">
<h4 class="anchored" data-anchor-id="precision-recall-curve">Precision-recall Curve</h4>
<pre><code>precision, recall, _ = precision_recall_curve(y_test, predicted)</code></pre>
<p>This returns the following graph.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/fe9b8f3a-a174-4ae8-8b4e-9c91762db4f1-4-ea05f3d5-684b-41ad-89a3-b7a5fd71c252.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">precision_recall_curve.JPG</figcaption>
</figure>
</div>
</section>
<section id="roc-curve-to-compare-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="roc-curve-to-compare-algorithms">ROC curve to compare algorithms</h4>
<p>Another useful tool in the performance toolbox is the ROC curve. ROC stands for receiver operating characteristic curve, and is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is very useful for comparing performance of different algorithms for your fraud detection problem. The “area under the ROC curve”-metric is easily obtained by getting the model probabilities like this:</p>
<pre><code>probs = model.predict_proba(X_test)</code></pre>
<p>and then comparing those with the actual labels.</p>
<pre><code>print(metrics.roc_auc_score(y_test, probs[:,1]))</code></pre>
</section>
<section id="confusion-matrix-and-classification-report" class="level4">
<h4 class="anchored" data-anchor-id="confusion-matrix-and-classification-report">Confusion matrix and classification report</h4>
<p>The confusion matrix and classification report are an absolute must have for fraud detection performance. You can obtain these from the scikit-learn metrics package.</p>
<pre><code>from sklearn.metrics import classification_report, confusion_matrix
predicted = model.predict(X_test)

print(classification_report(y_test,predicted))</code></pre>
<p>You need the model predictions for these, so not the probabilities. The classification report gives you precision, recall, and F1-score across the different labels. The confusion matrix plots the false negatives, false positives, etc for you.</p>
<pre><code>print(confusion_matrix(y_test, predicted))</code></pre>
</section>
</section>
<section id="performance-metrics-for-the-rf-model" class="level3">
<h3 class="anchored" data-anchor-id="performance-metrics-for-the-rf-model">2.2.1 Performance metrics for the RF model</h3>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the packages to get the different performance metrics</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix, roc_auc_score</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the predictions from our random forest model </span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> model.predict(X_test)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict probabilities</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the ROC curve, classification report and confusion matrix</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(roc_auc_score(y_test, probs[:,<span class="dv">1</span>]))</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, predicted))</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, predicted))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9406216622833715
              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.94      0.77      0.85       147

    accuracy                           1.00     85443
   macro avg       0.97      0.88      0.92     85443
weighted avg       1.00      1.00      1.00     85443

[[85289     7]
 [   34   113]]</code></pre>
</div>
</div>
<p>We have now obtained more meaningful performance metrics that tell us how well the model performs, given the highly imbalanced data that we’re working with. The model predicts 120 cases of fraud, out of which 113 are actual fraud. We have only 7 false positives. This is really good, and as a result we have a very high precision score. We however don’t catch 34 cases of actual fraud. Recall is therefore not as good as precision. Let’s try to improve that.</p>
</section>
<section id="plotting-the-precision-recall-curve" class="level3">
<h3 class="anchored" data-anchor-id="plotting-the-precision-recall-curve">2.2.2 Plotting the Precision Recall Curve</h3>
<p>We can also plot a Precision-Recall curve, to investigate the trade-off between the two in our model. In this curve Precision and Recall are inversely related; as Precision increases, Recall falls and vice-versa. A balance between these two needs to be achieved in our model, otherwise we might end up with many false positives, or not enough actual fraud cases caught. To achieve this and to compare performance, the precision-recall curves come in handy.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> average_precision_score</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate average precision and the PR curve</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>average_precision <span class="op">=</span> average_precision_score(y_test, predicted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_pr_curve(recall, precision, average_precision):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>  plt.step(recall, precision, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, where<span class="op">=</span><span class="st">'post'</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>  plt.fill_between(recall, precision, step<span class="op">=</span><span class="st">'post'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">'Recall'</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">'Precision'</span>)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>  plt.ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>  plt.xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>  plt.title(<span class="st">'2-class Precision-Recall curve: AP=</span><span class="sc">{0:0.2f}</span><span class="st">'</span>.<span class="bu">format</span>(average_precision))</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>  plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain precision and recall </span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, predicted)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the recall precision tradeoff</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>plot_pr_curve(recall, precision, average_precision)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="adjusting-your-algorithm-weights" class="level3">
<h3 class="anchored" data-anchor-id="adjusting-your-algorithm-weights">2.3 Adjusting your algorithm weights</h3>
<section id="balanced-weights" class="level4">
<h4 class="anchored" data-anchor-id="balanced-weights">Balanced weights</h4>
<p>When training a model for fraud detection, you want to try different options and setting to get the best recall-precision tradeoff possible. In scikit-learn there are two simple options to tweak your model for heavily imbalanced fraud data.</p>
<p>There is the <code>balanced mode</code>, and <code>balanced_subsample</code> mode, that you can assign to the weight argument when defining the model.</p>
<pre><code>model = RandomForestClassifier(class_weight='balanced')
model = RandomForestClassifier(class_weight='balanced_subsample')</code></pre>
<p>The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data. The balanced_subsample mode is the same as the balanced option, except that weights are calculated again at each iteration of growing a tree in the random forest. This latter option is therefore only applicable for the random forest model. The balanced option is however also available for many other classifiers, for example the logistic regression has the option, as well as the SVM model.</p>
</section>
<section id="hyperparameter-tuning-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="hyperparameter-tuning-for-fraud-detection">Hyperparameter tuning for fraud detection</h4>
<p>The weight option also takes a manual input. This allows you to adjust weights not based on the value counts relative to sample, but to whatever ratio you like. So if you just want to upsample your minority class slightly, then this is a good option. All the classifiers that have the weight option available should have this manual setting also. Moreover, the random forest takes many other options you can use to optimize the model; you call this hyperparameter tuning.</p>
<pre><code>model = RandomForestClassifier(class_weight={0:1, 1:4}, random_state=1)
model = LogisticRegression(class_weight={0:1, 1:4}, random_state=1)</code></pre>
<p>You can, for example, change the shape and size of the trees in the random forest by adjusting leaf size and tree depth. One of the most important settings are the number of trees in the forest, called number of <code>estimators</code>, and the number of <code>features</code> considered for splitting at each leaf node, indicated by <code>max_features</code>. Moreover, you can change the way the data is split at each node, as the default is to split on the <code>gini coefficient</code>.</p>
<pre><code>model = RandomForestClassifier(n_estimators=10, criterion = 'gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='auto', n_jobs=1, class_weight=None)</code></pre>
</section>
<section id="using-gridsearchcv" class="level4">
<h4 class="anchored" data-anchor-id="using-gridsearchcv">Using GridSearchCV</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/63f716bb-785d-4ea8-9e87-0dff0a4d32cc-1-a1f68959-e801-47ee-b8e5-7617d87a621d.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">grid_Search.JPG</figcaption>
</figure>
</div>
<p>A smarter way of hyperparameter tuning your model is to use <code>GridSearchCV</code>. GridSearchCV evaluates all combinations of parameters we define in the parameter grid. This is an example of a parameter grid specifically for a random forest model. Let’s define the machine learning model we’ll use. And now, let’s put it into a grid search. You pass in the model, the parameter grid, and we’ll tell it how often to cross-validate. Most importantly, you need to define a scoring metric to evaluate the models on. This is incredibly important in fraud detection. The default option here would be accuracy, so if you don’t define this, your models are ranked based on accuracy, which you already know is useless. You therefore need to pass the option precision, recall, or F1 here. Let’s go with F1 for this example:</p>
</section>
<section id="finding-the-best-model-with-gridsearchcv" class="level4">
<h4 class="anchored" data-anchor-id="finding-the-best-model-with-gridsearchcv">Finding the best model with GridSearchCV</h4>
<p>Once you have fitted your GridSearchCV and model to the data, you can obtain the parameters belonging to the optimal model by using the best parameters function.</p>
<pre><code>grid_search_model.fit(X_train, y_train)
grid_search_model.best_params_</code></pre>
<p>Mind you, GridSearchCV is computationally very heavy to run. Depending on the size of your data, and number of parameters in the grid, this can take up to many hours to complete, so make sure to save the results. You can easily get the results for the best_estimator that gives the best_score; these results are all stored. The best score is the mean cross-validated score of the best_estimator, which of course also depends on the scoring option you gave earlier. As you chose F1 before, you’ll get the best F1 sore here.</p>
<pre><code>grid_search.best_estimator_
grid_search.best_score_</code></pre>
</section>
</section>
<section id="model-adjustments" class="level3">
<h3 class="anchored" data-anchor-id="model-adjustments">2.3.1 Model adjustments</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model with balanced subsample</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(class_weight<span class="op">=</span><span class="st">'balanced_subsample'</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit your training model to your training set</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the predicted values and probabilities from the model </span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> model.predict(X_test)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the roc_auc_score, the classification report and confusion matrix</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(roc_auc_score(y_test, probs[:,<span class="dv">1</span>]))</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, predicted))</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, predicted))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.9407294501931329
              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.95      0.76      0.84       147

    accuracy                           1.00     85443
   macro avg       0.97      0.88      0.92     85443
weighted avg       1.00      1.00      1.00     85443

[[85290     6]
 [   36   111]]</code></pre>
</div>
</div>
<p>We can see that the model results don’t improve drastically. We now have 6 false positives (compared to 7), and now 36 instead of 34 false negatives, i.e.&nbsp;cases of fraud we are not catching.</p>
</section>
<section id="adjusting-our-random-forest-to-fraud-detection" class="level3">
<h3 class="anchored" data-anchor-id="adjusting-our-random-forest-to-fraud-detection">2.3.2 Adjusting our Random Forest to fraud detection</h3>
<p>In this example we will explore the options for the random forest classifier, by assigning weights and tweaking the shape of the decision trees in the forest. We’ll define weights manually, to be able to off-set that imbalance slightly.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>df.Class.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>Class
0    284315
1       492
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>In our case we have 492 fraud to 284,315 non-fraud cases, so by setting the weight ratio to 1:290, we get to <code>142,680 : 284315</code> (1/3 fraud to 2/3 non-fraud) ratio, which is good enough for training the model on.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model_results(X_train, y_train, X_test, y_test, model):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>  model.fit(X_train, y_train)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>  predicted <span class="op">=</span> model.predict(X_test)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span> (classification_report(y_test, predicted))</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span> (confusion_matrix(y_test, predicted))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Change the model options</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(bootstrap<span class="op">=</span><span class="va">True</span>, class_weight<span class="op">=</span>{<span class="dv">0</span>:<span class="dv">1</span>, <span class="dv">1</span>:<span class="dv">290</span>}, criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Change depth of model</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>            max_depth<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Change the number of samples in leaf nodes</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>            min_samples_leaf<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Change the number of trees to use</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>            n_estimators<span class="op">=</span><span class="dv">20</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the function get_model_results</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>get_model_results(X_train, y_train, X_test, y_test, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.85      0.80      0.83       147

    accuracy                           1.00     85443
   macro avg       0.92      0.90      0.91     85443
weighted avg       1.00      1.00      1.00     85443

[[85275    21]
 [   29   118]]</code></pre>
</div>
</div>
<p>WE can see by smartly defining more options in the model, we can obtain better predictions. We have effectively reduced the number of false negatives (from 36 to 29), i.e.&nbsp;we are catching more cases of fraud, whilst keeping the number of false positives relatively low (21). In this example we manually changed the options of the model. There is a smarter way of doing it, by using GridSearchCV, which you’ll see in the next section.</p>
</section>
<section id="gridsearchcv-to-find-optimal-parameters" class="level3">
<h3 class="anchored" data-anchor-id="gridsearchcv-to-find-optimal-parameters">2.3.3 GridSearchCV to find optimal parameters</h3>
<p>In this example we will tweak our model in a less “random” way, by leveraging GridSearchCV.</p>
<p>With GridSearchCV you can define which performance metric to score the options on. Since for fraud detection we are mostly interested in catching as many fraud cases as possible, we can optimize your model settings to get the best possible<code>Recall</code> score. If we also cared about reducing the number of false positives, we could optimize on F1-score, which provides a Precision-Recall trade-off.</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameter sets to test</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="co"># n_estimators = number of trees</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion refer to way trees split</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {<span class="st">'n_estimators'</span>: [<span class="dv">1</span>, <span class="dv">30</span>], <span class="st">'max_features'</span>: [<span class="st">'auto'</span>, <span class="st">'log2'</span>],  <span class="st">'max_depth'</span>: [<span class="dv">4</span>, <span class="dv">8</span>], <span class="st">'criterion'</span>: [<span class="st">'gini'</span>, <span class="st">'entropy'</span>]</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model to use</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the parameter sets with the defined model</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>CV_model <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>model, param_grid<span class="op">=</span>param_grid, cv<span class="op">=</span><span class="dv">5</span>, scoring<span class="op">=</span><span class="st">'recall'</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to our training data and obtain best parameters</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>CV_model.fit(X_train, y_train)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>CV_model.best_params_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.
  warn(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>{'criterion': 'entropy',
 'max_depth': 8,
 'max_features': 'log2',
 'n_estimators': 30}</code></pre>
</div>
</div>
</section>
<section id="model-results-using-gridsearchcv" class="level3">
<h3 class="anchored" data-anchor-id="model-results-using-gridsearchcv">2.3.4 Model results using GridSearchCV</h3>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input the optimal parameters in the model</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier(class_weight<span class="op">=</span>{<span class="dv">0</span>:<span class="dv">1</span>,<span class="dv">1</span>:<span class="dv">290</span>}, criterion<span class="op">=</span><span class="st">'entropy'</span>,</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>            n_estimators<span class="op">=</span><span class="dv">30</span>, max_features<span class="op">=</span><span class="st">'log2'</span>,  min_samples_leaf<span class="op">=</span><span class="dv">10</span>, max_depth<span class="op">=</span><span class="dv">8</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get results from your model</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>get_model_results(X_train, y_train, X_test, y_test, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.83      0.82      0.82       147

    accuracy                           1.00     85443
   macro avg       0.92      0.91      0.91     85443
weighted avg       1.00      1.00      1.00     85443

[[85272    24]
 [   27   120]]</code></pre>
</div>
</div>
<p>We managed to improve our model even further. The number of false negatives has now been slightly reduced even further (from 29 to 27), which means we are catching more cases of fraud. However, we can see that the number of false positives actually went up (from 21 to 24).</p>
<p>That is that Precision-Recall trade-off in action.</p>
<p>To decide which final model is best, we need to take into account how bad it is not to catch fraudsters, versus how many false positives the fraud analytics team can deal with. Ultimately, this final decision should be made by you and the fraud team together.</p>
</section>
<section id="ensemble-methods" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-methods">2.4 Ensemble methods</h3>
<section id="what-are-ensemble-methods-bagging-versus-stacking" class="level4">
<h4 class="anchored" data-anchor-id="what-are-ensemble-methods-bagging-versus-stacking">What are ensemble methods: bagging versus stacking</h4>
<p>Ensemble methods are techniques that create multiple machine learning models and then combine them to produce a final result. Ensemble methods usually produce more accurate predictions than a single model would. In fact, you’ve already worked with an ensemble method during the exercises. The random forest classifier is an ensemble of decision trees, and is described as a bootstrap aggregation, or bagging ensemble method. In a random forest, you train models on random subsamples of your data and aggregate the results by taking the average prediction of all of the trees.</p>
</section>
<section id="stacking-ensemble-methods" class="level4">
<h4 class="anchored" data-anchor-id="stacking-ensemble-methods">Stacking ensemble methods</h4>
<p>In this picture, you see a stacking ensemble method. In this case, multiple models are combined via a voting rule on the model outcome. The base level models are each trained based on the complete training set. So, unlike with the bagging method, you do not train your models on a subsample. In the stacking ensemble method, you can combine algorithms of different types. We’ll practice this in the exercises.</p>
</section>
<section id="why-use-ensemble-methods-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="why-use-ensemble-methods-for-fraud-detection">Why use ensemble methods for fraud detection</h4>
<p>The goal of any machine learning problem is to find a single model that will best predict the wanted outcome. Rather than making one model, and hoping this model is the best or most accurate predictor, you can make use of ensemble methods. Ensemble methods take a myriad of models into account, and average those models to produce one final model. This ensures that your predictions are robust and less likely to be the result of overfitting. Moreover, ensemble methods can improve overall performance of fraud detection, especially combining models with different recall and precision scores. They have therefore been a winning formula at many Kaggle competitions recently.</p>
</section>
<section id="voting-classifier" class="level4">
<h4 class="anchored" data-anchor-id="voting-classifier">Voting classifier</h4>
<p>he voting classifier available in scikit-learn is an easy way to implement an ensemble model. You start by importing the voting classifier, available from the ensemble methods package. Let’s define three models to use in our ensemble model, in this case let’s use a random forest, a logistic regression, and a Naive Bayes model. The next step is to combine these three into the ensemble model like this, and assign a rule to combine the model results. In this case, let’s use a hard voting rule. That option uses the predicted class labels and takes the majority vote. The other option is soft voting. This rule takes the average probability by combining the predicted probabilities of the individual models. You can then simply use the ensemble_model as you would any other machine learning model, ie you can fit and use the model to predict classes. Last thing to mention is that you can also assign weights to the model predictions in the ensemble, which can be useful, for example, when you know one model outperforms the others significantly.</p>
</section>
<section id="reliable-labels-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="reliable-labels-for-fraud-detection">Reliable labels for fraud detection</h4>
<p>In this section we have seen how to detect fraud when there are labels to train a model on. However, in real life, it is unlikely that you will have truly unbiased reliable labels for you model. For example, in credit card fraud you often will have reliable labels, in which case you want to use these methods you’ve just learned. However, in most other cases, you will need to rely on <code>unsupervised</code> learning techniques to detect fraud. You will learn how to do this in a later section.</p>
</section>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">2.4.1 Logistic Regression</h3>
<p>We wil now combine three algorithms into one model with the <code>VotingClassifier</code>. This allows us to benefit from the different aspects from all models, and hopefully improve overall performance and detect more fraud. The first model, the <code>Logistic Regression</code>, has a slightly higher recall score than our optimal <code>Random Forest</code> model, but gives a lot more false positives. We’ll also add a <code>Decision Tree</code> with balanced weights to it.</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/creditcard.csv'</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Time</th>
<th data-quarto-table-cell-role="th">V1</th>
<th data-quarto-table-cell-role="th">V2</th>
<th data-quarto-table-cell-role="th">V3</th>
<th data-quarto-table-cell-role="th">V4</th>
<th data-quarto-table-cell-role="th">V5</th>
<th data-quarto-table-cell-role="th">V6</th>
<th data-quarto-table-cell-role="th">V7</th>
<th data-quarto-table-cell-role="th">V8</th>
<th data-quarto-table-cell-role="th">V9</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">V21</th>
<th data-quarto-table-cell-role="th">V22</th>
<th data-quarto-table-cell-role="th">V23</th>
<th data-quarto-table-cell-role="th">V24</th>
<th data-quarto-table-cell-role="th">V25</th>
<th data-quarto-table-cell-role="th">V26</th>
<th data-quarto-table-cell-role="th">V27</th>
<th data-quarto-table-cell-role="th">V28</th>
<th data-quarto-table-cell-role="th">Amount</th>
<th data-quarto-table-cell-role="th">Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.0</td>
<td>-1.359807</td>
<td>-0.072781</td>
<td>2.536347</td>
<td>1.378155</td>
<td>-0.338321</td>
<td>0.462388</td>
<td>0.239599</td>
<td>0.098698</td>
<td>0.363787</td>
<td>...</td>
<td>-0.018307</td>
<td>0.277838</td>
<td>-0.110474</td>
<td>0.066928</td>
<td>0.128539</td>
<td>-0.189115</td>
<td>0.133558</td>
<td>-0.021053</td>
<td>149.62</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.0</td>
<td>1.191857</td>
<td>0.266151</td>
<td>0.166480</td>
<td>0.448154</td>
<td>0.060018</td>
<td>-0.082361</td>
<td>-0.078803</td>
<td>0.085102</td>
<td>-0.255425</td>
<td>...</td>
<td>-0.225775</td>
<td>-0.638672</td>
<td>0.101288</td>
<td>-0.339846</td>
<td>0.167170</td>
<td>0.125895</td>
<td>-0.008983</td>
<td>0.014724</td>
<td>2.69</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1.0</td>
<td>-1.358354</td>
<td>-1.340163</td>
<td>1.773209</td>
<td>0.379780</td>
<td>-0.503198</td>
<td>1.800499</td>
<td>0.791461</td>
<td>0.247676</td>
<td>-1.514654</td>
<td>...</td>
<td>0.247998</td>
<td>0.771679</td>
<td>0.909412</td>
<td>-0.689281</td>
<td>-0.327642</td>
<td>-0.139097</td>
<td>-0.055353</td>
<td>-0.059752</td>
<td>378.66</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.0</td>
<td>-0.966272</td>
<td>-0.185226</td>
<td>1.792993</td>
<td>-0.863291</td>
<td>-0.010309</td>
<td>1.247203</td>
<td>0.237609</td>
<td>0.377436</td>
<td>-1.387024</td>
<td>...</td>
<td>-0.108300</td>
<td>0.005274</td>
<td>-0.190321</td>
<td>-1.175575</td>
<td>0.647376</td>
<td>-0.221929</td>
<td>0.062723</td>
<td>0.061458</td>
<td>123.50</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2.0</td>
<td>-1.158233</td>
<td>0.877737</td>
<td>1.548718</td>
<td>0.403034</td>
<td>-0.407193</td>
<td>0.095921</td>
<td>0.592941</td>
<td>-0.270533</td>
<td>0.817739</td>
<td>...</td>
<td>-0.009431</td>
<td>0.798278</td>
<td>-0.137458</td>
<td>0.141267</td>
<td>-0.206010</td>
<td>0.502292</td>
<td>0.219422</td>
<td>0.215153</td>
<td>69.99</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">284802</td>
<td>172786.0</td>
<td>-11.881118</td>
<td>10.071785</td>
<td>-9.834783</td>
<td>-2.066656</td>
<td>-5.364473</td>
<td>-2.606837</td>
<td>-4.918215</td>
<td>7.305334</td>
<td>1.914428</td>
<td>...</td>
<td>0.213454</td>
<td>0.111864</td>
<td>1.014480</td>
<td>-0.509348</td>
<td>1.436807</td>
<td>0.250034</td>
<td>0.943651</td>
<td>0.823731</td>
<td>0.77</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">284803</td>
<td>172787.0</td>
<td>-0.732789</td>
<td>-0.055080</td>
<td>2.035030</td>
<td>-0.738589</td>
<td>0.868229</td>
<td>1.058415</td>
<td>0.024330</td>
<td>0.294869</td>
<td>0.584800</td>
<td>...</td>
<td>0.214205</td>
<td>0.924384</td>
<td>0.012463</td>
<td>-1.016226</td>
<td>-0.606624</td>
<td>-0.395255</td>
<td>0.068472</td>
<td>-0.053527</td>
<td>24.79</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">284804</td>
<td>172788.0</td>
<td>1.919565</td>
<td>-0.301254</td>
<td>-3.249640</td>
<td>-0.557828</td>
<td>2.630515</td>
<td>3.031260</td>
<td>-0.296827</td>
<td>0.708417</td>
<td>0.432454</td>
<td>...</td>
<td>0.232045</td>
<td>0.578229</td>
<td>-0.037501</td>
<td>0.640134</td>
<td>0.265745</td>
<td>-0.087371</td>
<td>0.004455</td>
<td>-0.026561</td>
<td>67.88</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">284805</td>
<td>172788.0</td>
<td>-0.240440</td>
<td>0.530483</td>
<td>0.702510</td>
<td>0.689799</td>
<td>-0.377961</td>
<td>0.623708</td>
<td>-0.686180</td>
<td>0.679145</td>
<td>0.392087</td>
<td>...</td>
<td>0.265245</td>
<td>0.800049</td>
<td>-0.163298</td>
<td>0.123205</td>
<td>-0.569159</td>
<td>0.546668</td>
<td>0.108821</td>
<td>0.104533</td>
<td>10.00</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">284806</td>
<td>172792.0</td>
<td>-0.533413</td>
<td>-0.189733</td>
<td>0.703337</td>
<td>-0.506271</td>
<td>-0.012546</td>
<td>-0.649617</td>
<td>1.577006</td>
<td>-0.414650</td>
<td>0.486180</td>
<td>...</td>
<td>0.261057</td>
<td>0.643078</td>
<td>0.376777</td>
<td>0.008797</td>
<td>-0.473649</td>
<td>-0.818267</td>
<td>-0.002415</td>
<td>0.013649</td>
<td>217.00</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>284807 rows × 31 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the Time column</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.drop(<span class="st">'Time'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prep_data(df):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df.iloc[:, <span class="dv">1</span>:<span class="dv">29</span>]</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array(X).astype(np.float64)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> df.iloc[:, <span class="dv">29</span>]</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>np.array(y).astype(np.float64)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X,y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create X and y from the prep_data function </span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>X_original, y_original <span class="op">=</span> prep_data(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the training and testing sets</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_original, y_original, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>X_train.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>(199364, 28)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>y_train</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>array([0., 0., 0., ..., 0., 0., 0.])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model_results(X_train, y_train, X_test, y_test, model):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>  model.fit(X_train, y_train)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>  predicted <span class="op">=</span> model.predict(X_test)</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>  probs <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span> (classification_report(y_test, predicted))</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span> (confusion_matrix(y_test, predicted))</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Logistic Regression model with weights</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(class_weight<span class="op">=</span>{<span class="dv">0</span>:<span class="dv">1</span>, <span class="dv">1</span>:<span class="dv">290</span>}, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the model results</span></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>get_model_results(X_train, y_train, X_test, y_test, model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         0.0       1.00      0.99      0.99     85296
         1.0       0.11      0.89      0.20       147

    accuracy                           0.99     85443
   macro avg       0.56      0.94      0.60     85443
weighted avg       1.00      0.99      0.99     85443

[[84255  1041]
 [   16   131]]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
</div>
</div>
<p>As you can see the Logistic Regression has quite different performance from the Random Forest. More false positives: 1,041 compared with 24, but also a better Recall, 0.89 compared to 0.82. It will therefore be a useful addition to the Random Forest in an ensemble model. Let’s give that a try.</p>
</section>
<section id="voting-classifier-1" class="level3">
<h3 class="anchored" data-anchor-id="voting-classifier-1">2.4.2 Voting Classifier</h3>
<p>Let’s now combine three machine learning models into one, to improve our Random Forest fraud detection model from before. We’ll combine our usual Random Forest model, with the Logistic Regression from the previous exercise, with a simple Decision Tree.</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the three classifiers to use in the ensemble</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>clf1 <span class="op">=</span> LogisticRegression(class_weight<span class="op">=</span>{<span class="dv">0</span>:<span class="dv">1</span>, <span class="dv">1</span>:<span class="dv">15</span>}, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>clf2 <span class="op">=</span> RandomForestClassifier(class_weight<span class="op">=</span>{<span class="dv">0</span>:<span class="dv">1</span>, <span class="dv">1</span>:<span class="dv">290</span>}, criterion<span class="op">=</span><span class="st">'gini'</span>, max_depth<span class="op">=</span><span class="dv">8</span>, max_features<span class="op">=</span><span class="st">'log2'</span>,</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>            min_samples_leaf<span class="op">=</span><span class="dv">10</span>, n_estimators<span class="op">=</span><span class="dv">30</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>clf3 <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">5</span>, class_weight<span class="op">=</span><span class="st">"balanced"</span>)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the classifiers in the ensemble model</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>ensemble_model <span class="op">=</span> VotingClassifier(estimators<span class="op">=</span>[(<span class="st">'lr'</span>, clf1), (<span class="st">'rf'</span>, clf2), (<span class="st">'dt'</span>, clf3)], voting<span class="op">=</span><span class="st">'soft'</span>) <span class="co"># predict_proba is not available when voting='hard'</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the results </span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>get_model_results(X_train, y_train, X_test, y_test, ensemble_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.84      0.80      0.82       147

    accuracy                           1.00     85443
   macro avg       0.92      0.90      0.91     85443
weighted avg       1.00      1.00      1.00     85443

[[85273    23]
 [   30   117]]</code></pre>
</div>
</div>
<p>We see that by combining the classifiers, we can take the best of multiple models. We’ve slightly decreased the cases of fraud we are catching from 120 to 117, and also reduced the number of false positives from 24 to 23. If you do care about catching as many fraud cases as you can, whilst keeping the false positives low, this is a pretty good trade-off. The Logistic Regression as a standalone was quite bad in terms of false positives, and the Random Forest was worse in terms of false negatives. By combining these together we managed to improve performance.</p>
</section>
<section id="adjust-weights-within-the-voting-classifier" class="level3">
<h3 class="anchored" data-anchor-id="adjust-weights-within-the-voting-classifier">2.4.3 Adjust weights within the Voting Classifier</h3>
<p>You’ve just seen that the Voting Classifier allows you to improve your fraud detection performance, by combining good aspects from multiple models. Now let’s try to adjust the weights we give to these models. By increasing or decreasing weights you can play with how much emphasis you give to a particular model relative to the rest. This comes in handy when a certain model has overall better performance than the rest, but you still want to combine aspects of the others to further improve your results.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the ensemble model, weighting 2nd classifier 4 to 1 with the rest</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>ensemble_model <span class="op">=</span> VotingClassifier(estimators<span class="op">=</span>[(<span class="st">'lr'</span>, clf1), (<span class="st">'rf'</span>, clf2), (<span class="st">'gnb'</span>, clf3)], voting<span class="op">=</span><span class="st">'soft'</span>, weights<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>], flatten_transform<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get results </span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>get_model_results(X_train, y_train, X_test, y_test, ensemble_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

         0.0       1.00      1.00      1.00     85296
         1.0       0.83      0.82      0.82       147

    accuracy                           1.00     85443
   macro avg       0.92      0.91      0.91     85443
weighted avg       1.00      1.00      1.00     85443

[[85272    24]
 [   27   120]]</code></pre>
</div>
</div>
<p>We have detected another 3 fraud cases and only conceded one additional false positive.</p>
<p>The weight option allows you to play with the individual models to get the best final mix for your fraud detection model. Now that we have finalized fraud detection with supervised learning, let’s have a look at how fraud detection can be done when you don’t have any labels to train on.</p>
</section>
</section>
<section id="fraud-detection-using-unlabeled-data" class="level2">
<h2 class="anchored" data-anchor-id="fraud-detection-using-unlabeled-data">3. Fraud detection using unlabeled data</h2>
<p>This section focuses on using unsupervised learning techniques to detect fraud. We will segment customers, use K-means clustering and other clustering algorithms to find suspicious occurrences in our data.</p>
<section id="normal-versus-abnormal-behaviour" class="level3">
<h3 class="anchored" data-anchor-id="normal-versus-abnormal-behaviour">3.1 Normal versus abnormal behaviour</h3>
<section id="fraud-detection-without-labels" class="level4">
<h4 class="anchored" data-anchor-id="fraud-detection-without-labels">Fraud detection without labels</h4>
<p>When you can’t rely on fraud labels, you can use unsupervised learning to detect suspicious behavior. Suspicious behavior is behavior that is very uncommon in your data, for example, very large transactions, or many transactions in a short period of time. Such behavior often is an indication of fraud, but of course can also just be uncommon but not fraudulent. This type of fraud detection is challenging, because you don’t have trustworthy labels to check your model results against. But, in fact, not having labels is the reality for many cases of fraud detection.</p>
</section>
<section id="what-is-normal-behavior" class="level4">
<h4 class="anchored" data-anchor-id="what-is-normal-behavior">What is normal behavior?</h4>
<p>In order to detect suspicious behavior, you need to understand your data very well. A good exploratory data analysis, including distribution plots, checking for outliers and correlations etc, is crucial. The fraud analysts can help you understand what are normal values for your data, and also what typifies fraudulent behavior. Moreover, you need to investigate whether your data is homogeneous, or whether different types of clients display very different behavior. What is normal for one does not mean it’s normal for another. For example, older age groups might have much higher total amount of health insurance claims than younger people. Or, a millionaire might make much larger transactions on average than a student. If that is the case in your data, you need to find homogeneous subgroups of data that are similar, such that you can look for abnormal behavior within subgroups.</p>
</section>
<section id="customer-segmentation-normal-behavior-within-segments" class="level4">
<h4 class="anchored" data-anchor-id="customer-segmentation-normal-behavior-within-segments">Customer segmentation: normal behavior within segments</h4>
<p>So what can you think about when checking for segments in your data? First of all, you need to make sure all your data points are the same type. By type I mean: are they individuals, groups of people, companies, or governmental organizations? Then, think about whether the data points differ on, for example spending patterns, age, location, or frequency of transactions. Especially for credit card fraud, location can be a big indication for fraud. But this also goes for e-commerce sites; where is the IP address located, and where is the product ordered to ship? If they are far apart that might not be normal for most clients, unless they indicate otherwise. Last thing to keep in mind, is that you have to create a separate model on each segment, because you want to detect suspicious behavior within each segment. But that means that you have to think about how to aggregate the many model results back into one final list.</p>
</section>
</section>
<section id="exploring-your-data" class="level3">
<h3 class="anchored" data-anchor-id="exploring-your-data">3.1.1 Exploring your data</h3>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/banksim.csv'</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">gender</th>
<th data-quarto-table-cell-role="th">category</th>
<th data-quarto-table-cell-role="th">amount</th>
<th data-quarto-table-cell-role="th">fraud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>171915</td>
<td>3</td>
<td>F</td>
<td>es_transportation</td>
<td>49.7100</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>426989</td>
<td>4</td>
<td>F</td>
<td>es_health</td>
<td>39.2900</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>310539</td>
<td>3</td>
<td>F</td>
<td>es_transportation</td>
<td>18.7600</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>215216</td>
<td>4</td>
<td>M</td>
<td>es_transportation</td>
<td>13.9500</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>569244</td>
<td>2</td>
<td>M</td>
<td>es_transportation</td>
<td>49.8700</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7195</td>
<td>260136</td>
<td>5</td>
<td>M</td>
<td>es_hotelservices</td>
<td>236.1474</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7196</td>
<td>56643</td>
<td>5</td>
<td>F</td>
<td>es_hotelservices</td>
<td>139.6000</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7197</td>
<td>495817</td>
<td>1</td>
<td>F</td>
<td>es_travel</td>
<td>236.1474</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7198</td>
<td>333170</td>
<td>1</td>
<td>M</td>
<td>es_hotelservices</td>
<td>236.1474</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7199</td>
<td>579286</td>
<td>4</td>
<td>F</td>
<td>es_health</td>
<td>236.1474</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>7200 rows × 6 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>df.drop(<span class="st">'Unnamed: 0'</span>, axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the dataframe shape</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>df.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>(7200, 5)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first 5 rows</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">gender</th>
<th data-quarto-table-cell-role="th">category</th>
<th data-quarto-table-cell-role="th">amount</th>
<th data-quarto-table-cell-role="th">fraud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3</td>
<td>F</td>
<td>es_transportation</td>
<td>49.71</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>F</td>
<td>es_health</td>
<td>39.29</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>3</td>
<td>F</td>
<td>es_transportation</td>
<td>18.76</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4</td>
<td>M</td>
<td>es_transportation</td>
<td>13.95</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2</td>
<td>M</td>
<td>es_transportation</td>
<td>49.87</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by 'category' and calculate the mean of 'value'</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>df.groupby(<span class="st">'category'</span>)[<span class="st">'fraud'</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>category
es_barsandrestaurants    0.022472
es_contents              0.000000
es_fashion               0.020619
es_food                  0.000000
es_health                0.242798
es_home                  0.208333
es_hotelservices         0.548387
es_hyper                 0.125000
es_leisure               1.000000
es_otherservices         0.600000
es_sportsandtoys         0.657895
es_tech                  0.179487
es_transportation        0.000000
es_travel                0.944444
es_wellnessandbeauty     0.060606
Name: fraud, dtype: float64</code></pre>
</div>
</div>
<p>You can see from the category averages that fraud is more prevalent in the leisure, travel and sports categories.</p>
</section>
<section id="customer-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="customer-segmentation">3.1.2 Customer segmentation</h3>
<p>In this example we’re going to check whether there are any obvious patterns for the clients in this data, thus whether we need to segment our data into groups, or whether the data is rather homogenous. We unfortunately don’t have a lot client information available; we can’t for example distinguish between the wealth levels of different clients. However, there is data on <strong>age</strong> available, so let’s see whether there is any significant difference between behaviour of age groups.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by age groups and get the mean</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.groupby(<span class="st">'age'</span>)[<span class="st">'fraud'</span>].mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>age
0    0.050000
1    0.026648
2    0.028718
3    0.023283
4    0.035966
5    0.023990
6    0.022293
U    0.000000
Name: fraud, dtype: float64</code></pre>
</div>
</div>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the values of the observations in each age group</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df[<span class="st">'age'</span>].value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>age
2    2333
3    1718
4    1279
5     792
1     713
6     314
0      40
U      11
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>Based on these results it does not make sense to divide our data into age segments before running a fraud detection algorithm, because the largest groups are relatively similar in terms of both amount spent and fraud occurence. Although age group ‘0’ has the highest proportion of fraud, only 40 cases in the entire dataset fall in this category, so it would not be feasible to split these out into a seperate group and run a model on that amount of observations. You have to keep in mind that you need to train the model seperately on each segment, so in this case it would not be possible.</p>
</section>
<section id="using-statistics-to-define-normal-behavior" class="level3">
<h3 class="anchored" data-anchor-id="using-statistics-to-define-normal-behavior">3.1.3 Using statistics to define normal behavior</h3>
<p>Let’s investigate the average amounts spend in normal transactions versus fraud transactions. This gives you an idea of how fraudulent transactions differ structurally from normal transactions.</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create two dataframes with fraud and non-fraud data </span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>df_fraud <span class="op">=</span> df.loc[df.fraud <span class="op">==</span> <span class="dv">1</span>] </span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>df_non_fraud <span class="op">=</span> df.loc[df.fraud <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histograms of the amounts in fraud and non-fraud data </span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>plt.hist(df_fraud.amount, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'fraud'</span>)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>plt.hist(df_non_fraud.amount, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'nonfraud'</span>)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"£ amount"</span>)</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"# observations"</span>)</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As the number of fraud observations is much smaller, it is difficult to see the full distribution. Nonetheless, you can see that the fraudulent transactions tend to be on the larger side relative to normal obervations. This is good news, as it helps us later in detecting fraud from non-fraud. In the next chapter you’re going to implement a clustering model to distinguish between normal and abnormal transactions, when the fraud labels are no longer available.</p>
</section>
<section id="clustering-methods-to-detect-fraud" class="level3">
<h3 class="anchored" data-anchor-id="clustering-methods-to-detect-fraud">3.2 Clustering methods to detect fraud</h3>
<section id="clustering-trying-to-detect-patterns-in-data" class="level4">
<h4 class="anchored" data-anchor-id="clustering-trying-to-detect-patterns-in-data">Clustering: trying to detect patterns in data</h4>
<p>The objective of any clustering model is to detect patterns in your data. More specifically, to group your data in distinct clusters, that is made up of data points that are very similar to each other, but distinct from the data points in the other clusters. We can use this for fraud detection to determine which data looks very similar to the data in the clusters, and which data you would have a hard time assigning to any cluster. You can flag such data as odd, or suspicious.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-3-7083d5d8-4f72-4c3e-9e96-5d2be520c8ec.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">cluster.JPG</figcaption>
</figure>
</div>
<p>In this image you see a clear example where a cloud of data is clustered into three distinct clusters.</p>
</section>
<section id="k-means-clustering-using-the-distance-to-cluster-centroids" class="level4">
<h4 class="anchored" data-anchor-id="k-means-clustering-using-the-distance-to-cluster-centroids">K-means clustering: using the distance to cluster centroids</h4>
<p>So, let’s talk about how we achieve this pattern detecting using K-means clustering. In this example, training samples are shown as dots and cluster centroids are shown as crosses. Let’s say we try to cluster the data in image A. We start by putting in an initial guess for two cluster centroids in figure B. You need to predefine the amount of clusters, therefore, at the start.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-5-abcca84f-a37a-49c8-abfe-ec8606896577.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A_B.JPG</figcaption>
</figure>
</div>
<p>You then calculate the distances of each sample in the data to the closest centroid, in figure C, which allows you to split your data into the first two clusters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-1-1d57ce82-ded3-4afc-b323-8213da1994d7.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">C.JPG</figcaption>
</figure>
</div>
<p>And based on these initial clusters, you can refine the location of the centroids to minimize the sum of all distances in the two clusters, as you can see here in picture D.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-2-6f255af0-c6d5-4aa5-ac4b-9d819d8f4c49.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">D.JPG</figcaption>
</figure>
</div>
<p>You then repeat the step of reassigning points that are nearest to the centroid, as shown in figure E, and so forth until it converges to the point where no sample gets reassigned to another cluster. The final clusters are depicted in picture F.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-6-e267fd2d-f0b7-4f30-ae90-990f3e0178f2.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">EF.JPG</figcaption>
</figure>
</div>
</section>
<section id="k-means-clustering-in-python" class="level4">
<h4 class="anchored" data-anchor-id="k-means-clustering-in-python">K-means clustering in Python</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-4-9c669bf6-3157-4336-99d4-e388a67e2398.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">k_means.JPG</figcaption>
</figure>
</div>
<p>Let’s see how to implement this in Python. You begin by importing the K-means model from scikit-learn, and also a scaling method. It is of utmost importance to scale your data before doing K-means clustering, or any algorithm that uses distances, for that matter. If you forget to scale, features on a larger scale will weigh more heavily in the algorithm, and you don’t want that. All features should weigh equally at this point. In the first step, you transform the data stored under df, into a NumPy array and make sure all the data is of the type float. Second, you apply the MinMaxScaler and use fit_transform on the data, as this returns the scaled data. Now you are ready to define the K-means model with 6 clusters, and fit that straight to the scaled data, as seen here. It is wise to fix the random-state, to be able to compare models.</p>
</section>
<section id="the-right-amount-of-clusters" class="level4">
<h4 class="anchored" data-anchor-id="the-right-amount-of-clusters">The right amount of clusters</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-8-f686b717-f9c9-4441-9f84-6f2207cc06c9.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">elbow_curve.JPG</figcaption>
</figure>
</div>
<p>The drawback of K-means clustering is that you need to assign the number of clusters beforehand. There are multiple ways to check what the right amount of clusters should be, such as the silhouette method or the elbow curve. Let’s do a quick refresher on the elbow curve. The objective of k-means is to minimize the sum of all distances between the data samples and their associated cluster centroids. The score is the inverse of that minimization, so you want the score to be close to zero.</p>
</section>
<section id="the-elbow-curve" class="level4">
<h4 class="anchored" data-anchor-id="the-elbow-curve">The elbow curve</h4>
<p>By running a k-means model on clusters varying from 1 to 10, like this, and saving the scores for each model under score, you can obtain the elbow curve. Then it is a matter of simply plotting the scores against the number of clusters like this. Which results in the following plot.</p>
<p>This is an example of a typical elbow curve. The slight angle at K equals 3 suggests that 3 clusters could be optimal, although the optimal cluster number is not very pronounced in this case.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2ad977c9-84cf-402e-a515-82aa36241480-7-f3b1b3d6-bed8-4636-9826-ad11643f3f8f.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">elbow_crv.JPG</figcaption>
</figure>
</div>
</section>
</section>
<section id="scaling-the-data" class="level3">
<h3 class="anchored" data-anchor-id="scaling-the-data">3.2.1 Scaling the data</h3>
<p>For ML algorithms using distance based metrics, it is crucial to always scale our data, as features using different scales will distort our results. K-means uses the Euclidian distance to assess distance to cluster centroids, therefore we first need to scale our data before continuing to implement the algorithm. Let’s do that first.</p>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>df_adj <span class="op">=</span> pd.read_csv(<span class="st">'data/banksim_adj.csv'</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>df_adj</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">amount</th>
<th data-quarto-table-cell-role="th">fraud</th>
<th data-quarto-table-cell-role="th">M</th>
<th data-quarto-table-cell-role="th">es_barsandrestaurants</th>
<th data-quarto-table-cell-role="th">es_contents</th>
<th data-quarto-table-cell-role="th">es_fashion</th>
<th data-quarto-table-cell-role="th">es_food</th>
<th data-quarto-table-cell-role="th">es_health</th>
<th data-quarto-table-cell-role="th">es_home</th>
<th data-quarto-table-cell-role="th">es_hotelservices</th>
<th data-quarto-table-cell-role="th">es_hyper</th>
<th data-quarto-table-cell-role="th">es_leisure</th>
<th data-quarto-table-cell-role="th">es_otherservices</th>
<th data-quarto-table-cell-role="th">es_sportsandtoys</th>
<th data-quarto-table-cell-role="th">es_tech</th>
<th data-quarto-table-cell-role="th">es_transportation</th>
<th data-quarto-table-cell-role="th">es_travel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0</td>
<td>3</td>
<td>49.7100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1</td>
<td>4</td>
<td>39.2900</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2</td>
<td>3</td>
<td>18.7600</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>3</td>
<td>4</td>
<td>13.9500</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>4</td>
<td>2</td>
<td>49.8700</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7184</td>
<td>7195</td>
<td>5</td>
<td>236.1474</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7185</td>
<td>7196</td>
<td>5</td>
<td>139.6000</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7186</td>
<td>7197</td>
<td>1</td>
<td>236.1474</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7187</td>
<td>7198</td>
<td>1</td>
<td>236.1474</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">7188</td>
<td>7199</td>
<td>4</td>
<td>236.1474</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>7189 rows × 19 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df_adj.fraud.values</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>array([0, 0, 0, ..., 1, 1, 1])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="74">
<pre><code>numpy.ndarray</code></pre>
</div>
</div>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># drop cols not required</span></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>df_adj <span class="op">=</span> df_adj.drop([<span class="st">'Unnamed: 0'</span>, <span class="st">'fraud'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the scaler</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Take the float values of df for X</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df_adj.values.astype(np.float64)</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the scaler and apply to the data</span></span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>X_scaled.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>(7189, 17)</code></pre>
</div>
</div>
</section>
<section id="k-means-clustering" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering">3.2.2 K-means clustering</h3>
<p>A very commonly used clustering algorithm is K-means clustering. For fraud detection, K-means clustering is straightforward to implement and relatively powerful in predicting suspicious cases. It is a good algorithm to start with when working on fraud detection problems. However, fraud data is oftentimes very large, especially when you are working with transaction data. MiniBatch K-means is an efficient way to implement K-means on a large dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import MiniBatchKmeans </span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MiniBatchKMeans</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model </span></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span><span class="dv">8</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model to the scaled data</span></span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>kmeans.fit(X_scaled)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We have now fitted our MiniBatch K-means model to the data. In the upcoming sections we will explore whether this model is any good at flagging fraud. But before doing that, we still need to figure our what the right number of clusters to use is.</p>
</section>
<section id="elbow-method" class="level3">
<h3 class="anchored" data-anchor-id="elbow-method">3.2.3 Elbow method</h3>
<p>In the previous exercise we implemented MiniBatch K-means with 8 clusters, without actually checking what the right amount of clusters should be. For our first fraud detection approach, it is important to get the number of clusters right, especially when we want to use the outliers of those clusters as fraud predictions. To decide which amount of clusters we are going to use, let’s apply the Elbow method and see what the optimal number clusters should be based on this method.</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the range of clusters to try</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>clustno <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">5</span>)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Run MiniBatch Kmeans over the number of clusters</span></span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> [MiniBatchKMeans(n_clusters<span class="op">=</span>i, random_state<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> i <span class="kw">in</span> clustno]</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the score for each model</span></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> [kmeans[i].fit(X_scaled).score(X_scaled) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(kmeans))]</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the models and their respective score </span></span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>plt.plot(clustno, score)</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Clusters'</span>)</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Score'</span>)</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Elbow Curve'</span>)</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-60-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Now we can see that the optimal number of clusters should probably be at around 3 clusters, as that is where the elbow is in the curve. We’ll use this in the next exercise as our baseline model, and see how well this does in detecting fraud.</p>
</section>
<section id="assigning-fraud-versus-non-fraud" class="level3">
<h3 class="anchored" data-anchor-id="assigning-fraud-versus-non-fraud">3.3 Assigning fraud versus non-fraud</h3>
<section id="starting-with-clustered-data" class="level4">
<h4 class="anchored" data-anchor-id="starting-with-clustered-data">Starting with clustered data</h4>
<p>It all starts with your optimized model, it can be k-means or any other clustering method, for that matter. In a nutshell, you’re going to take the outliers of each cluster, and flag those as fraud. In this example, you’re looking at three clusters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2239973a-7069-46d7-8df7-586d13c03f3c-3-80e41e26-e10d-404b-81be-cc281a345df0.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">clust.JPG</figcaption>
</figure>
</div>
</section>
<section id="assign-the-cluster-centroids" class="level4">
<h4 class="anchored" data-anchor-id="assign-the-cluster-centroids">Assign the cluster centroids</h4>
<p>In the first step, you need to collect and store the cluster centroids in memory, as that is the starting point to decide what’s normal and what’s not.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2239973a-7069-46d7-8df7-586d13c03f3c-4-b9999b8e-554d-4ccb-8d29-402c29089507.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">centroids.JPG</figcaption>
</figure>
</div>
</section>
<section id="define-distances-from-the-cluster-centroid" class="level4">
<h4 class="anchored" data-anchor-id="define-distances-from-the-cluster-centroid">Define distances from the cluster centroid</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2239973a-7069-46d7-8df7-586d13c03f3c-5-ba436bf6-c77a-44e5-bc58-29054d6bffeb.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">euclid.JPG</figcaption>
</figure>
</div>
<p>The next step is to calculate the distance of each point in the dataset, to their own cluster centroid. In this case, I use the Euclidean distance, hence you see these depicted as round circles. You then also need to define a cut-off point for the distances to define what is an outlier. You do this based on the distributions of the distances collected. Suppose you decide everything that has a bigger distance than the top 95th percentile, should be considered an outlier, ie you take the tail of the distribution of distances. In this case, that would mean that anything that falls outside the round circles, is considered an outlier.</p>
</section>
<section id="flag-fraud-for-those-furthest-away-from-cluster-centroid" class="level4">
<h4 class="anchored" data-anchor-id="flag-fraud-for-those-furthest-away-from-cluster-centroid">Flag fraud for those furthest away from cluster centroid</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2239973a-7069-46d7-8df7-586d13c03f3c-1-253fa9cb-66e1-46fc-a1d0-c3c315bcf041.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">odd.JPG</figcaption>
</figure>
</div>
<p>As you see in the example here, that means that you are indeed mostly flagging the odd samples that lie very far outside of the cluster centroids. These are definitely outliers and can thus be described as abnormal or suspicious. However, keep in mind that it doesn’t necessarily mean that these observations are also fraudulent. They are, compared to the majority of normal behavior, just odd.</p>
</section>
<section id="flagging-fraud-based-on-distance-to-centroid" class="level4">
<h4 class="anchored" data-anchor-id="flagging-fraud-based-on-distance-to-centroid">Flagging fraud based on distance to centroid</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2239973a-7069-46d7-8df7-586d13c03f3c-6-dbef3c24-b8f9-4299-87bf-af7a95532422.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">fraud_centroid_dist.JPG</figcaption>
</figure>
</div>
<p>In python, the steps to do this are exactly the steps that I’ve just described in pictures. It all starts with your trained clustering model, in this case, k-means. You then need to assign to which cluster each data point belongs to with the predict function, and store those results. Next, you need to save the cluster-centers with this function. Then, it’s time to calculate the distance of each data point to its cluster centroid. As you can see, I use the norm function from NumPy’s linear algebra package, which returns the vector norm, ie the vector of distance for each data point to their assigned cluster. Last, you use the percentiles of the distances to determine which samples are outliers. Here, I take the 93rd percentile using NumPy’s percentile function, and flag it with a one if it is bigger than that. Those are the final fraud predictions.</p>
</section>
<section id="validating-your-model-results" class="level4">
<h4 class="anchored" data-anchor-id="validating-your-model-results">Validating your model results</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/2239973a-7069-46d7-8df7-586d13c03f3c-2-45b76979-eb40-41e2-809f-04f7ee9c8f40.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">validate.JPG</figcaption>
</figure>
</div>
<p>Normally, this is where it gets difficult. If you don’t have original fraud labels, you can’t run the usual performance metrics, hence you need some other way to sense check your results. The best way to do so is to collaborate closely with your fraud expert, and let them have a look at the predictions and investigate further. Second, you want to understand why these cases are outliers. Are they truly fraudulent or just very rare cases of legit data in your sample? If it is just a rare but non-fraudulent cases, you can avoid that by deleting certain features, or removing those cases from the data altogether. If you do have some past cases of fraud, a good way is to see whether your model can actually predict those when you test your model on historic data. In the exercises, you’ll use original fraud labels to check our model performance, but do keep in mind this is usually not possible.</p>
</section>
</section>
<section id="detecting-outliers" class="level3">
<h3 class="anchored" data-anchor-id="detecting-outliers">3.3.1 Detecting outliers</h3>
<p>Let’s use the K-means algorithm to predict fraud, and compare those predictions to the actual labels that are saved, to sense check our results. The fraudulent transactions are typically flagged as the observations that are furthest aways from the cluster centroid. We’ll learn how to do this and how to determine the cut-off.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test set</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_scaled, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define K-means model </span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> MiniBatchKMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit(X_train)</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain predictions and calculate distance from cluster centroid</span></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>X_test_clusters <span class="op">=</span> kmeans.predict(X_test)</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>X_test_clusters_centers <span class="op">=</span> kmeans.cluster_centers_</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> [np.linalg.norm(x<span class="op">-</span>y) <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(X_test, X_test_clusters_centers[X_test_clusters])]</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create fraud predictions based on outliers on clusters </span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>km_y_pred <span class="op">=</span> np.array(dist)</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>km_y_pred[dist <span class="op">&gt;=</span> np.percentile(dist, <span class="dv">95</span>)] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>km_y_pred[dist <span class="op">&lt;</span> np.percentile(dist, <span class="dv">95</span>)] <span class="op">=</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/stephen137/mambaforge/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(</code></pre>
</div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the ROC score</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(roc_auc_score(y_test,km_y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.8197704982668266</code></pre>
</div>
</div>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a confusion matrix</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>km_cm <span class="op">=</span> confusion_matrix(y_test, km_y_pred)</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the confusion matrix in a figure to visualize results </span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_confusion_matrix(km_cm):</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>  df_cm <span class="op">=</span> pd.DataFrame(km_cm, [<span class="st">'True Normal'</span>,<span class="st">'True Fraud'</span>],[<span class="st">'Pred Normal'</span>,<span class="st">'Pred Fraud'</span>])</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>  plt.figure(figsize <span class="op">=</span> (<span class="dv">8</span>,<span class="dv">4</span>))</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>  sn.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.4</span>) </span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>  sn.heatmap(df_cm, annot<span class="op">=</span><span class="va">True</span>,annot_kws<span class="op">=</span>{<span class="st">"size"</span>: <span class="dv">16</span>},fmt<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>plot_confusion_matrix(km_cm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Fraud_detection_in_Python_files/figure-html/cell-63-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>If we were to lower the threshold for cases flagged as fraud from the current 95th percentile, we would flag more cases overall but thereby also get more false positives.</p>
</section>
<section id="other-clustering-fraud-detection-methods" class="level3">
<h3 class="anchored" data-anchor-id="other-clustering-fraud-detection-methods">3.4 Other clustering fraud detection methods</h3>
<section id="there-are-many-different-clustering-methods" class="level4">
<h4 class="anchored" data-anchor-id="there-are-many-different-clustering-methods">There are many different clustering methods</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/4a01ac45-3281-441a-92d3-e49374a05564-1-1424f11c-0aab-49c7-a447-f882b46bf558.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">cluster_alg.JPG</figcaption>
</figure>
</div>
<p>Each clustering method has its pros and cons. K-means works well when your data is clustered in normal, round shapes. As you can see in this picture, when data is clustered in very different shapes, it does not perform so well. In this picture, you see the clustering method DBSCAN performing quite well, in fact.</p>
</section>
<section id="and-different-ways-of-flagging-fraud-using-smallest-clusters" class="level4">
<h4 class="anchored" data-anchor-id="and-different-ways-of-flagging-fraud-using-smallest-clusters">And different ways of flagging fraud: using smallest clusters</h4>
<p>Apart from other clustering methods, there are also other ways to flag fraud, not just based on cluster outliers. Rather than treating fraud as the oddball outlier in the existing clusters, you can also use the smallest clusters as an indication of fraud, as pictured here.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/4a01ac45-3281-441a-92d3-e49374a05564-2-762d13be-bfd4-47d8-919b-bd6ebb41fe66.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">fraud_small_cluster.JPG</figcaption>
</figure>
</div>
<p>You can use this approach when fraudulent behavior has commonalities, and thus will cluster together in your data. In that sense, you would expect it to cluster in tiny groups, rather than be the outliers in the larger clusters.</p>
</section>
<section id="in-reality-it-looks-more-like-this" class="level4">
<h4 class="anchored" data-anchor-id="in-reality-it-looks-more-like-this">In reality it looks more like this</h4>
<p>The previous image was a perfect world example, but in reality, you will likely be looking at data that looks more like this.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/4a01ac45-3281-441a-92d3-e49374a05564-4-c5ba42d6-6f6f-48c0-a86c-1cc21e1ba708.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">reality.JPG</figcaption>
</figure>
</div>
<p>In this case, you see three obvious clusters, and a few dots that are clearly separate from the rest. As you can see, those smallest dots are outliers and outside of what you would describe as normal behavior. However, there are also medium to small clusters closely connected to the red cluster, so it’s not very straightforward. In fact, if you can visualize your data with, for example, PCA, it can be quite helpful to do so.</p>
</section>
<section id="dbscan-versus-k-means" class="level4">
<h4 class="anchored" data-anchor-id="dbscan-versus-k-means">DBSCAN versus K-means</h4>
<p>So let’s talk a bit more about DBSCAN. DBSCAN stands for <code>Density-Based Spatial Clustering of Applications with Noise</code>. One benefit is that you do not need to predefine the number of clusters. The algorithm finds core samples of high density and expands clusters from them. This works well on data which contains clusters of similar density. <strong>This is a type of algorithm you can use to identify fraud as very small clusters.</strong></p>
<p>Things you do need to assign in the DBSCAN model are the maximum allowed distance between data within clusters, and the minimal number of data points in clusters. As you already saw before, DBSCAN performs well on weirdly shaped data, but is computationally much heavier than, for example, mini-batch K-means.</p>
</section>
<section id="implementing-dbscan" class="level4">
<h4 class="anchored" data-anchor-id="implementing-dbscan">Implementing DBSCAN</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/4a01ac45-3281-441a-92d3-e49374a05564-3-acf3a65f-1188-4ca2-a451-73095152c153.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">DBSCAN.JPG</figcaption>
</figure>
</div>
<p>Implementing DBSCAN is relatively straightforward.</p>
<p>You start by defining:</p>
<ul>
<li>epsilon, <code>eps</code>. This is the <code>distance between data points</code> allowed from which the cluster expands.</li>
<li>minimum samples in the cluster.</li>
</ul>
<p>Conventional DBSCAN cannot produce the optimal value of epsilon, and it requires sophisticated DBSCAN modifications to determine the optimal epsilon value automatically, which is beyond the scope of this course. You need to fit DBSCAN to your scaled data. You can use the labels function to get the assigned cluster labels for each data point. You can also count the number of clusters by counting the unique cluster labels from the cluster label predictions. I use the length of the predicted labels here to do so, but you can do this in different ways.</p>
</section>
<section id="checking-the-size-of-the-clusters" class="level4">
<h4 class="anchored" data-anchor-id="checking-the-size-of-the-clusters">Checking the size of the clusters</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/4a01ac45-3281-441a-92d3-e49374a05564-5-f0d13b18-af6c-48d1-83e8-d8617700ee32.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">cluster_size.JPG</figcaption>
</figure>
</div>
<p>The DBSCAN model can also have performance metrics, such as the average silhouette score. Suppose you want to calculate the size of each cluster. You can use NumPy’s <code>.bincount()</code> function for this.</p>
<p>Bincount counts the number of occurrences of each value in a NumPy array, but only works on non-negative numbers. You can use this to calculate the size of each cluster. From here, you can sort on size and decide how many of the smaller clusters you want to flag as fraud. This last bit is trial and error, and will also depend on how many fraud cases the fraud team can deal with on a regular basis.</p>
</section>
</section>
<section id="dbscan" class="level3">
<h3 class="anchored" data-anchor-id="dbscan">3.4.1 DBSCAN</h3>
<p>The advantage of DBSCAN is that you do not need to define the number of clusters beforehand. Also, DBSCAN can handle weirdly shaped data (i.e.&nbsp;non-convex) much better than K-means can. This time, you are not going to take the outliers of the clusters and use that for fraud, but take the smallest clusters in the data and label those as fraud.</p>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import DBSCAN</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and fit the DBSCAN model</span></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>db <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.9</span>, min_samples<span class="op">=</span><span class="dv">10</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>).fit(X_scaled)</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain the predicted labels and calculate number of clusters</span></span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>pred_labels <span class="op">=</span> db.labels_</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="bu">len</span>(<span class="bu">set</span>(pred_labels)) <span class="op">-</span> (<span class="dv">1</span> <span class="cf">if</span> <span class="op">-</span><span class="dv">1</span> <span class="kw">in</span> y <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Print performance metrics for DBSCAN</span></span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Estimated number of clusters: </span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> n_clusters)</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Homogeneity: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> homogeneity_score(y, pred_labels))</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Silhouette Coefficient: </span><span class="sc">%0.3f</span><span class="st">"</span> <span class="op">%</span> silhouette_score(X_scaled, pred_labels))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated number of clusters: 23
Homogeneity: 0.612
Silhouette Coefficient: 0.713</code></pre>
</div>
</div>
<p>The number of clusters is much higher than with K-means. For fraud detection this is for now OK, as we are only interested in the smallest clusters, since those are considered as abnormal. Now let’s have a look at those clusters and decide which one to flag as fraud.</p>
</section>
<section id="assessing-smallest-clusters" class="level3">
<h3 class="anchored" data-anchor-id="assessing-smallest-clusters">3.4.2 Assessing smallest clusters</h3>
<p>We need to:</p>
<ul>
<li>figure out how big the clusters are, and filter out the smallest</li>
<li>take the smallest ones and flag those as fraud</li>
<li>check with the original labels whether this does actually do a good job in detecting fraud</li>
</ul>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count observations in each cluster number</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.bincount(pred_labels[pred_labels <span class="op">&gt;=</span> <span class="dv">0</span>])</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(counts)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[3252  145 2714   55  174  119  122   98   54   15   76   15   43   25
   51   47   42   15   25   20   19   10]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort the sample counts of the clusters and take the top 3 smallest clusters</span></span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>smallest_clusters <span class="op">=</span> np.argsort(counts)[:<span class="dv">3</span>]</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results </span></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The smallest clusters are clusters:"</span>)      </span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(smallest_clusters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The smallest clusters are clusters:
[21 17  9]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the counts of the smallest clusters only</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Their counts are:"</span>)      </span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(counts[smallest_clusters])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Their counts are:
[10 15 15]</code></pre>
</div>
</div>
<p>So now we know which smallest clusters we could flag as fraud. If we were to take more of the smallest clusters, we cast our net wider and catch more fraud, but most likely also more false positives. It is up to the fraud analyst to find the right amount of cases to flag and to investigate</p>
</section>
<section id="checking-results" class="level3">
<h3 class="anchored" data-anchor-id="checking-results">3.4.3 Checking results</h3>
<p>We wil now check the results of your DBSCAN fraud detection model. In reality, we often don’t have reliable labels and this where a fraud analyst can help you validate the results. They can check your results and see whether the cases you flagged are indeed suspicious. WE can also check historically known cases of fraud and see whether our model flags them.</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataframe of the predicted cluster numbers and fraud labels </span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'clusternr'</span>:pred_labels,<span class="st">'fraud'</span>:y})</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a condition flagging fraud for the smallest clusters </span></span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'predicted_fraud'</span>] <span class="op">=</span> np.where((df[<span class="st">'clusternr'</span>]<span class="op">==</span><span class="dv">21</span>)<span class="op">|</span> (df[<span class="st">'clusternr'</span>]<span class="op">==</span><span class="dv">17</span>) <span class="op">|</span> (df[<span class="st">'clusternr'</span>]<span class="op">==</span><span class="dv">9</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a crosstab on the results </span></span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pd.crosstab(df[<span class="st">'fraud'</span>], df[<span class="st">'predicted_fraud'</span>], rownames<span class="op">=</span>[<span class="st">'Actual Fraud'</span>], colnames<span class="op">=</span>[<span class="st">'Flagged Fraud'</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Flagged Fraud     0   1
Actual Fraud           
0              6973  16
1               176  24</code></pre>
</div>
</div>
<p>The good thing is: out of all flagged cases, 16 out of 40, or 60% are actually fraud. Since we only take the three smallest clusters, by definition we flag less cases of fraud, so we catch less but also have less false positives. However, we are missing quite a lot of fraud cases. Increasing the amount of smallest clusters we flag could improve that, at the cost of more false positives of course.</p>
<p>In the next section we will learn how to further improve fraud detection models by including text analysis.</p>
</section>
</section>
<section id="fraud-detection-using-text" class="level2">
<h2 class="anchored" data-anchor-id="fraud-detection-using-text">4. Fraud detection using text</h2>
<p>This section focuses on using unsupervised learning techniques to detect fraud. We will segment customers, use K-means clustering and other clustering algorithms to find suspicious occurrences in our data.</p>
<section id="using-text-data" class="level3">
<h3 class="anchored" data-anchor-id="using-text-data">4.1 Using text data</h3>
<section id="you-will-often-encounter-text-data-during-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="you-will-often-encounter-text-data-during-fraud-detection">You will often encounter text data during fraud detection</h4>
<p>During fraud analysis, almost always, there will be some sort of textual data available that you can use to your advantage. It can be emails between the company and its clients, or emails within the company. Descriptions of bank transactions are a good example also. It can be that a client service team leaves notes on a client account. Insurance claims are full of textual data, and it may even be the case that there are recorded telephone conversations. And this list goes on. It is, therefore, important to know how to handle textual data, when working on fraud detection problems.</p>
</section>
<section id="text-mining-techniques-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="text-mining-techniques-for-fraud-detection">Text mining techniques for fraud detection</h4>
<p>When using text data in fraud analysis, there are multiple ways to use the data. A common application is a word search, to flag any data mentioning certain terms. <code>Sentiment analysis</code>, aka measuring how positive or negative a text is, can be another interesting application, which you could also combine with a word search. More straightforward, you can check whether text data associated with fraud tends to be more positive or negative, relative to the normal text. Topic analysis and counting the frequency of certain words of interest, is another powerful application for fraud detection. We will cover this in more detail later. A last way to use text data is to analyze the style of fraud data and search for text that is similar in style to flag for fraud.</p>
</section>
<section id="word-search-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="word-search-for-fraud-detection">Word search for fraud detection</h4>
<p>Suppose you want to flag all client transactions that mention a certain gambling company, as this company has received bad press lately. A simple word search on all transactions and client emails can easily filter whether any of your clients mention this company. You can then either use these results as a filter, or a flag on its own, or simply as an additional feature in your machine learning model. You can do all this with a few simple lines of code. Let’s have a look at how it’s done.</p>
</section>
<section id="word-counts-to-flag-fraud-with-pandas" class="level4">
<h4 class="anchored" data-anchor-id="word-counts-to-flag-fraud-with-pandas">Word counts to flag fraud with pandas</h4>
<p>Pandas has functions that allow you to do operations on text data within a pandas series or DataFrame.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/74722787-7565-4e1a-9dc3-7ad0e0d5f562-1-1bc4a23d-9a59-4aae-95d1-e5b6812a1476.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">pandas_word_count.JPG</figcaption>
</figure>
</div>
<p>In this example, I use the string-dot-contains function to find all rows that contain the words money laundering. You can very easily use this command to select the rows that contain these words in your DataFrame. You need to use na equals False to ignore all rows containing missing values, otherwise the indexer won’t work. Suppose you want to filter on a list of words, rather than just one. This is also easily done, by the using string contains function. You need to join the list of words with the or command, so you search on whether the text contains this or that word. From then on it is easy to create a flag for data that contain these words. By using the NumPy where function, you can simply create a new variable that flags one where the condition is met, and zero otherwise.</p>
</section>
</section>
<section id="word-search-with-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="word-search-with-dataframes">4.1.1 Word search with dataframes</h3>
<p>In this exercise we will work with text data, containing emails from Enron employees. The Enron scandal is a famous fraud case. Enron employees covered up the bad financial position of the company, thereby keeping the stock price artificially high. Enron employees sold their own stock options, and when the truth came out, Enron investors were left with nothing. The goal is to find all emails that mention specific words, such as “sell enron stock”.</p>
<p>By using string operations on dataframes, we can easily sift through messy email data and create flags based on word-hits.</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span> pd.read_csv(<span class="st">'data/enron_emails_clean.csv'</span>)</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>2090</code></pre>
</div>
</div>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Message-ID</th>
<th data-quarto-table-cell-role="th">From</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Date</th>
<th data-quarto-table-cell-role="th">content</th>
<th data-quarto-table-cell-role="th">clean_content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>&lt;8345058.1075840404046.JavaMail.evans@thyme&gt;</td>
<td>('advdfeedback@investools.com')</td>
<td>('advdfeedback@investools.com')</td>
<td>2002-01-29 23:20:55</td>
<td>INVESTools Advisory\nA Free Digest of Trusted ...</td>
<td>investools advisory free digest trusted invest...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>&lt;1512159.1075863666797.JavaMail.evans@thyme&gt;</td>
<td>('richard.sanders@enron.com')</td>
<td>('richard.sanders@enron.com')</td>
<td>2000-09-20 19:07:00</td>
<td>----- Forwarded by Richard B Sanders/HOU/ECT o...</td>
<td>forwarded richard b sanders hou ect pm justin ...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>&lt;26118676.1075862176383.JavaMail.evans@thyme&gt;</td>
<td>('m..love@enron.com')</td>
<td>('m..love@enron.com')</td>
<td>2001-10-30 16:15:17</td>
<td>hey you are not wearing your target purple shi...</td>
<td>hey wearing target purple shirt today mine wan...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>&lt;10369289.1075860831062.JavaMail.evans@thyme&gt;</td>
<td>('leslie.milosevich@kp.org')</td>
<td>('leslie.milosevich@kp.org')</td>
<td>2002-01-30 17:54:18</td>
<td>Leslie Milosevich\n1042 Santa Clara Avenue\nAl...</td>
<td>leslie milosevich santa clara avenue alameda c...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>&lt;26728895.1075860815046.JavaMail.evans@thyme&gt;</td>
<td>('rtwait@graphicaljazz.com')</td>
<td>('rtwait@graphicaljazz.com')</td>
<td>2002-01-30 19:36:01</td>
<td>Rini Twait\n1010 E 5th Ave\nLongmont, CO 80501...</td>
<td>rini twait e th ave longmont co rtwait graphic...</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find all cleaned emails that contain 'sell enron stock'</span></span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> df[<span class="st">'clean_content'</span>].<span class="bu">str</span>.contains(<span class="st">'sell enron stock'</span>, na<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>mask.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>clean_content
False    2089
True        1
Name: count, dtype: int64</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the data from df using the mask</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.loc[mask])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                       Message-ID                        From  \
154  &lt;6336501.1075841154311.JavaMail.evans@thyme&gt;  ('sarah.palmer@enron.com')   

                             To                 Date  \
154  ('sarah.palmer@enron.com')  2002-02-01 14:53:35   

                                               content  \
154  \nJoint Venture: A 1997 Enron Meeting Belies O...   

                                         clean_content  
154  joint venture enron meeting belies officers cl...  </code></pre>
</div>
</div>
<p>We can see that searching for particular string values in a dataframe can be relatively easy, and allows us to include textual data into our model or analysis. We can use this word search as an additional flag, or as a feauture in our fraud detection model. Let’s now have a look at how to filter the data using multiple search terms.</p>
</section>
<section id="using-list-of-terms" class="level3">
<h3 class="anchored" data-anchor-id="using-list-of-terms">4.1.2 Using list of terms</h3>
<p>Sometimes you don’t want to search on just one term. You probably can create a full “fraud dictionary” of terms that could potentially flag fraudulent clients and/or transactions. Fraud analysts often will have an idea what should be in such a dictionary. In this example we will flag a multitude of terms, and create a new flag variable out of it. The ‘flag’ can be used either directly in a machine learning model as a feature, or as an additional filter on top of our machine learning model results.</p>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a list of terms to search for</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>searchfor <span class="op">=</span> [<span class="st">'enron stock'</span>, <span class="st">'sell stock'</span>, <span class="st">'stock bonus'</span>, <span class="st">'sell enron stock'</span>]</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter cleaned emails on searchfor list and select from df </span></span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>filtered_emails <span class="op">=</span> df.loc[df[<span class="st">'clean_content'</span>].<span class="bu">str</span>.contains(<span class="st">'|'</span>.join(searchfor), na<span class="op">=</span><span class="va">False</span>)]</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(filtered_emails)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                         Message-ID  \
0      &lt;8345058.1075840404046.JavaMail.evans@thyme&gt;   
1      &lt;1512159.1075863666797.JavaMail.evans@thyme&gt;   
2     &lt;26118676.1075862176383.JavaMail.evans@thyme&gt;   
3     &lt;10369289.1075860831062.JavaMail.evans@thyme&gt;   
4     &lt;26728895.1075860815046.JavaMail.evans@thyme&gt;   
...                                             ...   
1151  &lt;15875618.1075860830584.JavaMail.evans@thyme&gt;   
1450  &lt;30798399.1075841348382.JavaMail.evans@thyme&gt;   
1473    &lt;957052.1075861359136.JavaMail.evans@thyme&gt;   
1557  &lt;18936682.1075861158419.JavaMail.evans@thyme&gt;   
1621   &lt;5472336.1075841501893.JavaMail.evans@thyme&gt;   

                                   From                                 To  \
0       ('advdfeedback@investools.com')    ('advdfeedback@investools.com')   
1         ('richard.sanders@enron.com')      ('richard.sanders@enron.com')   
2                 ('m..love@enron.com')              ('m..love@enron.com')   
3          ('leslie.milosevich@kp.org')       ('leslie.milosevich@kp.org')   
4          ('rtwait@graphicaljazz.com')       ('rtwait@graphicaljazz.com')   
...                                 ...                                ...   
1151             ('bandersn@loyno.edu')             ('bandersn@loyno.edu')   
1450       ('chairman.enron@enron.com')       ('chairman.enron@enron.com')   
1473         ('chairman.ken@enron.com')         ('chairman.ken@enron.com')   
1557      ('resources.human@enron.com')      ('resources.human@enron.com')   
1621  ('announcements.enron@enron.com')  ('announcements.enron@enron.com')   

                     Date                                            content  \
0     2002-01-29 23:20:55  INVESTools Advisory\nA Free Digest of Trusted ...   
1     2000-09-20 19:07:00  ----- Forwarded by Richard B Sanders/HOU/ECT o...   
2     2001-10-30 16:15:17  hey you are not wearing your target purple shi...   
3     2002-01-30 17:54:18  Leslie Milosevich\n1042 Santa Clara Avenue\nAl...   
4     2002-01-30 19:36:01  Rini Twait\n1010 E 5th Ave\nLongmont, CO 80501...   
...                   ...                                                ...   
1151  2002-01-30 17:54:12  Blanca Anderson\n1310 Cadiz\nNew orleans, LA 7...   
1450  2002-01-16 14:45:55  \nEnron announced today that its common stock ...   
1473  2001-11-09 23:48:54  \nToday, we announced plans to merge with Dyne...   
1557  2001-11-25 23:15:46  We've updated the Merger Q&amp;A document on our E...   
1621  2002-01-23 20:51:34  \nPLEASE READ THIS IMPORTANT INFORMATION CONCE...   

                                          clean_content  
0     investools advisory free digest trusted invest...  
1     forwarded richard b sanders hou ect pm justin ...  
2     hey wearing target purple shirt today mine wan...  
3     leslie milosevich santa clara avenue alameda c...  
4     rini twait e th ave longmont co rtwait graphic...  
...                                                 ...  
1151  blanca anderson cadiz new orleans la bandersn ...  
1450  enron announced today common stock traded coun...  
1473  today announced plans merge dynegy major playe...  
1557  updated merger q document enron updates site h...  
1621  please read important information concerning e...  

[314 rows x 6 columns]</code></pre>
</div>
</div>
<p>By joining the search terms with the ‘or’ sign, i.e.&nbsp;<code>|</code>, we can search on a multitude of terms in our dataset very easily. Let’s now create a flag from this which we can use as a feature in a machine learning model.</p>
</section>
<section id="creating-a-flag" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-flag">4.1.3 Creating a flag</h3>
<p>Let’s create an actual flag variable that gives a <code>1</code> when the emails get a hit on the search terms of interest, and <code>0</code> otherwise. This is the last step we need to make in order to actually use the text data content as a feature in a machine learning model, or as an actual flag on top of model results.</p>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create flag variable where the emails match the searchfor terms</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'flag'</span>] <span class="op">=</span> np.where((df[<span class="st">'clean_content'</span>].<span class="bu">str</span>.contains(<span class="st">'|'</span>.join(searchfor)) <span class="op">==</span> <span class="va">True</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the values of the flag variable</span></span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> df[<span class="st">'flag'</span>].value_counts()</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(count)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>flag
0    1776
1     314
Name: count, dtype: int64</code></pre>
</div>
</div>
<p>We have now managed to search for a list of strings in several lines of text data. These skills come in handy when you want to flag certain words based on what you discovered in your topic model, or when you know beforehand what you want to search for. In the next sections we will learn how to clean text data and to create our own topic model to further look for indications of fraud in our text data.</p>
</section>
<section id="text-mining-to-detect-fraud" class="level3">
<h3 class="anchored" data-anchor-id="text-mining-to-detect-fraud">4.2 Text mining to detect fraud</h3>
<section id="cleaning-your-text-data" class="level4">
<h4 class="anchored" data-anchor-id="cleaning-your-text-data">Cleaning your text data</h4>
<p>Whenever you work with text data, be it for word search, topic modeling, sentiment analysis, or text style, you need to do some rigorous text cleaning in order to be able to work with the data. Here are four steps you must take, before working with the data further :</p>
<ol type="1">
<li><p>split the text into sentences, and the sentences into words. Transform everything into lowercase and remove punctuation. This is called tokenization.</p></li>
<li><p>remove all stopwords as they mess up your data. Luckily, there are dictionaries for this to help you do that.</p></li>
<li><p>lemmatize words. For example, this means changing words from third person into first person, changing verbs in past and future tenses into present tenses. This allows you to combine all words that point to the same thing.</p></li>
<li><p>all verbs need to be stemmed, such that they are reduced to their root form. For example, walking and walked are reduced to just their stem, walk.</p></li>
</ol>
<p>When you take these four steps, it allows you to go from this type of data:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/768f6099-8484-43f7-b871-c567c3982ba4-3-831e356d-4bce-49cc-a81f-ed5b90bd503e.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">dirty_data.JPG</figcaption>
</figure>
</div>
<p>to this nice, clean, structured list of words per dataframe row:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/768f6099-8484-43f7-b871-c567c3982ba4-1-06d51e92-7d9c-495b-9188-c43c109711aa.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">clean_text.JPG</figcaption>
</figure>
</div>
</section>
<section id="data-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h4>
<p>You begin by tokenizing the text data into words. Tokenizers divide strings into lists of substrings. The standard <code>nltk</code> word tokenizer can be used to find the words and punctuation in a string. It splits the words on, for example, white space, and separates the punctuations out.</p>
<p>You then use <code>rstrip</code> to remove all the whitespaces from the beginning and end of the strings, and, finally, you make sure all text is lower case, by replacing all letters with their lowercase counterpart, using <code>regular expressions</code>.</p>
<p>You then clean the text further by removing stopwords and punctuation. You can use the tokenized text and get rid of all punctuation. Nltk has a stopwords list for the English language that you can use. Since every row consists of a list of strings, you need to create small loops that select the words you want to keep. I use join here to separate the words I want to keep with a space.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/768f6099-8484-43f7-b871-c567c3982ba4-4-88807839-c260-4275-93ba-19f2b0798daf.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">tokenization.JPG</figcaption>
</figure>
</div>
<p>The next step is to <strong>lemmatize</strong> the words, this can be easily done with the nltk <code>WordNetLemmatizer</code>. Again, I loop over the words and make sure they are joined together with a space between them. Stemming your verbs is equally simple, you can use the nltk <code>PorterStemmer</code> for this. After this work your text data should be nice and clean. It consists of lists of cleaned words for each row in your dataframe.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/768f6099-8484-43f7-b871-c567c3982ba4-2-419167a0-1d32-4494-b0ef-6dec8c618427.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">lemmatize.JPG</figcaption>
</figure>
</div>
</section>
</section>
<section id="removing-stopwords" class="level3">
<h3 class="anchored" data-anchor-id="removing-stopwords">4.2.1 Removing stopwords</h3>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/text.csv'</span>)</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">clean_content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>enrolled following class electric business und...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>start date hourahead hour ancillary schedules ...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>information contained herein based sources bel...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>list looks complete walt zimmerman eott com pm...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>reid spice hayes st san francisco ca reidspice...</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>In the following exercises we will clean the Enron emails, in order to be able to use the data in a topic model. Text cleaning can be challenging, so we’ll learn some steps to do this well.</p>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import nltk packages and string</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'omw-1.4'</span>)</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define stopwords to exclude</span></span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>stop <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a>stop.update((<span class="st">"to"</span>,<span class="st">"cc"</span>,<span class="st">"subject"</span>,<span class="st">"http"</span>,<span class="st">"from"</span>,<span class="st">"sent"</span>, <span class="st">"ect"</span>, <span class="st">"u"</span>, <span class="st">"fwd"</span>, <span class="st">"www"</span>, <span class="st">"com"</span>))</span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define punctuations to exclude and lemmatizer</span></span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a>exclude <span class="op">=</span> <span class="bu">set</span>(string.punctuation)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package omw-1.4 to
[nltk_data]     /home/stephen137/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!</code></pre>
</div>
</div>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the lemmatizer from nltk</span></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem.wordnet <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>lemma <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define word cleaning function</span></span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean(text, stop):</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.rstrip()</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>    stop_free <span class="op">=</span> <span class="st">" "</span>.join([i <span class="cf">for</span> i <span class="kw">in</span> text.lower().split() <span class="cf">if</span>((i <span class="kw">not</span> <span class="kw">in</span> stop) <span class="kw">and</span> (<span class="kw">not</span> i.isdigit()))])</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a>    punc_free <span class="op">=</span> <span class="st">''</span>.join(i <span class="cf">for</span> i <span class="kw">in</span> stop_free <span class="cf">if</span> i <span class="kw">not</span> <span class="kw">in</span> exclude)</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a>    normalized <span class="op">=</span> <span class="st">" "</span>.join(lemma.lemmatize(i) <span class="cf">for</span> i <span class="kw">in</span> punc_free.split())      </span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> normalized</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean the emails in df and print results</span></span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a>text_clean<span class="op">=</span>[]</span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> df[<span class="st">'clean_content'</span>]:</span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a>    text_clean.append(clean(text, stop).split())    </span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text_clean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[['enrolled', 'following', 'class', 'electric', 'business', 'understanding', 'class', 'day', 'time', 'pm', 'room', 'location', 'eb', 'houston', 'last', 'day', 'cancel', 'participant', 'fee', 'note', 'please', 'review', 'prerequisite', 'material', 'validate', 'eligibility', 'class', 'company', 'rc', 'charged', 'question', 'please', 'call', 'development', 'center', 'team', 'thank'], ['start', 'date', 'hourahead', 'hour', 'ancillary', 'schedule', 'awarded', 'variance', 'detected', 'log', 'message', 'parsing', 'file', 'portland', 'westdesk', 'california', 'scheduling', 'iso', 'final', 'schedule', 'txt', 'error', 'invalid', 'variant', 'type', 'conversion', 'energy', 'import', 'export', 'schedule', 'final', 'schedule', 'found', 'preferred', 'schedule', 'detail', 'trans', 'type', 'final', 'sc', 'id', 'ectstca', 'mkt', 'type', 'trans', 'date', 'tie', 'point', 'pverde', 'devers', 'interchg', 'id', 'epmi', 'ciso', 'engy', 'type', 'firm'], ['information', 'contained', 'herein', 'based', 'source', 'believe', 'reliable', 'represent', 'accurate', 'complete', 'nothing', 'contained', 'herein', 'considered', 'offer', 'sell', 'solicitation', 'offer', 'buy', 'financial', 'instrument', 'discussed', 'herein', 'opinion', 'expressed', 'herein', 'solely', 'author', 'may', 'differ', 'material', 'respect', 'expressed', 'published', 'behalf', 'carr', 'future', 'officer', 'director', 'employee', 'affiliate', 'carr', 'future', 'chart', 'available', 'web', 'clicking', 'hot', 'link', 'contained', 'email', 'reason', 'unable', 'receive', 'chart', 'via', 'web', 'please', 'contact', 'via', 'email', 'email', 'chart', 'attachment', 'crude', 'carrfut', 'research', 'energy', 'crude', 'pdf', 'natural', 'gas', 'carrfut', 'research', 'energy', 'nga', 'pdf', 'feb', 'wti', 'brent', 'spread', 'carrfut', 'research', 'energy', 'clg', 'qog', 'pdf', 'feb', 'heat', 'crack', 'carrfut', 'research', 'energy', 'heatcrack', 'pdf', 'feb', 'gas', 'crack', 'carrfut', 'research', 'energy', 'gascrack', 'pdf', 'feb', 'gas', 'heat', 'spread', 'carrfut', 'research', 'energy', 'hug', 'hog', 'pdf', 'june', 'gas', 'heat', 'spread', 'carrfut', 'research', 'energy', 'hum', 'hom', 'pdf', 'march', 'gas', 'heat', 'spread', 'carrfut', 'research', 'energy', 'huh', 'hoh', 'pdf', 'feb', 'may', 'unlead', 'spread', 'carrfut', 'research', 'energy', 'hug', 'huk', 'pdf', 'feb', 'july', 'crude', 'oil', 'spread', 'carrfut', 'research', 'energy', 'clg', 'cln', 'pdf', 'nat', 'gas', 'strip', 'matrix', 'carrfut', 'research', 'energy', 'stripmatrixng', 'pdf', 'nat', 'gas', 'spread', 'matrix', 'carrfut', 'research', 'energy', 'spreadmatrixng', 'pdf', 'crude', 'product', 'spread', 'matrix', 'carrfut', 'research', 'energy', 'spreadmatrixcl', 'pdf'], ['list', 'look', 'complete', 'walt', 'zimmerman', 'eott', 'pm', 'mary', 'ellen', 'coombe', 'eott', 'dana', 'gibbs', 'eott', 'stan', 'horton', 'eott', 'annual', 'election', 'officer', 'may', 'meeting', 'eott', 'board', 'director', 'typically', 'elect', 'eott', 'slate', 'officer', 'next', 'month', 'attached', 'draft', 'resolution', 'provides', 'reelection', 'eott', 'current', 'slate', 'officer', 'please', 'let', 'know', 'need', 'make', 'change', 'proposed', 'slate', 'included', 'board', 'material', 'thank', 'assistance', 'see', 'attached', 'file', 'w', 'doc', 'walter', 'w', 'zimmerman', 'senior', 'counsel', 'eott', 'energy', 'corp', 'p', 'box', 'houston', 'texas', 'phone', 'fax', 'e', 'mail', 'walt', 'zimmerman', 'eott', 'w', 'doc'], ['reid', 'spice', 'hayes', 'st', 'san', 'francisco', 'ca', 'reidspice', 'hotmail', 'mr', 'ken', 'lay', 'writing', 'urge', 'donate', 'million', 'dollar', 'made', 'selling', 'enron', 'stock', 'company', 'declared', 'bankruptcy', 'fund', 'enron', 'employee', 'transition', 'fund', 'reach', 'benefit', 'company', 'employee', 'lost', 'retirement', 'saving', 'provide', 'relief', 'low', 'income', 'consumer', 'california', 'afford', 'pay', 'energy', 'bill', 'enron', 'made', 'million', 'pocketbook', 'california', 'consumer', 'effort', 'employee', 'indeed', 'netted', 'well', 'million', 'many', 'enron', 'employee', 'financially', 'devastated', 'company', 'declared', 'bankruptcy', 'retirement', 'plan', 'wiped', 'enron', 'made', 'astronomical', 'profit', 'california', 'energy', 'crisis', 'last', 'year', 'result', 'thousand', 'consumer', 'unable', 'pay', 'basic', 'energy', 'bill', 'largest', 'utility', 'state', 'bankrupt', 'new', 'york', 'time', 'reported', 'sold', 'million', 'worth', 'enron', 'stock', 'aggressively', 'urging', 'company', 'employee', 'keep', 'buying', 'please', 'donate', 'money', 'fund', 'set', 'help', 'repair', 'life', 'american', 'hurt', 'enron', 'underhanded', 'dealing', 'sincerely', 'reid', 'spice']]</code></pre>
</div>
</div>
<p>Now that you have cleaned your data entirely with the necessary steps, including splitting the text into words, removing stopwords and punctuations, and lemmatizing your words. You are now ready to run a topic model on this data</p>
</section>
<section id="topic-modeling-on-fraud" class="level3">
<h3 class="anchored" data-anchor-id="topic-modeling-on-fraud">4.3 Topic modeling on fraud</h3>
<section id="topic-modeling-discover-hidden-patterns-in-text-data" class="level4">
<h4 class="anchored" data-anchor-id="topic-modeling-discover-hidden-patterns-in-text-data">Topic modeling: discover hidden patterns in text data</h4>
<p>Topic modeling can be a powerful tool when searching for fraud in text data. Topic modeling allows you to discover abstract topics that occur in a collection of documents. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently. Topic modeling, therefore, tells us in a very efficient way what the text is about, based on the words it contains. Conceptually, it is similar to clustering, as it clusters words belonging to the same topic together. If you have text data of known fraud cases, it allows you to check what are the most common topics for those fraud cases, and use that to compare unknown cases. Without known labels, you can inspect which topics seem to point to fraudulent behavior and are interesting to further investigate.</p>
</section>
<section id="latent-dirichlet-allocation-lda" class="level4">
<h4 class="anchored" data-anchor-id="latent-dirichlet-allocation-lda">Latent Dirichlet Allocation (LDA)</h4>
<p>Latent Dirichlet Allocation, or LDA, is an example of topic model and is commonly used. It creates a “topic per text item” model and “words per topic” model, which are called Dirichlet distributions. Implementation of LDA is straightforward :</p>
<ul>
<li>clean your data as described in the previous section. This is the most work.</li>
<li>create a dictionary containing which words appear how often in all of the text</li>
<li>create a corpus, containing, for each text line in your data, the count of words that appear</li>
</ul>
<p>The results you get from this model are twofold.</p>
<ol type="1">
<li>you see how each word in your total data is associated with each topic.</li>
<li>you can also see how each text item in your data associates with topics, also in the form of probabilities.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/0914e75a-80e1-43a6-8ab7-63706380f048-1-096bebf7-6962-4522-984b-4829eee45688.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">LDA.JPG</figcaption>
</figure>
</div>
<p>You can see this in the image here on the right. This image comes from a <a href="https://www.datacamp.com/tutorial/lda2vec-topic-model">blogpost on DataCamp about LDA</a>, which I encourage you to read if you want to learn more in detail.</p>
</section>
<section id="bag-of-words-dictionary-and-corpus" class="level4">
<h4 class="anchored" data-anchor-id="bag-of-words-dictionary-and-corpus">Bag of words: dictionary and corpus</h4>
<p>Let’s talk about the implementation of an LDA model. You start by importing the corpora function from gensim.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/0914e75a-80e1-43a6-8ab7-63706380f048-2-538a9921-b54c-4694-8173-1501da29450c.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">bag_of_words.JPG</figcaption>
</figure>
</div>
<p>I use the dictionary function in corpora to create a dictionary from our text data, in this case, from the cleaned emails. The dictionary contains the number of times a word appears for each word. You then filter out words that appear in less than 5 emails and keep only the 50000 most frequent words, in a way of cleaning out the outlier noise of the text data. Last, you create a corpus that tells you, for each email, how many words it contains and how many times those words appear. You can use the doc2bow function for this. Doc2bow stands for document to bag of words. This function converts our text data into a bag-of-words format. That means, each row in our data is now a list of words with their associated word count.</p>
</section>
<section id="latent-dirichlet-allocation-lda-with-gensim" class="level4">
<h4 class="anchored" data-anchor-id="latent-dirichlet-allocation-lda-with-gensim">Latent Dirichlet Allocation (LDA) with gensim</h4>
<p>After cleaning the text data, and creating dictionary and corpus, you are now ready to run your LDA model. I use gensim again for this.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/0914e75a-80e1-43a6-8ab7-63706380f048-3-d1f624c7-2056-4cb6-9b8a-e7131922d399.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">LDA_gensim.JPG</figcaption>
</figure>
</div>
<p>You need to pass the corpus and dictionary into the LDA model. As with K-means, you need to pick the number of topics you want beforehand, even if you’re not sure yet what the topics are. The LDA model calculated here, now contains the associated words for each topic, and the topic scores per email. You can obtain the top words from the three topics with the function print_topics. As you can see, after running the model, I print the three topics and the four top keywords associated with the topic, for a first interpretation of the results.</p>
</section>
</section>
<section id="create-dictionary-and-corpus" class="level3">
<h3 class="anchored" data-anchor-id="create-dictionary-and-corpus">4.3.1 Create dictionary and corpus</h3>
<p>In order to run an LDA topic model, we first need to define our dictionary and corpus first, as those need to go into the model. We will continue working on the cleaned text data from the previous section.</p>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the packages</span></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the dictionary</span></span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary(text_clean)</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the corpus </span></span>
<span id="cb138-9"><a href="#cb138-9" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(text) <span class="cf">for</span> text <span class="kw">in</span> text_clean]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print corpus and dictionary</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dictionary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dictionary&lt;289 unique tokens: ['business', 'call', 'cancel', 'center', 'charged']...&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print corpus and dictionary</span></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corpus)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)], [(33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 5), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 4), (75, 1), (76, 1), (77, 1)], [(18, 1), (21, 1), (43, 13), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 13), (90, 3), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 3), (98, 2), (99, 4), (100, 1), (101, 1), (102, 1), (103, 3), (104, 1), (105, 2), (106, 6), (107, 1), (108, 2), (109, 7), (110, 1), (111, 4), (112, 1), (113, 4), (114, 1), (115, 1), (116, 1), (117, 1), (118, 2), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 3), (129, 2), (130, 2), (131, 1), (132, 1), (133, 1), (134, 2), (135, 1), (136, 1), (137, 1), (138, 13), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 13), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 8), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 2), (160, 2), (161, 1)], [(15, 1), (18, 1), (21, 1), (22, 1), (29, 1), (43, 1), (48, 1), (94, 1), (101, 1), (129, 1), (135, 3), (162, 1), (163, 1), (164, 2), (165, 2), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 2), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 9), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 3), (204, 1), (205, 1), (206, 1), (207, 3), (208, 2), (209, 1), (210, 3)], [(6, 4), (16, 1), (21, 1), (30, 1), (35, 3), (43, 3), (104, 5), (157, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 2), (217, 1), (218, 1), (219, 2), (220, 1), (221, 1), (222, 3), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 7), (231, 1), (232, 1), (233, 3), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 3), (248, 1), (249, 4), (250, 1), (251, 1), (252, 1), (253, 1), (254, 2), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 2), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 2), (274, 1), (275, 1), (276, 2), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1)]]</code></pre>
</div>
</div>
<p>These are the two ingredients we need to run our topic model on the enron emails. We are now ready to create our fraud detection topic model.</p>
</section>
<section id="lda-model" class="level3">
<h3 class="anchored" data-anchor-id="lda-model">4.3.2 LDA model</h3>
<p>Now it’s time to build the LDA model. Using the dictionary and corpus, we are ready to discover which topics are present in the Enron emails. With a quick print of words assigned to the topics, we can do a first exploration about whether there are any obvious topics that jump out. Be mindful that the topic model is heavy to calculate so it will take a while to run. Let’s give it a try!</p>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the LDA model</span></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>ldamodel <span class="op">=</span> gensim.models.ldamodel.LdaModel(corpus, num_topics<span class="op">=</span><span class="dv">5</span>, id2word<span class="op">=</span>dictionary, passes<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the topics and top 5 words</span></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a>topics <span class="op">=</span> ldamodel.print_topics(num_words<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> topics:</span>
<span id="cb143-9"><a href="#cb143-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(topic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(0, '0.040*"enron" + 0.029*"employee" + 0.023*"company" + 0.023*"million" + 0.018*"energy"')
(1, '0.042*"schedule" + 0.034*"type" + 0.026*"final" + 0.018*"id" + 0.018*"trans"')
(2, '0.006*"eott" + 0.004*"slate" + 0.004*"energy" + 0.004*"schedule" + 0.004*"company"')
(3, '0.051*"eott" + 0.018*"please" + 0.018*"class" + 0.018*"officer" + 0.018*"zimmerman"')
(4, '0.055*"research" + 0.055*"carrfut" + 0.055*"energy" + 0.055*"pdf" + 0.034*"spread"')</code></pre>
</div>
</div>
<p>We have now successfully created a topic model on the Enron email data. However, the print of words doesn’t really give us enough information to find a topic that might lead us to signs of fraud. We need to closely inspect the model results in order to be able to detect anything that can be related to fraud in our data.</p>
</section>
<section id="flagging-fraud-based-on-topics" class="level3">
<h3 class="anchored" data-anchor-id="flagging-fraud-based-on-topics">4.4 Flagging fraud based on topics</h3>
<section id="using-our-lda-model-results-for-fraud-detection" class="level4">
<h4 class="anchored" data-anchor-id="using-our-lda-model-results-for-fraud-detection">Using our LDA model results for fraud detection</h4>
<p>If you don’t have labels, you can first of all check for frequency of suspicious words within topics, and check whether topics seem to describe the fraudulent behavior. For the Enron email data, a suspicious topic would be one where employees are discussing stock bonuses, selling stock, the Enron stock price, and perhaps mentions of accounting or weak financials.</p>
<p>Defining suspicious topics does require some pre-knowledge about the fraudulent behavior. If the fraudulent topic is noticeable, you can flag all instances that have a high probability for this topic. If you have previous cases of fraud, you could run a topic model on the fraud text only, as well as on the non-fraud text. Check whether the results are similar, ie whether the frequency of the topics are the same in fraud versus non-fraud. Lastly, you can check whether fraud cases have a higher probability score for certain topics. If so, you can run a topic model on new data and create a flag directly on the instances that score high on those topics.</p>
</section>
<section id="to-understand-topics-you-need-to-visualize" class="level4">
<h4 class="anchored" data-anchor-id="to-understand-topics-you-need-to-visualize">To understand topics, you need to visualize</h4>
<p>Interpretation of the abstract topics can sometimes be difficult, so you need good visualization tools to dive deeper and try to understand what the underlying topics mean. There is a visualization tool called <code>pyLDAvis</code> for gensim available, that does an excellent job. Be mindful though, this tool only works with Jupyter notebooks. Once you have created your model, you can create a detailed visualization in just two lines of code.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/84f35f9a-6fb5-473f-a564-ba815e81710c-1-143b775c-7146-4394-8b52-84127f540e5e.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">LDAviz.JPG</figcaption>
</figure>
</div>
<p>As you can see here, I input the model, the corpus, and the dictionary into the pyLDAvis library, and then simply display the results.</p>
</section>
<section id="inspecting-how-topics-differ" class="level4">
<h4 class="anchored" data-anchor-id="inspecting-how-topics-differ">Inspecting how topics differ</h4>
<p>The display looks like this and is interactive.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/84f35f9a-6fb5-473f-a564-ba815e81710c-2-7543f1f5-4617-43c8-a309-890ff06d7e29.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">text_viz.JPG</figcaption>
</figure>
</div>
<p>So how to interpret this output? Each bubble on the left-hand side represents a topic. The larger the bubble, the more prevalent that topic is. You can click on each topic to get the details per topic in the right panel. The words are the most important keywords that form the selected topic. A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart. A model with too many topics, will typically have many overlaps, or small sized bubbles, clustered in one region. In our case, there is a slight overlap between topic two and three, so that may point to one topic too many.</p>
</section>
<section id="assign-topics-to-your-original-data" class="level4">
<h4 class="anchored" data-anchor-id="assign-topics-to-your-original-data">Assign topics to your original data</h4>
<p>The function can be applied as follows.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/84f35f9a-6fb5-473f-a564-ba815e81710c-3-d1962a1f-3c2a-4211-9291-a2668a438067.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">topic_details_funct.JPG</figcaption>
</figure>
</div>
<p>I take the original text data and combine that with the output of the get_topic_details function. The results looks like this. Each row contains the dominant topic number, the probability score with that topic, and the original text data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Fraud_detection_in_Python_files/figure-html/84f35f9a-6fb5-473f-a564-ba815e81710c-4-febec6e2-9944-4758-9959-93b3d89d4b1b.JPG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">assign_topics.JPG</figcaption>
</figure>
</div>
</section>
</section>
<section id="finding-fraudsters-based-on-topic" class="level3">
<h3 class="anchored" data-anchor-id="finding-fraudsters-based-on-topic">4.4.1 Finding fraudsters based on topic</h3>
<p>Let’s pull together the results from the topic model back to our original data. We learned that we want to flag everything related to topic 3. As we will see, this is actually not that straightforward.</p>
<p>We will use a custom function <code>get_topic_details()</code> which takes the arguments ldamodel and corpus. It retrieves the details of the topics for each line of text. With that function, we can append the results back to our original data. If you want to learn more detail on how to work with the model results, you’re highly encouraged to read <a href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/">this article</a>.</p>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_topic_details(ldamodel, corpus):</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a>    topic_data <span class="op">=</span> []  <span class="co"># Create a list to hold the data for each row</span></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, row <span class="kw">in</span> <span class="bu">enumerate</span>(ldamodel[corpus]):</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> <span class="bu">sorted</span>(row, key<span class="op">=</span><span class="kw">lambda</span> x: (x[<span class="dv">1</span>]), reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, (topic_num, prop_topic) <span class="kw">in</span> <span class="bu">enumerate</span>(row):</span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> j <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># =&gt; dominant topic</span></span>
<span id="cb145-7"><a href="#cb145-7" aria-hidden="true" tabindex="-1"></a>                wp <span class="op">=</span> ldamodel.show_topic(topic_num)</span>
<span id="cb145-8"><a href="#cb145-8" aria-hidden="true" tabindex="-1"></a>                topic_keywords <span class="op">=</span> <span class="st">", "</span>.join([word <span class="cf">for</span> word, prop <span class="kw">in</span> wp])</span>
<span id="cb145-9"><a href="#cb145-9" aria-hidden="true" tabindex="-1"></a>                topic_data.append([<span class="bu">int</span>(topic_num), <span class="bu">round</span>(prop_topic, <span class="dv">4</span>), topic_keywords])</span>
<span id="cb145-10"><a href="#cb145-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-11"><a href="#cb145-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create the DataFrame outside the loop</span></span>
<span id="cb145-12"><a href="#cb145-12" aria-hidden="true" tabindex="-1"></a>    topic_details_df <span class="op">=</span> pd.DataFrame(topic_data, columns<span class="op">=</span>[<span class="st">'Dominant_Topic'</span>, <span class="st">'% Score'</span>, <span class="st">'Topic_Keywords'</span>])</span>
<span id="cb145-13"><a href="#cb145-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> topic_details_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run get_topic_details function and check the results</span></span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(get_topic_details(ldamodel, corpus))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Dominant_Topic  % Score                                     Topic_Keywords
0               3   0.9789  eott, please, class, officer, zimmerman, w, sl...
1               1   0.9862  schedule, type, final, id, trans, date, energy...
2               4   0.9956  research, carrfut, energy, pdf, spread, gas, f...
3               3   0.9901  eott, please, class, officer, zimmerman, w, sl...
4               0   0.9934  enron, employee, company, million, energy, con...</code></pre>
</div>
</div>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add original text to topic details in a dataframe</span></span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>contents <span class="op">=</span> pd.DataFrame({<span class="st">'Original text'</span>: text_clean})</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>topic_details <span class="op">=</span> pd.concat([get_topic_details(ldamodel, corpus), contents], axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create flag for text highest associated with topic 3</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>topic_details[<span class="st">'flag'</span>] <span class="op">=</span> np.where((topic_details[<span class="st">'Dominant_Topic'</span>] <span class="op">==</span> <span class="fl">3.0</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(topic_details.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Dominant_Topic  % Score                                     Topic_Keywords  \
0               3   0.9789  eott, please, class, officer, zimmerman, w, sl...   
1               1   0.9862  schedule, type, final, id, trans, date, energy...   
2               4   0.9956  research, carrfut, energy, pdf, spread, gas, f...   
3               3   0.9901  eott, please, class, officer, zimmerman, w, sl...   
4               0   0.9934  enron, employee, company, million, energy, con...   

                                       Original text  flag  
0  [enrolled, following, class, electric, busines...     1  
1  [start, date, hourahead, hour, ancillary, sche...     0  
2  [information, contained, herein, based, source...     0  
3  [list, look, complete, walt, zimmerman, eott, ...     1  
4  [reid, spice, hayes, st, san, francisco, ca, r...     0  </code></pre>
</div>
</div>
<p>We have now flagged all data that is highest associated with topic 3, that seems to cover internal conversation about enron stock options. This project has demonstrated that text mining and topic modeling can be a powerful tool for fraud detection.</p>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(topic_details.iloc[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dominant_Topic                                                    3
% Score                                                      0.9789
Topic_Keywords    eott, please, class, officer, zimmerman, w, sl...
Original text     [enrolled, following, class, electric, busines...
flag                                                              1
Name: 0, dtype: object</code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<section id="working-with-imbalanced-data" class="level3">
<h3 class="anchored" data-anchor-id="working-with-imbalanced-data">Working with imbalanced data</h3>
<ul>
<li>learned how to rebalance data using resampling methods such as <code>Random Over and Under Sampling</code>, and the Synthetic Minority Oversample Technique, or <code>SMOTE</code>, to improve the perforamce of fraud detections models.</li>
</ul>
</section>
<section id="fraud-detection-with-labeled-data" class="level3">
<h3 class="anchored" data-anchor-id="fraud-detection-with-labeled-data">Fraud detection with labeled data</h3>
<ul>
<li>refreshed <code>supervised</code> learning techniques such as <code>random forests</code></li>
<li>looked at <code>performance metrics</code>, such as the <code>confusion matrix</code>, and explored the importance of the <code>precision-recall</code> curve.</li>
<li>how to adjust the parameters of our machine learning models to improve fraud detection</li>
<li>how to apply ensemble methods to combine multiple machine learning models to improve performance.</li>
</ul>
<p>In reality, truly reliable labels are often not available, so applying supervised learning is not always possible.</p>
</section>
<section id="fraud-detection-without-labels-1" class="level3">
<h3 class="anchored" data-anchor-id="fraud-detection-without-labels-1">Fraud detection without labels</h3>
<ul>
<li>importance of segmentation to set a <strong>baseline</strong> for what is normal and not normal behavior in your fraud data.</li>
<li><code>K-mean</code> clustering and the <code>elbow method</code> to apply clustering to fraud.</li>
<li>how to tag suspicious behavior by using outliers of the clusters, or the smallest clusters.</li>
<li>explored the <code>DBSCAN</code> clustering method for fraud, which has the benefit of not assigning cluster numbers beforehand.</li>
</ul>
<p>In reality, there are no easy ways to check validity of clustering models. It requires strong collaboration with the fraud analysts to sense check your results. The benefits of clustering for fraud detection is that your model adapts to the current data and is therefore dynamic. It doesn’t rely on historically known fraud cases to learn from.</p>
</section>
<section id="text-mining-for-fraud-detection" class="level3">
<h3 class="anchored" data-anchor-id="text-mining-for-fraud-detection">Text mining for fraud detection</h3>
<ul>
<li>how to leverage text data into your fraud analysis, by creating flags from word searches and topic models.</li>
<li>how to properly clean text data, such that you can analyze it.</li>
</ul>
<p>In many cases of fraud detection you will have some sort of text data available, such as transaction descriptions, client emails, incident reports, etc. That makes text mining an important skill to have for fraud detection.</p>
</section>
<section id="further-learning-for-fraud-detection" class="level3">
<h3 class="anchored" data-anchor-id="further-learning-for-fraud-detection">Further learning for fraud detection</h3>
<ul>
<li><code>Network analysis</code> is an important tool not only to flag individuals committing fraud, but also to lay bare entire networks evolving around the same fraud case.</li>
<li>explore <code>neural networks</code> for fraud detection. Neural nets typically outperform other algorithms when data gets very large and complex, which can be the typical fraud data.</li>
<li>explore distributed computing systems such as <code>Spark</code> to boost your analysis.</li>
</ul>
<p>Fraud data can be incredibly large, especially if you’re working with money transactions, website traffic, or large amounts of text data.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>