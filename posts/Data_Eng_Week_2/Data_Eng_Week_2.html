<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stephen Barrie">
<meta name="dcterms.date" content="2023-02-14">

<title>Into the Unknown - Data Engineering Zoomcamp - Week 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">Stephen Barrie</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Data Engineering Zoomcamp - Week 2</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Data Lake</div>
                <div class="quarto-category">Prefect</div>
                <div class="quarto-category">DataTalksClub</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stephen Barrie </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 14, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#week-2---workflow-orchestration" id="toc-week-2---workflow-orchestration" class="nav-link active" data-scroll-target="#week-2---workflow-orchestration">Week 2 - Workflow Orchestration</a>
  <ul class="collapse">
  <li><a href="#data-lakes" id="toc-data-lakes" class="nav-link" data-scroll-target="#data-lakes">2.1.1 Data Lakes</a></li>
  <li><a href="#introduction-to-workflow-orchestration-data-flow-logistics" id="toc-introduction-to-workflow-orchestration-data-flow-logistics" class="nav-link" data-scroll-target="#introduction-to-workflow-orchestration-data-flow-logistics">2.2.1 Introduction to Workflow Orchestration (data flow logistics)</a></li>
  <li><a href="#introdution-to-prefect" id="toc-introdution-to-prefect" class="nav-link" data-scroll-target="#introdution-to-prefect">2.2.2 Introdution to Prefect</a></li>
  <li><a href="#create-prefect-gcp-blocks" id="toc-create-prefect-gcp-blocks" class="nav-link" data-scroll-target="#create-prefect-gcp-blocks">Create Prefect GCP blocks</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="week-2---workflow-orchestration" class="level2">
<h2 class="anchored" data-anchor-id="week-2---workflow-orchestration">Week 2 - Workflow Orchestration</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/4e3e08f6-0273-4434-b889-b6921482a0d6.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">week_2.JPG</figcaption><p></p>
</figure>
</div>
<section id="data-lakes" class="level3">
<h3 class="anchored" data-anchor-id="data-lakes">2.1.1 Data Lakes</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/63571a21-e79a-4984-9ae2-064605e666d9.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">data_lake.JPG</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/99ac080c-c55c-4967-9b59-6ee28c178207.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">data_lake_origin.JPG</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/8d8ec4af-26c4-4148-9113-958b9cec5bf8.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">lake_vs_warehouse.JPG</figcaption><p></p>
</figure>
</div>
<p><img src="Data_Eng_Week_2_files/figure-html/32a74388-8691-46cb-af3c-ca2f3e11c4c2.JPG" class="img-fluid" alt="ELT.JPG"> <img src="Data_Eng_Week_2_files/figure-html/e8510545-0ce2-41c5-911b-b155778c8a6b.JPG" class="img-fluid" alt="ETL.JPG"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/6ae5b11c-ef1d-4bda-bb11-92e54d61126b.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">data_lake_cloud.JPG</figcaption><p></p>
</figure>
</div>
</section>
<section id="introduction-to-workflow-orchestration-data-flow-logistics" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-workflow-orchestration-data-flow-logistics">2.2.1 Introduction to Workflow Orchestration (data flow logistics)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/8e933809-32f3-4068-b99c-4af53575eac5.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">data_engineering_map.JPG</figcaption><p></p>
</figure>
</div>
<p>Just like a physical transport logistics system, it is important to have a smooth data logistics system. This process is also known as <code>Workflow Orchestration</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/7bb92351-add5-415c-a679-d0ca038ddd66.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">workflow_orchestration.JPG</figcaption><p></p>
</figure>
</div>
<p>Workflow orchestration allows us to turn any code into a workflow that we can schedule, run and observe.</p>
<p>Core features:</p>
<ul>
<li>remote execution</li>
<li>sceduling</li>
<li>retries</li>
<li>caching</li>
<li>integration with external systems (APIs, databases)</li>
<li>ad-hoc runs</li>
<li>parametrization</li>
<li>alert when something fails</li>
</ul>
</section>
<section id="introdution-to-prefect" class="level3">
<h3 class="anchored" data-anchor-id="introdution-to-prefect">2.2.2 Introdution to Prefect</h3>
<p><a href="https://docs.prefect.io/">Prefect</a> is air traffic control for the modern data stack. Monitor, coordinate, and orchestrate dataflows between and across your applications. Build pipelines, deploy them anywhere, and configure them remotely. You might just love your workflows again.</p>
<p>In this session, we are going to take a look at a basic python script that pulls the yellow taxi data into a postgres db and then transforms that script to be orchestrated with Prefect.</p>
<p>Prefect is the modern open source dataflow automation platform that will allow us to add observability and orchestration by utilizing python to write code as workflows to build,run and monitor pipelines at scale.</p>
<p>First let’s clone the <a href="https://github.com/discdiver/prefect-zoomcamp">Prefect repo</a> from the command line:</p>
<pre><code>git clone https://github.com/discdiver/prefect-zoomcamp.git</code></pre>
<p>Next, create a python environment :</p>
<pre><code>conda create -n zoomcamp python=3.9   </code></pre>
<p>Once created we need to activate it:</p>
<pre><code>conda activate zoomcamp</code></pre>
<p>To deactivate an environment use:</p>
<pre><code>conda deactivate  </code></pre>
<p>Note from the terminal that we are no longer running in <code>base</code> but our newly created <code>zoomcamp</code> environment:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/b25f0936-402d-48f8-9a9c-e62ccca914ad.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">zoomcamp_environ.JPG</figcaption><p></p>
</figure>
</div>
<p>Then install all package dependencies with:</p>
<pre><code>pip install -r requirements.txt</code></pre>
<p>Once that’s done we can check that has installed successfully and which version we have from the command line:</p>
<pre><code>prefect version</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/d77efb41-a9d9-41ac-b90d-16651906a4b2.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">prefect_version.JPG</figcaption><p></p>
</figure>
</div>
<p>I started Docker Desktop and executed these commands :</p>
<p>docker run -d<br>
-e POSTGRES_USER=“root”<br>
-e POSTGRES_PASSWORD=“root”<br>
-e POSTGRES_DB=“ny_taxi”<br>
-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data<br>
-p 5432:5432<br>
postgres:13</p>
<p>Then, I executed the <code>ingest_data.py</code> file and ran with</p>
<pre><code>python ingest_data.py</code></pre>
<p>#!/usr/bin/env python # coding: utf-8 import os import argparse from time import time import pandas as pd from sqlalchemy import create_engine</p>
<p>def ingest_data(user, password, host, port, db, table_name, url):</p>
<pre><code># the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {url} -O {csv_name}")
postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'
engine = create_engine(postgres_url)

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')

df.to_sql(name=table_name, con=engine, if_exists='append')


while True: 

    try:
        t_start = time()
        
        df = next(df_iter)

        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

        df.to_sql(name=table_name, con=engine, if_exists='append')

        t_end = time()

        print('inserted another chunk, took %.3f second' % (t_end - t_start))

    except StopIteration:
        print("Finished ingesting data into the postgres database")
        break</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>’: user = “root” password = “root” host = “localhost” port = “5432” db = “ny_taxi” table_name = “yellow_taxi_trips” csv_url = “https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz”</p>
<pre><code>ingest_data(user, password, host, port, db, table_name, csv_url)</code></pre>
<p>I then opened up <code>pgcli</code></p>
<pre><code>pgcli -h localhost -p 5432 -u root -d ny_taxi</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/d6190ac2-543f-455c-8e42-d68a7bab06d6.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">data_ingest.JPG</figcaption><p></p>
</figure>
</div>
<p>So we can see that the data ingested into the postgres db. This is great but we had to manually trigger this python script. Using a workflow orchestration tool will allow us to add a scheduler so that we won’t have to trigger this script manually anymore. Additionally, we’ll get all the functionality that comes with workflow orchestation such as visibility, and resilience to the dataflow with automatic retries or caching and more.</p>
<p>Let’s transform this into a Prefect flow. A flow is the most basic Prefect object that is a container for workflow logic and allows you to interact and understand the state of the workflow. Flows are like functions, they take inputs, preform work, and return an output. We can start by using the <code>@flow decorator</code>to a <code>main_flow</code> function.</p>
<ul>
<li>import prefect with <code>from prefect import flow, task</code></li>
<li>move everything that was in our <code>'if __name__ == '__main__'</code> function to a new <code>def main():</code> function (replace with a reference to <code>main()</code></li>
<li>add <code>@flow(name="Ingest Flow")</code> above a new <code>def main()</code> function</li>
<li>remove the <code>while True</code> part of our original script</li>
</ul>
<p>I started Docker Desktop and executed these commands:</p>
<p>docker run -d<br>
-e POSTGRES_USER=“root”<br>
-e POSTGRES_PASSWORD=“root”<br>
-e POSTGRES_DB=“ny_taxi”<br>
-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data<br>
-p 5432:5432<br>
postgres:13</p>
<p>Then start the Prefect Orion orchestration engine using:</p>
<pre><code>prefect orion start</code></pre>
<p>Open another terminal window and run the following command:</p>
<pre><code>prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api</code></pre>
<p>#!/usr/bin/env python # coding: utf-8 import os import argparse from time import time import pandas as pd from sqlalchemy import create_engine from prefect import flow, task # Added</p>
<p>def ingest_data(user, password, host, port, db, table_name, url):</p>
<pre><code># the backup files are gzipped, and it's important to keep the correct extension
# for pandas to be able to open the file
if url.endswith('.csv.gz'):
    csv_name = 'yellow_tripdata_2021-01.csv.gz'
else:
    csv_name = 'output.csv'

os.system(f"wget {url} -O {csv_name}")
postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'
engine = create_engine(postgres_url)

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')

df.to_sql(name=table_name, con=engine, if_exists='append')</code></pre>
<p><span class="citation" data-cites="flow">@flow</span>(name=“Ingest Flow”) # Added def main_flow(): user = “root” password = “root” host = “localhost” port = “5432” db = “ny_taxi” table_name = “yellow_taxi_trips” csv_url = “https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz”</p>
<pre><code>ingest_data(user, password, host, port, db, table_name, csv_url)</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>’: main_flow() # everything that was here moved into the new def main(): function</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/e81304af-7105-4e54-9c72-ba74c0ee69e2.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">flow_run1.JPG</figcaption><p></p>
</figure>
</div>
<p>We successfully completed a <code>flow run</code>.</p>
<p>Flows contain tasks so let’s transform ingest_data into a task by adding the <span class="citation" data-cites="task">@task</span> decorator. Tasks are not required for flows but tasks are special because they receive metadata about upstream dependencies and the state of those dependencies before the function is run, which gives you the opportunity to have a task wait on the completion of another task before executing.</p>
<p>We can simplify this script and transform it into an extract and transform before we load the data into the postgres db. We start by breaking apart the large <code>ingest_data</code> function into multiple functions so that we can get more <code>visibility</code> into the <code>tasks</code> that are running or potentially causing failures.</p>
<p>Let’s create a new task called <code>extract data</code> that will take the url for the csv and the task will actually return the results. Since this is pulling data from external my system (something we may not control) we want to add automatic <code>retries</code> and also add a <code>caching</code> so that if this task has already been run, it will not need to run again.</p>
<pre><code>import from prefect.tasks import task_input_hash</code></pre>
<p>If we look at the data in PotsgreSQL we can see that on row 4, there is a passenger count of 0. So let’s do a transformation step to cleanse the data before we load the data to postgres. We can create a new task called <code>transform_data</code> for this.</p>
<p>Lastly, let’s actually simplify the original <code>ingest_data()</code> function and rename this to <code>load_data()</code></p>
<p>#!/usr/bin/env python # coding: utf-8 import os import argparse from time import time import pandas as pd from sqlalchemy import create_engine from prefect import flow, task from prefect.tasks import task_input_hash from datetime import timedelta</p>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True, tags=[“extract”], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1)) def extract_data(url: str): # the backup files are gzipped, and it’s important to keep the correct extension # for pandas to be able to open the file if url.endswith(‘.csv.gz’): csv_name = ‘yellow_tripdata_2021-01.csv.gz’ else: csv_name = ‘output.csv’</p>
<pre><code>os.system(f"wget {url} -O {csv_name}")

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

return df</code></pre>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True) def transform_data(df): print(f”pre: missing passenger count: {df[‘passenger_count’].isin([0]).sum()}“) df = df[df[‘passenger_count’] != 0] print(f”post: missing passenger count: {df[‘passenger_count’].isin([0]).sum()}“) return df</p>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True, retries=3) def load_data(user, password, host, port, db, table_name, df): postgres_url = f’postgresql://{user}:{password}<span class="citation" data-cites="host">@host</span>:{port}/{db}’ engine = create_engine(postgres_url) df.head(n=0).to_sql(name=table_name, con=engine, if_exists=‘replace’) df.to_sql(name=table_name, con=engine, if_exists=‘append’)</p>
<p><span class="citation" data-cites="flow">@flow</span>(name=“Ingest Flow”) def main_flow(): user = “root” password = “root” host = “localhost” port = “5432” db = “ny_taxi” table_name = “yellow_taxi_trips” csv_url = “https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz”</p>
<pre><code>raw_data=extract_data(csv_url)
data = transform_data(raw_data)
load_data(user, password, host, port, db, table_name, data)</code></pre>
<p>if <strong>name</strong> == ‘<strong>main</strong>’: main_flow()</p>
<p>There’s a lot more we can add by sprinkling in Prefect to our flow. We could parameterize the flow to take a table name so that we could change the table name loaded each time the flow was run.</p>
<p>Flows can also contain other flows - and so we can create a <code>sub-flow</code> :</p>
<p>#!/usr/bin/env python # coding: utf-8 import os import argparse from time import time import pandas as pd from sqlalchemy import create_engine from prefect import flow, task from prefect.tasks import task_input_hash from datetime import timedelta</p>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True, tags=[“extract”], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1)) def extract_data(url: str): # the backup files are gzipped, and it’s important to keep the correct extension # for pandas to be able to open the file if url.endswith(‘.csv.gz’): csv_name = ‘yellow_tripdata_2021-01.csv.gz’ else: csv_name = ‘output.csv’</p>
<pre><code>os.system(f"wget {url} -O {csv_name}")

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

return df</code></pre>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True) def transform_data(df): print(f”pre: missing passenger count: {df[‘passenger_count’].isin([0]).sum()}“) df = df[df[‘passenger_count’] != 0] print(f”post: missing passenger count: {df[‘passenger_count’].isin([0]).sum()}“) return df</p>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True, retries=3) def load_data(user, password, host, port, db, table_name, df): postgres_url = f”postgresql://{user}:{password}<span class="citation" data-cites="host">@host</span>:{port}/{db}” engine = create_engine(postgres_url) df.head(n=0).to_sql(name=table_name, con=engine, if_exists=‘replace’) df.to_sql(name=table_name, con=engine, if_exists=‘append’)</p>
<p><span class="citation" data-cites="flow">@flow</span>(name=“Subflow”, log_prints=True) def log_subflow(table_name:str): print(“Logging Subflow for: {table_name}”)</p>
<p><span class="citation" data-cites="flow">@flow</span>(name=“Ingest Flow”) def main_flow(table_name: str): user = “root” password = “root” host = “localhost” port = “5432” db = “ny_taxi” csv_url = “https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz” log_subflow = (table_name) raw_data = extract_data(csv_url) data = transform_data(raw_data) load_data(user, password, host, port, db, table_name, data)</p>
<p>if <strong>name</strong> == ‘<strong>main</strong>’: main_flow(“yellow_taxi_trips”)</p>
<p>That has run successfully:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/ad18c860-f545-43e8-b325-1dd463a19f8d.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">sub_flow.JPG</figcaption><p></p>
</figure>
</div>
<p>Let’s now open the open source UI to visualise our flow runs :</p>
<pre><code>prefect orion start</code></pre>
<p><img src="Data_Eng_Week_2_files/figure-html/25cc8011-10b0-403c-9a51-91ce959b7c26.JPG" class="img-fluid" alt="prefect_orion.JPG"><img src="Data_Eng_Week_2_files/figure-html/964d7165-c296-4d3e-9dd3-7a4525ab9915.jpg" class="img-fluid" alt="download.jpg"></p>
<p>This should default but if you are having problems or just want to make sure you set the prefect config to point to the api URL:</p>
<pre><code>prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/ac55d25f-10b8-4a61-b207-a2d8633faff9.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">prefect_api_url.JPG</figcaption><p></p>
</figure>
</div>
<p>This is especially important if you are going to host the Url somewhere else and need to change the url for the api that your flows are communicating with.</p>
<p>Opening up the localhost we can see the Prefect UI, which gives us a nice dashboard to see all of our flow run history.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/818033db-c98d-4b52-8a51-e4fdb40fa5ca.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">prefect_UI.JPG</figcaption><p></p>
</figure>
</div>
<p>We can then drill down into the runs to obtain more details :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/a1f37a03-bb12-4a0f-97df-563cdfdbd6b1.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">optimistic_flounder.JPG</figcaption><p></p>
</figure>
</div>
<p>A quick navigation lets us dive into the logs of that flow run, navigate around. You’ll notice over on the side we have <code>Deployments</code>, <code>Work Queues</code>, <code>Blocks</code>, <code>Notifications</code>, and <code>Task Run Concurrency</code>.</p>
<p>Blocks are a primitive within Prefect that enable the storage of configuration and provide an interface with interacting with external systems. There are several different types of blocks you can build, and you can even create your own. Block names are immutable so they can be reused across multiple flows. Blocks can also build upon blocks or be installed as part of Intergration collection which is prebuilt tasks and blocks that are pip installable. For example, a lot of users use the SqlAlchemy.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/6d20c728-7a32-4b11-b1ff-1f6bb02de81f.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">prefect_blocks.JPG</figcaption><p></p>
</figure>
</div>
<p>We can add <code>Blocks</code> from the command line.</p>
<p>Let’s actually take our postgres configuration and store that in a block - by amending our previous <code>flow</code> Python file :</p>
<p>#!/usr/bin/env python # coding: utf-8 import os import argparse from time import time import pandas as pd from sqlalchemy import create_engine from prefect import flow, task from prefect.tasks import task_input_hash from datetime import timedelta from prefect_sqlalchemy import SqlAlchemyConnector</p>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True, tags=[“extract”], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1)) def extract_data(url: str): # the backup files are gzipped, and it’s important to keep the correct extension # for pandas to be able to open the file if url.endswith(‘.csv.gz’): csv_name = ‘yellow_tripdata_2021-01.csv.gz’ else: csv_name = ‘output.csv’</p>
<pre><code>os.system(f"wget {url} -O {csv_name}")

df_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)

df = next(df_iter)

df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)
df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)

return df</code></pre>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True) def transform_data(df): print(f”pre: missing passenger count: {df[‘passenger_count’].isin([0]).sum()}“) df = df[df[‘passenger_count’] != 0] print(f”post: missing passenger count: {df[‘passenger_count’].isin([0]).sum()}“) return df</p>
<p><span class="citation" data-cites="task">@task</span>(log_prints=True, retries=3) def load_data(table_name, df): connection_block = SqlAlchemyConnector.load(“postgres-connector”)</p>
<pre><code>with connection_block.get_connection(begin=False) as engine:
    df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')
    df.to_sql(name=table_name, con=engine, if_exists='append')</code></pre>
<p><span class="citation" data-cites="flow">@flow</span>(name=“Subflow”, log_prints=True) def log_subflow(table_name:str): print(“Logging Subflow for: {table_name}”)</p>
<p><span class="citation" data-cites="flow">@flow</span>(name=“Ingest Flow”) def main_flow(table_name: str): csv_url = “https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz” log_subflow(table_name) raw_data = extract_data(csv_url) data = transform_data(raw_data) load_data(table_name, data)</p>
<p>if <strong>name</strong> == ‘<strong>main</strong>’: main_flow(“yellow_taxi_trips”)</p>
</section>
<section id="create-prefect-gcp-blocks" class="level3">
<h3 class="anchored" data-anchor-id="create-prefect-gcp-blocks">Create Prefect GCP blocks</h3>
<p>Create a GCP Credentials block in the UI.</p>
<p>Paste your service account information from your <code>Google Cloud Platform (GCP)</code> JSON file into the Service Account Info block’s field.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/091a8e83-8dda-481a-b421-22ada9d87640.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">gcp_credentials.JPG</figcaption><p></p>
</figure>
</div>
<p>Create a GCS Bucket block in UI :</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Data_Eng_Week_2_files/figure-html/2b88d814-c944-4d13-92c3-f86ac88c93e4.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">gcs_bucket.JPG</figcaption><p></p>
</figure>
</div>
<p>valueError: Unable to find block document named postgres-connector for block type sqlalchemy-connector</p>
<p>Solution?</p>
<pre><code>pip install prefect-sqlalchemy</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>