<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stephen Barrie">
<meta name="dcterms.date" content="2022-11-18">

<title>Into the Unknown - Excelosaurus meets Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Stephen Barrie</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Excelosaurus meets Python</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">fastai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stephen Barrie </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 18, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#aim-of-the-project" id="toc-aim-of-the-project" class="nav-link active" data-scroll-target="#aim-of-the-project">Aim of the project</a></li>
  <li><a href="#data-source" id="toc-data-source" class="nav-link" data-scroll-target="#data-source">Data source</a></li>
  <li><a href="#set-path" id="toc-set-path" class="nav-link" data-scroll-target="#set-path">Set path</a></li>
  <li><a href="#install-required-packages" id="toc-install-required-packages" class="nav-link" data-scroll-target="#install-required-packages">Install required packages</a></li>
  <li><a href="#loading-the-data" id="toc-loading-the-data" class="nav-link" data-scroll-target="#loading-the-data">Loading the data</a></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis">Exploratory data analysis</a></li>
  <li><a href="#numeric-features" id="toc-numeric-features" class="nav-link" data-scroll-target="#numeric-features">Numeric features</a></li>
  <li><a href="#histogram-of-max_weight" id="toc-histogram-of-max_weight" class="nav-link" data-scroll-target="#histogram-of-max_weight">Histogram of Max_weight</a></li>
  <li><a href="#feature-engineering" id="toc-feature-engineering" class="nav-link" data-scroll-target="#feature-engineering">Feature engineering</a></li>
  <li><a href="#categorical-features" id="toc-categorical-features" class="nav-link" data-scroll-target="#categorical-features">Categorical features</a></li>
  <li><a href="#create-our-independentpredictors-and-dependenttarget-variables" id="toc-create-our-independentpredictors-and-dependenttarget-variables" class="nav-link" data-scroll-target="#create-our-independentpredictors-and-dependenttarget-variables">Create our independent(predictors) and dependent(target) variables</a></li>
  <li><a href="#setting-up-a-linear-model" id="toc-setting-up-a-linear-model" class="nav-link" data-scroll-target="#setting-up-a-linear-model">Setting up a linear model</a></li>
  <li><a href="#doing-a-gradient-descent-step" id="toc-doing-a-gradient-descent-step" class="nav-link" data-scroll-target="#doing-a-gradient-descent-step">Doing a gradient descent step</a></li>
  <li><a href="#training-the-linear-model" id="toc-training-the-linear-model" class="nav-link" data-scroll-target="#training-the-linear-model">Training the linear model</a></li>
  <li><a href="#measuring-accuracy" id="toc-measuring-accuracy" class="nav-link" data-scroll-target="#measuring-accuracy">Measuring accuracy</a></li>
  <li><a href="#using-sigmoid" id="toc-using-sigmoid" class="nav-link" data-scroll-target="#using-sigmoid">Using sigmoid</a></li>
  <li><a href="#using-matrix-multiplication" id="toc-using-matrix-multiplication" class="nav-link" data-scroll-target="#using-matrix-multiplication">Using Matrix Multiplication</a></li>
  <li><a href="#a-neural-network" id="toc-a-neural-network" class="nav-link" data-scroll-target="#a-neural-network">A neural network</a></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning">Deep learning</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final thoughts</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This is my follow up to <a href="https://www.youtube.com/watch?v=_rXzeWq4C6w"><strong><em>Lesson 5: Practical Deep Learning for Coders 2022</em></strong></a> in which Jeremy builds a linear regresson model and neural net from scratch using Python.</p>
<section id="aim-of-the-project" class="level2">
<h2 class="anchored" data-anchor-id="aim-of-the-project">Aim of the project</h2>
<p>The aim of this project is :</p>
<ul>
<li>to build a linear and neural network model from scratch within Python that predicts whether a dino is a meat-eater or a vegetarian</li>
</ul>
</section>
<section id="data-source" class="level2">
<h2 class="anchored" data-anchor-id="data-source">Data source</h2>
<p>I created my own dataset in Excel from my son’s favourite dino book.</p>
<p><img src="Dinozaur.jpg" style="width:400px;height:400px"></p>
</section>
<section id="set-path" class="level2">
<h2 class="anchored" data-anchor-id="set-path">Set path</h2>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="install-required-packages" class="level2">
<h2 class="anchored" data-anchor-id="install-required-packages">Install required packages</h2>
<p>We’ll be using <a href="https://numpy.org/">NumPy</a> and <a href="https://pytorch.org/">Pytorch</a> for array calculations in this notebook, and <a href="https://pandas.pydata.org/docs/index.html">pandas</a> for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>np.set_printoptions(linewidth<span class="op">=</span><span class="dv">140</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>torch.set_printoptions(linewidth<span class="op">=</span><span class="dv">140</span>, sci_mode<span class="op">=</span><span class="va">False</span>, edgeitems<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.width'</span>, <span class="dv">140</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="loading-the-data" class="level2">
<h2 class="anchored" data-anchor-id="loading-the-data">Loading the data</h2>
<p>The starting point for this project is the csv file that I created as part of my Lesson 3 project to build a linear model and neural network from scratch within Excel. See my earlier post titled <strong><em>Excelosaurus</em></strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Excel_meets_Python_files/figure-html/e40f5552-4f02-4ac3-baa3-5b6e4e75d6b2-1-3d825e94-c96d-4c3a-8550-70446c121297.PNG" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Features.PNG</figcaption>
</figure>
</div>
<p>The data is in the form of a table, as a Comma Separated Values (CSV) file. We can open it using the <a href="https://pandas.pydata.org/docs/index.html"><strong><em>pandas</em></strong></a> library, which will create a <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"><strong><em>DataFrame</em></strong></a>.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the path and load in the spreadsheet</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>dinos <span class="op">=</span> pd.read_csv(<span class="st">'Data/Dinos.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="exploratory-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="exploratory-data-analysis">Exploratory data analysis</h2>
<p>We can automate some of the exploratory data analysis by writing a function:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> initial_eda(df):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(df, pd.DataFrame):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        total_na <span class="op">=</span> df.isna().<span class="bu">sum</span>().<span class="bu">sum</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Dimensions : </span><span class="sc">%d</span><span class="st"> rows, </span><span class="sc">%d</span><span class="st"> columns"</span> <span class="op">%</span> (df.shape[<span class="dv">0</span>], df.shape[<span class="dv">1</span>]))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Total NA Values : </span><span class="sc">%d</span><span class="st"> "</span> <span class="op">%</span> (total_na))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="sc">%38s</span><span class="st"> </span><span class="sc">%10s</span><span class="st">     </span><span class="sc">%10s</span><span class="st"> </span><span class="sc">%10s</span><span class="st">"</span> <span class="op">%</span> (<span class="st">"Column Name"</span>, <span class="st">"Data Type"</span>, <span class="st">"#Distinct"</span>, <span class="st">"NA Values"</span>))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        col_name <span class="op">=</span> df.columns</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        dtyp <span class="op">=</span> df.dtypes</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        uniq <span class="op">=</span> df.nunique()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        na_val <span class="op">=</span> df.isna().<span class="bu">sum</span>()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df.columns)):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"</span><span class="sc">%38s</span><span class="st"> </span><span class="sc">%10s</span><span class="st">   </span><span class="sc">%10s</span><span class="st"> </span><span class="sc">%10s</span><span class="st">"</span> <span class="op">%</span> (col_name[i], dtyp[i], uniq[i], na_val[i]))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Expect a DataFrame but got a </span><span class="sc">%15s</span><span class="st">"</span> <span class="op">%</span> (<span class="bu">type</span>(df)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>initial_eda(dinos)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions : 60 rows, 10 columns
Total NA Values : 0 
                           Column Name  Data Type      #Distinct  NA Values
                                  Name     object           60          0
                                Period     object            3          0
                            Discovered     object            8          0
                               Min_Len    float64           21          0
                               Max_Len    float64           22          0
                            Min_Height    float64           15          0
                            Max_Height    float64           15          0
                            Min_Weight    float64           29          0
                            Max_weight    float64           27          0
                                  Meat      int64            2          0</code></pre>
</div>
</div>
<p>As we can see there are no missing values, amd the dataset includes:</p>
<ul>
<li>60 rows, each representing a unique dinosaur, and</li>
<li>10 columns, representing 9 features and the target variable.</li>
</ul>
<p>Note that the <em>Period</em> and <em>Discovered</em> columns are of data type <strong><em>object</em></strong>. We will deal with these later in the section on <strong><em>Categorical Features</em></strong>. The other columnns are numeric (float64 or int64).</p>
<p>Straight off the bat, we can safely drop the *Name” column as the dino name is clearly not an indicator of whether it is a meat eater or a plant eater:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>dinos <span class="op">=</span> dinos.drop([<span class="st">'Name'</span>], axis<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>dinos[<span class="st">'Meat'</span>].value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>1    36
0    24
Name: Meat, dtype: int64</code></pre>
</div>
</div>
<ul>
<li>There are two possible prediction outputs so this is a <a href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a> problem. It’s a small dataset but reasonably balanced; 36 meat-eating dinos [indexed 1] and 24 veggies [indexed 0].</li>
</ul>
</section>
<section id="numeric-features" class="level2">
<h2 class="anchored" data-anchor-id="numeric-features">Numeric features</h2>
<p>Here’s how we get a quick summary of all the numeric columns in the dataset:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>dinos.describe(include<span class="op">=</span>(np.number))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Min_Len</th>
<th data-quarto-table-cell-role="th">Max_Len</th>
<th data-quarto-table-cell-role="th">Min_Height</th>
<th data-quarto-table-cell-role="th">Max_Height</th>
<th data-quarto-table-cell-role="th">Min_Weight</th>
<th data-quarto-table-cell-role="th">Max_weight</th>
<th data-quarto-table-cell-role="th">Meat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>60.000000</td>
<td>60.000000</td>
<td>60.000000</td>
<td>60.000000</td>
<td>60.000000</td>
<td>60.000000</td>
<td>60.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>8.480000</td>
<td>9.015000</td>
<td>2.788333</td>
<td>3.075000</td>
<td>6.666750</td>
<td>7.333992</td>
<td>0.600000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>7.152356</td>
<td>7.757661</td>
<td>2.548518</td>
<td>2.754326</td>
<td>13.310511</td>
<td>14.709908</td>
<td>0.494032</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>0.300000</td>
<td>0.400000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000500</td>
<td>0.000500</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>3.750000</td>
<td>4.500000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.175000</td>
<td>0.175000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>6.500000</td>
<td>7.000000</td>
<td>2.000000</td>
<td>2.750000</td>
<td>2.500000</td>
<td>3.000000</td>
<td>1.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>10.250000</td>
<td>11.000000</td>
<td>4.000000</td>
<td>4.000000</td>
<td>7.000000</td>
<td>8.000000</td>
<td>1.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>35.000000</td>
<td>40.000000</td>
<td>14.000000</td>
<td>14.000000</td>
<td>70.000000</td>
<td>80.000000</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<ul>
<li>We need to watch out for dominant features as these can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will strongly influence the result.</li>
<li><em>Min_Weight</em> and <em>Max_Weight</em> contain mainly values of around 0.0005 to 8, but there are a few much bigger ones.</li>
</ul>
<p>Let’s illustrate this visually using a <a href="https://en.wikipedia.org/wiki/Histogram"><strong><em>histogram</em></strong></a> :</p>
</section>
<section id="histogram-of-max_weight" class="level2">
<h2 class="anchored" data-anchor-id="histogram-of-max_weight">Histogram of Max_weight</h2>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>dinos [<span class="st">'Max_weight'</span>].hist()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Excel_meets_Python_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="feature-engineering" class="level2">
<h2 class="anchored" data-anchor-id="feature-engineering">Feature engineering</h2>
<p>As you can see the distribution is long-tailed which linear models don’t like. To fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the log of zero will return NaN (not a number) so the workaround is to include +1</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>dinos[<span class="st">'Max_weight'</span>] <span class="op">=</span> np.log(dinos[<span class="st">'Max_weight'</span>]<span class="op">+</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The histogram now shows a more even distribution of values without the long tail:</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>dinos [<span class="st">'Max_weight'</span>].hist()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>&lt;AxesSubplot: &gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Excel_meets_Python_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s do the same for <strong><em>Min_Weight</em></strong> :</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>dinos[<span class="st">'Min_Weight'</span>] <span class="op">=</span> np.log(dinos[<span class="st">'Min_Weight'</span>]<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dinos [<span class="st">'Min_Weight'</span>].hist()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>&lt;AxesSubplot: &gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="Excel_meets_Python_files/figure-html/cell-13-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="categorical-features" class="level2">
<h2 class="anchored" data-anchor-id="categorical-features"><a href="https://towardsdatascience.com/categorical-variables-for-machine-learning-algorithms-d2768d587ab6#:~:text=As%20categorical%20features%2C%20they%20take,%E2%80%9350%2C%20etc">Categorical features</a></h2>
<p>Here’s how we get a quick summary of all the non-numeric columns in the dataset:</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>dinos.describe(include<span class="op">=</span>[<span class="bu">object</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Period</th>
<th data-quarto-table-cell-role="th">Discovered</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>60</td>
<td>60</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">unique</td>
<td>3</td>
<td>8</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">top</td>
<td>Cretaceous</td>
<td>North America</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">freq</td>
<td>41</td>
<td>36</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dinos.tail(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Period</th>
<th data-quarto-table-cell-role="th">Discovered</th>
<th data-quarto-table-cell-role="th">Min_Len</th>
<th data-quarto-table-cell-role="th">Max_Len</th>
<th data-quarto-table-cell-role="th">Min_Height</th>
<th data-quarto-table-cell-role="th">Max_Height</th>
<th data-quarto-table-cell-role="th">Min_Weight</th>
<th data-quarto-table-cell-role="th">Max_weight</th>
<th data-quarto-table-cell-role="th">Meat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">55</td>
<td>Cretaceous</td>
<td>Africa</td>
<td>12.0</td>
<td>12.0</td>
<td>1.5</td>
<td>1.5</td>
<td>2.197225</td>
<td>2.197225</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">56</td>
<td>Cretaceous</td>
<td>North America</td>
<td>11.0</td>
<td>11.0</td>
<td>1.5</td>
<td>1.5</td>
<td>2.302585</td>
<td>2.302585</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">57</td>
<td>Jurassic</td>
<td>Europe</td>
<td>0.3</td>
<td>0.4</td>
<td>0.2</td>
<td>0.2</td>
<td>0.001000</td>
<td>0.001000</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">58</td>
<td>Triassic</td>
<td>Europe</td>
<td>5.0</td>
<td>6.0</td>
<td>2.0</td>
<td>3.0</td>
<td>0.470004</td>
<td>0.470004</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">59</td>
<td>Triassic</td>
<td>Europe</td>
<td>4.0</td>
<td>5.0</td>
<td>3.0</td>
<td>3.0</td>
<td>2.302585</td>
<td>2.302585</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>As you can see from the above our data includes the following categorical data -</p>
<ul>
<li>the period when the dinos lived (Cretaceous, Jurassic, Triassic)</li>
<li>the place of discovery (Africa, North America, Europe etc.)</li>
</ul>
<p>Clearly we can’t multiply strings like Jurassic or Africa by coefficients, so we need to replace those with numbers.</p>
<p>We do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Period = ‘Cretaceous’, which would be a new column containing 1 for rows where Period is ‘Cretaceous’, and 0 for rows where it isn’t, and apply the same logic for the ‘Discovered’ values.</p>
<p>Pandas can create these automatically using get_dummies, which also remove the original columns. We’ll create dummy variables for <strong><em>Period</em></strong> and <strong><em>Discovered</em></strong></p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>dinos <span class="op">=</span> pd.get_dummies(dinos, columns<span class="op">=</span>[<span class="st">"Period"</span>,<span class="st">"Discovered"</span>])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>dinos.columns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>Index(['Min_Len', 'Max_Len', 'Min_Height', 'Max_Height', 'Min_Weight', 'Max_weight', 'Meat', 'Period_Cretaceous', 'Period_Jurassic',
       'Period_Triassic', 'Discovered_Africa', 'Discovered_Antartica', 'Discovered_Asia', 'Discovered_Australia', 'Discovered_Europe',
       'Discovered_North America', 'Discovered_South America', 'Discovered_UK'],
      dtype='object')</code></pre>
</div>
</div>
<p>We can see that columns have been added to the end – one for each of the possible values for <strong><em>Period</em></strong> and <strong><em>Discovered</em></strong>, and that the original <strong><em>Period</em></strong> and <strong><em>Discovered</em></strong> columns have been removed.</p>
<p>Here’s what the first few rows of those newly added columns look like:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>added_cols <span class="op">=</span> [<span class="st">'Period_Cretaceous'</span>, <span class="st">'Period_Jurassic'</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>       <span class="st">'Period_Triassic'</span>, <span class="st">'Discovered_Africa'</span>, <span class="st">'Discovered_Antartica'</span>, <span class="st">'Discovered_Asia'</span>, <span class="st">'Discovered_Australia'</span>, <span class="st">'Discovered_Europe'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>       <span class="st">'Discovered_North America'</span>, <span class="st">'Discovered_South America'</span>, <span class="st">'Discovered_UK'</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>dinos[added_cols].head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Period_Cretaceous</th>
<th data-quarto-table-cell-role="th">Period_Jurassic</th>
<th data-quarto-table-cell-role="th">Period_Triassic</th>
<th data-quarto-table-cell-role="th">Discovered_Africa</th>
<th data-quarto-table-cell-role="th">Discovered_Antartica</th>
<th data-quarto-table-cell-role="th">Discovered_Asia</th>
<th data-quarto-table-cell-role="th">Discovered_Australia</th>
<th data-quarto-table-cell-role="th">Discovered_Europe</th>
<th data-quarto-table-cell-role="th">Discovered_North America</th>
<th data-quarto-table-cell-role="th">Discovered_South America</th>
<th data-quarto-table-cell-role="th">Discovered_UK</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let’s review our dataset now following the changes prior to building a model:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>initial_eda(dinos)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dimensions : 60 rows, 18 columns
Total NA Values : 0 
                           Column Name  Data Type      #Distinct  NA Values
                               Min_Len    float64           21          0
                               Max_Len    float64           22          0
                            Min_Height    float64           15          0
                            Max_Height    float64           15          0
                            Min_Weight    float64           29          0
                            Max_weight    float64           27          0
                                  Meat      int64            2          0
                     Period_Cretaceous      uint8            2          0
                       Period_Jurassic      uint8            2          0
                       Period_Triassic      uint8            2          0
                     Discovered_Africa      uint8            2          0
                  Discovered_Antartica      uint8            2          0
                       Discovered_Asia      uint8            2          0
                  Discovered_Australia      uint8            2          0
                     Discovered_Europe      uint8            2          0
              Discovered_North America      uint8            2          0
              Discovered_South America      uint8            2          0
                         Discovered_UK      uint8            2          0</code></pre>
</div>
</div>
</section>
<section id="create-our-independentpredictors-and-dependenttarget-variables" class="level2">
<h2 class="anchored" data-anchor-id="create-our-independentpredictors-and-dependenttarget-variables">Create our independent(predictors) and dependent(target) variables</h2>
<ul>
<li>They both need to be PyTorch tensors. Our dependent variable is <strong>Meat</strong> :</li>
</ul>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> tensor</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>t_dep <span class="op">=</span> tensor(dinos.Meat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Our independent variables are all the continuous variables of interest plus all the dummy variables we created earlier:</li>
</ul>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>indep_cols <span class="op">=</span> [<span class="st">"Min_Len"</span>,<span class="st">"Max_Len"</span>,<span class="st">"Min_Height"</span>,<span class="st">"Max_Height"</span>,<span class="st">"Min_Weight"</span>,<span class="st">"Max_weight"</span>] <span class="op">+</span> added_cols</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>t_indep <span class="op">=</span> tensor(dinos[indep_cols].values, dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>t_indep</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor([[   35.0000,    40.0000,    11.0000,    11.0000,     4.2627,     4.3944,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     0.0000,     1.0000,     0.0000],
        [   25.0000,    30.0000,    10.0000,    12.0000,     3.9318,     3.9318,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [   30.0000,    30.0000,     5.0000,     6.0000,     2.4849,     2.5649,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [   20.0000,    22.0000,     5.0000,     6.0000,     3.0445,     3.1781,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [   25.0000,    25.0000,    14.0000,    14.0000,     3.9318,     3.9318,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [    7.0000,     7.0000,     2.5000,     2.5000,     1.3863,     1.3863,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [   12.0000,    12.0000,     4.0000,     4.0000,     1.6094,     1.6094,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        ...,
        [    5.0000,     5.0000,     2.0000,     2.0000,     0.4055,     0.4055,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [   20.0000,    20.0000,     4.0000,     4.0000,     3.7136,     3.9318,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [   12.0000,    12.0000,     1.5000,     1.5000,     2.1972,     2.1972,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     0.0000,     0.0000,     0.0000],
        [   11.0000,    11.0000,     1.5000,     1.5000,     2.3026,     2.3026,     1.0000,  ...,     0.0000,     0.0000,     0.0000,
             0.0000,     1.0000,     0.0000,     0.0000],
        [    0.3000,     0.4000,     0.2000,     0.2000,     0.0010,     0.0010,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             1.0000,     0.0000,     0.0000,     0.0000],
        [    5.0000,     6.0000,     2.0000,     3.0000,     0.4700,     0.4700,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             1.0000,     0.0000,     0.0000,     0.0000],
        [    4.0000,     5.0000,     3.0000,     3.0000,     2.3026,     2.3026,     0.0000,  ...,     0.0000,     0.0000,     0.0000,
             1.0000,     0.0000,     0.0000,     0.0000]])</code></pre>
</div>
</div>
<ul>
<li>Here’s the number of rows and columns we have for our independent variables:</li>
</ul>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>t_indep.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>torch.Size([60, 17])</code></pre>
</div>
</div>
<ul>
<li>So 60 rows or examples, and 17 columns, or features.</li>
</ul>
</section>
<section id="setting-up-a-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-a-linear-model">Setting up a linear model</h2>
<p>Now that we’ve got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we’re going to manually do a single step of calculating predictions and loss for every row of our data.</p>
<p>Our first model will be a simple linear model. We’ll need a coefficient for each column in t_indep. We’ll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">442</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>n_coeff <span class="op">=</span> t_indep.shape[<span class="dv">1</span>]</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> torch.rand(n_coeff)<span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>coeffs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625,  0.1722,  0.2324,
        -0.3575, -0.0010, -0.1833])</code></pre>
</div>
</div>
<p>Our predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don’t need a separate constant term (also known as a “bias” or “intercept” term), or a column of all 1s to give the same effect has having a constant term. That’s because our dummy variables already cover the entire dataset – e.g.&nbsp;there’s a column for “Cretaceous”, “Jurassic”, and “Triassic”, and every dino in the dataset was around during exactly one of these; therefore, we don’t need a separate intercept term to cover rows that aren’t otherwise part of a column.</p>
<p>Here’s what the multiplication looks like:</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>t_indep<span class="op">*</span>coeffs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([[   -16.2015,      5.5432,      2.6499,     -2.4877,     -1.1221,     -1.3830,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.0000,     -0.0010,     -0.0000],
        [   -11.5725,      4.1574,      2.4090,     -2.7138,     -1.0350,     -1.2374,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [   -13.8870,      4.1574,      1.2045,     -1.3569,     -0.6541,     -0.8072,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -9.2580,      3.0487,      1.2045,     -1.3569,     -0.8015,     -1.0002,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [   -11.5725,      3.4645,      3.3726,     -3.1662,     -1.0350,     -1.2374,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -3.2403,      0.9701,      0.6023,     -0.5654,     -0.3649,     -0.4363,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -5.5548,      1.6629,      0.9636,     -0.9046,     -0.4237,     -0.5065,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        ...,
        [    -2.3145,      0.6929,      0.4818,     -0.4523,     -0.1067,     -0.1276,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -9.2580,      2.7716,      0.9636,     -0.9046,     -0.9776,     -1.2374,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -5.5548,      1.6629,      0.3614,     -0.3392,     -0.5784,     -0.6915,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000],
        [    -5.0919,      1.5244,      0.3614,     -0.3392,     -0.6062,     -0.7247,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.1389,      0.0554,      0.0482,     -0.0452,     -0.0003,     -0.0003,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],
        [    -2.3145,      0.8315,      0.4818,     -0.6785,     -0.1237,     -0.1479,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],
        [    -1.8516,      0.6929,      0.7227,     -0.6785,     -0.6062,     -0.7247,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000]])</code></pre>
</div>
</div>
<p>We can see we’ve got a problem here. The sums of each row will be dominated by the first three columns, which represent average length, height, and weight, since these values are bigger on average than all the others.</p>
<p>Let’s make all the columns contain numbers from -1 to 1, by dividing each column by its max():</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>vals,indices <span class="op">=</span> t_indep.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>t_indep <span class="op">=</span> t_indep <span class="op">/</span> vals</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As we see, that removes the problem of one column dominating all the others:</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>t_indep<span class="op">*</span>coeffs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>tensor([[    -0.4629,      0.1386,      0.1893,     -0.1777,     -0.2632,     -0.3147,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.0000,     -0.0010,     -0.0000],
        [    -0.3306,      0.1039,      0.1721,     -0.1938,     -0.2428,     -0.2816,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.3968,      0.1039,      0.0860,     -0.0969,     -0.1535,     -0.1837,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.2645,      0.0762,      0.0860,     -0.0969,     -0.1880,     -0.2276,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.3306,      0.0866,      0.2409,     -0.2262,     -0.2428,     -0.2816,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.0926,      0.0243,      0.0430,     -0.0404,     -0.0856,     -0.0993,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.1587,      0.0416,      0.0688,     -0.0646,     -0.0994,     -0.1153,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        ...,
        [    -0.0661,      0.0173,      0.0344,     -0.0323,     -0.0250,     -0.0290,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.2645,      0.0693,      0.0688,     -0.0646,     -0.2293,     -0.2816,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.1587,      0.0416,      0.0258,     -0.0242,     -0.1357,     -0.1574,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000],
        [    -0.1455,      0.0381,      0.0258,     -0.0242,     -0.1422,     -0.1649,      0.4876,  ...,      0.0000,      0.0000,
              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],
        [    -0.0040,      0.0014,      0.0034,     -0.0032,     -0.0001,     -0.0001,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],
        [    -0.0661,      0.0208,      0.0344,     -0.0485,     -0.0290,     -0.0337,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],
        [    -0.0529,      0.0173,      0.0516,     -0.0485,     -0.1422,     -0.1649,      0.0000,  ...,      0.0000,      0.0000,
              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000]])</code></pre>
</div>
</div>
<p>Note this line of code in particular:</p>
<pre><code>t_indep = t_indep / vals</code></pre>
<p>That is dividing a [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics) by a [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics). The trick here is that we’re taking advantage of a technique in <a href="https://numpy.org/">Numpy</a> and <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a> (and many other languages, going all the way back to APL) called <a href="https://towardsdatascience.com/performing-multidimensional-matrix-operations-using-numpys-broadcasting-cf33e3029170">broadcasting</a>. In short, this acts as if there’s a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn’t actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we’re using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it’s well worth studying and practicing.</p>
<p>We can now create predictions from our linear model, by adding up the rows of the product:</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> (t_indep<span class="op">*</span>coeffs).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s take a look at the first few:</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>preds[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor([-0.4040, -0.8168, -0.5108, -0.6587, -0.6236, -0.1205, -0.1975, -0.1085, -0.1350, -0.1344])</code></pre>
</div>
</div>
<p>Of course, these predictions aren’t going to be any use, since our coefficients are random – they’re just a starting point for our gradient descent process.</p>
<pre><code>To do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:</code></pre>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> torch.<span class="bu">abs</span>(preds<span class="op">-</span>t_dep).mean()</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>tensor(0.6421)</code></pre>
</div>
</div>
<ul>
<li>Now that we’ve tested out a way of calculating predictions, and loss, let’s pop them into functions to make life easier:</li>
</ul>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_preds(coeffs, indeps): <span class="cf">return</span> (indeps<span class="op">*</span>coeffs).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_loss(coeffs, indeps, deps): <span class="cf">return</span> torch.<span class="bu">abs</span>(calc_preds(coeffs, indeps)<span class="op">-</span>deps).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="doing-a-gradient-descent-step" class="level2">
<h2 class="anchored" data-anchor-id="doing-a-gradient-descent-step">Doing a gradient descent step</h2>
<p>In this section, we’re going to do a single “epoch” of gradient descent manually. The only thing we’re going to automate is calculating gradients, because let’s face it that’s pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we’ll need to call requires_grad_() on our coeffs (if you’re not sure why, review the previous notebook, How does a neural net really work?, before continuing):</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>coeffs.requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625,  0.1722,  0.2324,
        -0.3575, -0.0010, -0.1833], requires_grad=True)</code></pre>
</div>
</div>
<p>Now when we calculate our loss, PyTorch will keep track of all the steps, so we’ll be able to get the gradients afterwards:</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> calc_loss(coeffs, t_indep, t_dep)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor(0.6421, grad_fn=&lt;MeanBackward0&gt;)</code></pre>
</div>
</div>
<p>Use backward() to ask PyTorch to calculate gradients now:</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see what they look like:</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>coeffs.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>tensor([-0.2290, -0.2129, -0.1825, -0.2030, -0.2670, -0.2669, -0.6167, -0.2000, -0.0500, -0.0667, -0.0167, -0.0833, -0.0167, -0.0500,
        -0.5667, -0.0500, -0.0167])</code></pre>
</div>
</div>
<p>Note that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. Let’s try running the above steps again:</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> calc_loss(coeffs, t_indep, t_dep)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>coeffs.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>tensor([-0.4579, -0.4258, -0.3650, -0.4060, -0.5340, -0.5337, -1.2333, -0.4000, -0.1000, -0.1333, -0.0333, -0.1667, -0.0333, -0.1000,
        -1.1333, -0.1000, -0.0333])</code></pre>
</div>
</div>
<p>As you see, our .grad values have doubled. That’s because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.</p>
<p>We can now do one gradient descent step, and check that our loss decreases:</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> calc_loss(coeffs, t_indep, t_dep)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>    coeffs.sub_(coeffs.grad <span class="op">*</span> <span class="fl">0.1</span>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>    coeffs.grad.zero_()</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(calc_loss(coeffs, t_indep, t_dep))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(0.4833)</code></pre>
</div>
</div>
</section>
<section id="training-the-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="training-the-linear-model">Training the linear model</h2>
<p>Before we begin training our model, we’ll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see <a href="https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners">“Getting started with NLP for absolute beginners”</a>.</p>
<p>let’s use RandomSplitter to get indices that will split our data into training and validation sets:</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.data.transforms <span class="im">import</span> RandomSplitter</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>trn_split,val_split<span class="op">=</span>RandomSplitter(seed<span class="op">=</span><span class="dv">137</span>)(dinos)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can apply those indicies to our independent and dependent variables:</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>trn_indep,val_indep <span class="op">=</span> t_indep[trn_split],t_indep[val_split]</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>trn_dep,val_dep <span class="op">=</span> t_dep[trn_split],t_dep[val_split]</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(trn_indep),<span class="bu">len</span>(val_indep)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>(48, 12)</code></pre>
</div>
</div>
<p>So our 60 examples have been split 80:20 between training set and validation set</p>
<p>We’ll create functions for the three things we did manually above:</p>
<ol type="1">
<li>updating coeffs</li>
<li>doing one full gradient descent step, and</li>
<li>initilising coeffs to random numbers:</li>
</ol>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># updating coeffs</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_coeffs(coeffs, lr):</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    coeffs.sub_(coeffs.grad <span class="op">*</span> lr)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    coeffs.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># doing one full gradient descent step</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_epoch(coeffs, lr):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> calc_loss(coeffs, trn_indep, trn_dep)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): update_coeffs(coeffs, lr)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">:.3f}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">"; "</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># initilising coeffs to random numbers</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_coeffs(): <span class="cf">return</span> (torch.rand(n_coeff)<span class="op">-</span><span class="fl">0.5</span>).requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now use these functions to train our model:</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(epochs<span class="op">=</span><span class="dv">30</span>, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    torch.manual_seed(<span class="dv">137</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    coeffs <span class="op">=</span> init_coeffs()</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs): one_epoch(coeffs, lr<span class="op">=</span>lr)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> coeffs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s try it. Our loss will print at the end of every step, so we hope we’ll see it going down:</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> train_model(<span class="dv">7</span>, lr<span class="op">=</span><span class="fl">0.9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.935; 0.722; 0.595; 0.535; 0.497; 0.463; 0.450; </code></pre>
</div>
</div>
<p>It does. Let’s take a look at the coefficients for each column:</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_coeffs(): <span class="cf">return</span> <span class="bu">dict</span>(<span class="bu">zip</span>(indep_cols, coeffs.requires_grad_(<span class="va">False</span>)))</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>show_coeffs()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>{'Min_Len': tensor(0.2610),
 'Max_Len': tensor(0.1809),
 'Min_Height': tensor(0.2024),
 'Max_Height': tensor(0.3190),
 'Min_Weight': tensor(-0.4248),
 'Max_weight': tensor(-0.2794),
 'Period_Cretaceous': tensor(0.5788),
 'Period_Jurassic': tensor(0.4300),
 'Period_Triassic': tensor(0.6414),
 'Discovered_Africa': tensor(0.4464),
 'Discovered_Antartica': tensor(0.3232),
 'Discovered_Asia': tensor(0.2413),
 'Discovered_Australia': tensor(-0.0671),
 'Discovered_Europe': tensor(0.1147),
 'Discovered_North America': tensor(0.2534),
 'Discovered_South America': tensor(-0.2927),
 'Discovered_UK': tensor(0.5903)}</code></pre>
</div>
</div>
</section>
<section id="measuring-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="measuring-accuracy">Measuring accuracy</h2>
<p>An alternative metric to absolute error (which is our loss function) is accuracy – the proportion of rows where we correctly predict meat-eater. Let’s see how accurate we were on the validation set. First, calculate the predictions:</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> calc_preds(coeffs, val_indep)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll assume that any dinosaur with a score of over 0.5 is predicted to be a meat-eater. So that means we’re correct for each row where preds&gt;0.5 is the same as the dependent variable:</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> val_dep.<span class="bu">bool</span>()<span class="op">==</span>(preds<span class="op">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>results[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>tensor([ True, False, False, False, False,  True, False,  True,  True, False])</code></pre>
</div>
</div>
<p>Let’s see what our average accuracy is:</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>results.<span class="bu">float</span>().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>tensor(0.4167)</code></pre>
</div>
</div>
<p>That’s not a great start, worse than a 50:50 guess. We’ll create a function so we can calcuate the accuracy easy for other models we train.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> acc(coeffs): <span class="cf">return</span> (val_dep.<span class="bu">bool</span>()<span class="op">==</span>(calc_preds(coeffs, val_indep)<span class="op">&gt;</span><span class="fl">0.5</span>)).<span class="bu">float</span>().mean()</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>acc(coeffs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>tensor(0.4167)</code></pre>
</div>
</div>
</section>
<section id="using-sigmoid" class="level2">
<h2 class="anchored" data-anchor-id="using-sigmoid">Using sigmoid</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> pip install sympy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Looking at our predictions, all of our predictions of the probability of meat-eater are between 0 and 1 so there is no benefit from using a sigmoid function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>preds[:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>tensor([0.7410, 0.8814, 0.8523, 0.8898, 0.8832, 0.8720, 0.7918, 0.9176, 0.5626, 0.6464, 0.6439, 0.7686])</code></pre>
</div>
</div>
<p>The sigmoid function, has a minimum at zero and maximum at one, and is defined as follows:</p>
<p>However, let’s proceed in any event for illustrative purposes:</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>sympy.plot(<span class="st">"1/(1+exp(-x))"</span>, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Excel_meets_Python_files/figure-html/cell-50-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch already defines that function for us, so we can modify calc_preds to use it:</p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_preds(coeffs, indeps): <span class="cf">return</span> torch.sigmoid((indeps<span class="op">*</span>coeffs).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s train a new model now, using this updated function to calculate predictions:</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> train_model(lr<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.515; 0.365; 0.357; 0.329; 0.340; 0.343; 0.277; 0.256; 0.299; 0.268; 0.319; 0.261; 0.221; 0.204; 0.195; 0.205; 0.353; 0.316; 0.234; 0.315; 0.300; 0.257; 0.196; 0.299; 0.284; 0.257; 0.248; 0.188; 0.230; 0.271; </code></pre>
</div>
</div>
<p>The loss has improved by a lot. Let’s check the accuracy:</p>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>acc(coeffs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>tensor(0.4167)</code></pre>
</div>
</div>
<p>As expected, that hasn’t improved. Here’s the coefficients of our trained model:</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>show_coeffs()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>{'Min_Len': tensor(1.1502),
 'Max_Len': tensor(0.5529),
 'Min_Height': tensor(-3.6500),
 'Max_Height': tensor(-1.4523),
 'Min_Weight': tensor(-8.8942),
 'Max_weight': tensor(-7.9555),
 'Period_Cretaceous': tensor(10.2919),
 'Period_Jurassic': tensor(8.6934),
 'Period_Triassic': tensor(7.9045),
 'Discovered_Africa': tensor(9.1483),
 'Discovered_Antartica': tensor(1.6134),
 'Discovered_Asia': tensor(7.2239),
 'Discovered_Australia': tensor(5.5307),
 'Discovered_Europe': tensor(-2.3057),
 'Discovered_North America': tensor(-1.3442),
 'Discovered_South America': tensor(3.7572),
 'Discovered_UK': tensor(3.2253)}</code></pre>
</div>
</div>
<p>These coefficients seem reasonable – in general, heavier dinos were less agile and therefore more likely to be veggie.</p>
</section>
<section id="using-matrix-multiplication" class="level2">
<h2 class="anchored" data-anchor-id="using-matrix-multiplication"><a href="https://en.wikipedia.org/wiki/Matrix_multiplication">Using Matrix Multiplication</a></h2>
<p>We can make things quite a bit neater…</p>
<p>Take a look at the inner-most calculation we’re doing to get the predictions:</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>(val_indep<span class="op">*</span>coeffs).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="74">
<pre><code>tensor([ 6.7534,  3.3826,  2.4559,  8.3519, 14.9058, 17.1442,  2.3888,  8.3883,  6.4539, -0.3698, -0.0583,  4.3533])</code></pre>
</div>
</div>
<p>Multiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> timedelta</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.monotonic()</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.monotonic()</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(timedelta(seconds<span class="op">=</span>end_time <span class="op">-</span> start_time))</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a>(val_indep<span class="op">*</span>coeffs).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a>val_indep<span class="op">@</span>coeffs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0:00:00.000017</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="75">
<pre><code>tensor([ 6.7534,  3.3826,  2.4559,  8.3519, 14.9058, 17.1442,  2.3888,  8.3883,  6.4539, -0.3698, -0.0583,  4.3533])</code></pre>
</div>
</div>
<p>It also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.</p>
<p>Let’s use this to replace how calc_preds works:</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_preds(coeffs, indeps): <span class="cf">return</span> torch.sigmoid(indeps<span class="op">@</span>coeffs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In order to do matrix-matrix products (which we’ll need in the next section), we need to turn coeffs into a column vector (i.e.&nbsp;a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:</p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_coeffs(): <span class="cf">return</span> (torch.rand(n_coeff, <span class="dv">1</span>)<span class="op">*</span><span class="fl">0.1</span>).requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>trn_dep <span class="op">=</span> trn_dep[:,<span class="va">None</span>]</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>val_dep <span class="op">=</span> val_dep[:,<span class="va">None</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now train our model as before and confirm we get identical outputs…:</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> train_model(lr<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.494; 0.365; 0.360; 0.358; 0.357; 0.357; 0.357; 0.356; 0.356; 0.356; 0.356; 0.356; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; </code></pre>
</div>
</div>
<p>…and identical accuracy:</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>acc(coeffs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="80">
<pre><code>tensor(0.4167)</code></pre>
</div>
</div>
</section>
<section id="a-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="a-neural-network">A neural network</h2>
<p>We’ve now got what we need to implement our neural network.</p>
<p>First, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.</p>
<p>Then our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_coeffs(n_hidden<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    layer1 <span class="op">=</span> (torch.rand(n_coeff, n_hidden)<span class="op">-</span><span class="fl">0.5</span>)<span class="op">/</span>n_hidden</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>    layer2 <span class="op">=</span> torch.rand(n_hidden, <span class="dv">1</span>)<span class="op">-</span><span class="fl">0.3</span></span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>    const <span class="op">=</span> torch.rand(<span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_preds(coeffs, indeps):</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    l1,l2,const <span class="op">=</span> coeffs</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> F.relu(indeps<span class="op">@</span>l1)</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>l2 <span class="op">+</span> const</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.sigmoid(res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, now that we have more than one set of coefficients, we need to add a loop to update each one:</p>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_coeffs(coeffs, lr):</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> coeffs:</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>        layer.sub_(layer.grad <span class="op">*</span> lr)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>        layer.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>That’s it – we’re now ready to train our model!</p>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> train_model(lr<span class="op">=</span><span class="fl">1.4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.456; 0.446; 0.437; 0.428; 0.421; 0.414; 0.408; 0.403; 0.398; 0.394; 0.391; 0.388; 0.385; 0.383; 0.381; 0.379; 0.377; 0.375; 0.374; 0.373; 0.372; 0.371; 0.370; 0.369; 0.368; 0.367; 0.366; 0.366; 0.365; 0.364; </code></pre>
</div>
</div>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> train_model(lr<span class="op">=</span><span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.456; 0.373; 0.363; 0.360; 0.358; 0.357; 0.357; 0.356; 0.356; 0.356; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; </code></pre>
</div>
</div>
<p>It’s looking good – our loss is lower than before. Let’s see if that translates to a better result on the validation set:</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>acc(coeffs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>tensor(0.4167)</code></pre>
</div>
</div>
<p>In this case our neural net isn’t showing better results than the linear model. That’s not surprising; this dataset is very small and very simple, and isn’t the kind of thing we’d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!</p>
</section>
<section id="deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning">Deep learning</h2>
<p>The neural net in the previous section only uses one hidden layer, so it doesn’t count as “deep” learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.</p>
<p>First, we’ll need to create additional coefficients for each layer:</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_coeffs():</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>    hiddens <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">10</span>]  <span class="co"># &lt;-- set this to the size of each hidden layer you want</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>    sizes <span class="op">=</span> [n_coeff] <span class="op">+</span> hiddens <span class="op">+</span> [<span class="dv">1</span>]</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(sizes)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [(torch.rand(sizes[i], sizes[i<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="fl">0.3</span>)<span class="op">/</span>sizes[i<span class="op">+</span><span class="dv">1</span>]<span class="op">*</span><span class="dv">4</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>    consts <span class="op">=</span> [(torch.rand(<span class="dv">1</span>)[<span class="dv">0</span>]<span class="op">-</span><span class="fl">0.5</span>)<span class="op">*</span><span class="fl">0.1</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> layers<span class="op">+</span>consts: l.requires_grad_()</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> layers,consts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You’ll notice here that there’s a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you’ll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days – it’s very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we’ll learn about in other notebooks.</p>
<p>Our deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_preds(coeffs, indeps):</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>    layers,consts <span class="op">=</span> coeffs</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(layers)</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> indeps</span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i,l <span class="kw">in</span> <span class="bu">enumerate</span>(layers):</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> res<span class="op">@</span>l <span class="op">+</span> consts[i]</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i<span class="op">!=</span>n<span class="op">-</span><span class="dv">1</span>: res <span class="op">=</span> F.relu(res)</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.sigmoid(res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We also need a minor update to update_coeffs since we’ve got layers and consts separated now:</p>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_coeffs(coeffs, lr):</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    layers,consts <span class="op">=</span> coeffs</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers<span class="op">+</span>consts:</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>        layer.sub_(layer.grad <span class="op">*</span> lr)</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>        layer.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s train our model…</p>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> train_model(lr<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.376; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; </code></pre>
</div>
</div>
<p>…and check its accuracy:</p>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>acc(coeffs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>tensor(0.4167)</code></pre>
</div>
</div>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final thoughts</h2>
<p>The main takeaway from this project, apart from hanging out with my son, and finding out about cool dinos, is that I have successfully managed to:</p>
<ul>
<li>clean the data using Python and carry out some Exploratory Data Analysis (EDA)</li>
<li>use Broadcasting to carry out matrix multiplicaiton</li>
</ul>
<p><img src="Matrix.jpg" style="width:200px;height:200px"></p>
<ul>
<li>create a real deep learning model from scratch using Python and train it</li>
</ul>
<p>The “real” deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you’ll recognise the basic steps are the same.</p>
<p>The biggest differences in practical models to what we have above are:</p>
<ul>
<li>How initialisation and normalisation is done to ensure the model trains correctly every time</li>
<li>Regularization (to avoid over-fitting)</li>
<li>Modifying the neural net itself to take advantage of knowledge of the problem domain</li>
<li>Doing gradient descent steps on smaller batches, rather than the whole dataset.</li>
</ul>
<p>Hopefully, some of the techniques included in this NoteBook prove to be helpful to other data science newcomers like me. Looking forward to Lesson 6!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>