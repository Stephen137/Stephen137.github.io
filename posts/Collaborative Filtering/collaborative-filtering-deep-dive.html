<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Stephen Barrie">
<meta name="dcterms.date" content="2023-01-13">

<title>Into the Unknown - Collaborative Filtering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Into the Unknown</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">Stephen Barrie</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Stephen137"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/sjbarrie"><i class="bi bi-linkedin" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Collaborative Filtering</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Collaborative Filtering</div>
                <div class="quarto-category">fastai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Stephen Barrie </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 13, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#recommendation-systems" id="toc-recommendation-systems" class="nav-link active" data-scroll-target="#recommendation-systems">Recommendation Systems</a>
  <ul class="collapse">
  <li><a href="#a-first-look-at-the-data" id="toc-a-first-look-at-the-data" class="nav-link" data-scroll-target="#a-first-look-at-the-data">A First Look at the Data</a></li>
  <li><a href="#collaborative-filtering---using-excel" id="toc-collaborative-filtering---using-excel" class="nav-link" data-scroll-target="#collaborative-filtering---using-excel">Collaborative filtering - using Excel</a></li>
  <li><a href="#using-pytorch-to-do-the-same-thing" id="toc-using-pytorch-to-do-the-same-thing" class="nav-link" data-scroll-target="#using-pytorch-to-do-the-same-thing">Using PyTorch to do the same thing</a></li>
  <li><a href="#embedding-layer" id="toc-embedding-layer" class="nav-link" data-scroll-target="#embedding-layer">Embedding layer</a></li>
  <li><a href="#creating-a-collaborative-filtering-model-in-pytorch-from-scratch" id="toc-creating-a-collaborative-filtering-model-in-pytorch-from-scratch" class="nav-link" data-scroll-target="#creating-a-collaborative-filtering-model-in-pytorch-from-scratch">Creating a Collaborative Filtering model in PyTorch from Scratch</a></li>
  <li><a href="#squeezing-our-predictions-using-sigmoid" id="toc-squeezing-our-predictions-using-sigmoid" class="nav-link" data-scroll-target="#squeezing-our-predictions-using-sigmoid">Squeezing our predictions using Sigmoid</a></li>
  <li><a href="#introducing-bias-into-our-model" id="toc-introducing-bias-into-our-model" class="nav-link" data-scroll-target="#introducing-bias-into-our-model">Introducing Bias into our model</a></li>
  <li><a href="#weight-decay-l2-regularization" id="toc-weight-decay-l2-regularization" class="nav-link" data-scroll-target="#weight-decay-l2-regularization">Weight Decay (L2 regularization)</a></li>
  <li><a href="#creating-our-own-embedding-module" id="toc-creating-our-own-embedding-module" class="nav-link" data-scroll-target="#creating-our-own-embedding-module">Creating Our Own Embedding Module</a></li>
  <li><a href="#interpreting-embeddings-and-biases" id="toc-interpreting-embeddings-and-biases" class="nav-link" data-scroll-target="#interpreting-embeddings-and-biases">Interpreting Embeddings and Biases</a></li>
  <li><a href="#using-fastai.collab" id="toc-using-fastai.collab" class="nav-link" data-scroll-target="#using-fastai.collab">Using fastai.collab</a></li>
  <li><a href="#embedding-distance" id="toc-embedding-distance" class="nav-link" data-scroll-target="#embedding-distance">Embedding Distance</a></li>
  <li><a href="#bootstrapping-a-collaborative-filtering-model" id="toc-bootstrapping-a-collaborative-filtering-model" class="nav-link" data-scroll-target="#bootstrapping-a-collaborative-filtering-model">Bootstrapping a Collaborative Filtering Model</a></li>
  <li><a href="#deep-learning-for-collaborative-filtering---from-scratch" id="toc-deep-learning-for-collaborative-filtering---from-scratch" class="nav-link" data-scroll-target="#deep-learning-for-collaborative-filtering---from-scratch">Deep Learning for Collaborative Filtering - from scratch</a></li>
  <li><a href="#deep-learning-for-collaborative-filtering---using-fast.ai" id="toc-deep-learning-for-collaborative-filtering---using-fast.ai" class="nav-link" data-scroll-target="#deep-learning-for-collaborative-filtering---using-fast.ai">Deep Learning for Collaborative Filtering - using fast.ai</a></li>
  <li><a href="#kwargs-and-delegates" id="toc-kwargs-and-delegates" class="nav-link" data-scroll-target="#kwargs-and-delegates">kwargs and Delegates</a></li>
  <li><a href="#natural-language-processing-nlp" id="toc-natural-language-processing-nlp" class="nav-link" data-scroll-target="#natural-language-processing-nlp">Natural Language Processing (NLP)</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This is my follow up to the second part of <a href="https://www.youtube.com/watch?v=p4ZZq0736Po"><strong><em>Lesson 7: Practical Deep Learning for Coders 2022</em></strong></a> in which Jeremy shows how to build a Collaborative Filtering model from scratch, within Excel, and also using PyTorch, and explains <code>latent factors</code> and <code>emdedding</code></p>
<section id="recommendation-systems" class="level2">
<h2 class="anchored" data-anchor-id="recommendation-systems">Recommendation Systems</h2>
<p>One very common problem to solve is when you have a number of users and a number of products, and you want to recommend which products are most likely to be useful for which users. There are many variations of this: for example, recommending movies (such as on Netflix), figuring out what to highlight for a user on a home page, deciding what stories to show in a social media feed, and so forth. There is a general solution to this problem, called <em>collaborative filtering</em>, which works like this: look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend other products that those users have used or liked.</p>
<p>For example, on Netflix you may have watched lots of movies that are science fiction, full of action, and were made in the 1970s. Netflix may not know these particular properties of the films you have watched, but it will be able to see that other people that have watched the same movies that you watched also tended to watch other movies that are science fiction, full of action, and were made in the 1970s. In other words, to use this approach we don’t necessarily need to know anything about the movies, except who like to watch them.</p>
<p>There is actually a more general class of problems that this approach can solve, not necessarily involving users and products. Indeed, for collaborative filtering we more commonly refer to <em>items</em>, rather than <em>products</em>. Items could be links that people click on, diagnoses that are selected for patients, and so forth.</p>
<p>The key foundational idea is that of <em>latent factors</em>. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.</p>
<blockquote class="blockquote">
<p><em>This is chapter 8 of the book <a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">Practical Deep Learning for Coders</a>, provided courtesy of O’Reilly Media. The full book is <a href="https://github.com/fastai/fastbook/">available as Jupyter Notebooks</a>. A free course that covers the book is <a href="https://course.fast.ai/">available here</a>.</em></p>
</blockquote>
<p>For this chapter we are going to work on this movie recommendation problem. We’ll start by getting some data suitable for a collaborative filtering model.</p>
<section id="a-first-look-at-the-data" class="level3">
<h3 class="anchored" data-anchor-id="a-first-look-at-the-data">A First Look at the Data</h3>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load required packages and set seed for reproducibility</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.collab <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.tabular.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We do not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can use, called <a href="https://grouplens.org/datasets/movielens/">MovieLens</a>. This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If you’re interested, it would be a great learning project to try and replicate this approach on the full 25-million recommendation dataset, which you can get from their website.</p>
<p>The dataset is available through the usual fastai function:</p>
<div class="cell" data-tags="[]" data-execution_count="29">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> untar_data(URLs.ML_100k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>According to the <em>README</em>, the main table is in the file <em>u.data</em>. It is tab-separated and the columns are, respectively user, movie, rating, and timestamp. Since those names are not encoded, we need to indicate them when reading the file with Pandas. Here is a way to open this table and take a look:</p>
<div class="cell" data-tags="[]" data-execution_count="30">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in table - specify colums names</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ratings <span class="op">=</span> pd.read_csv(path<span class="op">/</span><span class="st">'u.data'</span>, delimiter<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, header<span class="op">=</span><span class="va">None</span>, <span class="co"># tab(t) separated file, instead of a comma(c) separated file</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                      names<span class="op">=</span>[<span class="st">'user'</span>,<span class="st">'movie'</span>,<span class="st">'rating'</span>,<span class="st">'timestamp'</span>]) <span class="co"># need to specify columns as not encoded</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># look at the first 5 rows</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>ratings.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>user</th>
      <th>movie</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>196</td>
      <td>242</td>
      <td>3</td>
      <td>881250949</td>
    </tr>
    <tr>
      <th>1</th>
      <td>186</td>
      <td>302</td>
      <td>3</td>
      <td>891717742</td>
    </tr>
    <tr>
      <th>2</th>
      <td>22</td>
      <td>377</td>
      <td>1</td>
      <td>878887116</td>
    </tr>
    <tr>
      <th>3</th>
      <td>244</td>
      <td>51</td>
      <td>2</td>
      <td>880606923</td>
    </tr>
    <tr>
      <th>4</th>
      <td>166</td>
      <td>346</td>
      <td>1</td>
      <td>886397596</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Although this has all the information we need, it is not a particularly helpful way for humans to look at this data. Here is the same data cross-tabulated into a human-friendly table:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/9cbc69a5-40ca-4406-9bde-0db7c9887870.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">image.png</figcaption><p></p>
</figure>
</div>
<p>We have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.</p>
<p>If we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and +1, with positive numbers indicating stronger matches and negative numbers weaker ones, and the categories are science-fiction, action, and old movies, then we could represent the movie <em>The Last Skywalker</em> as:</p>
<div class="cell" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># embed features of the movie The Last Skywalker by creating vector of values between -1 and +1</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># science fiction 0.98, action 0.9, old movies -0.9</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>last_skywalker <span class="op">=</span> np.array([<span class="fl">0.98</span>,<span class="fl">0.9</span>,<span class="op">-</span><span class="fl">0.9</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, for instance, we are scoring <em>very science-fiction</em> as 0.98, <em>very action</em> as 0.9, and <em>very not old</em> as -0.9. We could represent a user who likes modern sci-fi action movies as:</p>
<div class="cell" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># embed the features of a user based on their movie preferences by creating vector of values between -1 and +1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># science fiction 0.9, action 0.8, old movies -0.6</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>user1 <span class="op">=</span> np.array([<span class="fl">0.9</span>,<span class="fl">0.8</span>,<span class="op">-</span><span class="fl">0.6</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and we can now calculate the match between this combination:</p>
<div class="cell" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the dot product of the two vectors to see whether LastSkywalker is a good match for user 1</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>(user1<span class="op">*</span>last_skywalker).<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>2.1420000000000003</code></pre>
</div>
</div>
<p>When we multiply two vectors together and add up the results, this is known as the <em>dot product</em>. It is used a lot in machine learning, and forms the basis of matrix multiplication. We will be looking a lot more at matrix multiplication and dot products later.</p>
<blockquote class="blockquote">
<p>jargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result.</p>
</blockquote>
<p>On the other hand, we might represent the movie <em>Casablanca</em> as:</p>
<div class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># embed features of the movie Casablanca by creating vector of values between -1 and +1</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># science fiction 0.98, action 0.9, old movies -0.9</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>casablanca <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.99</span>,<span class="op">-</span><span class="fl">0.3</span>,<span class="fl">0.8</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The match between this combination is:</p>
<div class="cell" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the dot product of the two vectors to see whether Casabalance is a good match for user 1</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>(user1<span class="op">*</span>casablanca).<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>-1.611</code></pre>
</div>
</div>
<p>Since we don’t know what <code>latent factors</code> actually are, and we don’t know how to score them for each user and movie, we should learn them.</p>
</section>
<section id="collaborative-filtering---using-excel" class="level3">
<h3 class="anchored" data-anchor-id="collaborative-filtering---using-excel">Collaborative filtering - using Excel</h3>
<p>The problem is we haven’t been given any information about the users, or the movies, and we might not even know what things about movies actually matter to users. But, not to worry, we can just use Stochastic Gradient Descent (SGD) to find them!</p>
<p>There is surprisingly little difference between specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general <code>gradient descent</code> approach.</p>
<blockquote class="blockquote">
<p>Step 1: randomly initialize some parameters</p>
</blockquote>
<p>These parameters will be a set of <code>latent factors</code> for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes let’s use 5 for now. Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle.</p>
<p>So, the initialized latent factors for <code>movieId 27</code> are 0.71, 0.81, 0.74, 0.04, 0.04 and the latent factors for <code>userID 14</code> are 0.19, 0.63, 0.31, 0.44, 0.51. We then multiply these together using the <code>MMULT</code> matrix multiplication function within Excel to obtain our initial predictions.</p>
<p>We don’t know what these factors are, but for example we can interpret that <code>userID 14</code> doesn’t feel very strongly, with a value of <code>0.19</code> about <code>movieID</code> factor 1 which has a value of <code>0.71</code></p>
<p>This is what it looks like in Microsoft Excel:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/0fb84ac2-fe09-4175-a46e-943bba1ea7f1.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">latent_factors.JPG</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Step 2: Calculate our predictions using Matrix Multiplication</p>
</blockquote>
<p>As we’ve discussed, we can do this by simply taking the dot product of each movie with each user. If, for instance, the first latent <code>user</code> factor represents how much the user likes action movies, and the first latent <code>movie</code> factor represents if the movie has a lot of action or not, the product of those will be particularly <code>high</code> if either <code>the user likes action movies and the movie has a lot of action in it</code> or <code>the user doesn't like action movies and the movie doesn't have any action in it</code>. On the other hand, if we have a mismatch (a user loves action movies but the movie isn’t an action film, or the user doesn’t like action movies and it is one), the product will be very low.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/7b668964-a0f6-4e7a-a92b-0cb7782686a1.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">initialized.JPG</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Step 3: calculate our loss</p>
</blockquote>
<p>We can use any loss function that we wish; let’s pick <code>mean squared error</code> for now, since that is one reasonable way to represent the accuracy of a prediction.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/f2041a2e-e5f0-480b-b75b-d2a6a5142c3b.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">initialized.JPG</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>Step 4: optimize using Stochastic Gradient Descent(SGD) - the Solver function in Excel approximates this</p>
</blockquote>
<p>That’s all we need. With this in place, we can optimize our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/b9ed0766-c860-44a6-9a09-3e4a3fc86baa.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">optimized.JPG</figcaption><p></p>
</figure>
</div>
<p>The above spreadsheet screenshot shows the updated predictions after applying Stohastic Gradient Descent using Excel’s inbuilt Solver function - note that the movie rating predictions are now much more in line with the actual ratings (with values betwen 0 and 5) and our loss function RMSE has reduced from 2.8 to 0.42.</p>
</section>
<section id="using-pytorch-to-do-the-same-thing" class="level3">
<h3 class="anchored" data-anchor-id="using-pytorch-to-do-the-same-thing">Using PyTorch to do the same thing</h3>
<p>To use the usual <code>Learner.fit</code> function we will need to get our data into a <code>DataLoaders</code>, so let’s focus on that now.</p>
<p>When showing the data, we would rather see movie titles than their IDs. The table <code>u.item</code> contains the correspondence of IDs to titles:</p>
<div class="cell" data-tags="[]" data-execution_count="31">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in movie titles table</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>movies <span class="op">=</span> pd.read_csv(path<span class="op">/</span><span class="st">'u.item'</span>,  delimiter<span class="op">=</span><span class="st">'|'</span>, encoding<span class="op">=</span><span class="st">'latin-1'</span>, <span class="co">#</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                     usecols<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">1</span>), names<span class="op">=</span>(<span class="st">'movie'</span>,<span class="st">'title'</span>), header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>movies.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>movie</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Toy Story (1995)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>GoldenEye (1995)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Four Rooms (1995)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Get Shorty (1995)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Copycat (1995)</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>We can merge this with our <code>ratings</code> table to get the user ratings by title:</p>
<div class="cell" data-tags="[]" data-execution_count="32">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># merge ratings and movie tables</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>ratings <span class="op">=</span> ratings.merge(movies)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>ratings.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>user</th>
      <th>movie</th>
      <th>rating</th>
      <th>timestamp</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>196</td>
      <td>242</td>
      <td>3</td>
      <td>881250949</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>63</td>
      <td>242</td>
      <td>3</td>
      <td>875747190</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>226</td>
      <td>242</td>
      <td>5</td>
      <td>883888671</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>154</td>
      <td>242</td>
      <td>3</td>
      <td>879138235</td>
      <td>Kolya (1996)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>306</td>
      <td>242</td>
      <td>5</td>
      <td>876503793</td>
      <td>Kolya (1996)</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>We can now build a <code>DataLoaders</code> object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of <code>item_name</code> in our case to use the titles instead of the IDs:</p>
<div class="cell" data-tags="[]" data-execution_count="33">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build a Collaborative Filtering DataLoaders from out ratings DataFrame</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># needs a user column and an item column - we have a user column called user so don't need to pass in</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> CollabDataLoaders.from_df(ratings, item_name<span class="op">=</span><span class="st">'title'</span>, bs<span class="op">=</span><span class="dv">64</span>) <span class="co"># need to pass in item_name to get title</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>dls.show_batch()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>user</th>
      <th>title</th>
      <th>rating</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>542</td>
      <td>My Left Foot (1989)</td>
      <td>4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>422</td>
      <td>Event Horizon (1997)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>311</td>
      <td>African Queen, The (1951)</td>
      <td>4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>595</td>
      <td>Face/Off (1997)</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>617</td>
      <td>Evil Dead II (1987)</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>158</td>
      <td>Jurassic Park (1993)</td>
      <td>5</td>
    </tr>
    <tr>
      <th>6</th>
      <td>836</td>
      <td>Chasing Amy (1997)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>474</td>
      <td>Emma (1996)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>466</td>
      <td>Jackie Chan's First Strike (1996)</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>554</td>
      <td>Scream (1996)</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>To represent collaborative filtering in PyTorch we can’t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:</p>
<div class="cell" data-tags="[]" data-execution_count="34">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>n_users  <span class="op">=</span> <span class="bu">len</span>(dls.classes[<span class="st">'user'</span>]) <span class="co"># set number of users = number of rows of users</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>n_movies <span class="op">=</span> <span class="bu">len</span>(dls.classes[<span class="st">'title'</span>]) <span class="co"># set number of movies = nuumber of rows of movies</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>n_factors <span class="op">=</span> <span class="dv">5</span> <span class="co"># set number of columns (latent factors) to whatever we want</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create initial random weightings for user latent factors</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># user EMBEDDING matrix</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>user_factors <span class="op">=</span> torch.randn(n_users, n_factors) <span class="co"># random tensors </span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># create initial random weightings for movie latent factors</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># movie EMBEDDING matrix</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>movie_factors <span class="op">=</span> torch.randn(n_movies, n_factors) <span class="co"># random tensors </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note fast.ai has a built in formula for setting an appropriate number of latent factors</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>user_factors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[-1.0827,  0.2138,  0.9310, -0.2739, -0.4359],
        [-0.5195,  0.7613, -0.4365,  0.1365,  1.3300],
        [-1.2804,  0.0705,  0.6489, -1.2110,  1.8266],
        ...,
        [ 0.8009, -0.4734, -0.8962, -0.7348, -0.0246],
        [ 0.3354, -0.8262, -0.1541,  0.4699,  0.4873],
        [ 2.4054, -0.2156, -1.4126, -0.2467,  1.0571]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>movie_factors</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>tensor([[-0.3978,  0.4563,  1.2301,  0.3745,  0.9689],
        [-1.1836, -0.5818, -0.5587, -0.4316,  0.2128],
        [ 0.0420,  1.3201, -0.7999,  1.1123, -0.7585],
        ...,
        [ 2.4743,  1.3068,  0.4540,  0.6958,  0.5228],
        [ 2.3970, -0.2559, -1.7196,  1.0440, -0.2662],
        [ 0.2786, -0.6593,  0.5260, -0.3416, -1.3938]])</code></pre>
</div>
</div>
<p>To calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But <code>look up in an index</code> is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.</p>
<p>Fortunately, it turns out that we can represent <em>look up in an index</em> as a <code>matrix product</code>. The trick is to <code>replace our indices with one-hot-encoded vectors</code>. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:</p>
<p>Taking the dot product of a one hot coded vector and something, is the same as looking up the index in an array.</p>
<div class="cell" data-tags="[]" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a one-hot encoded vector of length n_users, with 2nd element set to 1 and everything else set to 0</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>one_hot_2 <span class="op">=</span> one_hot(<span class="dv">2</span>, n_users).<span class="bu">float</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-tags="[]" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># matrix multiplication - users</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># .t transposes cols and rows to enable matrix multiplication</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># @ is the symbol for matrix multipy</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>user_factors.t() <span class="op">@</span> one_hot_2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor([-1.2804,  0.0705,  0.6489, -1.2110,  1.8266])</code></pre>
</div>
</div>
<p>It gives us the same vector as the one at index 2 in the <code>user_factor</code> matrix as shown previously.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a one-hot encoded vector of length n_users, with 1st element set to 1 and everything else set to 0</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>one_hot_1 <span class="op">=</span> one_hot(<span class="dv">1</span>, n_movies).<span class="bu">float</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># matrix multiplication - movie</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># .t transposes cols and rows to enable matrix multiplication</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># @ is the symbol for matrix multipy</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>movie_factors.t() <span class="op">@</span> one_hot_1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>tensor([-1.1836, -0.5818, -0.5587, -0.4316,  0.2128])</code></pre>
</div>
</div>
<p>It gives us the same vector as the one at index 1 in the <code>movie_factors</code> matrix as shown previously.</p>
</section>
<section id="embedding-layer" class="level3">
<h3 class="anchored" data-anchor-id="embedding-layer">Embedding layer</h3>
<p>If we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a <code>matrix multiplication</code>! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one — <code>we should just be able to index into an array directly with an integer</code>. Therefore, most deep learning libraries, including <code>PyTorch</code>, include a <code>special layer</code> that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an <code>embedding</code>.</p>
<blockquote class="blockquote">
<p>jargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the <code>embedding matrix</code>.</p>
</blockquote>
<p>In computer vision, we have a very easy way to get all the information of a pixel through its RGB values: each pixel in a colored image is represented by three numbers. Those three numbers give us the redness, the greenness and the blueness, which is enough to get our model to work afterward (with values between 0 and 255).</p>
<p>For the problem at hand, we don’t have the same easy way to characterize a user or a movie. There are probably relations with genres: if a given user likes romance, they are likely to give higher scores to romance movies. Other factors might be whether the movie is more action-oriented versus heavy on dialogue, or the presence of a specific actor that a user might particularly like.</p>
<p>How do we determine numbers to characterize those? The answer is, we don’t. We will let our model <code>learn</code> them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not. This is what <code>embeddings</code> are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, <code>n_factors=5</code>), and we will make those <code>learnable parameters</code>. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will <code>compute the gradients of the loss with respect to those embedding vectors</code> and update them with the rules of SGD (or another optimizer).</p>
<p>At the beginning, those numbers don’t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data about the relations between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance, and so on.</p>
<p>We are now in a position that we can create our whole model from scratch.</p>
</section>
<section id="creating-a-collaborative-filtering-model-in-pytorch-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-collaborative-filtering-model-in-pytorch-from-scratch">Creating a Collaborative Filtering model in PyTorch from Scratch</h3>
<p>Before we can write a model in PyTorch, we first need to learn the basics of <code>object-oriented programming</code> and Python. If you haven’t done any object-oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and getting some practice before moving on.</p>
<p>The key idea in object-oriented programming is the <code>class</code>. A model is a <code>class</code>. We have been using classes throughout this book, such as <code>DataLoader</code>, <code>string</code>, and <code>Learner</code>. Python also makes it easy for us to create new classes. Here is an example of a simple class:</p>
<div class="cell" data-tags="[]" data-execution_count="26">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example of a simple class</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Example:</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, a): <span class="va">self</span>.a <span class="op">=</span> a <span class="co"># __init__ any method surrounded in double underscores like this is considered special</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> say(<span class="va">self</span>,x): <span class="cf">return</span> <span class="ss">f'Hello </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>a<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">.'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The most important piece of this is the special method called <code>__init__</code> (pronounced <em>dunder init</em>). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behavior associated with this method name. In the case of <code>__init__</code>, this is the method <code>Python will call when your new object is created</code>. So, this is where you can <code>set up any state that needs to be initialized upon object creation</code>.</p>
<p>Any parameters included when the user constructs an instance of your class will be passed to the <code>__init__</code> method as parameters. Note that the <code>first parameter</code> to any method defined inside a class is <code>self</code>, so you can use this to <code>set and get any attributes that you will need</code>:</p>
<div class="cell" data-tags="[]" data-execution_count="27">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ex <span class="op">=</span> Example(<span class="st">'Sylvain'</span>) <span class="co"># so self.a now equals Sylvain</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>ex.say(<span class="st">'nice to meet you'</span>) <span class="co"># x is now 'nice to meet you - we can access the say function within the Example class using .say</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>'Hello Sylvain, nice to meet you.'</code></pre>
</div>
</div>
<p>Also note that creating a new PyTorch module requires inheriting from <code>Module</code>. <em>Inheritance</em> is an important object-oriented concept that we will not discuss in detail here—in short, it means that we can add additional behavior to an existing class. PyTorch already provides a <code>Module</code> class, which provides some basic foundations that we want to build on. So, we add the name of this <code>superclass</code> after the name of the class that we are defining, as shown in the following example.</p>
<p>The final thing that you need to know to <code>create a new PyTorch module</code> is that when your module is called, PyTorch will call a method in your class called <code>forward</code>, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:</p>
<div class="cell" data-tags="[]" data-execution_count="31">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a class to define our dot product module</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DotProduct(Module): <span class="co"># putting something in parentheses after a class name creates a SUPERclass</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_users, n_movies, n_factors): <span class="co"># specify number of users, movies, and latent factors</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_factors <span class="op">=</span> Embedding(n_users, n_factors) <span class="co"># create Embedding matrix for users - we will cover how to create Embedding Class later</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.movie_factors <span class="op">=</span> Embedding(n_movies, n_factors) <span class="co"># create Embedding matrix for movies - we will cover how to create Embedding Class later</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculation of our model has to be defined in a function called forward</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):  <span class="co"># pass the object itself and thing calculating on - user and movie for a batch</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>                           <span class="co"># each row will be one user and movie combination, columns will be users and movies</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        users <span class="op">=</span> <span class="va">self</span>.user_factors(x[:,<span class="dv">0</span>]) <span class="co"># grab first column i.e every row, and look it up using our user Embedding matrix</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        movies <span class="op">=</span> <span class="va">self</span>.movie_factors(x[:,<span class="dv">1</span>]) <span class="co"># grab second column i.e every row, and look it up using our movie Embedding matrix</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (users <span class="op">*</span> movies).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># calculate the dot product - # dim = 1 because we are summing across COLUMNS for each row # dim = 0 would sum across ROWS</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you haven’t seen object-oriented programming before, then don’t worry, you won’t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax.</p>
<p>Note that the input of the model is a tensor of shape <code>batch_size x 2</code>, where the first column (<code>x[:, 0]</code>) contains the user IDs and the second column (<code>x[:, 1]</code>) contains the movie IDs. As explained before, we use the <em>embedding</em> layers to represent our matrices of user and movie latent factors:</p>
<div class="cell" data-tags="[]" data-execution_count="30">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inputs to the model are 64 rows x 2 columns - column 0 user IDs and column 1 movie IDs</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>x,y <span class="op">=</span> dls.one_batch()</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>torch.Size([64, 2])</code></pre>
</div>
</div>
<p>Now that we have defined our architecture, and created our parameter matrices, we need to create a <code>Learner</code> to optimize our model. In the past we have used special functions, such as <code>cnn_learner</code>, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain <code>Learner</code> class:</p>
<div class="cell" data-tags="[]" data-execution_count="32">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define our Dot Product model</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProduct(n_users, n_movies, <span class="dv">50</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># we can pass our Dot Product class to our learner</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We are now ready to fit our model:</p>
<div class="cell" data-tags="[]" data-execution_count="33">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit (train) our model</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>) <span class="co"># 5 epochs, learning rate 5e^-3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.385412</td>
      <td>1.293633</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.061318</td>
      <td>1.070560</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.968811</td>
      <td>0.976037</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.862989</td>
      <td>0.883624</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.797610</td>
      <td>0.869864</td>
      <td>00:04</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</section>
<section id="squeezing-our-predictions-using-sigmoid" class="level3">
<h3 class="anchored" data-anchor-id="squeezing-our-predictions-using-sigmoid">Squeezing our predictions using Sigmoid</h3>
<p>The first thing we can do to make this model a little bit better is to force those predictions to be between 0 and 5. For this, we just need to use <code>sigmoid_range</code>. Sigmoid on its own squeezes values between 0 and 1 but if we multiply by 5 that wil ensure the values are between 0 and 5. One thing we discovered empirically is that it’s better to have the range go a little bit over 5, so we use <code>(0, 5.5)</code>:</p>
<div class="cell" data-tags="[]" data-execution_count="34">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tweak our Dot Product Class to squeeze preds between 0 and 5</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DotProduct(Module):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_users, n_movies, n_factors, y_range<span class="op">=</span>(<span class="dv">0</span>,<span class="fl">5.5</span>)): <span class="co"># set range for predictions between 0 and 5 (with a little bit extra for comfort) </span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_factors <span class="op">=</span> Embedding(n_users, n_factors) <span class="co"># create Embedding matrix for users - we will cover how to create Embedding Class later</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.movie_factors <span class="op">=</span> Embedding(n_movies, n_factors) <span class="co"># create Embedding matrix for movies - we will cover how to create Embedding Class later</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y_range <span class="op">=</span> y_range <span class="co"># range of predictions specified</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        users <span class="op">=</span> <span class="va">self</span>.user_factors(x[:,<span class="dv">0</span>]) <span class="co"># grab first column i.e every row, and look it up using our user Embedding matrix</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        movies <span class="op">=</span> <span class="va">self</span>.movie_factors(x[:,<span class="dv">1</span>]) <span class="co"># grab second column i.e every row, and look it up using our movie Embedding matrix</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid_range((users <span class="op">*</span> movies).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>), <span class="op">*</span><span class="va">self</span>.y_range) <span class="co"># force predictions to be between 0 and 5 using sigmoid function</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-tags="[]" data-execution_count="35">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># redefine our Dot Product model</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProduct(n_users, n_movies, <span class="dv">50</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pass in our Dot Product class to our learner as before</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit (train) our model</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>) <span class="co"># 5 epochs, learning rate 5e^-3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.991383</td>
      <td>0.971459</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.862119</td>
      <td>0.888047</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.677498</td>
      <td>0.857523</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.464585</td>
      <td>0.863056</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.384263</td>
      <td>0.867252</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>This is negligibly better, but we cann improve on this.</p>
</section>
<section id="introducing-bias-into-our-model" class="level3">
<h3 class="anchored" data-anchor-id="introducing-bias-into-our-model">Introducing Bias into our model</h3>
<p>One obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it.</p>
<p>That’s because at this point we only have weights; we do not have <code>biases</code>. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. Let’s first look at this in Excel - we simply initialize an additional randomized <code>bias factor</code> to add to our existing <code>latent factors</code> and then optimize as before. This results in an improvement - our RMSE drops from 0.42 to 0.35 - see spreadsheet screenshot below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/116f9d3a-7fa8-4c06-af73-d5f545c8db8a.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">bias.JPG</figcaption><p></p>
</figure>
</div>
<p>Let’s jump back to Python and adjust our model architecture there to introduce bias into our model:</p>
<div class="cell" data-tags="[]" data-execution_count="36">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create new Class to include bias </span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DotProductBias(Module):</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_users, n_movies, n_factors, y_range<span class="op">=</span>(<span class="dv">0</span>,<span class="fl">5.5</span>)): <span class="co"># set range for predictions between 0 and 5 (with a little bit extra for comfort as sigmoid won't return as high as 1)</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_factors <span class="op">=</span> Embedding(n_users, n_factors) <span class="co"># create Embedding matrix for users - we will cover how to create Embedding Class later</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_bias <span class="op">=</span> Embedding(n_users, <span class="dv">1</span>) <span class="co"># account for user BIAS (other factors outside of our latent factors)</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.movie_factors <span class="op">=</span> Embedding(n_movies, n_factors) <span class="co"># create Embedding matrix for movies - we will cover how to create Embedding Class later</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.movie_bias <span class="op">=</span> Embedding(n_movies, <span class="dv">1</span>) <span class="co"># account for movie BIAS (other factors outside of our latent factors)</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y_range <span class="op">=</span> y_range <span class="co"># range of predictions specified</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        users <span class="op">=</span> <span class="va">self</span>.user_factors(x[:,<span class="dv">0</span>]) <span class="co"># grab first column i.e every row, and look it up using our user Embedding matrix</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        movies <span class="op">=</span> <span class="va">self</span>.movie_factors(x[:,<span class="dv">1</span>]) <span class="co"># grab second column i.e every row, and look it up using our movie Embedding matrix</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> (users <span class="op">*</span> movies).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># calculate the dot product - # dim = 1 because we are summing across COLUMNS for each row # dim = 0 would sum across ROWS</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>        res <span class="op">+=</span> <span class="va">self</span>.user_bias(x[:,<span class="dv">0</span>]) <span class="op">+</span> <span class="va">self</span>.movie_bias(x[:,<span class="dv">1</span>]) <span class="co"># update dor product results for BIAS</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid_range(res, <span class="op">*</span><span class="va">self</span>.y_range) <span class="co"># force predictions to be between 0 and 5 using sigmoid function</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s try training this and see how it goes:</p>
<div class="cell" data-tags="[]" data-execution_count="37">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define our Dot Product Bias model</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProductBias(n_users, n_movies, <span class="dv">50</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pass in our Dot Product Bias class to our learner as before</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit (train) our model</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>) <span class="co"># 5 epochs, learning rate 5e^-3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.951611</td>
      <td>0.925811</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.819404</td>
      <td>0.855196</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.616164</td>
      <td>0.856704</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.403988</td>
      <td>0.885035</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.294023</td>
      <td>0.891860</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Unlike in Excel, instead of being better, in PyTorch our validation loss has actually gone up (at least by the end of training)! Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As we’ve seen, <code>this is a clear indication of overfitting</code>. In this case, there is no way to use data augmentation, so we will have to use another <code>regularization</code> technique. One way to help avoid overfitting is an approach called <code>weight decay</code>.</p>
</section>
<section id="weight-decay-l2-regularization" class="level3">
<h3 class="anchored" data-anchor-id="weight-decay-l2-regularization">Weight Decay (L2 regularization)</h3>
<p>Weight decay, or <code>L2 regularization</code>, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will <code>encourage the weights to be as small as possible</code>.</p>
<p>Why would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, <code>y = a * (x**2)</code>, the larger <code>a</code> is, the more <em>narrow</em> the parabola is:</p>
<div class="cell" data-hide_input="true" data-tags="[]" data-execution_count="38">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example illustrating imapct of using weight decay</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">100</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>a_s <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">50</span>] </span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> [a <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> a <span class="kw">in</span> a_s]</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>_,ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a,y <span class="kw">in</span> <span class="bu">zip</span>(a_s,ys): ax.plot(x,y, label<span class="op">=</span><span class="ss">f'a=</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="dv">0</span>,<span class="dv">5</span>])</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>ax.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>So, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to <code>overfitting</code>.</p>
<p>Limiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just <code>wd</code>) is a parameter that controls that sum of squares we add to our loss (assuming <code>parameters</code> is a tensor of all parameters):</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>loss_with_wd <span class="op">=</span> loss <span class="op">+</span> wd <span class="op">*</span> (parameters<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of <code>p**2</code> with respect to <code>p</code> is <code>2*p</code>, so adding that big sum to our loss is exactly the same as doing:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>parameters.grad <span class="op">+=</span> wd <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> parameters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In practice, since <code>wd</code> is a parameter that we choose, we can just make it twice as big, so we don’t even need the <code>*2</code> in this equation. To use weight decay in fastai, just pass <code>wd</code> in your call to <code>fit</code> or <code>fit_one_cycle</code>:</p>
<p>The whole reason for calculating the loss is to then calculate the gradient of the loss, by taking the derivative. The derivative of parameters^2 is 2*parameters.</p>
<section id="weight-decay-value-0.1" class="level4">
<h4 class="anchored" data-anchor-id="weight-decay-value-0.1">Weight decay value 0.1</h4>
<p>A higher weight decay value forces the weights lower, reducing the capacity of our model to make good prediction, but reducing the risk of overfitting.</p>
<div class="cell" data-tags="[]" data-execution_count="39">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define our Dot Product Bias model</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProductBias(n_users, n_movies, <span class="dv">50</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pass in our Dot Product Bias class to our learner as before</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit (train) our model</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.1</span>) <span class="co"># 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.976209</td>
      <td>0.929432</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.867723</td>
      <td>0.859258</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.751625</td>
      <td>0.823332</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.580325</td>
      <td>0.811122</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.485529</td>
      <td>0.811769</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>That’s much better! The key to regularization is to find the right balance of the magnitude of the weights of the coefficients - low enough so we don’t overfit, but high enough so that we can make useful predictions. We can’t reduce them too much (then we end up with underfitting) - but if the weights are increased too much then our model will start to overfit. If there are latent factors in our model that don’t have any influence on overall prediciton, it will just set the co-efficient for that latent factor to zero.</p>
</section>
<section id="weight-decay-value-0.01" class="level4">
<h4 class="anchored" data-anchor-id="weight-decay-value-0.01">Weight decay value 0.01</h4>
<p>A lower weight decay value keeps the weights higher, increasing the capacity of our model to make good predictions, but increasing the risk of overfitting.</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define our Dot Product Bias model</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProductBias(n_users, n_movies, <span class="dv">50</span>) <span class="co"># set number of latent factors = 50</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pass in our Dot Product Bias class to our learner as before</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit (train) our model</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.01</span>) <span class="co"># 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.937280</td>
      <td>0.919222</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.836111</td>
      <td>0.858221</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.594563</td>
      <td>0.858991</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.416554</td>
      <td>0.887284</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.282974</td>
      <td>0.894385</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>As we can see we start off with an improvement and then from epoch 2 performance gets worse, suggesting <code>overfitting</code>.</p>
</section>
<section id="weight-decay-value-0.001" class="level4">
<h4 class="anchored" data-anchor-id="weight-decay-value-0.001">Weight decay value 0.001</h4>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define our Dot Product Bias model</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProductBias(n_users, n_movies, <span class="dv">50</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pass in our Dot Product Bias class to our learner as before</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># fit (train) our model</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.001</span>) <span class="co"># 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.922303</td>
      <td>0.922695</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.856747</td>
      <td>0.854244</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.600128</td>
      <td>0.864396</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.404001</td>
      <td>0.894145</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.283558</td>
      <td>0.902557</td>
      <td>00:04</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Again, we start off with an improvement but then from epoch 2 performance gets worse, suggesting <code>overfitting</code> So, our original weight decay factor of 0.1 looks pretty optimal.</p>
</section>
</section>
<section id="creating-our-own-embedding-module" class="level3">
<h3 class="anchored" data-anchor-id="creating-our-own-embedding-module">Creating Our Own Embedding Module</h3>
<p>If the following section proves to be difficult to follow then it would be a useful exercise to revisit the <a href="https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch">Linear model and neural net from scratch</a> NoteBook.</p>
<p>In that Notebook we created functions to set initital weights, added layers, including bias, and created a further function to update the gradients i.e.&nbsp;perform gradient descent by calculating the layer gradients usind <code>layer.grad * learning_rate</code>. When using PyTorch a lot of this functionality is taken care of - PyTorch looks inside our Module and keeps track of anything that looks like a neural network parameter.</p>
<p>So far, we’ve used <code>Embedding</code> without thinking about how it really works. Let’s re-create <code>DotProductBias</code> <em>without</em> using this class. We’ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall that optimizers require that they can get all the parameters of a module from the module’s <code>parameters</code> method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a <code>Module</code>, it will not be included in <code>parameters</code>:</p>
<div class="cell" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a simple module which only includes a tensor</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> T(Module):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>): <span class="va">self</span>.a <span class="op">=</span> torch.ones(<span class="dv">3</span>) <span class="co"># add a tensor as an attribute to our Module</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>L(T().parameters()) <span class="co"># T() instantiates our Module, capital L in Fastcore returns a list of items</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(#0) []</code></pre>
</div>
</div>
<p>Note that the tensor is <code>not</code> included in parameters. To tell <code>Module</code> that we want to treat a tensor as a parameter, we have to wrap it in the <code>nn.Parameter</code> class. This class doesn’t actually add any functionality (other than automatically calling <code>requires_grad_</code> for us). It’s only used as a “marker” to show what to include in <code>parameters</code>:</p>
<div class="cell" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a simple module which only includes a tensor</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> T(Module):</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>): <span class="va">self</span>.a <span class="op">=</span> nn.Parameter(torch.ones(<span class="dv">3</span>)) <span class="co"># for PyTorch to recognise the parameters, we need to include the nn.Parameter wrapper</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>L(T().parameters()) <span class="co"># T() instantiates our Module, capital L in Fastcore returns a list of the parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>(#1) [Parameter containing:
tensor([1., 1., 1.], requires_grad=True)]</code></pre>
</div>
</div>
<p>Now that we have included the tensor in an <code>nn.Parameter</code> wrapper, PyTorch can read the parameters and we can return these using Fastcore’s <code>L</code>.</p>
<p>All PyTorch modules use <code>nn.Parameter</code> for any trainable parameters, which is why we haven’t needed to explicitly use this wrapper up until now:</p>
<div class="cell" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a simple module which only includes a tensor</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> T(Module):</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>): <span class="va">self</span>.a <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">3</span>, bias<span class="op">=</span><span class="va">False</span>) <span class="co"># we can create our tensor as before but use nn.Linear which flags that parameters are included </span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>                                                             <span class="co"># no bias term, nn.Linear returns randomly initialized tensor values, size as defined, 1 x 3 </span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> T()</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>L(t.parameters())  <span class="co"># T() instantiates our Module, capital L in Fastcore returns a list of the parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(#1) [Parameter containing:
tensor([[ 0.7645],
        [ 0.8300],
        [-0.2343]], requires_grad=True)]</code></pre>
</div>
</div>
<p>Now that we have included the tensor in an <code>nn.Linear</code> wrapper, PyTorch can read the parameters and we can return these using Fastcore’s <code>L</code>.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find out what the attribute a is</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>t.a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>Linear(in_features=1, out_features=3, bias=False)</code></pre>
</div>
</div>
<div class="cell" data-tags="[]" data-execution_count="17">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find out what type the attribute a is </span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(t.a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>torch.nn.modules.linear.Linear</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># find out what type the attribute a is </span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>t.a.weight</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>Parameter containing:
tensor([[ 0.7645],
        [ 0.8300],
        [-0.2343]], requires_grad=True)</code></pre>
</div>
</div>
<p>We can create a tensor as a parameter, with random initialization, like so:</p>
<div class="cell" data-tags="[]" data-execution_count="36">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create params function - poss in size (in case below n_users x n_factors)</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_params(size):</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn.Parameter(torch.zeros(<span class="op">*</span>size).normal_(<span class="dv">0</span>, <span class="fl">0.01</span>)) <span class="co"># creates a tensor of zeros of requested size, then Gaussian distribution with mean=0 and Std Dev = 0.01</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># normal_ modifies eplaces inline with the values specified in brackets </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use this to create <code>DotProductBias</code> again, but without <code>Embedding</code> i.e let’s create PyTorch’s Embedding Matrix from scratch:</p>
<div class="cell" data-tags="[]" data-execution_count="37">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create PyTorch's embedding matrix from scratch</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DotProductBias(Module):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_users, n_movies, n_factors, y_range<span class="op">=</span>(<span class="dv">0</span>,<span class="fl">5.5</span>)):</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_factors <span class="op">=</span> create_params([n_users, n_factors]) <span class="co"># create our user Embedding matrix of normally randomized values of size n_users x n_factors</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_bias <span class="op">=</span> create_params([n_users]) <span class="co"># build user bias into our model - vector of size n_users</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.movie_factors <span class="op">=</span> create_params([n_movies, n_factors]) <span class="co"># create our movie Embedding matrix of normally randomized values of size n_users x n_factors</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.movie_bias <span class="op">=</span> create_params([n_movies]) <span class="co"># build movie bias into our model - vector of size n_movies</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y_range <span class="op">=</span> y_range <span class="co"># range of predictions as set above, between 0 and 5.5</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        users <span class="op">=</span> <span class="va">self</span>.user_factors[x[:,<span class="dv">0</span>]] <span class="co"># user latent factors - note we can index into it</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>        movies <span class="op">=</span> <span class="va">self</span>.movie_factors[x[:,<span class="dv">1</span>]] <span class="co"># movie latent factors - note we can index into it</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> (users<span class="op">*</span>movies).<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># matrix multiplication</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        res <span class="op">+=</span> <span class="va">self</span>.user_bias[x[:,<span class="dv">0</span>]] <span class="op">+</span> <span class="va">self</span>.movie_bias[x[:,<span class="dv">1</span>]] <span class="co"># add bias - note we ca </span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid_range(res, <span class="op">*</span><span class="va">self</span>.y_range) <span class="co"># force predictions to be between 0 and 5 using sigmoid function</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then let’s train it again to check we get around the same results we saw in the previous section:</p>
<div class="cell" data-tags="[]" data-execution_count="38">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define our Dot Product Bias model</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DotProductBias(n_users, n_movies, <span class="dv">50</span>) <span class="co"># latent factors set to 50</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># # pass in our Dot Product Bias class to our learner as before</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co"># train for 5 epochs, lr = 5e^-3, weight decay factor = 0.1</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.960358</td>
      <td>0.956795</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.869042</td>
      <td>0.874685</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.737840</td>
      <td>0.839419</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.589841</td>
      <td>0.823726</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.472334</td>
      <td>0.824282</td>
      <td>00:05</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Now, let’s take a look at what our model has learned.</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what's inside movie bias?</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.movie_bias,<span class="bu">len</span>(model.movie_bias))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([-0.0010, -0.1098, -0.0022,  ..., -0.0443,  0.0685,  0.0255],
       requires_grad=True) 1665</code></pre>
</div>
</div>
<p>Movie bias parameters that have been trained - 1,665 being the number of movies we have.</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what is the shape of our movie bias vector?</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>model.movie_bias.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>torch.Size([1665])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what's inside movie factors?</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.movie_factors,<span class="bu">len</span>(model.movie_factors))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([[-0.0039, -0.0022,  0.0021,  ...,  0.0041, -0.0011,  0.0016],
        [-0.1175, -0.1778, -0.0984,  ...,  0.0191,  0.0929,  0.0216],
        [ 0.0109,  0.0653,  0.0031,  ..., -0.0156,  0.0204,  0.0313],
        ...,
        [-0.1234, -0.0363, -0.0474,  ..., -0.0825, -0.0893, -0.1314],
        [ 0.0995,  0.1521,  0.0754,  ...,  0.0901,  0.1230,  0.1518],
        [ 0.0164, -0.0041,  0.0183,  ..., -0.0054,  0.0122, -0.0150]],
       requires_grad=True) 1665</code></pre>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what is the shape of our movie factors Embedding matrix?</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>model.movie_factors.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>torch.Size([1665, 50])</code></pre>
</div>
</div>
<p>1,665 movies, and 50 latent factors.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what's inside user factors?</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.user_factors,<span class="bu">len</span>(model.user_factors))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([[ 1.2866e-03,  7.8120e-04, -7.0611e-04,  ...,  8.2220e-06,
         -3.2568e-03,  2.7836e-03],
        [ 1.6745e-01,  9.3676e-02, -5.2638e-03,  ..., -2.9528e-02,
         -1.1926e-01,  3.1058e-01],
        [ 4.6036e-02, -4.4877e-03,  1.5233e-01,  ...,  9.4287e-02,
          1.1350e-01,  1.4557e-01],
        ...,
        [ 6.7316e-02,  1.0262e-01,  2.9921e-01,  ...,  1.2235e-01,
          4.4754e-02,  2.5394e-01],
        [-8.0669e-03,  1.0943e-01,  2.0522e-01,  ...,  1.6869e-02,
          1.7104e-01,  1.5911e-01],
        [ 7.9618e-02,  2.9292e-01,  2.3172e-01,  ...,  1.1354e-01,
          1.2088e-01,  9.0374e-02]], requires_grad=True) 944</code></pre>
</div>
</div>
<p>A bunch of user parameters (weights) that have been trained - 944 being the number of users we have.</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what is the shape of our user factors Embedding matrix?</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>model.user_factors.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>torch.Size([944, 50])</code></pre>
</div>
</div>
<p>944 users, and 50 latent factors.</p>
</section>
<section id="interpreting-embeddings-and-biases" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-embeddings-and-biases">Interpreting Embeddings and Biases</h3>
<p>Our model is already useful, in that it can provide us with movie recommendations for our users — but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector:</p>
<div class="cell" data-tags="[]" data-execution_count="58">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get movie_bias values</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>movie_bias <span class="op">=</span> learn.model.movie_bias.squeeze() <span class="co"># </span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="co"># find out which movie id's have the lowest bias parameters</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> movie_bias.argsort()[:<span class="dv">5</span>] <span class="co"># argsort sorts in ascending order by default - let's grab first 5 </span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="co"># look inside our DataLoaders to grab the names of those movies from the indexes</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>[dls.classes[<span class="st">'title'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>['Children of the Corn: The Gathering (1996)',
 'Robocop 3 (1993)',
 'Lawnmower Man 2: Beyond Cyberspace (1996)',
 'Amityville 3-D (1983)',
 'Mortal Kombat: Annihilation (1997)']</code></pre>
</div>
</div>
<p>Think about what this means. What it’s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don’t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias:</p>
<div class="cell" data-tags="[]" data-execution_count="64">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sort indexes by descending will give us movies with highest bias values</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="co"># i.e movies that are popular even amongst users who don't normally like that kind of movie</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> movie_bias.argsort(descending<span class="op">=</span><span class="va">True</span>)[:<span class="dv">5</span>]</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>[dls.classes[<span class="st">'title'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>['Titanic (1997)',
 'L.A. Confidential (1997)',
 'Silence of the Lambs, The (1991)',
 'Shawshank Redemption, The (1994)',
 'Star Wars (1977)']</code></pre>
</div>
</div>
<p>So, for instance, even if you don’t normally enjoy detective movies, you might enjoy <em>LA Confidential</em>!</p>
<p>It is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying <em>directions</em> in such a matrix, called <code>principal component analysis (PCA)</code>. If you are interested then we suggest you check out the fast.ai course <a href="https://github.com/fastai/numerical-linear-algebra">Computational Linear Algebra for Coders</a>. Here’s what our movies look like based on two of the strongest PCA components:</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> ratings.groupby(<span class="st">'title'</span>)[<span class="st">'rating'</span>].count()</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>title
'Til There Was You (1997)                  9
1-900 (1994)                               5
101 Dalmatians (1996)                    109
12 Angry Men (1957)                      125
187 (1997)                                41
                                        ... 
Young Guns II (1990)                      44
Young Poisoner's Handbook, The (1995)     41
Zeus and Roxanne (1997)                    6
unknown                                    9
Á köldum klaka (Cold Fever) (1994)         1
Name: rating, Length: 1664, dtype: int64</code></pre>
</div>
</div>
<div class="cell" data-hide_input="true" data-tags="[]" data-execution_count="68">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># group movies by title and rating </span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> ratings.groupby(<span class="st">'title'</span>)[<span class="st">'rating'</span>].count()</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sort top movies by rating - top 1000</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>top_movies <span class="op">=</span> g.sort_values(ascending<span class="op">=</span><span class="va">False</span>).index.values[:<span class="dv">1000</span>]</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a><span class="co"># get the indexes of the sorted top movies using: object to index (o2i)</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>top_idxs <span class="op">=</span> tensor([learn.dls.classes[<span class="st">'title'</span>].o2i[m] <span class="cf">for</span> m <span class="kw">in</span> top_movies])</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>movie_w <span class="op">=</span> learn.model.movie_factors[top_idxs].cpu().detach()</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Compress our 50 latent factors into just 3 most important factors</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>movie_pca <span class="op">=</span> movie_w.pca(<span class="dv">3</span>)</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a><span class="co"># draw a chart of these features - Visualized Embeddings</span></span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>fac0,fac1,fac2 <span class="op">=</span> movie_pca.t() <span class="co"># .t transposes the array</span></span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">50</span>)) <span class="co"># restrict number of movies plotted to 50</span></span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> fac0[idxs]</span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> fac2[idxs]</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">12</span>))</span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, Y)</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x, y <span class="kw">in</span> <span class="bu">zip</span>(top_movies[idxs], X, Y):</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>    plt.text(x,y,i, color<span class="op">=</span>np.random.rand(<span class="dv">3</span>)<span class="op">*</span><span class="fl">0.7</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can see here that the model seems to have discovered a concept of <em>classic</em> versus <em>pop culture</em> movies, or perhaps it is <em>critically acclaimed</em> that is represented here.</p>
<blockquote class="blockquote">
<p>j: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!</p>
</blockquote>
<p>We defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. We’ll look at how to do that next.</p>
</section>
<section id="using-fastai.collab" class="level3">
<h3 class="anchored" data-anchor-id="using-fastai.collab">Using fastai.collab</h3>
<p>We can create and train a <code>collaborative filtering model</code> using the exact structure shown earlier by using fastai’s <code>collab_learner</code>. Let’s have a peek under the hood and see what is going on inside:</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's take a look at what's going on under the hood</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>collab_learner??</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>
collab_learner<span class="ansi-blue-fg">(</span>
    dls<span class="ansi-blue-fg">,</span>
    n_factors<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">50</span><span class="ansi-blue-fg">,</span>
    use_nn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    emb_szs<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    layers<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    loss_func<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span>
    opt_func<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&lt;</span>function Adam at <span class="ansi-cyan-fg">0x7f6f614f4700</span><span class="ansi-blue-fg">&gt;</span><span class="ansi-blue-fg">,</span>
    lr<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.001</span><span class="ansi-blue-fg">,</span>
    splitter<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'callable'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">&lt;</span>function trainable_params at <span class="ansi-cyan-fg">0x7f6f63949870</span><span class="ansi-blue-fg">&gt;</span><span class="ansi-blue-fg">,</span>
    cbs<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    metrics<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    path<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    model_dir<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">'models'</span><span class="ansi-blue-fg">,</span>
    wd<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    wd_bn_bias<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    train_bn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    moms<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0.95</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">0.85</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">0.95</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>
    default_cbs<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'bool'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Source:</span>   
<span class="ansi-blue-fg">@</span>delegates<span class="ansi-blue-fg">(</span>Learner<span class="ansi-blue-fg">.</span>__init__<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">def</span> collab_learner<span class="ansi-blue-fg">(</span>dls<span class="ansi-blue-fg">,</span> n_factors<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">50</span><span class="ansi-blue-fg">,</span> use_nn<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span> emb_szs<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> layers<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> config<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> loss_func<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
    <span class="ansi-blue-fg">"Create a Learner for collaborative filtering on `dls`."</span>
    emb_szs <span class="ansi-blue-fg">=</span> get_emb_sz<span class="ansi-blue-fg">(</span>dls<span class="ansi-blue-fg">,</span> ifnone<span class="ansi-blue-fg">(</span>emb_szs<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">{</span><span class="ansi-blue-fg">}</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">if</span> loss_func <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> loss_func <span class="ansi-blue-fg">=</span> MSELossFlat<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">if</span> config <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> config <span class="ansi-blue-fg">=</span> tabular_config<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">if</span> y_range <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> config<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">'y_range'</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> y_range
    <span class="ansi-green-fg">if</span> layers <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> layers <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>n_factors<span class="ansi-blue-fg">]</span>
    <span class="ansi-green-fg">if</span> use_nn<span class="ansi-blue-fg">:</span> model <span class="ansi-blue-fg">=</span> EmbeddingNN<span class="ansi-blue-fg">(</span>emb_szs<span class="ansi-blue-fg">=</span>emb_szs<span class="ansi-blue-fg">,</span> layers<span class="ansi-blue-fg">=</span>layers<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>config<span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>      model <span class="ansi-blue-fg">=</span> EmbeddingDotBias<span class="ansi-blue-fg">.</span>from_classes<span class="ansi-blue-fg">(</span>n_factors<span class="ansi-blue-fg">,</span> dls<span class="ansi-blue-fg">.</span>classes<span class="ansi-blue-fg">,</span> y_range<span class="ansi-blue-fg">=</span>y_range<span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">return</span> Learner<span class="ansi-blue-fg">(</span>dls<span class="ansi-blue-fg">,</span> model<span class="ansi-blue-fg">,</span> loss_func<span class="ansi-blue-fg">=</span>loss_func<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">File:</span>      ~/mambaforge/lib/python3.10/site-packages/fastai/collab.py
<span class="ansi-red-fg">Type:</span>      function
</pre>
</div>
</div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's take a look at what's going on under the hood</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>EmbeddingDotBias??</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span> EmbeddingDotBias<span class="ansi-blue-fg">(</span>n_factors<span class="ansi-blue-fg">,</span> n_users<span class="ansi-blue-fg">,</span> n_items<span class="ansi-blue-fg">,</span> y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Source:</span>        
<span class="ansi-green-fg">class</span> EmbeddingDotBias<span class="ansi-blue-fg">(</span>Module<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
    <span class="ansi-blue-fg">"Base dot model for collaborative filtering."</span>
    <span class="ansi-green-fg">def</span> __init__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> n_factors<span class="ansi-blue-fg">,</span> n_users<span class="ansi-blue-fg">,</span> n_items<span class="ansi-blue-fg">,</span> y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        self<span class="ansi-blue-fg">.</span>y_range <span class="ansi-blue-fg">=</span> y_range
        <span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>u_weight<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>i_weight<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>u_bias<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>i_bias<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>Embedding<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>o<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> o <span class="ansi-green-fg">in</span> <span class="ansi-blue-fg">[</span>
            <span class="ansi-blue-fg">(</span>n_users<span class="ansi-blue-fg">,</span> n_factors<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">(</span>n_items<span class="ansi-blue-fg">,</span> n_factors<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">(</span>n_users<span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">(</span>n_items<span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span>
    <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        users<span class="ansi-blue-fg">,</span>items <span class="ansi-blue-fg">=</span> x<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span>x<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span>
        dot <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>u_weight<span class="ansi-blue-fg">(</span>users<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">*</span> self<span class="ansi-blue-fg">.</span>i_weight<span class="ansi-blue-fg">(</span>items<span class="ansi-blue-fg">)</span>
        res <span class="ansi-blue-fg">=</span> dot<span class="ansi-blue-fg">.</span>sum<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> self<span class="ansi-blue-fg">.</span>u_bias<span class="ansi-blue-fg">(</span>users<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> self<span class="ansi-blue-fg">.</span>i_bias<span class="ansi-blue-fg">(</span>items<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>y_range <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> res
        <span class="ansi-green-fg">return</span> torch<span class="ansi-blue-fg">.</span>sigmoid<span class="ansi-blue-fg">(</span>res<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">*</span> <span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>y_range<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">-</span>self<span class="ansi-blue-fg">.</span>y_range<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> self<span class="ansi-blue-fg">.</span>y_range<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span>
    <span class="ansi-blue-fg">@</span>classmethod
    <span class="ansi-green-fg">def</span> from_classes<span class="ansi-blue-fg">(</span>cls<span class="ansi-blue-fg">,</span> n_factors<span class="ansi-blue-fg">,</span> classes<span class="ansi-blue-fg">,</span> user<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> item<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-blue-fg">"Build a model with `n_factors` by inferring `n_users` and  `n_items` from `classes`"</span>
        <span class="ansi-green-fg">if</span> user <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> user <span class="ansi-blue-fg">=</span> list<span class="ansi-blue-fg">(</span>classes<span class="ansi-blue-fg">.</span>keys<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span>
        <span class="ansi-green-fg">if</span> item <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> item <span class="ansi-blue-fg">=</span> list<span class="ansi-blue-fg">(</span>classes<span class="ansi-blue-fg">.</span>keys<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span>
        res <span class="ansi-blue-fg">=</span> cls<span class="ansi-blue-fg">(</span>n_factors<span class="ansi-blue-fg">,</span> len<span class="ansi-blue-fg">(</span>classes<span class="ansi-blue-fg">[</span>user<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> len<span class="ansi-blue-fg">(</span>classes<span class="ansi-blue-fg">[</span>item<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> y_range<span class="ansi-blue-fg">=</span>y_range<span class="ansi-blue-fg">)</span>
        res<span class="ansi-blue-fg">.</span>classes<span class="ansi-blue-fg">,</span>res<span class="ansi-blue-fg">.</span>user<span class="ansi-blue-fg">,</span>res<span class="ansi-blue-fg">.</span>item <span class="ansi-blue-fg">=</span> classes<span class="ansi-blue-fg">,</span>user<span class="ansi-blue-fg">,</span>item
        <span class="ansi-green-fg">return</span> res
    <span class="ansi-green-fg">def</span> _get_idx<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> arr<span class="ansi-blue-fg">,</span> is_item<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-blue-fg">"Fetch item or user (based on `is_item`) for all in `arr`"</span>
        <span class="ansi-green-fg">assert</span> hasattr<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">'classes'</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">"Build your model with `EmbeddingDotBias.from_classes` to use this functionality."</span>
        classes <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>classes<span class="ansi-blue-fg">[</span>self<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">]</span> <span class="ansi-green-fg">if</span> is_item <span class="ansi-green-fg">else</span> self<span class="ansi-blue-fg">.</span>classes<span class="ansi-blue-fg">[</span>self<span class="ansi-blue-fg">.</span>user<span class="ansi-blue-fg">]</span>
        c2i <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">{</span>v<span class="ansi-blue-fg">:</span>k <span class="ansi-green-fg">for</span> k<span class="ansi-blue-fg">,</span>v <span class="ansi-green-fg">in</span> enumerate<span class="ansi-blue-fg">(</span>classes<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">}</span>
        <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">return</span> tensor<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">[</span>c2i<span class="ansi-blue-fg">[</span>o<span class="ansi-blue-fg">]</span> <span class="ansi-green-fg">for</span> o <span class="ansi-green-fg">in</span> arr<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">except</span> KeyError <span class="ansi-green-fg">as</span> e<span class="ansi-blue-fg">:</span>
            message <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">f"You're trying to access {'an item' if is_item else 'a user'} that isn't in the training data. If it was in your original data, it may have been split such that it's only in the validation set now."</span>
            <span class="ansi-green-fg">raise</span> modify_exception<span class="ansi-blue-fg">(</span>e<span class="ansi-blue-fg">,</span> message<span class="ansi-blue-fg">,</span> replace<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">def</span> bias<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> arr<span class="ansi-blue-fg">,</span> is_item<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-blue-fg">"Bias for item or user (based on `is_item`) for all in `arr`"</span>
        idx <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_get_idx<span class="ansi-blue-fg">(</span>arr<span class="ansi-blue-fg">,</span> is_item<span class="ansi-blue-fg">)</span>
        layer <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>i_bias <span class="ansi-green-fg">if</span> is_item <span class="ansi-green-fg">else</span> self<span class="ansi-blue-fg">.</span>u_bias<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>eval<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>cpu<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">return</span> to_detach<span class="ansi-blue-fg">(</span>layer<span class="ansi-blue-fg">(</span>idx<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>gather<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">def</span> weight<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> arr<span class="ansi-blue-fg">,</span> is_item<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-blue-fg">"Weight for item or user (based on `is_item`) for all in `arr`"</span>
        idx <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_get_idx<span class="ansi-blue-fg">(</span>arr<span class="ansi-blue-fg">,</span> is_item<span class="ansi-blue-fg">)</span>
        layer <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>i_weight <span class="ansi-green-fg">if</span> is_item <span class="ansi-green-fg">else</span> self<span class="ansi-blue-fg">.</span>u_weight<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>eval<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>cpu<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">return</span> to_detach<span class="ansi-blue-fg">(</span>layer<span class="ansi-blue-fg">(</span>idx<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>gather<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">File:</span>           ~/mambaforge/lib/python3.10/site-packages/fastai/collab.py
<span class="ansi-red-fg">Type:</span>           PrePostInitMeta
<span class="ansi-red-fg">Subclasses:</span>     
</pre>
</div>
</div>
</div>
<p>OK, let’s now reproduce what we did from scratch earlier using the fast.ai functionality with just a few lines of code:</p>
<div class="cell" data-tags="[]" data-execution_count="75">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a collaborative filtering model using fastai</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> collab_learner(dls, n_factors<span class="op">=</span><span class="dv">50</span>, y_range<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">5.5</span>)) <span class="co"># latebt factors =50, predictions between 0 and 5.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-tags="[]" data-execution_count="76">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train for 5 epochs, learning rate = 5e^-3, weight decay = 0.1</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.940161</td>
      <td>0.954125</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.845409</td>
      <td>0.871870</td>
      <td>00:04</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.732785</td>
      <td>0.837964</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.581802</td>
      <td>0.822925</td>
      <td>00:05</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.483456</td>
      <td>0.823324</td>
      <td>00:04</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>The names of the layers can be seen by printing the model:</p>
<div class="cell" data-tags="[]" data-execution_count="77">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's look at the layers of our model</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>learn.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>EmbeddingDotBias(
  (u_weight): Embedding(944, 50)
  (i_weight): Embedding(1665, 50)
  (u_bias): Embedding(944, 1)
  (i_bias): Embedding(1665, 1)
)</code></pre>
</div>
</div>
<p>Note the slight difference in terminology. u = users, and i=items. So, we have the user Embedding layer (u_weight), and the movie Embedding layer (i_weight) and our bias layers.</p>
<p>We can use these to replicate any of the analyses we did in the previous section — for instance:</p>
<div class="cell" data-tags="[]" data-execution_count="78">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can look at the movie bias and grab the weights</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>movie_bias <span class="op">=</span> learn.model.i_bias.weight.squeeze()</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="co"># get indexes of top 5 movies by bias factor</span></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> movie_bias.argsort(descending<span class="op">=</span><span class="va">True</span>)[:<span class="dv">5</span>]</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a><span class="co"># get title of top 5 movies by bias factor</span></span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>[dls.classes[<span class="st">'title'</span>][i] <span class="cf">for</span> i <span class="kw">in</span> idxs]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>['L.A. Confidential (1997)',
 "Schindler's List (1993)",
 'Titanic (1997)',
 'Shawshank Redemption, The (1994)',
 'Silence of the Lambs, The (1991)']</code></pre>
</div>
</div>
<p>We get much the same results as before, that is LA Confidential is watched even by those that don’t normally watch that kind of movie.</p>
<p>Another interesting thing we can do with these learned embeddings is to look at<code>distance</code>.</p>
</section>
<section id="embedding-distance" class="level3">
<h3 class="anchored" data-anchor-id="embedding-distance">Embedding Distance</h3>
<p>On a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: <span class="math inline">\(\sqrt{x^{2}+y^{2}}\)</span> (assuming that <em>x</em> and <em>y</em> are the distances between the coordinates on each axis). For a 50-dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.</p>
<p>If there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two movies’ embedding vectors can define that similarity. We can use this to find the most similar movie to <em>Silence of the Lambs</em>:</p>
<div class="cell" data-tags="[]" data-execution_count="84">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>movie_factors <span class="op">=</span> learn.model.i_weight.weight</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="co"># convert Silence of the Lambs into its class ID using 'object to index' (o2i)</span></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> dls.classes[<span class="st">'title'</span>].o2i[<span class="st">'Silence of the Lambs, The (1991)'</span>]</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the 'distance' betweeen the Silence of the Lambs and other movie vectors</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>distances <span class="op">=</span> nn.CosineSimilarity(dim<span class="op">=</span><span class="dv">1</span>)(movie_factors, movie_factors[idx][<span class="va">None</span>]) <span class="co"># Cosine Similarity normalizes the angle between the vectors</span></span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a><span class="co"># sort distances from closes</span></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> distances.argsort(descending<span class="op">=</span><span class="va">True</span>)[<span class="dv">1</span>]</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a><span class="co"># attach movie titles to the movie indexes</span></span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>dls.classes[<span class="st">'title'</span>][idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="84">
<pre><code>"One Flew Over the Cuckoo's Nest (1975)"</code></pre>
</div>
</div>
<p>Now that we have succesfully trained a model, let’s see how to deal with the situation where we have no data for a user. How can we make recommendations to new users?</p>
</section>
<section id="bootstrapping-a-collaborative-filtering-model" class="level3">
<h3 class="anchored" data-anchor-id="bootstrapping-a-collaborative-filtering-model">Bootstrapping a Collaborative Filtering Model</h3>
<p>The biggest challenge with using collaborative filtering models in practice is the <em>bootstrapping problem</em>. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?</p>
<p>But even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of <em>use your common sense</em>. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent <em>average taste</em>.</p>
<p>Better still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)</p>
<p>One thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don’t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of <em>best ever movies</em> lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.</p>
<p>Such a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.</p>
<p>In a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It’s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.</p>
<p>Our dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as <em>probabilistic matrix factorization</em> (PMF). Another approach, which generally works similarly well given the same data, is deep learning.</p>
</section>
<section id="deep-learning-for-collaborative-filtering---from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-for-collaborative-filtering---from-scratch">Deep Learning for Collaborative Filtering - from scratch</h3>
<p>To turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way. Since we’ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function <code>get_emb_sz</code> that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:</p>
<div class="cell" data-tags="[]" data-execution_count="85">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use fast.ai recommended embedding sizes</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>embs <span class="op">=</span> get_emb_sz(dls)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>embs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>[(944, 74), (1665, 102)]</code></pre>
</div>
</div>
<p>So the suggested number of latent factors for our 944 users is 74, and the suggested number of latent factors for our 1,665 movies is 102.</p>
<p>Let’s implement this class:</p>
<div class="cell" data-tags="[]" data-execution_count="88">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build a Collaborative Filtering neural net from scratch</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CollabNN(Module):</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, user_sz, item_sz, y_range<span class="op">=</span>(<span class="dv">0</span>,<span class="fl">5.5</span>), n_act<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.user_factors <span class="op">=</span> Embedding(<span class="op">*</span>user_sz)</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.item_factors <span class="op">=</span> Embedding(<span class="op">*</span>item_sz)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(user_sz[<span class="dv">1</span>]<span class="op">+</span>item_sz[<span class="dv">1</span>], n_act), <span class="co"># </span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(), <span class="co"># Rectified Linear Unit</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(n_act, <span class="dv">1</span>)) <span class="co"># Linear layer at the end to create a single output</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.y_range <span class="op">=</span> y_range</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>        embs <span class="op">=</span> <span class="va">self</span>.user_factors(x[:,<span class="dv">0</span>]),<span class="va">self</span>.item_factors(x[:,<span class="dv">1</span>])</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layers(torch.cat(embs, dim<span class="op">=</span><span class="dv">1</span>)) <span class="co"># concatenate user and item embeddings together</span></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> sigmoid_range(x, <span class="op">*</span><span class="va">self</span>.y_range)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And use it to create a model:</p>
<div class="cell" data-tags="[]" data-execution_count="89">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># instantiate our model </span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CollabNN(<span class="op">*</span>embs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>CollabNN</code> creates our <code>Embedding</code> layers in the same way as previous classes in this chapter, except that we now use the <code>embs</code> sizes. <code>self.layers</code> is identical to the mini-neural net we created in the chapter for MNIST. Then, in <code>forward</code>, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply <code>sigmoid_range</code> as we have in previous models.</p>
<p>Let’s see if it trains:</p>
<div class="cell" data-tags="[]" data-execution_count="90">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, loss_func<span class="op">=</span>MSELossFlat())</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co"># train for 5 epochs, learning rate 5e^-3, weight decay = 0.01</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.960975</td>
      <td>0.944082</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.899966</td>
      <td>0.908818</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.877111</td>
      <td>0.890931</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.791085</td>
      <td>0.869468</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.771323</td>
      <td>0.869940</td>
      <td>00:06</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>fastai provides this model in <code>fastai.collab</code> if you pass <code>use_nn=True</code> in your call to <code>collab_learner</code> (including calling <code>get_emb_sz</code> for you), and it lets you easily create more layers. For instance, here we’re creating two hidden layers, of size 100 and 50, respectively:</p>
</section>
<section id="deep-learning-for-collaborative-filtering---using-fast.ai" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-for-collaborative-filtering---using-fast.ai">Deep Learning for Collaborative Filtering - using fast.ai</h3>
<div class="cell" data-tags="[]" data-execution_count="91">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create our Collab Filtering learner, define neural net layesr</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> collab_learner(dls, use_nn<span class="op">=</span><span class="va">True</span>, y_range<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">5.5</span>), layers<span class="op">=</span>[<span class="dv">100</span>,<span class="dv">50</span>]) <span class="co"># use_nn = True allows us to create a neural network, with 2 hidden layers</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="co"># train for 5 epochs, learning rate 5e^-3, weight decay = 0.01</span></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">5e-3</span>, wd<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.957651</td>
      <td>0.987930</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.894093</td>
      <td>0.919895</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.907125</td>
      <td>0.892506</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.863961</td>
      <td>0.864401</td>
      <td>00:06</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.766643</td>
      <td>0.866521</td>
      <td>00:06</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<p>Deep learning models really come into play when we have a lot of metadata e.g.&nbsp;information about our users, where are they from, when did they sign up, what sex are they etc and for our movies e.g.&nbsp;when was it released, what genre is it etc. In our scenario here where we don’t have this information to hand, the deep learning model scores a bit worse than our dot product model, which is taking advantage of our understanding of the problem domain. In practice we often create a model which has a dot product component and a neural net component.</p>
<p><code>learn.model</code> is an object of type <code>EmbeddingNN</code>. Let’s take a look at fastai’s code for this class:</p>
<div class="cell" data-tags="[]" data-execution_count="92">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="at">@delegates</span>(TabularModel)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EmbeddingNN(TabularModel):</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_szs, layers, <span class="op">**</span>kwargs):</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(emb_szs, layers<span class="op">=</span>layers, n_cont<span class="op">=</span><span class="dv">0</span>, out_sz<span class="op">=</span><span class="dv">1</span>, <span class="op">**</span>kwargs) <span class="co"># n_cont=0 means number of continuous variables is zero</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wow, that’s not a lot of code! This class <em>inherits</em> from <code>TabularModel</code>, which is where it gets all its functionality from. In <code>__init__</code> it calls the same method in <code>TabularModel</code>, passing <code>n_cont=0</code> and <code>out_sz=1</code>; other than that, it only passes along whatever arguments it received.</p>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>TabularModel??</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Init signature:</span>
TabularModel<span class="ansi-blue-fg">(</span>
    emb_szs<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'list'</span><span class="ansi-blue-fg">,</span>
    n_cont<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'int'</span><span class="ansi-blue-fg">,</span>
    out_sz<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'int'</span><span class="ansi-blue-fg">,</span>
    layers<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'list'</span><span class="ansi-blue-fg">,</span>
    ps<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'float | list'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    embed_p<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'float'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">0.0</span><span class="ansi-blue-fg">,</span>
    y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    use_bn<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'bool'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    bn_final<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'bool'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    bn_cont<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'bool'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    act_cls<span class="ansi-blue-fg">=</span>ReLU<span class="ansi-blue-fg">(</span>inplace<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>
    lin_first<span class="ansi-blue-fg">:</span> <span class="ansi-blue-fg">'bool'</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">Source:</span>        
<span class="ansi-green-fg">class</span> TabularModel<span class="ansi-blue-fg">(</span>Module<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
    <span class="ansi-blue-fg">"Basic model for tabular data."</span>
    <span class="ansi-green-fg">def</span> __init__<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> 
        emb_szs<span class="ansi-blue-fg">:</span>list<span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Sequence of (num_embeddings, embedding_dim) for each categorical variable</span>
        n_cont<span class="ansi-blue-fg">:</span>int<span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Number of continuous variables</span>
        out_sz<span class="ansi-blue-fg">:</span>int<span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Number of outputs for final `LinBnDrop` layer</span>
        layers<span class="ansi-blue-fg">:</span>list<span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Sequence of ints used to specify the input and output size of each `LinBnDrop` layer</span>
        ps<span class="ansi-blue-fg">:</span>float<span class="ansi-blue-fg">|</span>list<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Sequence of dropout probabilities for `LinBnDrop`</span>
        embed_p<span class="ansi-blue-fg">:</span>float<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0.</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Dropout probability for `Embedding` layer</span>
        y_range<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Low and high for `SigmoidRange` activation </span>
        use_bn<span class="ansi-blue-fg">:</span>bool<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Use `BatchNorm1d` in `LinBnDrop` layers</span>
        bn_final<span class="ansi-blue-fg">:</span>bool<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Use `BatchNorm1d` on final layer</span>
        bn_cont<span class="ansi-blue-fg">:</span>bool<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Use `BatchNorm1d` on continuous variables</span>
        act_cls<span class="ansi-blue-fg">=</span>nn<span class="ansi-blue-fg">.</span>ReLU<span class="ansi-blue-fg">(</span>inplace<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-red-fg"># Activation type for `LinBnDrop` layers</span>
        lin_first<span class="ansi-blue-fg">:</span>bool<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span> <span class="ansi-red-fg"># Linear layer is first or last in `LinBnDrop` layers</span>
    <span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        ps <span class="ansi-blue-fg">=</span> ifnone<span class="ansi-blue-fg">(</span>ps<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">*</span>len<span class="ansi-blue-fg">(</span>layers<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">if</span> <span class="ansi-green-fg">not</span> is_listy<span class="ansi-blue-fg">(</span>ps<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span> ps <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>ps<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">*</span>len<span class="ansi-blue-fg">(</span>layers<span class="ansi-blue-fg">)</span>
        self<span class="ansi-blue-fg">.</span>embeds <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>ModuleList<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">[</span>Embedding<span class="ansi-blue-fg">(</span>ni<span class="ansi-blue-fg">,</span> nf<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> ni<span class="ansi-blue-fg">,</span>nf <span class="ansi-green-fg">in</span> emb_szs<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
        self<span class="ansi-blue-fg">.</span>emb_drop <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Dropout<span class="ansi-blue-fg">(</span>embed_p<span class="ansi-blue-fg">)</span>
        self<span class="ansi-blue-fg">.</span>bn_cont <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>BatchNorm1d<span class="ansi-blue-fg">(</span>n_cont<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">if</span> bn_cont <span class="ansi-green-fg">else</span> <span class="ansi-green-fg">None</span>
        n_emb <span class="ansi-blue-fg">=</span> sum<span class="ansi-blue-fg">(</span>e<span class="ansi-blue-fg">.</span>embedding_dim <span class="ansi-green-fg">for</span> e <span class="ansi-green-fg">in</span> self<span class="ansi-blue-fg">.</span>embeds<span class="ansi-blue-fg">)</span>
        self<span class="ansi-blue-fg">.</span>n_emb<span class="ansi-blue-fg">,</span>self<span class="ansi-blue-fg">.</span>n_cont <span class="ansi-blue-fg">=</span> n_emb<span class="ansi-blue-fg">,</span>n_cont
        sizes <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>n_emb <span class="ansi-blue-fg">+</span> n_cont<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">+</span> layers <span class="ansi-blue-fg">+</span> <span class="ansi-blue-fg">[</span>out_sz<span class="ansi-blue-fg">]</span>
        actns <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>act_cls <span class="ansi-green-fg">for</span> _ <span class="ansi-green-fg">in</span> range<span class="ansi-blue-fg">(</span>len<span class="ansi-blue-fg">(</span>sizes<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">2</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">+</span> <span class="ansi-blue-fg">[</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">]</span>
        _layers <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>LinBnDrop<span class="ansi-blue-fg">(</span>sizes<span class="ansi-blue-fg">[</span>i<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> sizes<span class="ansi-blue-fg">[</span>i<span class="ansi-blue-fg">+</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> bn<span class="ansi-blue-fg">=</span>use_bn <span class="ansi-green-fg">and</span> <span class="ansi-blue-fg">(</span>i<span class="ansi-blue-fg">!=</span>len<span class="ansi-blue-fg">(</span>actns<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">-</span><span class="ansi-cyan-fg">1</span> <span class="ansi-green-fg">or</span> bn_final<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> p<span class="ansi-blue-fg">=</span>p<span class="ansi-blue-fg">,</span> act<span class="ansi-blue-fg">=</span>a<span class="ansi-blue-fg">,</span> lin_first<span class="ansi-blue-fg">=</span>lin_first<span class="ansi-blue-fg">)</span>
                       <span class="ansi-green-fg">for</span> i<span class="ansi-blue-fg">,</span><span class="ansi-blue-fg">(</span>p<span class="ansi-blue-fg">,</span>a<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">in</span> enumerate<span class="ansi-blue-fg">(</span>zip<span class="ansi-blue-fg">(</span>ps<span class="ansi-blue-fg">+</span><span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0.</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span>actns<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">]</span>
        <span class="ansi-green-fg">if</span> y_range <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> _layers<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>SigmoidRange<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>y_range<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
        self<span class="ansi-blue-fg">.</span>layers <span class="ansi-blue-fg">=</span> nn<span class="ansi-blue-fg">.</span>Sequential<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>_layers<span class="ansi-blue-fg">)</span>
    <span class="ansi-green-fg">def</span> forward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x_cat<span class="ansi-blue-fg">,</span> x_cont<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
        <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>n_emb <span class="ansi-blue-fg">!=</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">:</span>
            x <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">[</span>e<span class="ansi-blue-fg">(</span>x_cat<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">:</span><span class="ansi-blue-fg">,</span>i<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> i<span class="ansi-blue-fg">,</span>e <span class="ansi-green-fg">in</span> enumerate<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>embeds<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">]</span>
            x <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>cat<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span>
            x <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>emb_drop<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>
        <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>n_cont <span class="ansi-blue-fg">!=</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">:</span>
            <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>bn_cont <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">not</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span> x_cont <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>bn_cont<span class="ansi-blue-fg">(</span>x_cont<span class="ansi-blue-fg">)</span>
            x <span class="ansi-blue-fg">=</span> torch<span class="ansi-blue-fg">.</span>cat<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">[</span>x<span class="ansi-blue-fg">,</span> x_cont<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>n_emb <span class="ansi-blue-fg">!=</span> <span class="ansi-cyan-fg">0</span> <span class="ansi-green-fg">else</span> x_cont
        <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>layers<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">)</span>
<span class="ansi-red-fg">File:</span>           ~/mambaforge/lib/python3.10/site-packages/fastai/tabular/model.py
<span class="ansi-red-fg">Type:</span>           PrePostInitMeta
<span class="ansi-red-fg">Subclasses:</span>     EmbeddingNN, EmbeddingNN
</pre>
</div>
</div>
</div>
</section>
<section id="kwargs-and-delegates" class="level3">
<h3 class="anchored" data-anchor-id="kwargs-and-delegates">kwargs and Delegates</h3>
<blockquote class="blockquote">
<p><code>EmbeddingNN</code> includes <code>**kwargs</code> as a parameter to <code>__init__</code>. In Python <code>**kwargs</code> in a parameter list means “put any additional keyword arguments into a dict called <code>kwargs</code>. And <code>**kwargs</code> in an argument list means”insert all key/value pairs in the <code>kwargs</code> dict as named arguments here”. This approach is used in many popular libraries, such as <code>matplotlib</code>, in which the main <code>plot</code> function simply has the signature <code>plot(*args, **kwargs)</code>. The <a href="https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot"><code>plot</code> documentation</a> says “The <code>kwargs</code> are <code>Line2D</code> properties” and then lists those properties.</p>
</blockquote>
<blockquote class="blockquote">
<p>We’re using <code>**kwargs</code> in <code>EmbeddingNN</code> to avoid having to write all the arguments to <code>TabularModel</code> a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesn’t know what parameters are available. Consequently things like tab completion of parameter names and pop-up lists of signatures won’t work.</p>
</blockquote>
<blockquote class="blockquote">
<p>fastai resolves this by providing a special <code>@delegates</code> decorator, which automatically changes the signature of the class or function (<code>EmbeddingNN</code> in this case) to insert all of its keyword arguments into the signature.</p>
</blockquote>
<p>Although the results of <code>EmbeddingNN</code> are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That’s exactly what <code>TabularModel</code> does. In fact, we’ve now seen that <code>EmbeddingNN</code> is just a <code>TabularModel</code>, with <code>n_cont=0</code> and <code>out_sz=1</code>. So, we’d better spend some time learning about <code>TabularModel</code>, and how to use it to get great results! We’ll do that in the next chapter.</p>
</section>
<section id="natural-language-processing-nlp" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-processing-nlp">Natural Language Processing (NLP)</h3>
<p>It’s possible you may have heard about <code>Embeddings before</code> in the context of <code>Natural Language Processing (NLP)</code>. We can turn words into integers using an embedding matrix. Let’s use the poem [Green Eggs and Ham] to illustrate:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="collaborative-filtering-deep-dive_files/figure-html/05150507-7e20-435c-86ab-c1f520086cba.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">embedding_NLP.JPG</figcaption><p></p>
</figure>
</div>
<p>From the spreadhseet screenshot above, we can see that each word that appears in the poem is given an index which can be arbitrary (in this case alphabetical) and then given 4 randomly initialized latent factors, and a bias factor. This allows the conversion from text to integers in the form of an Embedding matrix, which allows our neural net to interpret the text.</p>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key takeaways</h3>
<p>This blog has explored <code>Collaborative Filtering</code> and we have seen how to:</p>
<ul>
<li>build a Collaborative Filtering model from scratch</li>
<li>create Embedding matrices from scratch</li>
<li>replicate the from-scratch model using PyTorch</li>
<li>replicate the from-scratch model using Fast.ai</li>
</ul>
<p>We have also learned how to build a Collaborative Filtering Model using <code>deep learning</code> again, doing this from scratch, using PyTorch’s functionality, and also using the Fast.ai methodology. We saw how gradient descent can learn intrinsic factors or biases about items from a history of ratings, which can then give us information about the data, which can be used to provide e.g.&nbsp;tailored movie recommendations.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>