[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Stephen Barrie",
    "section": "",
    "text": "My background is in finance, most recently as client relationship manager for a portfolio of clients from a wide range of industries, where I developed an aptitude for translating raw data into business insights to facilitate data-driven strategic decision making. I have a solid understanding of accounting with extensive data wrangling, analytical, stakeholder management, and communication skills. As an experienced auditor, I also understand business risk, data pipelines, systems of control and know how to exercise professional scepticism and document evidence.\nI decided to develop my data science skills further and recently completed the Google Data Analytics certificate which focused on how to prepare, process, analyze and share data using SQL, Tableau and R. I followed that up with the Machine Learning Specialization certificate which covered supervised learning (linear regression, logistic regression, decision trees, neural networks), unsupervised learning (clustering, anomaly detection), recommender systems (Collaborative Filtering) and reinforcement learning.\nI am currently working through Practical Deep Learning for Coders which covers the fast.ai Python library. The course is very ‘hands-on’ and includes the practical application of machine learning, which complements the underlying theory acquired in Machine Learning Specialization. I have already built an image classification model and deployed to Hugging Face and know how to build a wide range of models ‘from scratch’ using Excel, Python (PyTorch, scikit-learn) and the fast.ai library.\nI am looking for a new challenge."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Into the Unknown",
    "section": "",
    "text": "Forecasting\n\n\nDataCamp\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nMulti-Label\n\n\nCross-Entropy\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Accumulation\n\n\nfastai\n\n\nEnsembling\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nfastai\n\n\nvision\n\n\ntimm\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nKaggle\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forests\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseaborn\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumPy\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nImage\n\n\nData Carpentry\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nData Carpentry\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nGeospatial\n\n\nData Carpentry\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nAudio\n\n\nlibrosa\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nPython\n\n\nDataCamp\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nDataCamp\n\n\nApache Spark\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nSQL\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nPython\n\n\n\n\n\n\n\nStephen Barrie\n\n\nNov 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nNov 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\nHugging Face\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Journey\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging\n\n\nQuarto\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html",
    "href": "posts/Programming_with_Python/Programming_with_Python.html",
    "title": "Programming with Python",
    "section": "",
    "text": "This blog has been produced after working through the Programming with Python lessons provided by Data Carpentry.\nThe best way to learn how to program is to do something useful, so this introduction to Python is built around a common scientific task: data analysis."
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#scenario-a-miracle-arthritis-inflammation-cure",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#scenario-a-miracle-arthritis-inflammation-cure",
    "title": "Programming with Python",
    "section": "Scenario: A Miracle Arthritis Inflammation Cure",
    "text": "Scenario: A Miracle Arthritis Inflammation Cure\nOur imaginary colleague “Dr. Maverick” has invented a new miracle drug that promises to cure arthritis inflammation flare-ups after only 3 weeks since initially taking the medication! Naturally, we wish to see the clinical trial data, and after months of asking for the data they have finally provided us with a CSV spreadsheet containing the clinical trial data.\nThe CSV file contains the number of inflammation flare-ups per day for the 60 patients in the initial clinical trial, with the trial lasting 40 days. Each row corresponds to a patient, and each column corresponds to a day in the trial. Once a patient has their first inflammation flare-up they take the medication and wait a few weeks for it to take effect and reduce flare-ups.\nTo see how effective the treatment is we would like to:\n\nCalculate the average inflammation per day across all patients.\nPlot the result to discuss and share with colleagues.\n\n\nData Format\nThe data sets are stored in comma-separated values (CSV) format:\n\neach row holds information for a single patient,\ncolumns represent successive days.\n\nThe first three rows of our first file look like this:\n\n\n\nthree_rows.JPG\n\n\nEach number represents the number of inflammation bouts that a particular patient experienced on a given day. For example, value “6” at row 3 column 7 of the data set above means that the third patient was experiencing inflammation six times on the seventh day of the clinical study.\nIn order to analyze this data and report to our colleagues, we’ll have to learn a little bit about programming."
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#python-fundamentals",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#python-fundamentals",
    "title": "Programming with Python",
    "section": "1. Python Fundamentals",
    "text": "1. Python Fundamentals\n\nVariables\nAny Python interpreter can be used as a calculator:\n\n3 + 5 * 4\n\n23\n\n\nThis is great but not very interesting. To do anything useful with data, we need to assign its value to a variable. In Python, we can assign a value to a variable, using the equals sign =. For example, we can track the weight of a patient who weighs 60 kilograms by assigning the value 60 to a variable weight_kg:\n\nweight_kg = 60\n\nFrom now on, whenever we use weight_kg, Python will substitute the value we assigned to it. In layperson’s terms, a variable is a name for a value.\nIn Python, variable names:\n\ncan include letters, digits, and underscores\ncannot start with a digit\nare case sensitive.\n\nThis means that, for example:\n\nweight0 is a valid variable name, whereas 0weight is not\nweight and Weight are different variables\n\n\n\nTypes of data\nPython knows various types of data. Three common ones are:\n\ninteger numbers\nfloating point numbers, and\nstrings.\n\nIn the example above, variable weight_kg has an integer value of 60. If we want to more precisely track the weight of our patient, we can use a floating point value by executing:\n\nweight_kg = 60.3\n\nTo create a string, we add single or double quotes around some text. To identify and track a patient throughout our study, we can assign each person a unique identifier by storing it in a string:\n\npatient_id = '001'\n\n\n\nUsing Variables in Python\nOnce we have data stored with variable names, we can make use of it in calculations. We may want to store our patient’s weight in pounds as well as kilograms:\n\nweight_lb = 2.2 * weight_kg\n\nWe might decide to add a prefix to our patient identifier:\n\npatient_id = 'inflam_' + patient_id\n\n\n\nBuilt-in Python functions\nTo carry out common tasks with data and variables in Python, the language provides us with several built-in functions. To display information to the screen, we use the print function:\n\nprint(weight_lb)\nprint(patient_id)\n\n132.66\ninflam_inflam_001\n\n\nWhen we want to make use of a function, referred to as calling the function, we follow its name by parentheses. The parentheses are important: if you leave them off, the function doesn’t actually run! Sometimes you will include values or variables inside the parentheses for the function to use. In the case of print, we use the parentheses to tell the function what value we want to display. We will learn more about how functions work and how to create our own in later sections.\nWe can display multiple things at once using only one print call:\n\nprint(patient_id, 'weight in kilograms:', weight_kg)\n\ninflam_inflam_inflam_001 weight in kilograms: 60.3\n\n\nWe can also call a function inside of another function call. For example, Python has a built-in function called type that tells you a value’s data type:\n\nprint(type(60.3))\nprint(type(patient_id))\n\n<class 'float'>\n<class 'str'>\n\n\nMoreover, we can do arithmetic with variables right inside the print function:\n\nprint('weight in pounds:', 2.2 * weight_kg)\n\nweight in pounds: 132.66\n\n\nThe above command, however, did not change the value of weight_kg:\n\nprint(weight_kg)\n\n60.3\n\n\nTo change the value of the weight_kg variable, we have to assign weight_kg a new value using the equals = sign:\n\nweight_kg = 65.0\nprint('weight in kilograms is now:', weight_kg)\n\nweight in kilograms is now: 65.0\n\n\n\n\nVariables as Sticky Notes\nA variable in Python is analogous to a sticky note with a name written on it: assigning a value to a variable is like putting that sticky note on a particular value. Using this analogy, we can investigate how assigning a value to one variable does not change values of other, seemingly related, variables. For example, let’s store the subject’s weight in pounds in its own variable:\n\n# There are 2.2 pounds per kilogram\nweight_lb = 2.2 * weight_kg\nprint('weight in kilograms:', weight_kg, 'and in pounds:', weight_lb)\n\nweight in kilograms: 65.0 and in pounds: 143.0\n\n\nSimilar to above, the expression 2.2 * weight_kg is evaluated to 143.0, and then this value is assigned to the variable weight_lb (i.e. the sticky note weight_lb is placed on 143.0). At this point, each variable is “stuck” to completely distinct and unrelated values.\nLet’s now change weight_kg:\n\nweight_kg = 100.0\nprint('weight in kilograms is now:', weight_kg, 'and weight in pounds is still:', weight_lb)\n\nweight in kilograms is now: 100.0 and weight in pounds is still: 143.0\n\n\n\nSince weight_lb doesn’t “remember” where its value comes from, it is not updated when we change weight_kg.\n\n\n\nSorting out references\nPython allows you to assign multiple values to multiple variables in one line by separating the variables and values with commas.\n\nfirst, second = 'Grace', 'Hopper'\nthird, fourth = second, first\nprint(third, fourth)\n\nHopper Grace\n\n\n\n\nSeeing Data Types\n\nplanet = 'Earth'\napples = 5\ndistance = 10.5\n\n\nprint(type(planet))\nprint(type(apples))\nprint(type(distance))\n\n<class 'str'>\n<class 'int'>\n<class 'float'>\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nbasic data types in Python include integers, strings, and floating-point numbers\nuse variable = value to assign a value to a variable in order to record it in memory\nvariables are created on demand whenever a value is assigned to them\nuse print(something) to display the value of something\nbuilt-in functions are always available to use"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-patient-data",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-patient-data",
    "title": "Programming with Python",
    "section": "2. Analyzing Patient Data",
    "text": "2. Analyzing Patient Data\n\nLoading data into Python\n\nimport numpy\n\n\nnumpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\n\narray([[0., 0., 1., ..., 3., 0., 0.],\n       [0., 1., 2., ..., 1., 0., 1.],\n       [0., 1., 1., ..., 2., 1., 1.],\n       ...,\n       [0., 1., 1., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 0., 2., 0.],\n       [0., 0., 1., ..., 1., 1., 0.]])\n\n\nThe expression numpy.loadtxt(…) is a function call that asks Python to run the function loadtxt which belongs to the NumPy library. The dot notation in Python is used most of all as an object attribute/property specifier or for invoking its method. object.property will give you the object.property value, object_name.method() will invoke on object_name method.\nAs an example, John Smith is the John that belongs to the Smith family. We could use the dot notation to write his name smith.john, just as loadtxt is a function that belongs to the numpy library.\nnumpy.loadtxt has two parameters: - the name of the file we want to read and - the delimiter that separates values on a line.\nThese both need to be character strings (or strings for short), so we put them in quotes.\nSince we haven’t told it to do anything else with the function’s output, the notebook displays it. In this case, that output is the data we just loaded. By default, only a few rows and columns are shown (with … to omit elements when displaying big arrays). Note that, to save space when displaying NumPy arrays, Python does not show us trailing zeros, so 1.0 becomes 1..\nOur call to numpy.loadtxt read our file but didn’t save the data in memory. To do that, we need to assign the array to a variable. In a similar manner to how we assign a single value to a variable, we can also assign an array of values to a variable using the same syntax. Let’s re-run numpy.loadtxt and save the returned data:\n\ndata = numpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\n\nThis statement doesn’t produce any output because we’ve assigned the output to the variable data. If we want to check that the data have been loaded, we can print the variable’s value:\n\nprint(data)\n\n[[0. 0. 1. ... 3. 0. 0.]\n [0. 1. 2. ... 1. 0. 1.]\n [0. 1. 1. ... 2. 1. 1.]\n ...\n [0. 1. 1. ... 1. 1. 1.]\n [0. 0. 0. ... 0. 2. 0.]\n [0. 0. 1. ... 1. 1. 0.]]\n\n\nNow that the data are in memory, we can manipulate them. First, let’s ask what type of thing data refers to:\n\nprint(type(data))\n\n<class 'numpy.ndarray'>\n\n\nThe output tells us that data currently refers to an N-dimensional array, the functionality for which is provided by the NumPy library. These data correspond to arthritis patients’ inflammation. The rows are the individual patients, and the columns are their daily inflammation measurements.\n\n\nData Type\nA Numpy array contains one or more elements of the same type. The type function will only tell you that a variable is a NumPy array but won’t tell you the type of thing inside the array. We can find out the type of the data contained in the NumPy array:\n\nprint(data.dtype)\n\nfloat64\n\n\nThis tells us that the NumPy array’s elements are floating-point numbers. With the following command, we can see the array’s shape:\n\nprint(data.shape)\n\n(60, 40)\n\n\nThe output tells us that the data array variable contains 60 rows and 40 columns. When we created the variable data to store our arthritis data, we did not only create the array; we also created information about the array, called members or attributes. This extra information describes data in the same way an adjective describes a noun. data.shape is an attribute of data which describes the dimensions of data. We use the same dotted notation for the attributes of variables that we use for the functions in libraries because they have the same part-and-whole relationship.\nIf we want to get a single number from the array, we must provide an index in square brackets after the variable name, just as we do in math when referring to an element of a matrix. Our inflammation data has two dimensions, so we will need to use two indices to refer to one specific value:\n\nprint('first value in data:', data[0, 0])\n\nfirst value in data: 0.0\n\n\n\nprint('middle value in data:', data[30, 20])\n\nmiddle value in data: 13.0\n\n\nThe expression data[30, 20] accesses the element at row 30, column 20. While this expression may not surprise you, data[0, 0] might. Programming languages like Fortran, MATLAB and R start counting at 1 because that’s what human beings have done for thousands of years. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because it represents an offset from the first value in the array (the second value is offset by one index from the first value). This is closer to the way that computers represent arrays (if you are interested in the historical reasons behind counting indices from zero, you can read Mike Hoye’s blog post). As a result, if we have an M×N array in Python, its indices go from 0 to M-1 on the first axis and 0 to N-1 on the second. It takes a bit of getting used to, but one way to remember the rule is that the index is how many steps we have to take from the start to get the item we want. \nWhat may also surprise you is that when Python displays an array, it shows the element with index [0, 0] in the upper left corner rather than the lower left. This is consistent with the way mathematicians draw matrices but different from the Cartesian coordinates. The indices are (row, column) instead of (column, row) for the same reason, which can be confusing when plotting data.\n\n\nSlicing data\n\nAn index like [30, 20] selects a single element of an array, but we can select whole sections as well. For example, we can select the first ten days (columns) of values for the first four patients (rows) like this:\n\nprint(data[0:4, 0:10])\n\n[[0. 0. 1. 3. 1. 2. 4. 7. 8. 3.]\n [0. 1. 2. 1. 2. 1. 3. 2. 2. 6.]\n [0. 1. 1. 3. 3. 2. 6. 2. 5. 9.]\n [0. 0. 2. 0. 4. 2. 2. 1. 6. 7.]]\n\n\nThe slice 0:4 means, “Start at index 0 and go up to, but not including, index 4”. Again, the up-to-but-not-including takes a bit of getting used to, but the rule is that the difference between the upper and lower bounds is the number of values in the slice.\nWe don’t have to start slices at 0:\n\nprint(data[5:10, 0:10])\n\n[[0. 0. 1. 2. 2. 4. 2. 1. 6. 4.]\n [0. 0. 2. 2. 4. 2. 2. 5. 5. 8.]\n [0. 0. 1. 2. 3. 1. 2. 3. 5. 3.]\n [0. 0. 0. 3. 1. 5. 6. 5. 5. 8.]\n [0. 1. 1. 2. 1. 3. 5. 3. 5. 8.]]\n\n\nWe also don’t have to include the upper and lower bound on the slice. If we don’t include the lower bound, Python uses 0 by default; if we don’t include the upper, the slice runs to the end of the axis, and if we don’t include either (i.e., if we use ‘:’ on its own), the slice includes everything:\n\nsmall = data[:3, 36:]\nprint('small is:')\nprint(small)\n\nsmall is:\n[[2. 3. 0. 0.]\n [1. 1. 0. 1.]\n [2. 2. 1. 1.]]\n\n\nThe above example selects rows 0 through 2 and columns 36 through to the end of the array.\n\n\nAnalyzing data\nNumPy has several useful functions that take an array as input to perform operations on its values. If we want to find the average inflammation for all patients on all days, for example, we can ask NumPy to compute data’s mean value:\n\nprint(numpy.mean(data))\n\n6.14875\n\n\nmean is a function that takes an array as an argument.\n\n\nNot all functions have inputs\nGenerally, a function uses inputs to produce outputs. However, some functions produce outputs without needing any input. For example, checking the current time doesn’t require any input.\n\nimport time\nprint(time.ctime())\n\nThu Dec 15 22:41:53 2022\n\n\nFor functions that don’t take in any arguments, we still need parentheses (()) to tell Python to go and do something for us. Let’s use three other NumPy functions to get some descriptive values about the dataset. We’ll also use multiple assignment, a convenient Python feature that will enable us to do this all in one line.\n\n# assigned the return value from numpy.max(data) to the variable maxval, the value from numpy.min(data) to minval, and so on\nmaxval, minval, stdval = numpy.max(data), numpy.min(data), numpy.std(data)\n\nprint('maximum inflammation:', maxval)\nprint('minimum inflammation:', minval)\nprint('standard deviation:', stdval)\n\nmaximum inflammation: 20.0\nminimum inflammation: 0.0\nstandard deviation: 4.613833197118566\n\n\n\n\nMystery Functions in IPython\nHow did we know what functions NumPy has and how to use them? If you are working in IPython or in a Jupyter Notebook, there is an easy way to find out. If you type the name of something followed by a dot, then you can use tab completion (e.g. type numpy. and then press Tab) to see a list of all functions and attributes that you can use. After selecting one, you can also add a question mark (e.g. numpy.cumprod?), and IPython will return an explanation of the method! This is the same as doing help(numpy.cumprod).\nSimilarly, if you are using the “plain vanilla” Python interpreter, you can type numpy. and press the Tab key twice for a listing of what is available. You can then use the help() function to see an explanation of the function you’re interested in, for example: help(numpy.cumprod).\nWhen analyzing data, though, we often want to look at variations in statistical values, such as the maximum inflammation per patient or the average inflammation per day. One way to do this is to create a new temporary array of the data we want, then ask it to do the calculation:\n\n# 0 on the first axis (rows), everything on the second (columns)\npatient_0 = data[0, :] \nprint('maximum inflammation for patient 0:', numpy.max(patient_0))\n\nmaximum inflammation for patient 0: 18.0\n\n\nEverything in a line of code following the ‘#’ symbol is a comment that is ignored by Python. Comments allow programmers to leave explanatory notes for other programmers or their future selves. We don’t actually need to store the row in a variable of its own. Instead, we can combine the selection and the function call:\n\nprint('maximum inflammation for patient 2:', numpy.max(data[2, :]))\n\nmaximum inflammation for patient 2: 19.0\n\n\nWhat if we need the maximum inflammation for each patient over all days (as in the next diagram on the left) or the average for each day (as in the diagram on the right)? As the diagram below shows, we want to perform the operation across an axis:\n\n\n\npython-operations-across-axes.png\n\n\nTo support this functionality, most array functions allow us to specify the axis we want to work on. If we ask for the average across axis 0 (rows in our 2D example), we get:\n\nprint(numpy.mean(data, axis=0))\n\n[ 0.          0.45        1.11666667  1.75        2.43333333  3.15\n  3.8         3.88333333  5.23333333  5.51666667  5.95        5.9\n  8.35        7.73333333  8.36666667  9.5         9.58333333 10.63333333\n 11.56666667 12.35       13.25       11.96666667 11.03333333 10.16666667\n 10.          8.66666667  9.15        7.25        7.33333333  6.58333333\n  6.06666667  5.95        5.11666667  3.6         3.3         3.56666667\n  2.48333333  1.5         1.13333333  0.56666667]\n\n\nAs a quick check, we can ask this array what its shape is:\n\nprint(numpy.mean(data, axis=0).shape)\n\n(40,)\n\n\nThe expression (40,) tells us we have an N×1 vector, so this is the average inflammation per day for all patients. If we average across axis 1 (columns in our 2D example), we get:\n\nprint(numpy.mean(data, axis=1))\n\n[5.45  5.425 6.1   5.9   5.55  6.225 5.975 6.65  6.625 6.525 6.775 5.8\n 6.225 5.75  5.225 6.3   6.55  5.7   5.85  6.55  5.775 5.825 6.175 6.1\n 5.8   6.425 6.05  6.025 6.175 6.55  6.175 6.35  6.725 6.125 7.075 5.725\n 5.925 6.15  6.075 5.75  5.975 5.725 6.3   5.9   6.75  5.925 7.225 6.15\n 5.95  6.275 5.7   6.1   6.825 5.975 6.725 5.7   6.25  6.4   7.05  5.9  ]\n\n\nwhich is the average inflammation per patient across all days.\n\n\nSlicing Strings\nA section of an array is called a slice. We can take slices of character strings as well:\n\nelement = 'oxygen'\n\nprint('first three characters:', element[0:3])\nprint('last three characters:', element[3:6])\nprint('first four characters:', element[:4])\nprint('last two characters:', element[4:])\nprint('all characters:', element[:])\nprint('last character:', element[-1])\nprint('second last character:', element[-2])\n\nfirst three characters: oxy\nlast three characters: gen\nfirst four characters: oxyg\nlast two characters: en\nall characters: oxygen\nlast character: n\ne\n\n\n\n\nThin slices\n\nprint(element[3:3])\n\n\n\n\n\ndata[3:3, 4:4]\n\narray([], shape=(0, 0), dtype=float64)\n\n\n\ndata[3:3, :]\n\narray([], shape=(0, 40), dtype=float64)\n\n\n\n\nStacking arrays\nArrays can be concatenated and stacked on top of one another, using NumPy’s vstack and hstack functions for vertical and horizontal stacking, respectively.\n\nA = numpy.array([[1,2,3], [4,5,6], [7, 8, 9]])\nprint('A = ')\nprint(A)\n\nB = numpy.hstack([A, A])\nprint('B = ')\nprint(B)\n\nC = numpy.vstack([A, A])\nprint('C = ')\nprint(C)\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nB = \n[[1 2 3 1 2 3]\n [4 5 6 4 5 6]\n [7 8 9 7 8 9]]\nC = \n[[1 2 3]\n [4 5 6]\n [7 8 9]\n [1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\nA ‘gotcha’ with array indexing is that singleton dimensions are dropped by default. That means A[:, 0] is a one dimensional array, which won’t stack as desired. To preserve singleton dimensions, the index itself can be a slice or array. For example, A[:, :1] returns a two dimensional array with one singleton dimension (i.e. a column vector).\n\nD = numpy.hstack((A[:, :1], A[:, -1:]))\nprint('D = ')\nprint(D)\n\nD = \n[[1 3]\n [4 6]\n [7 9]]\n\n\nAn alternative way to achieve the same result is to use Numpy’s delete function to remove the second column of A:\n\nD = numpy.delete(A, 1, 1)\nprint('D = ')\nprint(D)\n\nD = \n[[1 3]\n [4 6]\n [7 9]]\n\n\n\n\nChange In Inflammation\nThe patient data is longitudinal in the sense that each row represents a series of observations relating to one individual. This means that the change in inflammation over time is a meaningful concept. Let’s find out how to calculate changes in the data contained in an array with NumPy.\nThe numpy.diff() function takes an array and returns the differences between two successive values. Let’s use it to examine the changes each day across the first week of patient 3 from our inflammation dataset.\n\npatient3_week1 = data[3, :7]\nprint(patient3_week1)\n\n[0. 0. 2. 0. 4. 2. 2.]\n\n\nCalling numpy.diff(patient3_week1) would do the following calculations\n\n[ 0 - 0, 2 - 0, 0 - 2, 4 - 0, 2 - 4, 2 - 2 ]\n\n[0, 2, -2, 4, -2, 0]\n\n\nand return the 6 difference values in a new array.\n\nnumpy.diff(patient3_week1)\n\narray([ 0.,  2., -2.,  4., -2.,  0.])\n\n\nNote that the array of differences is shorter by one element (length 6).\nWhen calling numpy.diff with a multi-dimensional array, an axis argument may be passed to the function to specify which axis to process. When applying numpy.diff to our 2D inflammation array data, which axis would we specify?\nSince the row axis (0) is patients, it does not make sense to get the difference between two arbitrary patients. The column axis (1) is in days, so the difference is the change in inflammation – a meaningful concept.\n\nnumpy.diff(data, axis=1)\n\narray([[ 0.,  1.,  2., ...,  1., -3.,  0.],\n       [ 1.,  1., -1., ...,  0., -1.,  1.],\n       [ 1.,  0.,  2., ...,  0., -1.,  0.],\n       ...,\n       [ 1.,  0.,  0., ..., -1.,  0.,  0.],\n       [ 0.,  0.,  1., ..., -2.,  2., -2.],\n       [ 0.,  1., -1., ..., -2.,  0., -1.]])\n\n\n\n\n\n\n\n\nIf the shape of an individual data file is (60, 40) (60 rows and 40 columns), what would the shape of the array be after you run the diff() function and why?\n\n\n\n\n\nThe shape will be (60, 39) because there is one fewer difference between columns than there are columns in the data.\n\n\n\n\n\n\n\n\n\nHow would you find the largest change in inflammation for each patient? Does it matter if the change in inflammation is an increase or a decrease?\n\n\n\n\n\nBy using the numpy.max() function after you apply the numpy.diff() function, you will get the largest difference between days.\n\n\n\n\nnumpy.max(numpy.diff(data, axis=1), axis=1)\n\narray([ 7., 12., 11., 10., 11., 13., 10.,  8., 10., 10.,  7.,  7., 13.,\n        7., 10., 10.,  8., 10.,  9., 10., 13.,  7., 12.,  9., 12., 11.,\n       10., 10.,  7., 10., 11., 10.,  8., 11., 12., 10.,  9., 10., 13.,\n       10.,  7.,  7., 10., 13., 12.,  8.,  8., 10., 10.,  9.,  8., 13.,\n       10.,  7., 10.,  8., 12., 10.,  7., 12.])\n\n\nIf inflammation values decrease along an axis, then the difference from one element to the next will be negative. If you are interested in the magnitude of the change and not the direction, the numpy.absolute() function will provide that.\nNotice the difference if you get the largest absolute difference between readings.\n\nnumpy.max(numpy.absolute(numpy.diff(data, axis=1)), axis=1)\n\narray([12., 14., 11., 13., 11., 13., 10., 12., 10., 10., 10., 12., 13.,\n       10., 11., 10., 12., 13.,  9., 10., 13.,  9., 12.,  9., 12., 11.,\n       10., 13.,  9., 13., 11., 11.,  8., 11., 12., 13.,  9., 10., 13.,\n       11., 11., 13., 11., 13., 13., 10.,  9., 10., 10.,  9.,  9., 13.,\n       10.,  9., 10., 11., 13., 10., 10., 12.])\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nimport a library into a program using import libraryname\nuse the numpy library to work with arrays in Python\nthe expression array.shape gives the shape of an array\nuse array[x, y] to select a single element from a 2D array\narray indices start at 0, not 1\nuse low:high to specify a slice that includes the indices from low to high-1\nuse # some kind of explanation to add comments to programs\nuse numpy.mean(array), numpy.max(array), and numpy.min(array) to calculate simple statistics\nuse numpy.mean(array, axis=0) or numpy.mean(array, axis=1) to calculate statistics across the specified axis"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#visualizing-tabular-data",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#visualizing-tabular-data",
    "title": "Programming with Python",
    "section": "3. Visualizing Tabular Data",
    "text": "3. Visualizing Tabular Data\n\nVisualizing data\nThe mathematician Richard Hamming once said, “The purpose of computing is insight, not numbers,” and the best way to develop insight is often to visualize data. Visualization deserves an entire lecture of its own, but we can explore a few features of Python’s matplotlib library here. While there is no official plotting library, matplotlib is the de facto standard. First, we will import the pyplot module from matplotlib and use two of its functions to create and display a heat map of our data:\n\nimport matplotlib.pyplot\nimage = matplotlib.pyplot.imshow(data)\nmatplotlib.pyplot.show()\n\n\n\n\nEach row in the heat map corresponds to a patient in the clinical trial dataset, and each column corresponds to a day in the dataset. Blue pixels in this heat map represent low values, while yellow pixels represent high values. As we can see, the general number of inflammation flare-ups for the patients rises and falls over a 40-day period.\nSo far so good as this is in line with our knowledge of the clinical trial and Dr. Maverick’s claims:\n\nthe patients take their medication once their inflammation flare-ups begin\nit takes around 3 weeks for the medication to take effect and begin reducing flare-ups\nand flare-ups appear to drop to zero by the end of the clinical trial\n\nNow let’s take a look at the average inflammation over time:\n\nave_inflammation = numpy.mean(data, axis=0)\nave_plot = matplotlib.pyplot.plot(ave_inflammation)\nmatplotlib.pyplot.show()\n\n\n\n\nHere, we have put the average inflammation per day across all patients in the variable ave_inflammation, then asked matplotlib.pyplot to create and display a line graph of those values. The result is a reasonably linear rise and fall, in line with Dr. Maverick’s claim that the medication takes 3 weeks to take effect.\nBut a good data scientist doesn’t just consider the average of a dataset, so let’s have a look at two other statistics:\n\n# maximum inflammation per day \nmax_plot = matplotlib.pyplot.plot(numpy.max(data, axis=0))\nmatplotlib.pyplot.show()\n\n\n\n\n\n# minimum inflammation per day \nmax_plot = matplotlib.pyplot.plot(numpy.min(data, axis=0))\nmatplotlib.pyplot.show()\n\n\n\n\nThe maximum value rises and falls linearly. The minimum seems to be a step function. Neither trend seems particularly likely, so either there’s a mistake in our calculations or something is wrong with our data. This insight would have been difficult to reach by examining the numbers themselves without visualization tools.\n\n\nGrouping plots\nYou can group similar plots in a single figure using subplots. This script below uses a number of new commands. The function matplotlib.pyplot.figure() creates a space into which we will place all of our plots. The parameter figsize tells Python how big to make this space. Each subplot is placed into the figure using its add_subplot method. The add_subplot method takes 3 parameters. The first denotes how many total rows of subplots there are, the second parameter refers to the total number of subplot columns, and the final parameter denotes which subplot your variable is referencing (left-to-right, top-to-bottom). Each subplot is stored in a different variable (axes1, axes2, axes3). Once a subplot is created, the axes can be titled using the set_xlabel() command (or set_ylabel()). Here are our three plots side by side:\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\naxes1 = fig.add_subplot(1, 3, 1)\naxes2 = fig.add_subplot(1, 3, 2)\naxes3 = fig.add_subplot(1, 3, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(data, axis=0))\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(data, axis=0))\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0))\n\nfig.tight_layout()\n\nmatplotlib.pyplot.savefig('inflammation.png')\nmatplotlib.pyplot.show()\n\n\n\n\nThe call to loadtxt reads our data, and the rest of the program tells the plotting library how large we want the figure to be, that we’re creating three subplots, what to draw for each one, and that we want a tight layout.\nIf we leave out that call to fig.tight_layout(), the graphs will actually be squeezed together more closely.\nThe call to savefig stores the plot as a graphics file. This can be a convenient way to store your plots for use in other documents, web pages etc. The graphics format is automatically determined by Matplotlib from the file name ending we specify; here PNG from ‘inflammation.png’. Matplotlib supports many different graphics formats, including SVG, PDF, and JPEG.\n\n\nImporting libraries with shortcuts\nIn this blog we use the import matplotlib.pyplot syntax to import the pyplot module of matplotlib. However, shortcuts such as import matplotlib.pyplot as plt are frequently used. Importing pyplot this way means that after the initial import, rather than writing matplotlib.pyplot.plot(…), you can now write plt.plot(…). Another common convention is to use the shortcut import numpy as np when importing the NumPy library. We then can write np.loadtxt(…) instead of numpy.loadtxt(…), for example.\nSome people prefer these shortcuts as it is quicker to type and results in shorter lines of code - especially for libraries with long names! You will frequently see Python code online using a pyplot function with plt, or a NumPy function with np, and it’s because they’ve used this shortcut. It makes no difference which approach you choose to take, but you must be consistent as if you use import matplotlib.pyplot as plt then matplotlib.pyplot.plot(…) will not work, and you must use plt.plot(…) instead. Because of this, when working with other people it is important you agree on how libraries are imported.\n\n\n\n\n\n\nWhy do all of our plots stop just short of the upper end of our graph?\n\n\n\n\n\nBecause matplotlib normally sets x and y axes limits to the min and max of our data (depending on data range). If we want to change this, we can use the set_ylim(min, max) method of each ‘axes’, for example:\n\n\n\n\n# One method\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0))\naxes3.set_ylim(0,6)\n\n(0.0, 6.0)\n\n\n\n# A more automated approach\nmin_data = numpy.min(data, axis=0)\naxes3.set_ylabel('min')\naxes3.plot(min_data)\naxes3.set_ylim(numpy.min(min_data), numpy.max(min_data) * 1.1)\n\n(0.0, 5.5)\n\n\n\n\nDrawing Straight Lines\nIn the center and right subplots above, we expect all lines to look like step functions because non-integer value are not realistic for the minimum and maximum values. However, you can see that the lines are not always vertical or horizontal, and in particular the step function in the subplot on the right looks slanted. Why is this?\nBecause matplotlib interpolates (draws a straight line) between the points. One way to avoid this is to use the Matplotlib drawstyle option:\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\naxes1 = fig.add_subplot(1, 3, 1)\naxes2 = fig.add_subplot(1, 3, 2)\naxes3 = fig.add_subplot(1, 3, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(data, axis=0), drawstyle='steps-mid')\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(data, axis=0), drawstyle='steps-mid')\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0), drawstyle='steps-mid')\n\nfig.tight_layout()\n\nmatplotlib.pyplot.show()\n\n\n\n\nLet’s now create a plot showing the standard deviation (numpy.std) of the inflammation data for each day across all patients:\n\nstd_plot = matplotlib.pyplot.plot(numpy.std(data, axis=0))\nmatplotlib.pyplot.show()\n\n\n\n\nWe can modify the program to display the three plots on top of one another instead of side by side:\n\n# change figsize (swap width and height)\nfig = matplotlib.pyplot.figure(figsize=(3.0, 10.0))\n\n# change add_subplot (swap first two parameters)\naxes1 = fig.add_subplot(3, 1, 1)\naxes2 = fig.add_subplot(3, 1, 2)\naxes3 = fig.add_subplot(3, 1, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(data, axis=0))\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(data, axis=0))\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0))\n\nfig.tight_layout()\n\nmatplotlib.pyplot.show()\n\n\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse the pyplot module from the matplotlib library for creating simple visualizations."
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#storing-multiple-values-in-lists",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#storing-multiple-values-in-lists",
    "title": "Programming with Python",
    "section": "4. Storing Multiple Values in Lists",
    "text": "4. Storing Multiple Values in Lists\nIn the previous section, we analyzed a single file of clinical trial inflammation data. However, after finding some peculiar and potentially suspicious trends in the trial data we ask Dr. Maverick if they have performed any other clinical trials. Surprisingly, they say that they have and provide us with 11 more CSV files for a further 11 clinical trials they have undertaken since the initial trial.\nOur goal now is to process all the inflammation data we have, which means that we still have eleven more files to go!\nThe natural first step is to collect the names of all the files that we have to process. In Python, a list is a way to store multiple values together. In this episode, we will learn how to store multiple values in a list as well as how to work with lists.\n\nPython lists\nUnlike NumPy arrays, lists are built into the language so we do not have to load a library to use them. We create a list by putting values inside [square brackets] and separating the values with commas:\n\nodds = [1, 3, 5, 7]\nprint('odds are:', odds)\n\nodds are: [1, 3, 5, 7]\n\n\nWe can access elements of a list using indices – numbered positions of elements in the list. These positions are numbered starting at 0, so the first element has an index of 0\n\nprint('first element:', odds[0])\nprint('last element:', odds[3])\nprint('\"-1\" element:', odds[-1])\n\nfirst element: 1\nlast element: 7\n\"-1\" element: 7\n\n\nYes, we can use negative numbers as indices in Python. When we do so, the index -1 gives us the last element in the list, -2 the second to last, and so on. Because of this, odds[3] and odds[-1] point to the same element here.\n\n\n\n\n\n\nThere is one important difference between lists and strings\n\n\n\nWe can change the values in a list, but we cannot change individual characters in a string.\n\n\nFor example:\n\nnames = ['Curie', 'Darwing', 'Turing']  # typo in Darwin's name\nprint('names is originally:', names)\nnames[1] = 'Darwin'  # correct the name\nprint('final value of names:', names)\n\nnames is originally: ['Curie', 'Darwing', 'Turing']\nfinal value of names: ['Curie', 'Darwin', 'Turing']\n\n\nworks, but:\n\nname = 'Darwin'\nname[0] = 'd'\n\nTypeError: 'str' object does not support item assignment\n\n\ndoes not.\n\n\nCh-Ch-Ch-Ch-Changes\n\n\n\nchanges.jpg\n\n\nData which can be modified in place is called mutable, while data which cannot be modified is called immutable. Strings and numbers are immutable. This does not mean that variables with string or number values are constants, but when we want to change the value of a string or number variable, we can only replace the old value with a completely new value.\nLists and arrays, on the other hand, are mutable: we can modify them after they have been created. We can change individual elements, append new elements, or reorder the whole list. For some operations, like sorting, we can choose whether to use a function that modifies the data in-place or a function that returns a modified copy and leaves the original unchanged.\n\n\n\n\n\n\nBe careful when modifying data in-place.\n\n\n\nIf two variables refer to the same list, and you modify the list value, it will change for both variables!\n\n\n\nsalsa = ['peppers', 'onions', 'cilantro', 'tomatoes']\nmy_salsa = salsa        # <-- my_salsa and salsa point to the *same* list data in memory\nsalsa[0] = 'hot peppers'\nprint('Ingredients in my salsa:', my_salsa)\n\nIngredients in my salsa: ['hot peppers', 'onions', 'cilantro', 'tomatoes']\n\n\nIf you want variables with mutable values to be independent, you must make a copy of the value when you assign it.\n\nsalsa = ['peppers', 'onions', 'cilantro', 'tomatoes']\nmy_salsa = list(salsa)        # <-- makes a *copy* of the list\nsalsa[0] = 'hot peppers'\nprint('Ingredients in my salsa:', my_salsa)\n\nIngredients in my salsa: ['peppers', 'onions', 'cilantro', 'tomatoes']\n\n\nBecause of pitfalls like this, code which modifies data in place can be more difficult to understand. However, it is often far more efficient to modify a large data structure in place than to create a modified copy for every small change. You should consider both of these aspects when writing your code.\n\n\nNested Lists\n\n\n\nnest.jpg\n\n\nSince a list can contain any Python variables, it can even contain other lists.For example, we could represent the products in the shelves of a small grocery shop:\n\nx = [['pepper', 'zucchini', 'onion'],\n     ['cabbage', 'lettuce', 'garlic'],\n     ['apple', 'pear', 'banana']]\n\nHere is a visual example of how indexing a list of lists x works:\n\n\n\nindexing_lists_python.png\n\n\nThanks to Hadley Wickham for the image above.\nUsing the previously declared list x, these would be the results of the index operations shown in the image:\n\nprint([x[0]])\n\n[['pepper', 'zucchini', 'onion']]\n\n\n\nprint(x[0])\n\n['pepper', 'zucchini', 'onion']\n\n\n\nprint(x[0][0])\n\npepper\n\n\n\n\nHeterogeneous Lists\nLists in Python can contain elements of different types. Example:\n\nsample_ages = [10, 12.5, 'Unknown']\n\nThere are many ways to change the contents of lists besides assigning new values to individual elements:\n\nodds = [1,3,5,7]\n\nodds.append(11)\nprint('odds after adding a value:', odds)\n\nodds after adding a value: [1, 3, 5, 7, 11]\n\n\n\nremoved_element = odds.pop(0)\nprint('odds after removing the first element:', odds)\nprint('removed_element:', removed_element)\n\nodds after removing the first element: [3, 5, 7, 11]\nremoved_element: 1\n\n\n\nodds.reverse()\nprint('odds after reversing:', odds)\n\nodds after reversing: [11, 7, 5, 3]\n\n\nWhile modifying in place, it is useful to remember that Python treats lists in a slightly counter-intuitive way. As we saw earlier, when we modified the salsa list item in-place, if we make a list, (attempt to) copy it and then modify this list, we can cause all sorts of trouble. This also applies to modifying the list using the above functions:\n\nodds = [3, 5, 7]\nprimes = odds\nprimes.append(2)\nprint('primes:', primes)\nprint('odds:', odds)\n\nprimes: [3, 5, 7, 2]\nodds: [3, 5, 7, 2]\n\n\nThis is because Python stores a list in memory, and then can use multiple names to refer to the same list. If all we want to do is copy a (simple) list, we can again use the list function, so we do not modify a list we did not mean to:\n\nodds = [3, 5, 7]\nprimes = list(odds)\nprimes.append(2)\nprint('primes:', primes)\nprint('odds:', odds)\n\nprimes: [3, 5, 7, 2]\nodds: [3, 5, 7]\n\n\nSubsets of lists and strings can be accessed by specifying ranges of values in brackets, similar to how we accessed ranges of positions in a NumPy array. This is commonly referred to as “slicing” the list/string.\n\nbinomial_name = 'Drosophila melanogaster'\ngroup = binomial_name[0:10]\nprint('group:', group)\n\nspecies = binomial_name[11:23]\nprint('species:', species)\n\nchromosomes = ['X', 'Y', '2', '3', '4']\nautosomes = chromosomes[2:5]\nprint('autosomes:', autosomes)\n\nlast = chromosomes[-1]\nprint('last:', last)\n\ngroup: Drosophila\nspecies: melanogaster\nautosomes: ['2', '3', '4']\nlast: 4\n\n\n\n\nSlicing from the end\nUse slicing to access only the last four characters of a string or entries of a list:\n\nstring_for_slicing = 'Observation date: 02-Feb-2013'\nlist_for_slicing = [['fluorine', 'F'],\n                    ['chlorine', 'Cl'],\n                    ['bromine', 'Br'],\n                    ['iodine', 'I'],\n                    ['astatine', 'At']]\n\n\n# string slicing\nstring_for_slicing[-4:]\n\n'2013'\n\n\n\n# list slicing\nlist_for_slicing [-4:]\n\n[['chlorine', 'Cl'], ['bromine', 'Br'], ['iodine', 'I'], ['astatine', 'At']]\n\n\n\n\nNon-Continuous Slices\nSo far we’ve seen how to use slicing to take single blocks of successive entries from a sequence. But what if we want to take a subset of entries that aren’t next to each other in the sequence? We can achieve this by providing a third argument to the range within the brackets, called the step size. The example below shows how we can take every third entry in a list:\n\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n\n# starting point:end point(not included):step size\nsubset = primes[0:12:3]\nprint('subset', subset)\n\nsubset [2, 7, 17, 29]\n\n\nNotice that the slice taken begins with the first entry in the range, followed by entries taken at equally-spaced intervals (the steps) thereafter. If you wanted to begin the subset with the third entry, you would need to specify that as the starting point of the sliced range:\n\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n\n# starting point:end point(not included):step size\nsubset = primes[2:12:3]\nprint('subset', subset)\n\nsubset [5, 13, 23, 37]\n\n\nLet’s use the step size argument to create a new string that contains only every other character in the string “In an octopus’s garden in the shade”. Start with creating a variable to hold the string:\n\nbeatles = \"In an octopus's garden in the shade\"\n\nbeatles[0:35:2]\n\n'I notpssgre ntesae'\n\n\nWe can also leave out the beginning and end of the slice to take the whole string and provide only the step argument to go every second element:\n\n# leave out beginning and end - only include step argument - will include first character\nbeatles[::2]\n\n'I notpssgre ntesae'\n\n\nIf we want to take a slice from the beginning of a sequence, we can omit the first index in the range:\n\ndate = 'Monday 4 January 2016'\nday = date[0:6]\nprint('Using 0 to begin range:', day)\n\n# omit first index - by default slicing starts at beginning\nday = date[:6]\nprint('Omitting beginning index:', day)\n\nUsing 0 to begin range: Monday\nOmitting beginning index: Monday\n\n\nAnd similarly, we can omit the ending index in the range to take a slice to the very end of the sequence:\n\nmonths = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\nsond = months[8:12]\nprint('With known last position:', sond)\n\n# we can use len(months) to reference the end point of our slicing - so we don't have to manually count!\nsond = months[8:len(months)]\nprint('Using len() to get last entry:', sond)\n\n# we can omit the ending index - by default will slice to the very end\nsond = months[8:]\nprint('Omitting ending index:', sond)\n\nWith known last position: ['sep', 'oct', 'nov', 'dec']\nUsing len() to get last entry: ['sep', 'oct', 'nov', 'dec']\nOmitting ending index: ['sep', 'oct', 'nov', 'dec']\n\n\n\n\nOverloading\n\n\n\n\n\n\n+ usually means addition, but when used on strings or lists, it means “concatenate”. Given that, what do you think the multiplication operator * does on lists? In particular, what will be the output of the following code?  counts = [2, 4, 6, 8, 10]  repeats = counts * 2  print(repeats)\n\n\n\n\n\n[2, 4, 6, 8, 10, 2, 4, 6, 8, 10]\n\n\n\nThe multiplication operator * used on a list replicates elements of the list and concatenates them together. It’s equivalent to:\n\ncounts + counts\n\n[2, 4, 6, 8, 10, 2, 4, 6, 8, 10]\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\n[value1, value2, value3, …] creates a list\nlists can contain any Python object, including lists (i.e., list of lists)\nLists are indexed and sliced with square brackets (e.g., list[0] and list[2:9]), in the same way as strings and arrays\nLists are mutable (i.e., their values can be changed in place)\nStrings are immutable (i.e., the characters in them cannot be changed)"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#repeating-actions-with-loops",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#repeating-actions-with-loops",
    "title": "Programming with Python",
    "section": "5. Repeating Actions with Loops",
    "text": "5. Repeating Actions with Loops\n\n\n\nloops.png\n\n\nIn the section about visualizing data, we wrote Python code that plots values of interest from our first inflammation dataset (inflammation-01.csv), which revealed some suspicious features in it. We have a dozen data sets right now and potentially more on the way if Dr. Maverick can keep up their surprisingly fast clinical trial rate. We want to create plots for all of our data sets with a single statement. To do that, we’ll have to teach the computer how to repeat things.\nAn example task that we might want to repeat is accessing numbers in a list, which we will do by printing each number on a line of its own:\n\nodds = [1, 3, 5, 7]\n\nprint(odds[0])\nprint(odds[1])\nprint(odds[2])\nprint(odds[3])\n\n1\n3\n5\n7\n\n\nThis is a bad approach for three reasons:\n\nNot scalable. Imagine you need to print a list that has hundreds of elements. It might be easier to type them in manually.\nDifficult to maintain. If we want to decorate each printed element with an asterisk or any other character, we would have to change four lines of code. While this might not be a problem for small lists, it would definitely be a problem for longer ones.\nFragile. If we use it with a list that has more elements than what we initially envisioned, it will only display part of the list’s elements. A shorter list, on the other hand, will cause an error because it will be trying to display elements of the list that do not exist.\n\n\nodds = [1, 3, 5]\nprint(odds[0])\nprint(odds[1])\nprint(odds[2])\nprint(odds[3])\n\n1\n3\n5\n\n\nIndexError: list index out of range\n\n\nHere’s a better approach: a for loop\n\nodds = [1, 3, 5, 7]\nfor num in odds:\n    print(num)\n\n1\n3\n5\n7\n\n\nThis is shorter — certainly shorter than something that prints every number in a hundred-number list — and more robust as well:\n\nodds = [1, 3, 5, 7, 9, 11]\nfor num in odds:\n    print(num)\n\n1\n3\n5\n7\n9\n11\n\n\nThe improved version uses a for loop to repeat an operation — in this case, printing — once for each thing in a sequence. The general form of a loop is:\nfor variable in collection: # do things using variable, such as print\nUsing the odds example above, the loop might look like this:\n\n\n\nloops_image_num.png\n\n\nwhere each number (num) in the variable odds is looped through and printed one number after another. The other numbers in the diagram denote which loop cycle the number was printed in (1 being the first loop cycle, and 6 being the final loop cycle).\n\n\n\n\n\n\nTip\n\n\n\nWe can call the loop variable anything we like, but there must be a colon at the end of the line starting the loop, and we must indent anything we want to run inside the loop. Unlike many other languages, there is no command to signify the end of the loop body (e.g. end for); what is indented after the for statement belongs to the loop.\n\n\n\nWhat’s in a name?\nIn the example above, the loop variable was given the name num as a mnemonic; it is short for ‘number’. We can choose any name we want for variables. We might just as easily have chosen the name banana for the loop variable, as long as we use the same name when we invoke the variable inside the loop:\n\nodds = [1, 3, 5, 7, 9, 11]\nfor banana in odds:\n    print(banana)\n\n1\n3\n5\n7\n9\n11\n\n\nIt is a good idea to choose variable names that are meaningful, otherwise it would be more difficult to understand what the loop is doing. Here’s another loop that repeatedly updates a variable:\n\nlength = 0\n\nnames = ['Curie', 'Darwin', 'Turing']\n\nfor value in names:\n    length = length + 1\nprint('There are', length, 'names in the list.')\n\nThere are 3 names in the list.\n\n\nIt’s worth tracing the execution of this little program step by step. Since there are three names in names, the statement on line 4 will be executed three times. The first time around, length is zero (the value assigned to it on line 1) and value is Curie. The statement adds 1 to the old value of length, producing 1, and updates length to refer to that new value. The next time around, value is Darwin and length is 1, so length is updated to be 2. After one more update, length is 3; since there is nothing left in names for Python to process, the loop finishes and the print function on line 5 tells us our final answer.\nNote that a loop variable is a variable that is being used to record progress in a loop. It still exists after the loop is over, and we can re-use variables previously defined as loop variables as well:\n\nname = 'Rosalind'\n\nfor name in ['Curie', 'Darwin', 'Turing']:\n    print(name)\nprint('after the loop, name is', name)\n\nCurie\nDarwin\nTuring\nafter the loop, name is Turing\n\n\nNote also that finding the length of an object is such a common operation that Python actually has a built-in function to do it called len:\n\nprint(len([0, 1, 2, 3]))\n\n4\n\n\n\n\n\n\n\n\nTip\n\n\n\nlen is much faster than any function we could write ourselves, and much easier to read than a two-line loop; it will also give us the length of many other things that we haven’t met yet, so we should always use it when we can.\n\n\n\n\nFrom 1 to N\nPython has a built-in function called range that generates a sequence of numbers. range can accept 1, 2, or 3 parameters.\n\nif one parameter is given, range generates a sequence of that length, starting at zero and incrementing by 1. For example, range(3) produces the numbers 0, 1, 2.\nif two parameters are given, range starts at the first and ends just before the second, incrementing by one. For example, range(2, 5) produces 2, 3, 4.\nif range is given 3 parameters, it starts at the first one, ends just before the second one, and increments by the third one. For example, range(3, 10, 2) produces 3, 5, 7, 9.\n\nUsing range, write a loop that uses range to print the first 3 natural numbers:\n\nfor number in range(1, 4):\n    print(number)\n\n1\n2\n3\n\n\n\n\nUnderstanding the loops\nGiven the following loop:\n\nword = 'oxygen'\nfor char in word:\n    print(char)\n\n\n\n\n\n\n\nHow many times is the body of the loop executed?\n\n\n\n\n\n6 times\n\n\n\n\n\nComputing Powers With Loops\nExponentiation is built into Python:\n\nprint(5 ** 3)\n\n125\n\n\nHowever, we can write a loop that calculates the same result as 5 ** 3 using multiplication (and without exponentiation):\n\nresult = 1\nfor number in range(0, 3):\n    result = result * 5\nprint(result)\n\n125\n\n\n\n\nSumming a list\n\nnumbers = [124, 402, 36]\nsummed = 0\nfor num in numbers:\n    summed = summed + num\nprint(summed)\n\n562\n\n\n\n\nEnumerate\nThe built-in function enumerate takes a sequence (e.g. a list) and generates a new sequence of the same length. Each element of the new sequence is a pair composed of the index (0, 1, 2,…) and the value from the original sequence:\nfor idx, val in enumerate(a_list): # Do something using idx and val\nThe code above loops through a_list, assigning the index to idx and the value to val. Suppose we have encoded a polynomial as a list of coefficients in the following way: the first element is the constant term, the second element is the coefficient of the linear term, the third is the coefficient of the quadratic term, etc.\n\nx = 5\ncoefs = [2, 4, 3]\ny = coefs[0] * x**0 + coefs[1] * x**1 + coefs[2] * x**2\nprint(y)\n\n97\n\n\nWe can achieve the same outcome more concisely with a loop using enumerate(coefs):\n\nx = 5\ncoefs = [2, 4, 3]\n\ny=0\n\nfor idx, coef in enumerate(coefs):\n    y = y + coef * x**idx\n    \nprint(y)\n\n97\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse for variable in sequence to process the elements of a sequence one at a time\nthe body of a for loop must be indented\nuse len(thing) to determine the length of something that contains other values"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-data-from-multiple-files",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-data-from-multiple-files",
    "title": "Programming with Python",
    "section": "6. Analyzing Data from Multiple Files",
    "text": "6. Analyzing Data from Multiple Files\nAs a final step in processing our inflammation data, we need a way to get a list of all the files in our data directory whose names start with inflammation- and end with .csv. The following library will help us to achieve this:\n\nimport glob\n\n\n\n\n\n\n\nThe glob library contains a function, also called glob, that finds files and directories whose names match a pattern. We provide those patterns as strings: the character * matches zero or more characters, while ? matches any one character. We can use this to get the names of all the CSV files in the current directory.\n\n\n\n\n\n\n\nprint(glob.glob('Data/inflammation*.csv'))\n\n['Data/inflammation-04.csv', 'Data/inflammation-11.csv', 'Data/inflammation-12.csv', 'Data/inflammation-05.csv', 'Data/inflammation-09.csv', 'Data/inflammation-06.csv', 'Data/inflammation-02.csv', 'Data/inflammation-03.csv', 'Data/inflammation-01.csv', 'Data/inflammation-07.csv', 'Data/inflammation-10.csv', 'Data/inflammation-08.csv']\n\n\nAs these examples show, glob.glob’s result is a list of file and directory paths in arbitrary order. This means we can loop over it to do something with each filename in turn. In our case, the “something” we want to do is generate a set of plots for each file in our inflammation dataset.\nIf we want to start by analyzing just the first three files in alphabetical order, we can use the sorted built-in function to generate a new sorted list from the glob.glob output:\n\nimport glob\nimport numpy\nimport matplotlib.pyplot\n\n# sort filenames in alphabetical order\nfilenames = sorted(glob.glob('Data/inflammation*.csv'))\n# select only the first three files\nfilenames = filenames[0:3]\n\nfor filename in filenames:\n    print(filename)\n\n    data = numpy.loadtxt(fname=filename, delimiter=',')\n\n    fig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\n    axes1 = fig.add_subplot(1, 3, 1)\n    axes2 = fig.add_subplot(1, 3, 2)\n    axes3 = fig.add_subplot(1, 3, 3)\n\n    axes1.set_ylabel('average')\n    axes1.plot(numpy.mean(data, axis=0))\n\n    axes2.set_ylabel('max')\n    axes2.plot(numpy.max(data, axis=0))\n\n    axes3.set_ylabel('min')\n    axes3.plot(numpy.min(data, axis=0))\n\n    fig.tight_layout()\n    matplotlib.pyplot.show()\n\nData/inflammation-01.csv\n\n\n\n\n\nData/inflammation-02.csv\n\n\n\n\n\nData/inflammation-03.csv\n\n\n\n\n\nThe plots generated for the second clinical trial file look very similar to the plots for the first file: their average plots show similar “noisy” rises and falls; their maxima plots show exactly the same linear rise and fall; and their minima plots show similar staircase structures.\nThe third dataset shows much noisier average and maxima plots that are far less suspicious than the first two datasets, however the minima plot shows that the third dataset minima is consistently zero across every day of the trial. If we produce a heat map for the third data file we see the following:\n\ndata = numpy.loadtxt(fname='Data/inflammation-03.csv', delimiter=',')\n\n# plot a heat map for third data file\nimage = matplotlib.pyplot.imshow(data)\nmatplotlib.pyplot.show()\n\n\n\n\nWe can see that there are zero values sporadically distributed across all patients and days of the clinical trial, suggesting that there were potential issues with data collection throughout the trial. In addition, we can see that the last patient in the study didn’t have any inflammation flare-ups at all throughout the trial, suggesting that they may not even suffer from arthritis!\n\nPlotting differences\nLet’s plot the difference between the average inflammations reported in the first and second datasets (stored in inflammation-01.csv and inflammation-02.csv, correspondingly), i.e., the difference between the leftmost plots of the first two figures.\n\nimport glob\nimport numpy\nimport matplotlib.pyplot\n\n# sort filenames in alphabetical order\nfilenames = sorted(glob.glob('Data/inflammation*.csv'))\n\ndata0 = numpy.loadtxt(fname=filenames[0], delimiter=',')\ndata1 = numpy.loadtxt(fname=filenames[1], delimiter=',')\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\nmatplotlib.pyplot.ylabel('Difference in average')\nmatplotlib.pyplot.plot(numpy.mean(data0, axis=0) - numpy.mean(data1, axis=0))\n\nfig.tight_layout()\nmatplotlib.pyplot.show()\n\n\n\n\n\n\nGenerate composite statistics\nWe can use each of the files once to generate a dataset containing values averaged over all patients. Then use pyplot to generate average, max, and min for all patients:\n\nfilenames = glob.glob('Data/inflammation*.csv')\n\ncomposite_data = numpy.zeros((60,40))\nfor filename in filenames:\n    # sum each new file's data into composite_data as it's read\n    # and then divide the composite_data by number of samples\n    data = numpy.loadtxt(fname = filename, delimiter=',')\n    composite_data = composite_data + data\n    \ncomposite_data = composite_data / len(filenames)\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\naxes1 = fig.add_subplot(1, 3, 1)\naxes2 = fig.add_subplot(1, 3, 2)\naxes3 = fig.add_subplot(1, 3, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(composite_data, axis=0))\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(composite_data, axis=0))\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(composite_data, axis=0))\n\nfig.tight_layout()\n\nmatplotlib.pyplot.show()\n\n\n\n\nAfter spending some time investigating the heat map and statistical plots, as well as doing the above exercises to plot differences between datasets and to generate composite patient statistics, we gain some insight into the twelve clinical trial datasets. The datasets appear to fall into two categories:\n\nseemingly “ideal” datasets that agree excellently with Dr. Maverick’s claims, but display suspicious maxima and minima (such as inflammation-01.csv and inflammation-02.csv)\n“noisy” datasets that somewhat agree with Dr. Maverick’s claims, but show concerning data collection issues such as sporadic missing values and even an unsuitable candidate making it into the clinical trial.\n\nIn fact, it appears that all three of the “noisy” datasets (inflammation-03.csv, inflammation-08.csv, and inflammation-11.csv) are identical down to the last value. Armed with this information, we confront Dr. Maverick about the suspicious data and duplicated files.\nDr. Maverick confesses that they fabricated the clinical data after they found out that the initial trial suffered from a number of issues, including unreliable data-recording and poor participant selection. They created fake data to prove their drug worked, and when we asked for more data they tried to generate more fake datasets, as well as throwing in the original poor-quality dataset a few times to try and make all the trials seem a bit more “realistic”.\nCongratulations! We’ve investigated the inflammation data and proven that the datasets have been synthetically generated.\nBut it would be a shame to throw away the synthetic datasets that have taught us so much already, so we’ll forgive the imaginary Dr. Maverick and continue to use the data to learn how to program.\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse glob.glob(pattern) to create a list of files whose names match a pattern\nuse * in a pattern to match zero or more characters, and ? to match any single character"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#making-choices",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#making-choices",
    "title": "Programming with Python",
    "section": "7. Making Choices",
    "text": "7. Making Choices\nIn the last section, we discovered something suspicious was going on in our inflammation data by drawing some plots. How can we use Python to automatically recognize the different features we saw, and take a different action for each? In this lesson, we’ll learn how to write code that runs only when certain conditions are true:\n\nConditionals\nWe can ask Python to take different actions, depending on a condition, with an if statement:\n\nnum = 37\nif num > 100:\n    print('greater')\nelse:\n    print('not greater')\nprint('done')\n\nnot greater\ndone\n\n\nThe second line of this code uses the keyword if to tell Python that we want to make a choice. If the test that follows the if statement is true, the body of the if (i.e., the set of lines indented underneath it) is executed, and “greater” is printed. If the test is false, the body of the else is executed instead, and “not greater” is printed. Only one or the other is ever executed before continuing on with program execution to print “done”:\n\n\n\npython-flowchart-conditional.png\n\n\nConditional statements don’t have to include an else. If there isn’t one, Python simply does nothing if the test is false:\n\nnum = 53\nprint('before conditional...')\nif num > 100:\n    print(num, 'is greater than 100')\nprint('...after conditional')\n\nbefore conditional...\n...after conditional\n\n\nWe can also chain several tests together using elif, which is short for “else if”. The following Python code uses elif to print the sign of a number:\n\nnum = -3\n\nif num > 0:\n    print(num, 'is positive')\nelif num == 0:\n    print(num, 'is zero')\nelse:\n    print(num, 'is negative')\n\n-3 is negative\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that to test for equality we use a double equals sign == rather than a single equals sign = which is used to assign values.\n\n\n\n\nComparing in Python\nAlong with the > and == operators we have already used for comparing values in our conditionals, there are a few more options to know about:\n > greater than\n < less than\n == equal to\n != does not equal\n >= greater than or equal to\n <= less than or equal to\nWe can also combine tests using and and or.\nand is only true if both parts are true:\n\nif (1 > 0) and (-1 >= 0):\n    print('both parts are true')\nelse:\n    print('at least one part is false')\n\nat least one part is false\n\n\nwhile or is true if at least one part is true:\n\nif (1 < 0) or (1 >= 0):\n    print('at least one test is true')\n\nat least one test is true\n\n\n\n\n\n\n\n\nNote\n\n\n\nTrue and False are special words in Python called booleans, which represent truth values. A statement such as 1 < 0 returns the value False, while -1 < 0 returns the value True.\n\n\n\n\nChecking our Data\nNow that we’ve seen how conditionals work, we can use them to check for the suspicious features we saw in our inflammation data. We are about to use functions provided by the numpy module again. Therefore, if you’re working in a new Python session, make sure to load the module with:\n\nimport numpy\n\nFrom the first couple of plots, we saw that maximum daily inflammation exhibits a strange behavior and raises one unit a day. Wouldn’t it be a good idea to detect such behavior and report it as suspicious? Let’s do that! However, instead of checking every single day of the study, let’s merely check if maximum inflammation in the beginning (day 0) and in the middle (day 20) of the study are equal to the corresponding day numbers:\nmax_inflammation_0 = numpy.max(data, axis=0)[0]\nmax_inflammation_20 = numpy.max(data, axis=0)[20]\n\nif max_inflammation_0 == 0 and max_inflammation_20 == 20:\n    print('Suspicious looking maxima!')   \nWe also saw a different problem in the third dataset; the minima per day were all zero (looks like a healthy person snuck into our study). We can also check for this with an elif condition:\nelif numpy.sum(numpy.min(data, axis=0)) == 0:\n    print('Minima add up to zero!')   \nAnd if neither of these conditions are true, we can use else to give the all-clear:\nelse:\n    print('Seems OK!')\nLet’s put that all together and test that out:\n\n# Study number 1\ndata = numpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\n\nmax_inflammation_0 = numpy.max(data, axis=0)[0]\nmax_inflammation_20 = numpy.max(data, axis=0)[20]\n\nif max_inflammation_0 == 0 and max_inflammation_20 == 20:\n    print('Suspicious looking maxima!')\nelif numpy.sum(numpy.min(data, axis=0)) == 0:\n    print('Minima add up to zero!')\nelse:\n    print('Seems OK!')\n\nSuspicious looking maxima!\n\n\n\n# Study number 3\ndata = numpy.loadtxt(fname='Data/inflammation-03.csv', delimiter=',')\n\nmax_inflammation_0 = numpy.max(data, axis=0)[0]\nmax_inflammation_20 = numpy.max(data, axis=0)[20]\n\nif max_inflammation_0 == 0 and max_inflammation_20 == 20:\n    print('Suspicious looking maxima!')\nelif numpy.sum(numpy.min(data, axis=0)) == 0:\n    print('Minima add up to zero!')\nelse:\n    print('Seems OK!')\n\nMinima add up to zero!\n\n\nIn this way, we have asked Python to do something different depending on the condition of our data. Here we printed messages in all cases, but we could also imagine not using the else catch-all so that messages are only printed when something is wrong, freeing us from having to manually examine every plot for features we’ve seen before.\n\n\nHow Many Paths?\nConsider this code:\n\nif 4 > 5:\n    print('A')\nelif 4 == 5:\n    print('B')\nelif 4 < 5:\n    print('C')\n\nC\n\n\nC gets printed because the first two conditions, 4 > 5 and 4 == 5, are not true, but 4 < 5 is true.\n\n\nWhat Is Truth?\nTrue and False booleans are not the only values in Python that are true and false. In fact, any value can be used in an if or elif. Consider the code below:\n\nif '':\n    print('empty string is true')\nif 'word':\n    print('word is true')\nif []:\n    print('empty list is true')\nif [1, 2, 3]:\n    print('non-empty list is true')\nif 0:\n    print('zero is true')\nif 1:\n    print('one is true')\n\nword is true\nnon-empty list is true\none is true\n\n\n\n\nThat’s Not Not What I Meant\nSometimes it is useful to check whether some condition is not true. The Boolean operator not can do this explicitly. After reading and running the code below, write some if statements that use not to test the rule that you formulated in the previous challenge.\n\nif not '':\n    print('empty string is not true')\nif not 'word':\n    print('word is not true')\nif not not True:\n    print('not not True is true')\n\nempty string is not true\nnot not True is true\n\n\n\n\nClose enough\nWrite some conditions that print True if the variable a is within 10% of the variable b and False otherwise:\n\na = 5\nb = 5.1\n\nif abs(a - b) <= 0.1 * abs(b):\n    print('True')\nelse:\n    print('False')\n\nTrue\n\n\n\nprint(abs(a - b) <= 0.1 * abs(b))\n\nTrue\n\n\nThis works because the Booleans True and False have string representations which can be printed.\n\n\nIn-Place Operators\nPython (and most other languages in the C family) provides in-place operators that work like this:\n\n# original value\nx = 1  \n\n# add one to x, assigning result back to x\nx += 1 \n\n# multiply x by 3\nx *= 3 \n\nprint(x)\n\n6\n\n\nWrite some code that sums the positive and negative numbers in a list separately, using in-place operators. Do you think the result is more or less readable than writing the same without in-place operators?\n\npositive_sum = 0\nnegative_sum = 0\ntest_list = [3, 4, 6, 1, -1, -5, 0, 7, -8]\n\nfor num in test_list:\n    if num > 0:\n        positive_sum += num\n    elif num == 0:\n        pass\n    else:\n        negative_sum += num\n        \nprint(positive_sum, negative_sum)\n\n\n\n\n\n\n\nImportant\n\n\n\npass means “don’t do anything”.\n\n\nIn this particular case, it’s not actually needed, since if num == 0 neither sum needs to change, but it illustrates the use of elif and pass.\n\n\nSorting a list into buckets\nIn our data folder, large data sets are stored in files whose names start with “inflammation-“ and small data sets – in files whose names start with “small-“. We also have some other files that we do not care about at this point. We’d like to break all these files into three lists called large_files, small_files, and other_files, respectively.\nAdd code to the template below to do this:\nfilenames = ['inflammation-01.csv',\n     'myscript.py',\n     'inflammation-02.csv',\n     'small-01.csv',\n     'small-02.csv']\nlarge_files = []\nsmall_files = []\nother_files = []\nNote that the string method startswith returns True if and only if the string it is called on starts with the string passed as an argument, that is:\n\n'String'.startswith('Str')\n\nTrue\n\n\nBut:\n\n'String'.startswith('str')\n\nFalse\n\n\n\nfilenames = ['inflammation-01.csv',\n         'myscript.py',\n         'inflammation-02.csv',\n         'small-01.csv',\n         'small-02.csv']\n\nlarge_files = []\nsmall_files = []\nother_files = []\n\nfor filename in filenames:\n    if filename.startswith('inflammation-'):\n        large_files.append(filename)\n    elif filename.startswith('small-'):\n        small_files.append(filename)\n    else:\n        other_files.append(filename)\n\nprint('large_files:', large_files)\nprint('small_files:', small_files)\nprint('other_files:', other_files)\n\nlarge_files: ['inflammation-01.csv', 'inflammation-02.csv']\nsmall_files: ['small-01.csv', 'small-02.csv']\nother_files: ['myscript.py']\n\n\n\n\nCounting vowels\nLet’s write a loop that counts the number of vowels in a character string, and test it on a few individual words and full sentences:\n\nvowels = 'aeiouAEIOU'\nsentence1 = 'Mary had a little lamb.'\ncount = 0\nfor char in sentence1:\n    if char in vowels:\n        count += 1\n\nprint('The number of vowels in this string is ' + str(count))\n\nThe number of vowels in this string is 6\n\n\n\nvowels = 'aeiouAEIOU'\nsentence2 = 'Is this a question?'\ncount = 0\nfor char in sentence2:\n    if char in vowels:\n        count += 1\n\nprint('The number of vowels in this string is ' + str(count))\n\nThe number of vowels in this string is 7\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse if condition to start a conditional statement, elif condition to provide additional tests, and else to provide a default\nthe bodies of the branches of conditional statements must be indented\nuse == to test for equality\nX and Y is only true if both X and Y are true\nX or Y is true if either X or Y, or both, are true\nzero, the empty string, and the empty list are considered false; all other numbers, strings, and lists are considered true\nTrue and False represent truth values"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#creating-functions",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#creating-functions",
    "title": "Programming with Python",
    "section": "8. Creating Functions",
    "text": "8. Creating Functions\nAt this point, we’ve written code to draw some interesting features in our inflammation data, loop over all our data files to quickly draw these plots for each of them, and have Python make decisions based on what it sees in our data. But, our code is getting pretty long and complicated.\nWhat if we had thousands of datasets, and didn’t want to generate a figure for every single one?\nCommenting out the figure-drawing code is a nuisance. Also, what if we want to use that code again, on a different dataset or at a different point in our program? Cutting and pasting it is going to make our code get very long and very repetitive, very quickly. We’d like a way to package our code so that it is easier to reuse, and Python provides for this by letting us define things called ‘functions’ — a shorthand way of re-executing longer pieces of code. Let’s start by defining a function fahr_to_celsius that converts temperatures from Fahrenheit to Celsius:\n\ndef fahr_to_celsius(temp):\n    return ((temp - 32) * (5/9))\n\n\nThe function definition opens with the keyword def followed by the name of the function (fahr_to_celsius) and a parenthesized list of parameter names (temp). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value.\nWhen we call the function, the values we pass to it are assigned to those variables so that we can use them inside the function. Inside the function, we use a return statement to send a result back to whoever asked for it. Let’s try running our function. This command should call our function, using “32” as the input and return the function value:\n\nfahr_to_celsius(32)\n\n0.0\n\n\nIn fact, calling our own function is no different from calling any other function:\n\nprint('freezing point of water:', fahr_to_celsius(32), 'C')\nprint('boiling point of water:', fahr_to_celsius(212), 'C')\n\nfreezing point of water: 0.0 C\nboiling point of water: 100.0 C\n\n\nWe’ve successfully called the function that we defined, and we have access to the value that we returned.\n\nComposing Functions\nNow that we’ve seen how to turn Fahrenheit into Celsius, we can also write the function to turn Celsius into Kelvin:\n\ndef celsius_to_kelvin(temp_c):\n    return temp_c + 273.15\n\nprint('freezing point of water in Kelvin:', celsius_to_kelvin(0.))\n\nfreezing point of water in Kelvin: 273.15\n\n\nWhat about converting Fahrenheit to Kelvin? We could write out the formula, but we don’t need to. Instead, we can compose the two functions we have already created:\n\ndef fahr_to_kelvin(temp_f):\n    temp_c = fahr_to_celsius(temp_f)\n    temp_k = celsius_to_kelvin(temp_c)\n    return temp_k\n\nprint('boiling point of water in Kelvin:', fahr_to_kelvin(212.0))\n\nboiling point of water in Kelvin: 373.15\n\n\nThis is our first taste of how larger programs are built:\n\nwe define basic operations, then\ncombine them in ever-larger chunks to get the effect we want.\n\nReal-life functions will usually be larger than the ones shown here — typically half a dozen to a few dozen lines — but they shouldn’t ever be much longer than that, or the next person who reads it won’t be able to understand what’s going on.\n\n\nVariable Scope\nIn composing our temperature conversion functions, we created variables inside of those functions, temp, temp_c, temp_f, and temp_k. We refer to these variables as local variables because they no longer exist once the function is done executing. If we try to access their values outside of the function, we will encounter an error:\n\nprint('Again, temperature in Kelvin was:', temp_k)\n\nNameError: name 'temp_k' is not defined\n\n\nIf we want to reuse the temperature in Kelvin after we have calculated it with fahr_to_kelvin, we can store the result of the function call in a variable:\n\ntemp_kelvin = fahr_to_kelvin(212.0)\nprint('temperature in Kelvin was:', temp_kelvin)\n\ntemperature in Kelvin was: 373.15\n\n\nThe variable temp_kelvin, being defined outside any function, is said to be global. Inside a function, we can read the value of such global variables:\n\ndef print_temperatures():\n  print('temperature in Fahrenheit was:', temp_fahr)\n  print('temperature in Kelvin was:', temp_kelvin)\n\ntemp_fahr = 212.0\ntemp_kelvin = fahr_to_kelvin(temp_fahr)\n\nprint_temperatures()\n\ntemperature in Fahrenheit was: 212.0\ntemperature in Kelvin was: 373.15\n\n\n\n\nTidying up\nNow that we know how to wrap bits of code up in functions, we can make our inflammation analysis easier to read and easier to reuse. First, let’s make a visualize function that generates our plots:\n\ndef visualize(filename):\n\n    data = numpy.loadtxt(fname=filename, delimiter=',')\n\n    fig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\n    axes1 = fig.add_subplot(1, 3, 1)\n    axes2 = fig.add_subplot(1, 3, 2)\n    axes3 = fig.add_subplot(1, 3, 3)\n\n    axes1.set_ylabel('average')\n    axes1.plot(numpy.mean(data, axis=0))\n\n    axes2.set_ylabel('max')\n    axes2.plot(numpy.max(data, axis=0))\n\n    axes3.set_ylabel('min')\n    axes3.plot(numpy.min(data, axis=0))\n\n    fig.tight_layout()\n    matplotlib.pyplot.show()\n\nand another function called detect_problems that checks for those systematics we noticed:\n\ndef detect_problems(filename):\n\n    data = numpy.loadtxt(fname=filename, delimiter=',')\n\n    if numpy.max(data, axis=0)[0] == 0 and numpy.max(data, axis=0)[20] == 20:\n        print('Suspicious looking maxima!')\n    elif numpy.sum(numpy.min(data, axis=0)) == 0:\n        print('Minima add up to zero!')\n    else:\n        print('Seems OK!')\n\nWait! Didn’t we forget to specify what both of these functions should return?\nWell, we didn’t. In Python, functions are not required to include a return statement and can be used for the sole purpose of grouping together pieces of code that conceptually do one thing. In such cases, function names usually describe what they do, e.g. visualize, detect_problems.\nNotice that rather than jumbling this code together in one giant for loop, we can now read and reuse both ideas separately. We can reproduce the previous analysis with a much simpler for loop:\n\nfilenames = sorted(glob.glob('inflammation*.csv'))\n\nfor filename in filenames[:3]:\n    print(filename)\n    visualize(filename)\n    detect_problems(filename)\n\nBy giving our functions human-readable names, we can more easily read and understand what is happening in the for loop. Even better, if at some later date we want to use either of those pieces of code again, we can do so in a single line.\n\n\nTesting and Documenting\nOnce we start putting things in functions so that we can re-use them, we need to start testing that those functions are working correctly. To see how to do this, let’s write a function to offset a dataset so that it’s mean value shifts to a user-defined value:\n\ndef offset_mean(data, target_mean_value):\n    return (data - numpy.mean(data)) + target_mean_value\n\nWe could test this on our actual data, but since we don’t know what the values ought to be, it will be hard to tell if the result was correct. Instead, let’s use NumPy to create a matrix of 0’s and then offset its values to have a mean value of 3:\n\nz = numpy.zeros((2,2))\nprint(offset_mean(z, 3))\n\n[[3. 3.]\n [3. 3.]]\n\n\nThat looks right, so let’s try offset_mean on our real data:\n\ndata = numpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\nprint(offset_mean(data, 0))\n\n[[-6.14875 -6.14875 -5.14875 ... -3.14875 -6.14875 -6.14875]\n [-6.14875 -5.14875 -4.14875 ... -5.14875 -6.14875 -5.14875]\n [-6.14875 -5.14875 -5.14875 ... -4.14875 -5.14875 -5.14875]\n ...\n [-6.14875 -5.14875 -5.14875 ... -5.14875 -5.14875 -5.14875]\n [-6.14875 -6.14875 -6.14875 ... -6.14875 -4.14875 -6.14875]\n [-6.14875 -6.14875 -5.14875 ... -5.14875 -5.14875 -6.14875]]\n\n\nIt’s hard to tell from the default output whether the result is correct, but there are a few tests that we can run to reassure us:\n\nprint('original min, mean, and max are:', numpy.min(data), numpy.mean(data), numpy.max(data))\noffset_data = offset_mean(data, 0)\nprint('min, mean, and max of offset data are:',\n      numpy.min(offset_data),\n      numpy.mean(offset_data),\n      numpy.max(offset_data))\n\noriginal min, mean, and max are: 0.0 6.14875 20.0\nmin, mean, and max of offset data are: -6.14875 2.842170943040401e-16 13.85125\n\n\nThat seems almost right: the original mean was about 6.1, so the lower bound from zero is now about -6.1. The mean of the offset data isn’t quite zero — we’ll explore why not later — but it’s pretty close. We can even go further and check that the standard deviation hasn’t changed:\n\nprint('std dev before and after:', numpy.std(data), numpy.std(offset_data))\n\nstd dev before and after: 4.613833197118566 4.613833197118566\n\n\nThose values look the same, but we probably wouldn’t notice if they were different in the sixth decimal place. Let’s do this instead:\n\nprint('difference in standard deviations before and after:',\n      numpy.std(data) - numpy.std(offset_data))\n\ndifference in standard deviations before and after: 0.0\n\n\nAgain, the difference is very small. It’s still possible that our function is wrong, but it seems unlikely enough that we should probably get back to doing our analysis. We have one more task first, though: we should write some documentation for our function to remind ourselves later what it’s for and how to use it.\nThe usual way to put documentation in software is to add comments like this:\n\n# offset_mean(data, target_mean_value):\n# return a new array containing the original data with its mean offset to match the desired value.\ndef offset_mean(data, target_mean_value):\n    return (data - numpy.mean(data)) + target_mean_value\n\nThere’s a better way, though. If the first thing in a function is a string that isn’t assigned to a variable, that string is attached to the function as its documentation:\n\ndef offset_mean(data, target_mean_value):\n    \"\"\"Return a new array containing the original data\n       with its mean offset to match the desired value.\"\"\"\n    return (data - numpy.mean(data)) + target_mean_value\n\nThis is better because we can now ask Python’s built-in help system to show us the documentation for the function:\n\nhelp(offset_mean)\n\nHelp on function offset_mean in module __main__:\n\noffset_mean(data, target_mean_value)\n    Return a new array containing the original data\n    with its mean offset to match the desired value.\n\n\n\nA string like this is called a docstring. We don’t need to use triple quotes when we write one, but if we do, we can break the string across multiple lines:\n\ndef offset_mean(data, target_mean_value):\n    \"\"\"Return a new array containing the original data\n       with its mean offset to match the desired value.\n\n    Examples\n    --------\n    >>> offset_mean([1, 2, 3], 0)\n    array([-1.,  0.,  1.])\n    \"\"\"\n    return (data - numpy.mean(data)) + target_mean_value\n\nhelp(offset_mean)\n\nHelp on function offset_mean in module __main__:\n\noffset_mean(data, target_mean_value)\n    Return a new array containing the original data\n       with its mean offset to match the desired value.\n    \n    Examples\n    --------\n    >>> offset_mean([1, 2, 3], 0)\n    array([-1.,  0.,  1.])\n\n\n\nWe have passed parameters to functions in two ways:\n\ndirectly, as in type(data), and\nby name, as in numpy.loadtxt(fname=‘something.csv’, delimiter=‘,’).\n\nIn fact, we can pass the filename to loadtxt without the fname=:\n\nnumpy.loadtxt('Data/inflammation-01.csv', delimiter=',')\n\narray([[0., 0., 1., ..., 3., 0., 0.],\n       [0., 1., 2., ..., 1., 0., 1.],\n       [0., 1., 1., ..., 2., 1., 1.],\n       ...,\n       [0., 1., 1., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 0., 2., 0.],\n       [0., 0., 1., ..., 1., 1., 0.]])\n\n\nbut we still need to say delimiter=:\n\nnumpy.loadtxt('Data/inflammation-01.csv', ',')\n\nSyntaxError: invalid syntax (<unknown>, line 1)\n\n\nTo understand what’s going on, and make our own functions easier to use, let’s re-define our offset_mean function like this:\n\ndef offset_mean(data, target_mean_value=0.0):\n    \"\"\"Return a new array containing the original data\n       with its mean offset to match the desired value, (0 by default).\n\n    Examples\n    --------\n    >>> offset_mean([1, 2, 3])\n    array([-1.,  0.,  1.])\n    \"\"\"\n    return (data - numpy.mean(data)) + target_mean_value\n\nThe key change is that the second parameter is now written target_mean_value=0.0 instead of just target_mean_value. If we call the function with two arguments, it works as it did before:\n\ntest_data = numpy.zeros((2, 2))\nprint(offset_mean(test_data, 3))\n\n[[3. 3.]\n [3. 3.]]\n\n\nBut we can also now call it with just one parameter, in which case target_mean_value is automatically assigned the default value of 0.0:\n\nmore_data = 5 + numpy.zeros((2, 2))\nprint('data before mean offset:')\nprint(more_data)\nprint('offset data:')\nprint(offset_mean(more_data))\n\ndata before mean offset:\n[[5. 5.]\n [5. 5.]]\noffset data:\n[[0. 0.]\n [0. 0.]]\n\n\nThis is handy.\n\n\n\n\n\n\nIf we usually want a function to work one way, but occasionally need it to do something else, we can allow people to pass a parameter when they need to but provide a default to make the normal case easier.\n\n\n\nThe example below shows how Python matches values to parameters:\n\n\n\ndef display(a=1, b=2, c=3):\n    print('a:', a, 'b:', b, 'c:', c)\n\n# display pre-defibed initial values af a, b, and c\nprint('no parameters:')\ndisplay()\n\n# replace the initial value of a, whilst retaining initial values for b and c\nprint('one parameter:')\ndisplay(55)\n\n# replace intial values of a and b, whilst retaining initial value for c\nprint('two parameters:')\ndisplay(55, 66)\n\nno parameters:\na: 1 b: 2 c: 3\none parameter:\na: 55 b: 2 c: 3\ntwo parameters:\na: 55 b: 66 c: 3\n\n\nAs this example shows, parameters are matched up from left to right, and any that haven’t been given a value explicitly get their default value. We can override this behaviour by naming the value as we pass it in:\n\nprint('only setting the value of c')\ndisplay(c=77)\n\nonly setting the value of c\na: 1 b: 2 c: 77\n\n\nWith that in hand, let’s look at the help for numpy.loadtxt:\nhelp(numpy.loadtxt)\nThere’s a lot of information here, but the most important part is the first couple of lines:\nloadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, use\ncols=None, unpack=False, ndmin=0, encoding='bytes')\nThis tells us that loadtxt has one parameter called fname that doesn’t have a default value, and eight others that do. If we call the function like this:\nnumpy.loadtxt('inflammation-01.csv', ',')\nthen the filename is assigned to fname (which is what we want), but the delimiter string ‘,’ is assigned to dtype rather than delimiter, because dtype is the second parameter in the list. However ‘,’ isn’t a known dtype so our code produced an error message when we tried to run it.\nWhen we call loadtxt we don’t have to provide fname= for the filename because it’s the first item in the list, but if we want the ‘,’ to be assigned to the variable delimiter, we do have to provide delimiter= for the second parameter since delimiter is not the second parameter in the list.\n\n\nReadable functions\nConsider these two functions:\n\n# Function 1 - calculate sample standard deviation\ndef s(p):\n    a = 0\n    for v in p:\n        a += v\n    m = a / len(p)\n    d = 0\n    for v in p:\n        d += (v - m) * (v - m)\n    return numpy.sqrt(d / (len(p) - 1))\n\n\n# Function 2 - calculate sample standard deviation\ndef std_dev(sample):\n    sample_sum = 0\n    for value in sample:\n        sample_sum += value\n\n    sample_mean = sample_sum / len(sample)\n\n    sum_squared_devs = 0\n    for value in sample:\n        sum_squared_devs += (value - sample_mean) * (value - sample_mean)\n\n    return numpy.sqrt(sum_squared_devs / (len(sample) - 1))\n\nFunctions 1 and 2 are computationally equivalent (they both calculate the sample standard deviation), but to a human reader, they look very different. You probably found Function 2 much easier to read and understand than Function 1.\nAs this example illustrates, both documentation and a programmer’s coding style combine to determine how easy it is for others to read and understand the programmer’s code. Choosing meaningful variable names and using blank spaces to break the code into logical “chunks” are helpful techniques for producing readable code. This is useful not only for sharing code with others, but also for the original programmer.\nIf you need to revisit code that you wrote months ago and haven’t thought about since then, you will appreciate the value of readable code!\n\n\nCombining Strings\n“Adding” two strings produces their concatenation:\n'a' + 'b' is 'ab'. \nLet’s write a function called fence that takes two parameters: original and wrapper and returns a new string that has the wrapper character at the beginning and end of the original:\n\ndef fence(original, wrapper):\n    return wrapper + original + wrapper\n\n\nfence(' in ', 'all')\n\n'all in all'\n\n\n\n\nReturn versus print\nNote that return and print are not interchangeable.\n\nprint is a Python function that prints data to the screen. It enables us, users, to see the data\nreturn statement, on the other hand, makes data visible to the program\n\nLet’s have a look at the following function:\n\ndef add(a, b):\n    print(a + b)\n    \nA = add(7, 3)\nprint(A)\n\n10\nNone\n\n\nPython first executes the function add with a = 7 and b = 3, and, therefore, prints 10. However, because function add does not have a line that starts with return (no return “statement”), it will, *by default, return nothing which, in Python world, is called None. Therefore, A is therefore assigned to None and the last line (print(A)) printed is None.\n\n\nSelecting characters from strings\nIf the variable s refers to a string, then s[0] is the string’s first character and s[-1] is its last. Let’s write a function called outer that returns a string made up of just the first and last characters of its input:\n\ndef outer(input):\n    return input[0] + input[-1:]\n\n\nouter('helium')\n\n'hm'\n\n\n\n\nRescaling an Array\nLet’s now write a function rescale that takes an array as input and returns a corresponding array of values scaled (normalized) to lie in the range 0.0 to 1.0.\nHint: If L and H are the lowest and highest values in the original array, then the replacement for a value v should be (v-L) / (H-L)\n\ndef rescale(input_array):\n    L = numpy.min(input_array)\n    H = numpy.max(input_array)\n    output_array = (input_array - L) / (H - L)\n    return output_array\n\n\ninput_array = [12,15,8,23,9,6,13,7]\n\n\nrescale(input_array)\n\narray([0.35294118, 0.52941176, 0.11764706, 1.        , 0.17647059,\n       0.        , 0.41176471, 0.05882353])\n\n\nNote that the largest value in the array, 23, has been rescaled to one, the smallest value 6, has been rescaled to zero, and all other values scaled proportionately.\n\n\nTesting and documenting our function\nFirst, let’s run the commands help(numpy.arange) and help(numpy.linspace) to see how to use these functions to generate regularly-spaced values.\nThen using these values we can test our rescale function, and add a docstring that explains what it does.\n\nrescale(numpy.arange(10.0))\n\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])\n\n\n\nrescale(numpy.linspace(0,100,5))\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\n“““Takes an array as input, and returns a corresponding array scaled so that 0 corresponds to the minimum and 1 to the maximum value of the input array.\nExamples: >>> rescale(numpy.arange(10.0)) array([ 0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ]) >>> rescale(numpy.linspace(0, 100, 5)) array([ 0. , 0.25, 0.5 , 0.75, 1. ]) ““”\n\n\nDefining defaults\nLet’s now rewrite the rescale function so that it scales data to lie between 0.0 and 1.0 by default, whilst also allowing the caller to specify lower and upper bounds if they want.\n\n# lower and upper bounds specified within function argument, for amendment if required)\ndef rescale(input_array, low_val=0.0, high_val=1.0):\n    \n    \"\"\"rescales input array values to lie between low_val and high_val\"\"\"\n    L = numpy.min(input_array)\n    H = numpy.max(input_array)\n    intermed_array = (input_array - L) / (H - L)\n    output_array = intermed_array * (high_val - low_val) + low_val\n    return output_array\n\n\n\nVariables inside and outside functions\n\nf = 0\nk = 0\n\ndef f2k(f):\n    k = ((f - 32) * (5.0 / 9.0)) + 273.15\n    return k\n\nprint(f2k(8))\nprint(f2k(41))\nprint(f2k(32))\n\nprint(k)\n\n259.81666666666666\n278.15\n273.15\n0\n\n\nk is 0 because the k inside the function f2k doesn’t know about the k defined outside the function. When the f2k function is called, it creates a local variable k. The function does not return any values and does not alter k outside of its local copy. Therefore the original value of k remains unchanged. Beware that a local k is created because f2k internal statements affect a new value to it. If k was only read, it would simply retrieve the global k value.\n\n\nMixing default and non-default parameters\n\n# second and fourth arguments are given default values\n# one and three MUST be included as arguments when function called, and must be BEFORE any default values\ndef numbers(one, two=2, three, four=4):\n    n = str(one) + str(two) + str(three) + str(four)\n    return n\n\nprint(numbers(1, three=3))\n\nSyntaxError: non-default argument follows default argument (4284410807.py, line 2)\n\n\nAttempting to define the numbers function results in a SyntaxError. The defined parameters two and four are given default values. Because one and three are not given default values, they are required to be included as arguments when the function is called and must be placed before any parameters that have default values in the function definition.\n\n# default values for b and c will be used if no value passed when function called\ndef func(a, b=3, c=6):\n    print('a: ', a, 'b: ', b, 'c:', c)\n\nfunc(-1, 2)\n\na:  -1 b:  2 c: 6\n\n\nThe given call to func displays a: -1 b: 2 c: 6. -1 is assigned to the first parameter a, 2 is assigned to the next parameter b, and c is not passed a value, so it uses its default value 6.\n\n\n\n\n\n\nKey Points\n\n\n\n\ndefine a function using def function_name(parameter)\nthe body of a function must be indented\ncall a function using function_name(value)\nnumbers are stored as integers or floating-point numbers\nvariables defined within a function can only be seen and used within the body of the function\nvariables created outside of any function are called global variables\nwithin a function, we can access global variables\nvariables created within a function override global variables if their names match\nuse help(thing) to view help for something\nput docstrings in functions to provide help for that function\nspecify default values for parameters when defining a function using name=value in the parameter list\nparameters can be passed by matching based on name, by position, or by omitting them (in which case the default value is used)\nput code whose parameters change frequently in a function, then call it with different parameter values to customize its behavior"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#errors-and-exceptions",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#errors-and-exceptions",
    "title": "Programming with Python",
    "section": "9. Errors and Exceptions",
    "text": "9. Errors and Exceptions\nEvery programmer encounters errors, both those who are just beginning, and those who have been programming for years. Encountering errors and exceptions can be very frustrating at times, and can make coding feel like a hopeless endeavour. However, understanding what the different types of errors are and when you are likely to encounter them can help a lot. Once you know why you get certain types of errors, they become much easier to fix.\nErrors in Python have a very specific form, called a traceback. Let’s examine one:\n\n# This code has an intentional error. You can type it directly or\n# use it for reference to understand the error message below.\ndef favorite_ice_cream():\n    ice_creams = [\n        'chocolate',\n        'vanilla',\n        'strawberry'\n    ]\n    print(ice_creams[3])\n\nfavorite_ice_cream()\n\nIndexError: list index out of range\n\n\nThis particular traceback has two levels. You can determine the number of levels by looking for the number of arrows on the left hand side. In this case:\n\nThe first shows code from the cell above, with an arrow pointing to Line 11 (which is favorite_ice_cream()).\nThe second shows some code in the function favorite_ice_cream, with an arrow pointing to Line 9 (which is print(ice_creams[3])).\n\nThe last level is the actual place where the error occurred. The other level(s) show what function the program executed to get to the next level down. So, in this case, the program first performed a function call to the function favorite_ice_cream. Inside this function, the program encountered an error on Line 6, when it tried to run the code print(ice_creams[3]).\n\nLong Tracebacks\nSometimes, you might see a traceback that is very long – sometimes they might even be 20 levels deep! This can make it seem like something horrible happened, but the length of the error message does not reflect severity, rather, it indicates that your program called many functions before it encountered the error. Most of the time, the actual place where the error occurred is at the bottom-most level, so you can skip down the traceback to the bottom.\nSo what error did the program actually encounter? In the last line of the traceback, Python helpfully tells us the category or type of error (in this case, it is an IndexError) and a more detailed error message (in this case, it says “list index out of range”). If you encounter an error and don’t know what it means, it is still important to read the traceback closely. That way, if you fix the error, but encounter a new one, you can tell that the error changed. Additionally, sometimes knowing where the error occurred is enough to fix it, even if you don’t entirely understand the message.\nIf you do encounter an error you don’t recognize, try looking at the official documentation on errors. However, note that you may not always be able to find the error there, as it is possible to create custom errors. In that case, hopefully the custom error message is informative enough to help you figure out what went wrong.\n\n\nSyntax Errors\nWhen you forget a colon at the end of a line, accidentally add one space too many when indenting under an if statement, or forget a parenthesis, you will encounter a syntax error. This means that Python couldn’t figure out how to read your program. This is similar to forgetting punctuation in English: for example, this text is difficult to read there is no punctuation there is also no capitalization why is this hard because you have to figure out where each sentence ends you also have to figure out where each sentence begins to some extent it might be ambiguous if there should be a sentence break or not\nPeople can typically figure out what is meant by text with no punctuation, but people are much smarter than computers. If Python doesn’t know how to read the program, it will give up and inform you with an error. For example:\n\ndef some_function()\n    msg = 'hello, world!'\n    print(msg)\n     return msg\n\nSyntaxError: expected ':' (1947819108.py, line 1)\n\n\nHere, Python tells us that there is a SyntaxError on line 1, and even puts a little arrow in the place where there is an issue. In this case the problem is that the function definition is missing a colon at the end.\nActually, the function above has two issues with syntax. If we fix the problem with the colon, we see that there is also an IndentationError, which means that the lines in the function definition do not all have the same indentation:\n\ndef some_function():\n    msg = 'hello, world!'\n    print(msg)\n     return msg\n\nIndentationError: unexpected indent (329577338.py, line 4)\n\n\nBoth SyntaxError and IndentationError indicate a problem with the syntax of your program, but an IndentationError is more specific: it always means that there is a problem with how your code is indented.\n\n\nTabs and Spaces\nSome indentation errors are harder to spot than others. In particular, mixing spaces and tabs can be difficult to spot because they are both whitespace. In the example below, the first two lines in the body of the function some_function are indented with tabs, while the third line — with spaces. If you’re working in a Jupyter notebook, be sure to copy and paste this example rather than trying to type it in manually because Jupyter automatically replaces tabs with spaces.\n\ndef some_function():\n    msg = 'hello, world!'\n    print(msg)\n        return msg\n\nTabError: inconsistent use of tabs and spaces in indentation (834453434.py, line 4)\n\n\nVisually it is impossible to spot the error. Fortunately, Python does not allow you to mix tabs and spaces.\n\n\nVariable Name Errors\nAnother very common type of error is called a NameError, and occurs when you try to use a variable that does not exist. For example:\n\nprint(i)\n\nNameError: name 'i' is not defined\n\n\nVariable name errors come with some of the most informative error messages, which are usually of the form “name ‘the_variable_name’ is not defined”.\nWhy does this error message occur? That’s a harder question to answer, because it depends on what your code is supposed to do. However, there are a few very common reasons why you might have an undefined variable. The first is that you meant to use a string, but forgot to put quotes around it:\n\nprint(hello)\n\nNameError: name 'hello' is not defined\n\n\nThe second reason is that you might be trying to use a variable that does not yet exist. In the following example, counter should have been defined (e.g., with counter = 0) before the for loop:\n\nfor number in range(10):\n    counter = counter + number\nprint('The count is:', counter)\n\nNameError: name 'counter' is not defined\n\n\nFinally, the third possibility is that you made a typo when you were writing your code. Let’s say we fixed the error above by adding the line Count = 0 before the for loop. Frustratingly, this actually does not fix the error. Remember that variables are **case-sensitive, so the variable counter is different from Counter. We still get the same error, because we still have not defined counter:\n\nCounter = 0\nfor number in range(10):\n    counter = counter + number\nprint('The count is:', counter)\n\nNameError: name 'counter' is not defined\n\n\n\n\nIndex Errors\nNext up are errors having to do with containers (like lists and strings) and the items within them. If you try to access an item in a list or a string that does not exist, then you will get an error. This makes sense: if you asked someone what day they would like to get coffee, and they answered “caturday”, you might be a bit annoyed. Python gets similarly annoyed if you try to ask it for an item that doesn’t exist:\n\nletters = ['a', 'b', 'c']\nprint('Letter #1 is', letters[0])\nprint('Letter #2 is', letters[1])\nprint('Letter #3 is', letters[2])\nprint('Letter #4 is', letters[3])\n\nLetter #1 is a\nLetter #2 is b\nLetter #3 is c\n\n\nIndexError: list index out of range\n\n\nHere, Python is telling us that there is an IndexError in our code, meaning we tried to access a list index that did not exist.\n\n\nFile Errors\nThe last type of error we’ll cover today are those associated with reading and writing files: FileNotFoundError. If you try to read a file that does not exist, you will receive a FileNotFoundError telling you so. If you attempt to write to a file that was opened read-only, Python 3 returns an UnsupportedOperationError. More generally, problems with input and output manifest as IOErrors or OSErrors, depending on the version of Python you use.\n\nfile_handle = open('myfile.txt', 'r')\n\nFileNotFoundError: [Errno 2] No such file or directory: 'myfile.txt'\n\n\nOne reason for receiving this error is that you specified an incorrect path to the file. For example, if I am currently in a folder called myproject, and I have a file in myproject/writing/myfile.txt, but I try to open myfile.txt, this will fail. The correct path would be writing/myfile.txt. It is also possible that the file name or its path contains a typo.\nA related issue can occur if you use the “read” flag instead of the “write” flag. Python will not give you an error if you try to open a file for writing when the file does not exist. However, if you meant to open a file for reading, but accidentally opened it for writing, and then try to read from it, you will get an UnsupportedOperation error telling you that the file was not opened for reading:\n\nfile_handle = open('myfile.txt', 'w')\nfile_handle.read()\n\nUnsupportedOperation: not readable\n\n\nThese are the most common errors with files, though many others exist. If you get an error that you’ve never seen before, searching the Internet for that error type often reveals common reasons why you might get that error.\n\n# This code has an intentional error. Do not type it directly;\n# use it for reference to understand the error message below.\ndef print_message(day):\n    messages = {\n        'monday': 'Hello, world!',\n        'tuesday': 'Today is Tuesday!',\n        'wednesday': 'It is the middle of the week.',\n        'thursday': 'Today is Donnerstag in German!',\n        'friday': 'Last day of the week!',\n        'saturday': 'Hooray for the weekend!',\n        'sunday': 'Aw, the weekend is almost over.'\n    }\n    print(messages[day])\n\ndef print_friday_message():\n    print_message('Friday')\n\nprint_friday_message()\n\nKeyError: 'Friday'\n\n\n\nHow many levels does the traceback have?\nthree levels as indicated by the —>\nWhat is the function name where the error occurred?\nprint_message\nOn which line number in this function did the error occur?\n13\nWhat is the type of error?\nKeyError\nWhat is the error message?\nThere isn’t really a message; you’re supposed to infer that Friday is not a key in messages\n\n\n\nIdentifying Syntax Errors\n\ndef another_function\n  print('Syntax errors are annoying.')\n   print('But at least Python tells us about them!')\n  print('So they are usually not too hard to fix.')\n\nSyntaxError: invalid syntax (3532211595.py, line 1)\n\n\n\ndef another_function():\n  print('Syntax errors are annoying.')\n   print('But at least Python tells us about them!')\n  print('So they are usually not too hard to fix.')\n\nIndentationError: unexpected indent (381338138.py, line 3)\n\n\n\ndef another_function():\n    print('Syntax errors are annoying.')\n    print('But at least Python tells us about them!')\n    print('So they are usually not too hard to fix.')\n\nFixed!\n\n\nIdentifying Variable Name Errors\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (Number % 3) == 0:\n        message = message + a\n    else:\n        message = message + 'b'\nprint(message)\n\nNameError: name 'Number' is not defined\n\n\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + a\n    else:\n        message = message + 'b'\nprint(message)\n\nNameError: name 'message' is not defined\n\n\n\nmessage = ''\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + a\n    else:\n        message = message + 'b'\nprint(message)\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nmessage = ''\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + 'a'\n    else:\n        message = message + 'b'\nprint(message)\n\nabbabbabba\n\n\nFixed!\n\n\nIdentifying Index Errors\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[4])\n\nIndexError: list index out of range\n\n\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[3])\n\nMy favorite season is  Winter\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\ntracebacks can look intimidating, but they give us a lot of useful information about what went wrong in our program, including where the error occurred and what type of error it was\nan error having to do with the ‘grammar’ or syntax of the program is called a SyntaxError. If the issue has to do with how the code is indented, then it will be called an IndentationError\na NameError will occur when trying to use a variable that does not exist. Possible causes are that a variable definition is missing, a variable reference differs from its definition in spelling or capitalization, or the code contains a string that is missing quotes around it\ncontainers like lists and strings will generate errors if you try to access items in them that do not exist. This type of error is called an IndexError\ntrying to read a file that does not exist will give you an FileNotFoundError. Trying to read a file that is open for writing, or writing to a file that is open for reading, will give you an IOError"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#defensive-programming",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#defensive-programming",
    "title": "Programming with Python",
    "section": "10. Defensive Programming",
    "text": "10. Defensive Programming\nOur previous sections have introduced the basic tools of programming: variables and lists, file I/O, loops, conditionals, and functions. What they haven’t done is show us how to tell whether a program is getting the right answer, and how to tell if it’s still getting the right answer as we make changes to it.\nTo achieve that, we need to:\n\nwrite programs that check their own operation\nwrite and run tests for widely-used functions\nmake sure we know what “correct” actually means\n\nThe good news is, doing these things will speed up our programming, not slow it down. As in real carpentry — the kind done with lumber — the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes.\n\nAssertions\nThe first step toward getting the right answers from our programs is to assume that mistakes will happen and to guard against them. This is called defensive programming, and the most common way to do it is to add assertions to our code so that it checks itself as it runs. An assertion is simply a statement that something must be true at a certain point in a program. When Python sees one, it evaluates the assertion’s condition. If it’s true, Python does nothing, but if it’s false, Python halts the program immediately and prints the error message if one is provided. For example, this piece of code halts as soon as the loop encounters a value that isn’t positive:\n\nnumbers = [1.5, 2.3, 0.7, -0.001, 4.4]\ntotal = 0.0\nfor num in numbers:\n    # assertion that numbers should be greater than zero - and error output where this condition not met\n    assert num > 0.0, 'Data should only contain positive values'\n    total += num\nprint('total is:', total)\n\nAssertionError: Data should only contain positive values\n\n\nPrograms like the Firefox browser are full of assertions: 10-20% of the code they contain are there to check that the other 80–90% are working correctly. Broadly speaking, assertions fall into three categories:\n\na precondition is something that must be true at the start of a function in order for it to work correctly.\na postcondition is something that the function guarantees is true when it finishes.\nan invariant is something that is always true at a particular point inside a piece of code.\n\nFor example, suppose we are representing rectangles using a tuple of four coordinates (x0, y0, x1, y1), representing the lower left and upper right corners of the rectangle. In order to do some calculations, we need to normalize the rectangle so that the lower left corner is at the origin and the longest side is 1.0 units long. This function does that, but checks that its input is correctly formatted and that its result makes sense:\n\ndef normalize_rectangle(rect):\n    \"\"\"Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis.\n    Input should be of the format (x0, y0, x1, y1).\n    (x0, y0) and (x1, y1) define the lower left and upper right corners\n    of the rectangle, respectively.\"\"\"\n    \n    # precondition to catch invalid input\n    assert len(rect) == 4, 'Rectangles must contain 4 coordinates'\n    x0, y0, x1, y1 = rect\n    \n    # preconditions to catch invalid inputs\n    assert x0 < x1, 'Invalid X coordinates'\n     # precondition to catch invalid input\n    assert y0 < y1, 'Invalid Y coordinates'\n\n    dx = x1 - x0\n    dy = y1 - y0\n    if dx > dy:\n        scaled = float(dx) / dy\n        upper_x, upper_y = 1.0, scaled\n    else:\n        scaled = float(dx) / dy\n        upper_x, upper_y = scaled, 1.0\n\n    # postconditions to help catch incorrect calculation\n    assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid'\n    \n    # postconditions to help catch incorrect calculation\n    assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid'\n\n    return (0, 0, upper_x, upper_y)\n\n\n# missing the fourth coordinate\nprint(normalize_rectangle( (0.0, 1.0, 2.0) )) \n\nAssertionError: Rectangles must contain 4 coordinates\n\n\n\n# correctly includes four coordinates\nprint(normalize_rectangle( (0.0, 0.0, 1.0, 5.0) ))\n\n(0, 0, 0.2, 1.0)\n\n\n\n# rectangle that is wider than it is tall\nprint(normalize_rectangle( (0.0, 0.0, 5.0, 1.0) ))\n\nAssertionError: Calculated upper Y coordinate invalid\n\n\nRe-reading our function, we realize that line 14 should divide dy by dx rather than dx by dy. In a Jupyter notebook, we can display line numbers by typing Ctrl+M followed by L. If we had left out the assertion at the end of the function, we would have created and returned something that had the right shape as a valid answer, but wasn’t. Detecting and debugging that would almost certainly have taken more time in the long run than writing the assertion.\nBut assertions aren’t just about catching errors: they also help people understand programs. Each assertion gives the person reading the program a chance to check (consciously or otherwise) that their understanding matches what the code is doing.\nMost good programmers follow two rules when adding assertions to their code:\n\nThe first is, fail early, fail often. The greater the distance between when and where an error occurs and when it’s noticed, the harder the error will be to debug, so good code catches mistakes as early as possible.\nThe second rule is, turn bugs into assertions or tests. Whenever you fix a bug, write an assertion that catches the mistake should you make it again. If you made a mistake in a piece of code, the odds are good that you have made other mistakes nearby, or will make the same mistake (or a related one) the next time you change it. Writing assertions to check that you haven’t regressed (i.e., haven’t re-introduced an old problem) can save a lot of time in the long run, and helps to warn people who are reading the code (including your future self) that this bit is tricky.\n\n\n\nTest-Driven Development\nAn assertion checks that something is true at a particular point in the program. The next step is to check the overall behavior of a piece of code, i.e., to make sure that it produces the right output when it’s given a particular input. For example, suppose we need to find where two or more time series overlap. The range of each time series is represented as a pair of numbers, which are the time the interval started and ended. The output is the largest range that they all include:\n\nMost novice programmers would solve this problem like this:\n\nwrite a function range_overlap\ncall it interactively on two or three different inputs\nif it produces the wrong answer, fix the function and re-run that test\n\nThis clearly works — after all, thousands of scientists are doing it right now — but there’s a better way:\n\nwrite a short function for each test\nwrite a range_overlap function that should pass those tests\nIf range_overlap produces any wrong answers, fix it and re-run the test functions\n\nWriting the tests before writing the function they exercise is called test-driven development (TDD). Its advocates believe it produces better code faster because:\n\nif people write tests after writing the thing to be tested, they are subject to confirmation bias, i.e., they subconsciously write tests to show that their code is correct, rather than to find errors.\nwriting tests helps programmers figure out what the function is actually supposed to do.\n\nHere are three test functions for range_overlap:\n\nassert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0)\nassert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0)\nassert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0)\n\nNameError: name 'range_overlap' is not defined\n\n\nThe error is actually reassuring: we haven’t written range_overlap yet, so if the tests passed, it would be a sign that someone else had and that we were accidentally using their function. And as a bonus of writing these tests, we’ve implicitly defined what our input and output look like: we expect a list of pairs as input, and produce a single pair as output.\nSomething important is missing, though. We don’t have any tests for the case where the ranges don’t overlap at all:\nassert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == ???\nWhat should range_overlap do in this case: fail with an error message, produce a special value like (0.0, 0.0) to signal that there’s no overlap, or something else? Any actual implementation of the function will do one of these things; writing the tests first helps us figure out which is best before we’re emotionally invested in whatever we happened to write before we realized there was an issue.\nAnd what about this case?\nassert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == ???\nDo two segments that touch at their endpoints overlap or not? Mathematicians usually say “yes”, but engineers usually say “no”. The best answer is “whatever is most useful in the rest of our program”, but again, any actual implementation of range_overlap is going to do something, and whatever it is ought to be consistent with what it does when there’s no overlap at all.\nSince we’re planning to use the range this function returns as the X axis in a time series chart, we decide that:\n\nevery overlap has to have non-zero width, and\nwe will return the special value None when there’s no overlap\n\nNone is built into Python, and means “nothing here”. (Other languages often call the equivalent value null or nil). With that decision made, we can finish writing our last two tests:\n\nassert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None\nassert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None\n\nNameError: name 'range_overlap' is not defined\n\n\nAgain, we get an error because we haven’t written our function, but we’re now ready to do so:\n\ndef range_overlap(ranges):\n    \"\"\"Return common overlap among a set of [left, right] ranges.\"\"\"\n    max_left = 0.0\n    min_right = 1.0\n    for (left, right) in ranges:\n        max_left = max(max_left, left)\n        min_right = min(min_right, right)\n    return (max_left, min_right)\n\nTake a moment to think about why we calculate the left endpoint of the overlap as the maximum of the input left endpoints, and the overlap right endpoint as the minimum of the input right endpoints. We’d now like to re-run our tests, but they’re scattered across three different cells. To make running them easier, let’s put them all in a function:\n\ndef test_range_overlap():\n    assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None\n    assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None\n    assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0)\n    assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0)\n    assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0)\n    assert range_overlap([]) == None\n\nWe can now test range_overlap with a single function call:\n\ntest_range_overlap()\n\nAssertionError: \n\n\nThe first test that was supposed to produce None fails, so we know something is wrong with our function. We don’t know whether the other tests passed or failed because Python halted the program as soon as it spotted the first error. Still, some information is better than none, and if we trace the behavior of the function with that input, we realize that we’re initializing max_left and min_right to 0.0 and 1.0 respectively, regardless of the input values. This violates another important rule of programming: always initialize from data.\n\n\nPre- and Post-Conditions\nSuppose we are writing a function called average that calculates the average of the numbers in a list. Possible pre and post-conditions are included below:\n\n# a possible pre-condition:\nassert len(input_list) > 0, 'List length must be non-zero'\n# a possible post-condition:\n\nassert numpy.min(input_list) <= average <= numpy.max(input_list),\n'Average should be between min and max of input values (inclusive)'\n\n\n\nTesting Assertions\nGiven a sequence of a number of cars, the function get_total_cars returns the total number of cars:\n\ndef get_total_cars(values):\n    \n    # assertion checks that the input sequence values is not empty. An empty sequence such as [] will make it fail\n    assert len(values) > 0\n    for element in values:\n        # assertion checks that each value in the list can be turned into an integer. Input such as [1, 2,'c', 3] will make it fail\n        assert int(element)\n    values = [int(element) for element in values]\n    total = sum(values)\n    # assertion checks that the total of the list is greater than 0. Input such as [-10, 2, 3] will make it fail\n    assert total > 0\n    return total\n\n\nget_total_cars([1, 2, 3, 4])\n\n10\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nprogram defensively, i.e., assume that errors are going to arise, and write code to detect them when they do\nput assertions in programs to check their state as they run, and to help readers understand how those programs are supposed to work\nuse preconditions to check that the inputs to a function are safe to use\nuse postconditions to check that the output from a function is safe to use\nwrite tests before writing code in order to help determine exactly what that code is supposed to do"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#debugging",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#debugging",
    "title": "Programming with Python",
    "section": "11. Debugging",
    "text": "11. Debugging\nOnce testing has uncovered problems, the next step is to fix them. Many novices do this by making more-or-less random changes to their code until it seems to produce the right answer, but that’s very inefficient (and the result is usually only correct for the one case they’re testing). The more experienced a programmer is, the more systematically they debug, and most follow some variation on the rules explained below.\n\nKnow What It’s Supposed to Do\nThe first step in debugging something is to know what it’s supposed to do. “My program doesn’t work” isn’t good enough: in order to diagnose and fix problems, we need to be able to tell correct output from incorrect. If we can write a test case for the failing case — i.e., if we can assert that with these inputs, the function should produce that result — then we’re ready to start debugging. If we can’t, then we need to figure out how we’re going to know when we’ve fixed things.\nBut writing test cases for scientific software is frequently harder than writing test cases for commercial applications, because if we knew what the output of the scientific code was supposed to be, we wouldn’t be running the software: we’d be writing up our results and moving on to the next program. In practice, scientists tend to do the following:\n\nTest with simplified data. Before doing statistics on a real data set, we should try calculating statistics for a single record, for two identical records, for two records whose values are one step apart, or for some other case where we can calculate the right answer by hand.\nTest a simplified case. If our program is supposed to simulate magnetic eddies in rapidly-rotating blobs of supercooled helium, our first test should be a blob of helium that isn’t rotating, and isn’t being subjected to any external electromagnetic fields. Similarly, if we’re looking at the effects of climate change on speciation, our first test should hold temperature, precipitation, and other factors constant.\nCompare to an oracle. A test oracle is something whose results are trusted, such as experimental data, an older program, or a human expert. We use test oracles to determine if our new program produces the correct results. If we have a test oracle, we should store its output for particular cases so that we can compare it with our new results as often as we like without re-running that program.\nCheck conservation laws. Mass, energy, and other quantities are conserved in physical systems, so they should be in programs as well. Similarly, if we are analyzing patient data, the number of records should either stay the same or decrease as we move from one analysis to the next (since we might throw away outliers or records with missing values). If “new” patients start appearing out of nowhere as we move through our pipeline, it’s probably a sign that something is wrong.\nVisualize. Data analysts frequently use simple visualizations to check both the science they’re doing and the correctness of their code (just as we did in the opening sections). This should only be used for debugging as a last resort, though, since it’s very hard to compare two visualizations automatically.\n\n\n\nMake It Fail Every Time\nWe can only debug something when it fails, so the second step is always to find a test case that makes it fail every time. The “every time” part is important because few things are more frustrating than debugging an intermittent problem: if we have to call a function a dozen times to get a single failure, the odds are good that we’ll scroll past the failure when it actually occurs.\nAs part of this, it’s always important to check that our code is “plugged in”, i.e., that we’re actually exercising the problem that we think we are. Every programmer has spent hours chasing a bug, only to realize that they were actually calling their code on the wrong data set or with the wrong configuration parameters, or are using the wrong version of the software entirely. Mistakes like these are particularly likely to happen when we’re tired, frustrated, and up against a deadline, which is one of the reasons late-night (or overnight) coding sessions are almost never worthwhile.\n\n\nMake It Fail Fast\nIf it takes 20 minutes for the bug to surface, we can only do three experiments an hour. This means that we’ll get less data in more time and that we’re more likely to be distracted by other things as we wait for our program to fail, which means the time we are spending on the problem is less focused. It’s therefore critical to make it fail fast.\nAs well as making the program fail fast in time, we want to make it fail fast in space, i.e., we want to localize the failure to the smallest possible region of code:\n\nThe smaller the gap between cause and effect, the easier the connection is to find. Many programmers therefore use a divide and conquer strategy to find bugs, i.e., if the output of a function is wrong, they check whether things are OK in the middle, then concentrate on either the first or second half, and so on.\nN things can interact in N! different ways, so every line of code that isn’t run as part of a test means more than one thing we don’t need to worry about.\n\n\n\nChange One Thing at a Time, For a Reason\nReplacing random chunks of code is unlikely to do much good. (After all, if you got it wrong the first time, you’ll probably get it wrong the second and third as well.) Good programmers therefore change one thing at a time, for a reason. They are either trying to gather more information (“is the bug still there if we change the order of the loops?”) or test a fix (“can we make the bug go away by sorting our data before processing it?”).\nEvery time we make a change, however small, we should re-run our tests immediately, because the more things we change at once, the harder it is to know what’s responsible for what (those N! interactions again). And we should re-run all of our tests: more than half of fixes made to code introduce (or re-introduce) bugs, so re-running all of our tests tells us whether we have regressed.\n\n\nKeep Track of What You’ve Done\nGood scientists keep track of what they’ve done so that they can reproduce their work, and so that they don’t waste time repeating the same experiments or running ones whose results won’t be interesting. Similarly, debugging works best when we keep track of what we’ve done and how well it worked. If we find ourselves asking, “Did left followed by right with an odd number of lines cause the crash? Or was it right followed by left? Or was I using an even number of lines?” then it’s time to step away from the computer, take a deep breath, and start working more systematically.\nRecords are particularly useful when the time comes to ask for help. People are more likely to listen to us when we can explain clearly what we did, and we’re better able to give them the information they need to be useful.\n\n\nVersion Control Revisited\nVersion control is often used to reset software to a known state during debugging, and to explore recent changes to code that might be responsible for bugs. In particular, most version control systems (e.g. git, Mercurial) have:\n\na blame command that shows who last changed each line of a file\na bisect command that helps with finding the commit that introduced an issue\n\n\n\nBe Humble\n\nAnd speaking of help: if we can’t find a bug in 10 minutes, we should be humble and ask for help. Explaining the problem to someone else is often useful, since hearing what we’re thinking helps us spot inconsistencies and hidden assumptions. If you don’t have someone nearby to share your problem description with, get a rubber duck!\nAsking for help also helps alleviate confirmation bias. If we have just spent an hour writing a complicated program, we want it to work, so we’re likely to keep telling ourselves why it should, rather than searching for the reason it doesn’t. People who aren’t emotionally invested in the code can be more objective, which is why they’re often able to spot the simple mistakes we have overlooked.\nPart of being humble is learning from our mistakes. Programmers tend to get the same things wrong over and over: either they don’t understand the language and libraries they’re working with, or their model of how things work is wrong. In either case, taking note of why the error occurred and checking for it next time quickly turns into not making the mistake at all.\nAnd that is what makes us most productive in the long run. As the saying goes, A week of hard work can sometimes save you an hour of thought. If we train ourselves to avoid making some kinds of mistakes, to break our code into modular, testable chunks, and to turn every assumption (or mistake) into an assertion, it will actually take us less time to produce working programs, not more.\n\n\nNot Supposed to be the Same\nImagine you are assisting a researcher with Python code that computes the Body Mass Index (BMI) of patients. The researcher is concerned because all patients seemingly have unusual and identical BMIs, despite having different physiques. BMI is calculated as weight in kilograms divided by the square of height in metres.\n\npatients = [[70, 1.8], [80, 1.9], [150, 1.7]]\n\ndef calculate_bmi(weight, height):\n    return weight / (height ** 2)\n\nfor patient in patients:\n    weight, height = patients[0]\n    bmi = calculate_bmi(height, weight)\n    print(\"Patient's BMI is:\", bmi)\n\nPatient's BMI is: 0.00036734693877551024\nPatient's BMI is: 0.00036734693877551024\nPatient's BMI is: 0.00036734693877551024\n\n\n\n\n\n\n\n\nUse the debugging principles in this section and locate problems with the above code. What suggestions would you give the researcher for ensuring any later changes they make work correctly?\n\n\n\n\n\n\nThe loop is not being utilised correctly. height and weight are always set as the first patient’s data during each iteration of the loop.\nThe height/weight variables are reversed in the function call to calculate_bmi(…), the correct BMIs are 21.604938, 22.160665 and 51.903114.\n\n\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nknow what code is supposed to do before trying to debug it.\nmake it fail every time.\nmake it fail fast.\nchange one thing at a time, and for a reason.\nkeep track of what you’ve done.\nbe humble."
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html",
    "title": "Image Processing with Python",
    "section": "",
    "text": "As computer systems have become faster and more powerful, and cameras and other imaging systems have become commonplace in many other areas of life, the need has grown for researchers to be able to process and analyse image data. Considering the large volumes of data that can be involved - high-resolution images that take up a lot of disk space/virtual memory, and/or collections of many images that must be processed together - and the time-consuming and error-prone nature of manual processing, it can be advantageous or even necessary for this processing and analysis to be automated as a computer program.\nThis blog introduces an open source toolkit for processing image data: the Python programming language and the scikit-image (skimage) library. With careful experimental design, Python code can be a powerful instrument in answering many different kinds of questions.\n\n\nAutomated processing can be used to analyse many different properties of an image, including the distribution and change in colours in the image, the number, size, position, orientation, and shape of objects in the image, and even - when combined with machine learning techniques for object recognition - the type of objects in the image.\nSome examples of image processing methods applied in research include:\n\nimaging a Black Hole\nestimating the population of Emperor Penguins\nthe global-scale analysis of marine plankton diversity\nsegmentation of liver and vessels from CT images\n\nThis blog aims to provide a thorough grounding in the fundamental concepts and skills of working with image data in Python. Most of the examples used focus on one particular class of image processing technique, morphometrics, but what we will learn can be used to solve a much wider range of problems.\n\n\n\nMorphometrics involves counting the number of objects in an image, analyzing the size of the objects, or analyzing the shape of the objects. For example, we might be interested in automatically counting the number of bacterial colonies growing in a Petri dish.\n\nWe could use image processing to find the colonies, count them, and then highlight their locations on the original image.\n\nNote that we can easily manually count the number of bacteria colonies shown in the morphometric example above. So, why should we learn how to write a Python program to do a task we could easily perform with our own eyes? There are at least two reasons to learn how to perform tasks like these with Python and skimage:\n\nWhat if there are many more bacteria colonies in the Petri dish? For example, suppose the image looked like this: Manually counting the colonies in that image would present more of a challenge. A Python program using skimage could count the number of colonies more accurately, and much more quickly, than a human could.\nWhat if we have hundreds, or thousands, of images to consider? Imagine having to manually count colonies on several thousand images like those above. A Python program using skimage could move through all of the images in seconds; how long would a graduate student require to do the task? Which process would be more accurate and repeatable?\n\nAs we can see, the simple image processing / computer vision techniques we will learn during this blog can be very valuable tools for scientific research. As we progress, we will learn image analysis methods useful for many different scientific problems. Let’s get started, by learning some basics about how images are represented and stored digitally.\n\n\n\n\n\n\nKey Points:\n\n\n\n\n\nSimple Python and skimage (scikit-image) techniques can be used to solve genuine image analysis problems\n\nMorphometric problems involve the number, shape, and / or size of the objects in an image"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#image-basics",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#image-basics",
    "title": "Image Processing with Python",
    "section": "1. Image Basics",
    "text": "1. Image Basics\nThe images we see on hard copy, view with our electronic devices, or process with our programs are represented and stored in the computer as numeric abstractions, approximations of what we see with our eyes in the real world. Before we begin to learn how to process images with Python programs, we need to spend some time understanding how these abstractions work.\n\nPixels\nIt is important to realise that images are stored as rectangular arrays of hundreds, thousands, or millions of discrete “picture elements,” otherwise known as pixels. Each pixel can be thought of as a single square point of coloured light. For example, consider this image of a maize seedling, with a square area designated by a red box:\n\nNow, if we zoomed in close enough to see the pixels in the red box, we would see something like this:\n\n\n\nmaize-seedling-enlarged.jpg\n\n\nNote that each square in the enlarged image area - each pixel - is all one colour, but that each pixel can have a different colour from its neighbours. Viewed from a distance, these pixels seem to blend together to form the image we see.\n\n\nWorking with Pixels\nAs noted, in practice, real world images will typically be made up of a vast number of pixels, and each of these pixels will be one of potentially millions of colours. While we will deal with pictures of such complexity shortly, let’s start our exploration with 15 pixels in a 5 X 3 matrix with 2 colours and work our way up to that complexity.\n\n\n\n\n\n\nMatrices, arrays, images and pixels\n\n\n\nThe matrix is mathematical concept - numbers evenly arranged in a rectangle. This can be a two dimensional rectangle, like the shape of the screen you’re looking at now. Or it could be a three dimensional equivalent, a cuboid, or have even more dimensions, but always keeping the evenly spaced arrangement of numbers. In computing, array refers to a structure in the computer’s memory where data is stored in evenly-spaced elements. This is strongly analogous to a matrix. A numpy array is a type of variable (a simpler example of a type is an integer). For our purposes, the distinction between matrices and arrays is not important, we don’t really care how the computer arranges our data in its memory. The important thing is that the computer stores values describing the pixels in images, as arrays. And the terms matrix and array can be used interchangeably.\n\n\nFirst, the necessary imports:\n\n# Python libraries for learning and performing image processing\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipympl\nimport imageio.v3 as iio\nimport skimage\n\n\n\n\n\n\n\nImport Statements in Python\n\n\n\nIn Python, the import statement is used to load additional functionality into a program. This is necessary when we want our code to do something more specialised, which cannot easily be achieved with the limited set of basic tools and data structures available in the default Python environment. Additional functionality can be loaded as a single function or object, a module defining several of these, or a library containing many modules. We will encounter several different forms of import statement.\n\n\n\n# form 1, load whole skimage library\nimport skimage  \n\n# form 2, load skimage.draw module only\nimport skimage.draw  \n\n# form 3, load only the disk function\nfrom skimage.draw import disk\n\n# form 4, load all of numpy into an object called np\nimport numpy as np           \n\nNow that we have our libraries loaded, we will run a Jupyter Magic Command that will ensure our images display in our Jupyter document with pixel information that will help us more efficiently run commands later in the session.\n\n%matplotlib inline\n\nWith that taken care of, let’s load our image data from disk using the imread function from the imageio.v3 module and display it using the imshow function from the matplotlib.pyplot module. imageio is a Python library for reading and writing image data. imageio.v3 is specifying that we want to use version 3 of imageio. This version has the benefit of supporting nD (multidimensional) image data natively (think of volumes, movies).\n\n\n\n\n\n\nWhy not use skimage.io.imread()\n\n\n\nThe skimage library has its own function to read an image, so you might be asking why we don’t use it here. Actually, skimage.io.imread() uses iio.imread() internally when loading an image into Python. It is certainly something you may use as you see fit in your own code. In this lesson, we use the imageio library to read or write (save) images, while skimage is dedicated to performing operations on the images. Using imageio gives us more flexibility, especially when it comes to handling metadata.\n\n\n\nfrom skimage import data, io\nfrom matplotlib import pyplot as plt\n\nimage = iio.imread(uri=\"Images/eight.tif\")\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f29490f2fe0>\n\n\n\n\n\nYou might be thinking, “That does look vaguely like an eight, and I see two colours but how can that be only 15 pixels”. The display of the eight you see does use a lot more screen pixels to display our eight so large, but that does not mean there is information for all those screen pixels in the file. All those extra pixels are a consequence of our viewer creating additional pixels through interpolation. It could have just displayed it as a tiny image using only 15 screen pixels if the viewer was designed differently.\nWhile many image file formats contain descriptive metadata that can be essential, the bulk of a picture file is just arrays of numeric information that, when interpreted according to a certain rule set, become recognizable as an image to us. Our image of an eight is no exception, and imageio.v3 stored that image data in an array of arrays making a 5 x 3 matrix of 15 pixels. We can demonstrate that by calling on the shape property of our image variable and see the matrix by printing our image variable to the screen.\n\nprint(image.shape)\nprint(image)\n\n(5, 3)\n[[0. 0. 0.]\n [0. 1. 0.]\n [0. 0. 0.]\n [0. 1. 0.]\n [0. 0. 0.]]\n\n\nThus if we have tools that will allow us to manipulate these arrays of numbers, we can manipulate the image. The numpy library can be particularly useful here, so let’s try that out using numpy array slicing. Notice that the default behavior of the imshow function appended row and column numbers that will be helpful to us as we try to address individual or groups of pixels. First let’s load another copy of our eight, and then make it look like a zero.\nTo make it look like a zero, we need to change the number underlying the centremost pixel to be 1. With the help of those row and column headers, at this small scale we can determine the centre pixel is in row labeled 2 and column labeled 1. Using array slicing, we can then address and assign a new value to that position.\nzero = iio.imread(uri=“Images/eight.tif”) zero[2,1]= 1.0 ““” The following line of code creates a new figure for imshow to use in displaying our output. Without it, plt.imshow() would overwrite our previous image in the cell above ““” fig, ax = plt.subplots() plt.imshow(zero) print(zero)\n\n\nCo-ordinate system\nWhen we process images, we can access, examine, and / or change the colour of any pixel we wish. To do this, we need some convention on how to access pixels individually; a way to give each one a name, or an address of a sort. The most common manner to do this, and the one we will use in our programs, is to assign a modified Cartesian coordinate system to the image. The coordinate system we usually see in mathematics has a horizontal x-axis and a vertical y-axis, like this:\n\n\n\ncartesian-coordinates.png\n\n\nThe modified coordinate system used for our images will have only positive coordinates, the origin will be in the upper left corner instead of the centre, and y coordinate values will get larger as they go down instead of up, like this:\n\n\n\nmodified-coordinates.png\n\n\nThis is called a left-hand coordinate system. If you hold your left hand in front of your face and point your thumb at the floor, your extended index finger will correspond to the x-axis while your thumb represents the y-axis.\n\n\n\nleft-hand-coordinates.png\n\n\n\n\n\n\n\n\nThe most common mistake made with coordinates is to forget that y coordinates get larger as they go down instead of up as in a normal Cartesian coordinate system.\n\n\n\nConsequently, it may be helpful to think in terms of counting down rows (r) for the y-axis and across columns (c) for the x-axis. This can be especially helpful in cases where you need to transpose image viewer data provided in x,y format to y,x format. Thus, we will use cx and ry where appropriate to help bridge these two approaches.\n\n\n\n\nChanging pixel values\nLet’s load another copy of eight named five, and then change the value of pixels so you have what looks like a 5 instead of an 8. There are many possible solutions, but one method would be . . .\n\nfive = iio.imread(uri=\"Images/eight.tif\")\nfive[1,2]= 1.0\nfive[3,0]= 1.0\nfig, ax = plt.subplots()\nplt.imshow(five)\nprint(five)\n\n[[0. 0. 0.]\n [0. 1. 1.]\n [0. 0. 0.]\n [1. 1. 0.]\n [0. 0. 0.]]\n\n\n\n\n\n\n\nMore colours\nUp to now, we only had a 2 colour matrix, but we can have more if we use other numbers or fractions. One common way is to use the numbers between 0 and 255 to allow for 256 different colours or 256 different levels of grey. Let’s try that out:\n\n# make a copy of eight\nthree_colours = iio.imread(uri=\"Images/eight.tif\")\n\n# multiply the whole matrix by 128\nthree_colours = three_colours * 128\n\n# set the middle row (index 2) to the value of 255.,\n# so you end up with the values 0., 128., and 255.\nthree_colours[2,:] = 255.\nfig, ax = plt.subplots()\nplt.imshow(three_colours)\nprint(three_colours)\n\n[[  0.   0.   0.]\n [  0. 128.   0.]\n [255. 255. 255.]\n [  0. 128.   0.]\n [  0.   0.   0.]]\n\n\n\n\n\nWe now have 3 colours, but are they the three colours you expected? They all appear to be on a continuum of dark purple on the low end and yellow on the high end. This is a consequence of the default colour map (cmap) in this library. You can think of a colour map as an association or mapping of numbers to a specific colour. However, the goal here is not to have one number for every possible colour, but rather to have a continuum of colours that demonstrate relative intensity. In our specific case here for example, 255 or the highest intensity is mapped to yellow, and 0 or the lowest intensity is mapped to a dark purple. The best colour map for your data will vary and there are many options built in, but this default selection was not arbitrary. A lot of science went into making this the default due to its robustness when it comes to how the human mind interprets relative colour values, grey-scale printability, and colour-blind friendliness (You can read more about this default colour map in a Matplotlib tutorial and an explanatory article by the authors). Thus it is a good place to start, and you should change it only with purpose and forethought. For now, let’s see how you can do that using an alternative map you have likely seen before where it will be even easier to see it as a mapped continuum of intensities: greyscale.\n\nfig, ax = plt.subplots()\nplt.imshow(three_colours,cmap=plt.cm.gray)\n\n<matplotlib.image.AxesImage at 0x7f294bdd79d0>\n\n\n\n\n\nAbove we have exactly the same underying data matrix, but in greyscale. Zero maps to black, 255 maps to white, and 128 maps to medium grey. Here we only have a single channel in the data and utilize a grayscale color map to represent the luminance, or intensity of the data and correspondingly this channel is referred to as the luminance channel.\n\n\nEven More Colours\nThis is all well and good at this scale, but what happens when we instead have a picture of a natural landscape that contains millions of colours. Having a one to one mapping of number to colour like this would be inefficient and make adjustments and building tools to do so very difficult. Rather than larger numbers, the solution is to have more numbers in more dimensions. Storing the numbers in a multi-dimensional matrix where each colour or property like transparency is associated with its own dimension allows for individual contributions to a pixel to be adjusted independently. This ability to manipulate properties of groups of pixels separately will be key to certain techniques explored in later chapters of this lesson.\nTo get started let’s see an example of how different dimensions of information combine to produce a set of pixels using a 4 X 4 matrix with 3 dimensions for the colours red, green, and blue. Rather than loading it from a file, we will generate this example using NumPy.\n\n# set the random seed so we all get the same matrix\npseudorandomizer = np.random.RandomState(2021)\n# create a 4 × 4 checkerboard of random colours\ncheckerboard = pseudorandomizer.randint(0, 255, size=(4, 4, 3))\n# restore the default map as you show the image\nfig, ax = plt.subplots()\nplt.imshow(checkerboard)\n# display the arrays\nprint(checkerboard)\n\n[[[116  85  57]\n  [128 109  94]\n  [214  44  62]\n  [219 157  21]]\n\n [[ 93 152 140]\n  [246 198 102]\n  [ 70  33 101]\n  [  7   1 110]]\n\n [[225 124 229]\n  [154 194 176]\n  [227  63  49]\n  [144 178  54]]\n\n [[123 180  93]\n  [120   5  49]\n  [166 234 142]\n  [ 71  85  70]]]\n\n\n\n\n\nPreviously we had one number being mapped to one colour or intensity. Now we are combining the effect of 3 numbers to arrive at a single colour value. Let’s see an example of that using the blue square at the end of the second row, which has the index [row, column][1, 3]. Note that in Python indexing starts at zero.\n\n# extract all the colour information for the blue square\nupper_right_square = checkerboard[1, 3, :]\nupper_right_square\n\narray([  7,   1, 110])\n\n\nThe integers in order represent\n\nRed: 7\nGreen: 1\nBlue: 3\n\nLooking at theses three values and knowing how they map, can help us understand why it is blue. If we divide each value by 255, which is the maximum, we can determine how much it is contributing relative to its maximum potential. Effectively, the red is at 7/255 or 2.8 percent of its potential, the green is at 1/255 or 0.4 percent, and blue is 110/255 or 43.1 percent of its potential. So when you mix those three intensities of colour, blue is winning by a wide margin, but the red and green still contribute to make it a slightly different shade of blue than 0,0,110 would be on its own.\nThese colours mapped to dimensions of the matrix may be referred to as channels. It may be helpful to display each of these channels independently, to help us understand what is happening. We can do that by multiplying our image array representation with a 1d matrix that has a one for the channel we want to keep and zeros for the rest.\n\n# transform array to focus only on the red channel (green and blue channels effectively removed by multiplying by zero)\nred_channel = checkerboard * [1, 0, 0]\nfig, ax = plt.subplots()\nplt.imshow(red_channel)\n\n<matplotlib.image.AxesImage at 0x7f2949357580>\n\n\n\n\n\n\n# transform array to focus only on the green channel (red and blue channels effectively removed by multiplying by zero)\ngreen_channel = checkerboard * [0, 1, 0]\nfig, ax = plt.subplots()\nplt.imshow(green_channel)\n\n<matplotlib.image.AxesImage at 0x7f29491dae60>\n\n\n\n\n\n\n# transform array to focus only on the blue channel (red and green channels effectively removed by multiplying by zero)\nblue_channel = checkerboard * [0, 0, 1]\nfig, ax = plt.subplots()\nplt.imshow(blue_channel)\n\n<matplotlib.image.AxesImage at 0x7f2949248700>\n\n\n\n\n\nIf we look at the upper [1, 3] square in all three figures, we can see each of those colour contributions in action. Notice that there are several squares in the blue figure that look even more intensely blue than square [1, 3]. When all three channels are combined though, the blue light of those squares is being diluted by the relative strength of red and green being mixed in with them.\n\n\n24-bit RGB Colour\nThis last colour model we used, known as the RGB (Red, Green, Blue) model, is the most common. As we saw, the RGB model is an additive colour model, which means that the primary colours are mixed together to form other colours. Most frequently, the amount of the primary colour added is represented as an integer in the closed range [0, 255] as seen in the example. Therefore, there are 256 discrete amounts of each primary colour that can be added to produce another colour. The number of discrete amounts of each colour, 256, corresponds to the number of bits used to hold the colour channel value, which is eight (28=256). Since we have three channels with 8 bits for each (8+8+8=24), this is called 24-bit colour depth.\nAny particular colour in the RGB model can be expressed by a triplet of integers in [0, 255], representing the red, green, and blue channels, respectively. A larger number in a channel means that more of that primary colour is present.\n\n\n\n\n\n\nSuppose that we represent colours as triples (r, g, b), where each of r, g, and b is an integer in [0, 255]. What colours are represented by each of these triples? 1.(255, 0, 0)  2.(0, 255, 0)  3.(0, 0, 255)  4.(255, 255, 255)  5.(0, 0, 0)  6.(128, 128, 128)\n\n\n\n\n\n\n(255, 0, 0) represents red, because the red channel is maximised, while the other two channels have the minimum values\n\n(0, 255, 0) represents green\n\n(0, 0, 255) represents blue\n\n(255, 255, 255) is a little harder. When we mix the maximum value of all three colour channels, we see the colour white\n\n(0, 0, 0) represents the absence of all colour, or black\n\n(128, 128, 128) represents a medium shade of gray. Note that the 24-bit RGB colour model provides at least 254 shades of gray, rather than only fifty\n\n\n\n\nNote that the RGB colour model may run contrary to your experience, especially if you have mixed primary colours of paint to create new colours. In the RGB model, the lack of any colour is black, while the maximum amount of each of the primary colours is white. With physical paint, we might start with a white base, and then add differing amounts of other paints to produce a darker shade.\nWe can look at some further examples of 24-bit RGB colours, in a visual way. The image below shows some colour names, their 24-bit RGB triplet values, and the colour itself:\n\n\n\n\n\n\nWe cannot really provide a complete table. To see why, answer this question: How many possible colours can be represented with the 24-bit RGB model?\n\n\n\n\n\nThere are 24 total bits in an RGB colour of this type, and each bit can be on or off, and so there are \\(2^{24}\\) = 16,777,216 possible colours with our additive, 24-bit RGB colour model.\n\n\n\nAlthough 24-bit colour depth is common, there are other options. We might have 8-bit colour (3 bits for red and green, but only 2 for blue, providing 8 × 8 × 4 = 256 colours) or 16-bit colour (4 bits for red, green, and blue, plus 4 more for transparency, providing 16 × 16 × 16 = 4096 colours), for example. There are colour depths with more than eight bits per channel, but as the human eye can only discern approximately 10 million different colours, these are not often used.\nIf you are using an older or inexpensive laptop screen or LCD monitor to view images, it may only support 18-bit colour, capable of displaying 64 × 64 × 64 = 262,144 colours. 24-bit colour images will be converted in some manner to 18-bit, and thus the colour quality you see will not match what is actually in the image.\nWe can combine our coordinate system with the 24-bit RGB colour model to gain a conceptual understanding of the images we will be working with. An image is a rectangular array of pixels, each with its own coordinate. Each pixel in the image is a square point of coloured light, where the colour is specified by a 24-bit RGB triplet. Such an image is an example of raster graphics.\n\n\nImage formats\nAlthough the images we will manipulate in our programs are conceptualised as rectangular arrays of RGB triplets, they are not necessarily created, stored, or transmitted in that format. There are several image formats we might encounter, and we should know the basics of at least of few of them. Some formats we might encounter, and their file extensions, are shown in this table:\n\n\n\nimage_formats.JPG\n\n\n\n\nDevice-Independent Bitmap (BMP)\nThe file format that comes closest to our preceding conceptualisation of images is the Device-Independent Bitmap, or BMP, file format. BMP files store raster graphics images as long sequences of binary-encoded numbers that specify the colour of each pixel in the image. Since computer files are one-dimensional structures, the pixel colours are stored one row at a time. That is, the first row of pixels (those with y-coordinate 0) are stored first, followed by the second row (those with y-coordinate 1), and so on. Depending on how it was created, a BMP image might have 8-bit, 16-bit, or 24-bit colour depth.\n24-bit BMP images have a relatively simple file format, can be viewed and loaded across a wide variety of operating systems, and have high quality. However, BMP images are not compressed, resulting in very large file sizes for any useful image resolutions.\nThe idea of image compression is important to us for two reasons: first, compressed images have smaller file sizes, and are therefore easier to store and transmit; and second, compressed images may not have as much detail as their uncompressed counterparts, and so our programs may not be able to detect some important aspect if we are working with compressed images. Since compression is important to us, we should take a brief detour and discuss the concept.\n\n\nImage compression\nBefore we talk specifically about images, we first need to understand how numbers are stored in a modern digital computer. When we think of a number, we do so using a decimal, or base-10 place-value number system. For example, a number like 659 is 6 × 102 + 5 × 101 + 9 × 100. Each digit in the number is multiplied by a power of 10, based on where it occurs, and there are 10 digits that can occur in each position (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\nIn principle, computers could be constructed to represent numbers in exactly the same way. But, the electronic circuits inside a computer are much easier to construct if we restrict the numeric base to only two, instead of 10. (It is easier for circuitry to tell the difference between two voltage levels than it is to differentiate among 10 levels.) So, values in a computer are stored using a binary, or base-2 place-value number system.\nIn this system, each symbol in a number is called a bit instead of a digit, and there are only two values for each bit (0 and 1). We might imagine a four-bit binary number, 1101. Using the same kind of place-value expansion as we did above for 659, we see that 1101 = 1 × 23 + 1 × 22 + 0 × 21 + 1 × 20, which if we do the math is 8 + 4 + 0 + 1, or 13 in decimal.\nInternally, computers have a minimum number of bits that they work with at a given time: eight. A group of eight bits is called a byte. The amount of memory (RAM) and drive space our computers have is quantified by terms like Megabytes (MB), Gigabytes (GB), and Terabytes (TB). The following table provides more formal definitions for these terms:\n\n\n\nbits.JPG\n\n\n\n\n\n\n\n\nImagine that we have a fairly large, but very boring image: a 5,000 × 5,000 pixel image composed of nothing but white pixels. If we used an uncompressed image format such as BMP, with the 24-bit RGB colour model, how much storage would be required for the file?\n\n\n\n\n\nIn such an image, there are 5,000 × 5,000 = 25,000,000 pixels, and 24 bits for each pixel, leading to 25,000,000 × 24 = 600,000,000 bits, or 75,000,000 bytes (71.5MB). That is quite a lot of space for a very uninteresting image!\n\n\n\nSince image files can be very large, various compression schemes exist for saving (approximately) the same information while using less space. These compression techniques can be categorised as lossless or lossy.\n\n\nLossless compression\nIn lossless image compression, we apply some algorithm (i.e., a computerised procedure) to the image, resulting in a file that is significantly smaller than the uncompressed BMP file equivalent would be. Then, when we wish to load and view or process the image, our program reads the compressed file, and reverses the compression process, resulting in an image that is identical to the original. Nothing is lost in the process – hence the term “lossless.”\nThe general idea of lossless compression is to somehow detect long patterns of bytes in a file that are repeated over and over, and then assign a smaller bit pattern to represent the longer sample. Then, the compressed file is made up of the smaller patterns, rather than the larger ones, thus reducing the number of bytes required to save the file. The compressed file also contains a table of the substituted patterns and the originals, so when the file is decompressed it can be made identical to the original before compression.\nTo provide you with a concrete example, consider the 71.5 MB white BMP image discussed above. When put through the zip compression utility on Microsoft Windows, the resulting .zip file is only 72 KB in size! That is, the .zip version of the image is three orders of magnitude smaller than the original, and it can be decompressed into a file that is byte-for-byte the same as the original. Since the original is so repetitious - simply the same colour triplet repeated 25,000,000 times - the compression algorithm can dramatically reduce the size of the file.\nIf you work with .zip or .gz archives, you are dealing with lossless compression.\n\n\nLossy compression\nLossy compression takes the original image and discards some of the detail in it, resulting in a smaller file format. The goal is to only throw away detail that someone viewing the image would not notice. Many lossy compression schemes have adjustable levels of compression, so that the image creator can choose the amount of detail that is lost. The more detail that is sacrificed, the smaller the image files will be - but of course, the detail and richness of the image will be lower as well.\nThis is probably fine for images that are shown on Web pages or printed off on 4 × 6 photo paper, but may or may not be fine for scientific work. You will have to decide whether the loss of image quality and detail are important to your work, versus the space savings afforded by a lossy compression format.\n\n\n\n\n\n\nIt is important to understand that once an image is saved in a lossy compression format, the lost detail is just that - lost.\n\n\n\nUnlike lossless formats, given an image saved in a lossy format, there is no way to reconstruct the original image in a byte-by-byte manner.\n\n\n\n\nJPEG\nJPEG images are perhaps the most commonly encountered digital images today. JPEG uses lossy compression, and the degree of compression can be tuned to your liking. It supports 24-bit colour depth, and since the format is so widely used, JPEG images can be viewed and manipulated easily on all computing platforms.\nLet us see the effects of image compression on image size with actual images. The following script creates a square white image 5000 X 5000 pixels, and then saves it as a BMP and as a JPEG image.\n\nimport imageio.v3 as iio\nimport numpy as np\n\ndim = 5000\n\nimg = np.zeros((dim, dim, 3), dtype=\"uint8\")\nimg.fill(255)\n\niio.imwrite(uri=\"Images/ws.bmp\", image=img)\niio.imwrite(uri=\"Images/ws.jpg\", image=img)\n\n\n\n\ndownload.jpg\n\n\nThe BMP file, ws.bmp, is 75,000,054 bytes, which matches our prediction very nicely. The JPEG file, ws.jpg, is 392,503 bytes, two orders of magnitude smaller than the bitmap version.\nLet us see a hands-on example of lossless versus lossy compression. Once again, open a terminal and navigate to the data/ directory. The two output images, ws.bmp and ws.jpg, should still be in the directory, along with another image, tree.jpg.\nWe can apply lossless compression to any file by using the zip command. Recall that the ws.bmp file contains 75,000,054 bytes. Apply lossless compression to this image by executing the following command:\nzip ws.zip ws.bmp. \nThis command tells the computer to create a new compressed file, ws.zip, from the original bitmap image. Execute a similar command on the tree JPEG file:\nzip tree.zip tree.jpg.\nHaving created the compressed file, use the ls -al command to display the contents of the directory.\n\nHow big are the compressed files?\nHow do those compare to the size of ws.bmp and tree.jpg?\nWhat can you conclude from the relative sizes?\n\n\n\n\nzipped.JPG\n\n\nThe tree.jpg has not reduced in size as this was already in a compressed format. However the regularity of the bitmap image (remember, it is a 5,000 x 5,000 pixel image containing only white pixels) allows the lossless compression scheme to compress the ws.bmp file quite effectivelym reducing the size from 7,500,054 bytes (71.52Mb) to 72,998 bytes (71.29kb).\nHere is an example showing how JPEG compression might impact image quality. Consider this image of several maize seedlings (scaled down here from 11,339 × 11,336 pixels in order to fit the display):\n\n\n\nquality-original.jpg\n\n\nNow, let us zoom in and look at a small section of the label in the original, first in the uncompressed format:\n\n\n\nquality-tif.jpg\n\n\nHere is the same area of the image, but in JPEG format. We used a fairly aggressive compression parameter to make the JPEG, in order to illustrate the problems you might encounter with the format.\n\n\n\nquality-jpg.jpg\n\n\nThe JPEG image is of clearly inferior quality. It has less colour variation and noticeable pixelation. Quality differences become even more marked when one examines the colour histograms for each image. A histogram shows how often each colour value appears in an image. The histograms for the uncompressed (left) and compressed (right) images are shown below:\n\n\n\nquality-histogram.jpg\n\n\nWe learn how to make histograms such as these later on in the workshop. The differences in the colour histograms are even more apparent than in the images themselves; clearly the colours in the JPEG image are different from the uncompressed version.\nIf the quality settings for your JPEG images are high (and the compression rate therefore relatively low), the images may be of sufficient quality for your work. It all depends on how much quality you need, and what restrictions you have on image storage space. Another consideration may be where the images are stored. For example,if your images are stored in the cloud and therefore must be downloaded to your system before you use them, you may wish to use a compressed image format to speed up file transfer time.\n\n\nPNG\nPNG images are well suited for storing diagrams. It uses a lossless compression and is hence often used in web applications for non-photographic images. The format is able to store RGB and plain luminance (single channel, without an associated color) data, among others. Image data is stored row-wise and then, per row, a simple filter, like taking the difference of adjacent pixels, can be applied to increase the compressability of the data. The filtered data is then compressed in the next step and written out to the disk.\n\n\nTIFF\nTIFF images are popular with publishers, graphics designers, and photographers. TIFF images can be uncompressed, or compressed using either lossless or lossy compression schemes, depending on the settings used, and so TIFF images seem to have the benefits of both the BMP and JPEG formats. The main disadvantage of TIFF images (other than the size of images in the uncompressed version of the format) is that they are not universally readable by image viewing and manipulation software.\n\n\nMetadata\nJPEG and TIFF images support the inclusion of metadata in images. Metadata is textual information that is contained within an image file. Metadata holds information about the image itself, such as when the image was captured, where it was captured, what type of camera was used and with what settings, etc. We normally don’t see this metadata when we view an image, but we can view it independently if we wish to (see Accessing Metadata, below). The important thing to be aware of at this stage is that you cannot rely on the metadata of an image being fully preserved when you use software to process that image.\n\n\n\n\n\n\nThe image reader/writer library that we use throughout this blog, imageio.v3, includes metadata when saving new images but may fail to keep certain metadata fields.\n\n\n\nIf metadata is important to you, take precautions to always preserve the original files.\n\n\nMetadata is served independently from pixel data. imageio.v3 provides a way to display or explore the metadata associated with an image:\n\n# read metadata\nmetadata = iio.immeta(uri='Images/eight.tif')\n\n# display the format-specific metadata\nmetadata\n\n{'is_fluoview': False,\n 'is_nih': False,\n 'is_micromanager': False,\n 'is_ome': False,\n 'is_lsm': False,\n 'is_reduced': False,\n 'is_shaped': True,\n 'is_stk': False,\n 'is_tiled': False,\n 'is_mdgel': False,\n 'compression': <COMPRESSION.NONE: 1>,\n 'predictor': 1,\n 'is_mediacy': False,\n 'description': '{\"shape\": [5, 3]}',\n 'description1': '',\n 'is_imagej': False,\n 'software': 'tifffile.py',\n 'resolution_unit': 1,\n 'resolution': (1.0, 1.0, 'NONE')}\n\n\nOther software exists that can help you handle metadata, such as Fiji and ImageMagick. You may want to explore these options if you need to work with the metadata of your images.\n\n\nSummary of image formats used in this blog\nThe following table summarises the characteristics of the BMP, JPEG, and TIFF image formats:\n\n\n\nformats.JPG\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\ndigital images are represented as rectangular arrays of square pixels.\ndigital images use a left-hand coordinate system, with the origin in the upper left corner, the x-axis running to the right, and the y-axis running down. Some learners may prefer to think in terms of counting down rows for the y-axis and across columns for the x-axis. Thus, we will make an effort to allow for both approaches in our lesson presentation.\nmost frequently, digital images use an additive RGB model, with eight bits for the red, green, and blue channels.\nskimage images are stored as multi-dimensional NumPy arrays.\nin skimage images, the red channel is specified first, then the green, then the blue, i.e., RGB.\nlossless compression retains all the details in an image, but lossy compression results in loss of some of the original image detail.\nBMP images are uncompressed, meaning they have high quality but also that their file sizes are large.\nJPEG images use lossy compression, meaning that their file sizes are smaller, but image quality may suffer.\nTIFF images can be uncompressed or compressed with lossy or lossless compression.\ndepending on the camera or sensor, various useful pieces of information may be stored in an image file, in the image metadata."
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#working-with-skimage",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#working-with-skimage",
    "title": "Image Processing with Python",
    "section": "2. Working with skimage",
    "text": "2. Working with skimage\nIn the previous section we covered much of how images are represented in computer software. In this section we will learn some more methods for accessing and changing digital images.\n\nReading, displaying, and saving images\nImageio provides intuitive functions for reading and writing (saving) images. All of the popular image formats, such as BMP, PNG, JPEG, and TIFF are supported, along with several more esoteric formats. Check the Supported Formats docs for a list of all formats. Matplotlib provides a large collection of plotting utilities.\nLet us examine a simple Python program to load, display, and save an image to a different format. First, we import the v3 module of imageio (imageio.v3) as iio so we can read and write images. Then, we use the iio.imread() function to read a JPEG image entitled chair.jpg. Here are the first few lines:\n\n# Python program to open, display, and save an image.\nimport imageio.v3 as iio\n\n# read image\nimage = iio.imread(uri='Images/chair.jpg')\n\nNext, we will do something with the image. Once we have the image in the program, we first call plt.subplots() so that we will have a fresh figure with a set of axis independent from our previous calls. Next we call plt.imshow() in order to display the image:\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f0839e0da80>\n\n\n\n\n\nNow, we will save the image in another format:\n\n# save a new version in .tif format\niio.imwrite(uri=\"Images/chair.tif\", image=image)\n\nThe final statement in the program, iio.imwrite(uri=“data/chair.tif”, image=image), writes the image to a file named chair.tif in our Images/ directory. The imwrite() function automatically determines the type of the file, based on the file extension we provide. In this case, the .tif extension causes the image to be saved as a TIFF.\n\n\n\n\n\n\nThe iio.imwrite() function automatically uses the file type we specify in the file name parameter’s extension.\n\n\n\nNote that this is not always the case. For example, if we are editing a document in Microsoft Word, and we save the document as paper.pdf instead of paper.docx, the file is not saved as a PDF document.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen we call functions in Python, there are two ways we can specify the necessary arguments. We can specify the arguments positionally, i.e., in the order the parameters appear in the function definition, or we can use named arguments.\nFor example, the iio.imwrite() function definition specifies two parameters, the resource to save the image to (e.g., a file name, an http address) and the image to write to disk. So, we could save the chair image in the sample code above using positional arguments like this:\niio.imwrite(\"data/chair.tif\", image)\nSince the function expects the first argument to be the file name, there is no confusion about what “data/chair.jpg” means. The same goes for the second argument.\nThe style we will use in this workshop is to name each argument, like this:\niio.imwrite(uri=\"data/chair.tif\", image=image)\nThis style will make it easier for you to learn how to use the variety of functions we will cover in this blog.\n\n\n\n\nResizing an image\nAdd import skimage.transform and import skimage.util to your list of imports. Using the chair.jpg image located in the data folder, write a Python script to read your image into a variable named image. Then, resize the image to 10 percent of its current size using these lines of code:\n\nimport skimage.transform, skimage.util\n\nimage = iio.imread(uri='Images/chair.jpg')\n\n# resize to 10% of original size\nnew_shape = (image.shape[0] // 10, image.shape[1] // 10, image.shape[2])\n\n# image file stored as whole numbers for space efficiency\nsmall = skimage.transform.resize(image=image, output_shape=new_shape)\n\n# converts image back to whole numbers before saving to disk\nsmall = skimage.util.img_as_ubyte(small)\n\nAs it is used here, the parameters to the skimage.transform.resize() function are the image to transform, image, the dimensions we want the new image to have, new_shape.\nNote that the pixel values in the new image are an approximation of the original values and should not be confused with actual, observed data. This is because skimage interpolates the pixel values when reducing or increasing the size of an image. skimage.transform.resize has a number of optional parameters that allow the user to control this interpolation. You can find more details in the scikit-image documentation.\nImage files on disk are normally stored as whole numbers for space efficiency, but transformations and other math operations often result in conversion to floating point numbers. Using the skimage.util.img_as_ubyte() method converts it back to whole numbers before we save it back to disk. If we don’t convert it before saving, iio.imwrite() may not recognise it as image data.\nNext, write the resized image out to a new file named resized.jpg in your data directory. Finally, use plt.imshow() with each of your image variables to display both images in your notebook. Don’t forget to use fig, ax = plt.subplots() so you don’t overwrite the first image with the second. Images may appear the same size in jupyter, but you can see the size difference by comparing the scales for each. You can also see the differnce in file storage size on disk by hovering your mouse cursor over the original and the new file in the jupyter file browser, using ls -l in your shell, or the OS file browser if it is configured to show file sizes.\n\n# write the resized image out to a new file named resized.jpg\niio.imwrite(uri=\"Images/resized.jpg\", image=small)\n\n# plot original image\nfig, ax = plt.subplots()\nplt.imshow(image)\nplt.title(\"Original image\")\n\n# plot resized image\nfig, ax = plt.subplots()\nplt.imshow(small)\nplt.title(\"Reduced to 10% of size of original image\")\n\nText(0.5, 1.0, 'Reduced to 10% of size of original image')\n\n\n\n\n\n\n\n\nWe can see from the above that our reduction to 10% of the original size appears to have been successsful. The axes of the reduced image are 0-300 and 0-400 as against 0-3000 and 0-4000.\nThe script has: - resized the data/chair.jpg image by a factor of 10 in both dimensions - saved the result to the data/resized.jpg file, and - displays original and resized for comparision\n\n\nManipulating pixels\n\nRetaining only high intensity pixels\nIn section 1, we individually manipulated the colours of pixels by changing the numbers stored in the image’s NumPy array. Let’s apply the principles learned there along with some new principles to a real world example. Suppose we are interested in this maize root cluster image.\n\nWe want to be able to focus our program’s attention on the roots themselves, while ignoring the black background.\nSince the image is stored as an array of numbers, we can simply look through the array for pixel colour values that are less than some threshold value. This process is called thresholding, and we will see more powerful methods to perform the thresholding task in the Thresholding section.\nHere, though, we will look at a simple and elegant NumPy method for thresholding. Let us develop a program that keeps only the pixel colour values in an image that have value greater than or equal to 128. This will keep the pixels that are brighter than half of “full brightness”, i.e., pixels that do not belong to the black background. We will start by reading the image and displaying it.\n\nimport imageio.v3 as iio\n\n# read input image\nimage = iio.imread(uri=\"Images/maize_cluster.jpg\")\n\n# display original image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f4d204fe320>\n\n\n\n\n\nNow we can threshold the image and display the result:\n\n# keep only high-intensity pixels\nimage[image < 128] = 0\n\n# display modified image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\nValueError: assignment destination is read-only\n\n\n\n\n\n\n\n\nCheck if the array is writeable with image.flags\n\n\n\n\nIf WRITEABLE is false, change it with img.setflags(write=1)\nIf after doing this you receive ValueError: cannot set WRITEABLE flag to True of this array, then as a workaround create a copy of the image using image_copy = image.copy()\n\n\n\nimage.flags\n\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : False\n  WRITEABLE : False\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n\n\n\nimage.setflags(write=1)\n\nValueError: cannot set WRITEABLE flag to True of this array\n\n\n\n# workaround to address the read-only issues\nimage_copy = image.copy()\n\n# keep only high-intensity pixels, by setting all low-intensity pixels < 128 to zero\nimage_copy[image_copy < 128] = 0\n\n# display modified image\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f4d11f9c4c0>\n\n\n\n\n\nThe result is an image in which the extraneous background detail has been removed.\n\n\n\nConverting colour images to grayscale\nIt is often easier to work with grayscale images, which have a single channel, instead of colour images, which have three channels. Skimage offers the function skimage.color.rgb2gray() to achieve this. This function adds up the three colour channels in a way that matches human colour perception, see the skimage documentation for details. It returns a grayscale image with floating point values in the range from 0 to 1. We can use the function skimage.util.img_as_ubyte() in order to convert it back to the original data type and the data range back 0 to 255. Note that it is often better to use image values represented by floating point values, because using floating point numbers is numerically more stable.\n\n\n\n\n\n\nskimage contains many modules and functions that include the US English spelling, color.\n\n\n\nThe exact spelling matters here, e.g. you will encounter an error if you try to run skimage.colour.rgb2gray(). To account for this, we will use the US English spelling, color, in example Python code throughout this blog. We will adopt a similar approach with “centre” and center.\n\n\n\nimport imageio.v3 as iio\nimport skimage.color\n\n# read input image\nimage = iio.imread(uri=\"Images/chair.jpg\")\n\n# display original image\nfig, ax = plt.subplots()\nplt.imshow(image)\nplt.title(\"Original image\")\n\n# convert to grayscale and display\ngray_image = skimage.color.rgb2gray(image)\nfig, ax = plt.subplots()\nplt.imshow(gray_image, cmap=\"gray\")\nplt.title(\"Grayscale image\")\n\nText(0.5, 1.0, 'Grayscale image')\n\n\n\n\n\n\n\n\nWe also load colour images as grayscale directly by passing the argument mode=“L”  to iio.imread():\n\nimport imageio.v3 as iio\nimport skimage.color\n\n# read input image, based on filename parameter\nimage = iio.imread(uri=\"Images/chair.jpg\", mode=\"L\")\n\n# display grayscale image\nfig, ax = plt.subplots()\nplt.imshow(image, cmap=\"gray\")\nplt.title(\"Grayscale image\")\n\nText(0.5, 1.0, 'Grayscale image')\n\n\n\n\n\n\nRetaining only low intensity pixels\nA little earlier, we showed how we could use Python and skimage to turn on only the high intensity pixels from an image, while turning all the low intensity pixels off. Now, let’s practice doing the opposite - keeping all the low intensity pixels while changing the high intensity ones, by turning all of the white pixels in the image to a light gray colour, say with the intensity of each formerly white pixel set to 0.75:\n\n# read input image, based on filename parameter\nsudoku = iio.imread(uri=\"Images/sudoku.png\", mode=\"L\")\n\n# display grayscale image\nfig, ax = plt.subplots()\nplt.imshow(sudoku, cmap=\"gray\")\nplt.title(\"Grayscale image\")\n\nText(0.5, 1.0, 'Grayscale image')\n\n\n\n\n\n\n# Clip all high intensity pixels > 0.75 to 0.75\nsudoku[sudoku > 0.75] = 0.75\n\n# display modified image\nfig, ax = plt.subplots()\nplt.imshow(sudoku, cmap=\"gray\", vmin=0, vmax=1)\n\n\n\n\n\nPlotting single channel images (cmap, vmin, vmax)\nCompared to a colour image, a grayscale image contains only a single intensity value per pixel. When we plot such an image with plt.imshow, matplotlib uses a colour map, to assign each intensity value a colour. The default colour map is called “viridis” and maps low values to purple and high values to yellow. We can instruct matplotlib to map low values to black and high values to white instead, by calling plt.imshow with cmap=“gray”. The documentation contains an overview of pre-defined colour maps.\nFurthermore, matplotlib determines the minimum and maximum values of the colour map dynamically from the image, by default. That means, that in an image, where the minimum is 0.25 and the maximum is 0.75, those values will be mapped to black and white respectively (and not dark gray and light gray as you might expect). If there are defined minimum and maximum vales, you can specify them via vmin and vmax to get the desired output. If you forget about this, it can lead to unexpected results.\n\n\nAccess via slicing\nAs noted in the previous section skimage images are stored as NumPy arrays, so we can use array slicing to select rectangular areas of an image. Then, we can save the selection as a new image, change the pixels in the image, and so on. It is important to remember that coordinates are specified in (ry, cx) order and that colour values are specified in (r, g, b) order when doing these manipulations.\nConsider this image of a whiteboard, and suppose that we want to create a sub-image with just the portion that says “odd + even = odd,” along with the red box that is drawn around the words.\n\n\n\nboard.jpg\n\n\nUsing the same display technique we have used throughout this blog, we can determine the coordinates of the corners of the area we wish to extract by hovering the mouse near the points of interest and noting the coordinates. If we do that, we might settle on a rectangular area with an upper-left coordinate of (135, 60) and a lower-right coordinate of (480, 150), as shown in this version of the whiteboard picture:\n\n\n\nboard-coordinates.jpg\n\n\nNote that the coordinates in the preceding image are specified in (cx, ry) order. Now if our entire whiteboard image is stored as an skimage image named image, we can create a new image of the selected region with a statement like this:\nclip = image[60:151, 135:481, :]\nOur array slicing specifies the range of y-coordinates or rows first, 60:151, and then the range of x-coordinates or columns, 135:481. Note we go one beyond the maximum value in each dimension, so that the entire desired area is selected. The third part of the slice, :, indicates that we want all three colour channels in our new image.\nA script to create the subimage would start by loading the image:\n\nimport imageio.v3 as iio\n\n# load and display original image\nimage = iio.imread(uri=\"Images/board.jpg\")\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f93077220>\n\n\n\n\n\nThen we use array slicing to create a new image with our selected area and then display the new image:\n\n# extract, display, and save sub-image\nclip = image[60:151, 135:481, :]\nfig, ax = plt.subplots()\nplt.imshow(clip)\niio.imwrite(uri=\"Images/clip.tif\", image=clip)\n\n\n\n\nWe can also change the values in an image.\n\nFirst, we sample a single pixel’s colour at a particular location of the image, saving it in a variable named color, which creates a 1 × 1 × 3 NumPy array with the blue, green, and red colour values for the pixel located at (ry = 330, cx = 90).\nThen, with the img[60:151, 135:481] = color command, we modify the image in the specified area. From a NumPy perspective, this changes all the pixel values within that range to array saved in the color variable. In this case, the command “erases” that area of the whiteboard, replacing the words with a beige colour, as shown in the final image produced by the program:\n\n\n# replace clipped area with sampled color\nimage_copy = image.copy()\ncolor = image_copy[330, 90]\nimage_copy[60:151, 135:481] = color\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f2f9102ffd0>\n\n\n\n\n\n\n::: {.callout-important}\n## Key Points:\n- images are read from disk with the iio.imread() function\n- we create a window that automatically scales the displayed image with matplotlib and calling show() on the global figure object\n- colour images can be transformed to grayscale using skimage.color.rgb2gray() or, in many cases, be read as grayscale directly by passing the argument mode=\"L\" to iio.imread()\n- we can resize images with the skimage.transform.resize() function\n- NumPy array commands, such as image[image < 128] = 0, can be used to manipulate the pixels of an image.\n- array slicing can be used to extract sub-images or modify areas of images, e.g., clip = image[60:150, 135:480, :]\n- metadata is not retained when images are loaded as skimage images\n:::\n\n\nimport imageio.v3 as iio\n\n# load and display original image\nimage = iio.imread(uri=\"Images/maize_cluster.jpg\")\nfig, ax = plt.subplots()\nplt.imshow(image)\nplt.title(\"Original image\")\n\n# extract, display, and save sub-image\n# WRITE YOUR CODE TO SELECT THE SUBIMAGE NAME clip HERE:\nclip = image[0:1750, 1500:2500, :]\nfig, ax = plt.subplots()\nplt.imshow(clip)\nplt.title(\"Clipped image\")\n\n# WRITE YOUR CODE TO SAVE clip HERE\niio.imwrite(uri=\"Images/clip.jpg\", image=clip)"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#drawing-and-bitwise-operations",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#drawing-and-bitwise-operations",
    "title": "Image Processing with Python",
    "section": "3. Drawing and Bitwise Operations",
    "text": "3. Drawing and Bitwise Operations\nThe next sections cover a basic toolkit of skimage operators. With these tools, we will be able to create programs to perform simple analyses of images based on changes in colour or shape.\n\nDrawing on images\nOften we wish to select only a portion of an image to analyze, and ignore the rest. Creating a rectangular sub-image with slicing, as we did in section 2 is one option for simple cases. Another option is to create another special image, of the same size as the original, with white pixels indicating the region to save and black pixels everywhere else. Such an image is called a mask. In preparing a mask, we sometimes need to be able to draw a shape - a circle or a rectangle, say - on a black image. skimage provides tools to do that.\nConsider this image of maize seedlings:\n\nNow, suppose we want to analyze only the area of the image containing the roots themselves; we do not care to look at the kernels, or anything else about the plants. Further, we wish to exclude the frame of the container holding the seedlings as well. Hovering over the image with our mouse, could tell us that the upper-left coordinate of the sub-area we are interested in is (44, 357), while the lower-right coordinate is (720, 740). These coordinates are shown in (x, y) order.\nA Python program to create a mask to select only that area of the image would start with a now-familiar section of code to open and display the original image.As before, we first import the v3 submodule of imageio (imageio.v3). We also import the NumPy library, which we need to create the initial mask image. Then, we import the draw submodule of skimage. We load and display the initial image in the same way we have done before.\n\nimport imageio.v3 as iio\nimport skimage.draw\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load and display the original image\nimage = iio.imread(uri=\"Images/maize-seedlings.tif\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f9073cd30>\n\n\n\n\n\n\n\nMasking\n\nNumPy allows indexing of images/arrays with “boolean” arrays of the same size. Indexing with a boolean array is also called mask indexing. The “pixels” in such a mask array can only take two values: True or False. When indexing an image with such a mask, only pixel values at positions where the mask is True are accessed. But first, we need to generate a mask array of the same size as the image. Luckily, the NumPy library provides a function to create just such an array. The next section of code shows how:\n\n# Create the basic mask\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\ncolor = \n\nThe first argument to the ones() function is the shape of the original image, so that our mask will be exactly the same size as the original. Notice, that we have only used the first two indices of our shape. We omitted the channel dimension. Indexing with such a mask will change all channel values simultaneously. The second argument, dtype = “bool”, indicates that the elements in the array should be booleans* - i.e., values are either True or False. Thus, even though we use np.ones() to create the mask, its pixel values are in fact not 1 but True. You could check this, e.g., by print(mask[0, 0]).\nNext, we draw a filled, rectangle on the mask. The parameters of the rectangle() function (357, 44) and (740, 720), are the coordinates of the upper-left (start) and lower-right (end) corners of a rectangle in (ry, cx) order. The function returns the rectangle as row (rr) and column (cc) coordinate arrays:\n\n# Draw filled rectangle on the mask image\n# co-ordinates are (row, column)\n# first co-ord is top-left, second co-ord is bottom-right\nrr, cc = skimage.draw.rectangle(start=(357, 44), end=(740, 720))\nmask[rr, cc] = False\n\n# Display mask image\nfig, ax = plt.subplots()\nplt.imshow(mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f90de33a0>\n\n\n\n\n\n\n\n\n\n\n\nCheck the documentation!\n\n\n\nWhen using an skimage function for the first time - or the fifth time - it is wise to check how the function is used, via the skimage documentation or other usage examples on programming-related sites such as Stack Overflow. Basic information about skimage functions can be found interactively in Python, via commands like help(skimage) or help(skimage.draw.rectangle). Take notes in your lab notebook. And, it is always wise to run some test code to verify that the functions your program uses are behaving in the manner you intend.\n\n\n\n\n\n\n\n\nVariable naming conventions\n\n\n\nYou may have wondered why we called the return values of the rectangle function rr and cc?! You may have guessed that r is short for row and c is short for column. However, the rectangle function returns mutiple rows and columns; thus we used a convention of doubling the letter r to rr (and c to cc) to indicate that those are multiple values. In fact it may have even been clearer to name those variables rows and columns; however this would have been also much longer. Whatever you decide to do, try to stick to some already existing conventions, such that it is easier for other people to understand your code.\n\n\n\n\nOther drawing operations\nThere are other functions for drawing on images, in addition to the skimage.draw.rectangle() function. We can draw circles, lines, text, and other shapes as well. These drawing functions may be useful later on, to help annotate images that our programs produce. Practice some of these functions here.\nCircles can be drawn with the skimage.draw.disk() function, which takes two parameters: the (ry, cx) point of the centre of the circle, and the radius of the circle. There is an optional shape parameter that can be supplied to this function. It will limit the output coordinates for cases where the circle dimensions exceed the ones of the image.\nLines can be drawn with the skimage.draw.line() function, which takes four parameters: the (ry, cx) coordinate of one end of the line, and the (ry, cx) coordinate of the other end of the line.\nOther drawing functions supported by skimage can be found in the skimage reference pages.\nFirst let’s make an empty, black image with a size of 800x600 pixels:\n\n# create the black canvas\nimage = np.zeros(shape=(600, 800, 3), dtype=\"uint8\")\n\n\n# Draw a blue circle with centre (200, 300) in (ry, cx) coordinates, and radius 100\nrr, cc = skimage.draw.disk(center=(200, 300), radius=100, shape=image.shape[0:2])\nimage[rr, cc] = (0, 0, 255)\n\n\n# Draw a green line from (400, 200) to (500, 700) in (ry, cx) coordinates\nrr, cc = skimage.draw.line(r0=400, c0=200, r1=500, c1=700)\nimage[rr, cc] = (0, 255, 0)\n\n\n# Display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f9064a6e0>\n\n\n\n\n\nWe could expand this solution, if we wanted, to draw rectangles, circles and lines at random positions within our black canvas. To do this, we could use the random python module, and the function random.randrange, which can produce random numbers within a certain range.\nLet’s draw 15 randomly placed circles:\n\nimport random\n\n# create the black canvas\nimage = np.zeros(shape=(600, 800, 3), dtype=\"uint8\")\n\n# draw a blue circle at a random location 15 times\nfor i in range(15):\n    rr, cc = skimage.draw.disk(center=(\n         random.randrange(600),\n         random.randrange(800)),\n         radius=50,\n         shape=image.shape[0:2],\n        )\n    image[rr, cc] = (0, 0, 255)\n\n# display the results\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f906bbaf0>\n\n\n\n\n\nWe could expand this even further to also randomly choose whether to plot a rectangle, a circle, or a square. Again, we do this with the random module, now using the function random.random that returns a random number between 0.0 and 1.0.\n\n# Draw 15 random shapes (rectangle, circle or line) at random positions\nfor i in range(15):\n    # generate a random number between 0.0 and 1.0 and use this to decide if we\n    # want a circle, a line or a sphere\n    x = random.random()\n    if x < 0.33:\n        # draw a blue circle at a random location\n        rr, cc = skimage.draw.disk(center=(\n            random.randrange(600),\n            random.randrange(800)),\n            radius=50,\n            shape=image.shape[0:2],\n        )\n        color = (0, 0, 255)\n    elif x < 0.66:\n        # draw a green line at a random location\n        rr, cc = skimage.draw.line(\n            r0=random.randrange(600),\n            c0=random.randrange(800),\n            r1=random.randrange(600),\n            c1=random.randrange(800),\n        )\n        color = (0, 255, 0)\n    else:\n        # draw a red rectangle at a random location\n        rr, cc = skimage.draw.rectangle(\n            start=(random.randrange(600), random.randrange(800)),\n            extent=(50, 50),\n            shape=image.shape[0:2],\n        )\n        color = (255, 0, 0)\n\n    image[rr, cc] = color\n\n# display the results\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f906fb5e0>\n\n\n\n\n\n\n\nImage modification\nAll that remains is the task of modifying the image using our mask in such a way that the areas with True pixels in the mask are not shown in the image any more.\n\n\n\n\n\n\nNow, consider the mask image we created above. The values of the mask that corresponds to the portion of the image we are interested in are all False, while the values of the mask that corresponds to the portion of the image we want to remove are all True. How do we change the original image using the mask?\n\n\n\n\n\nWhen indexing the image using the mask, we access only those pixels at positions where the mask is True. So, when indexing with the mask, one can set those values to 0, and effectively remove them from the image.\n\n\n\nNow we can write a Python program to use a mask to retain only the portions of our maize roots image that actually contains the seedling roots. We load the original image and create the mask in the same way as before:\n\n# Load the original image\nimage = iio.imread(uri=\"Images/maize-seedlings.tif\")\n\n# Create the basic mask\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n# Draw a filled rectangle on the mask image\nrr, cc = skimage.draw.rectangle(start=(357, 44), end=(740, 720))\nmask[rr, cc] = False\n\nThen, we use numpy indexing to remove the portions of the image, where the mask is True:\n\n# Apply the mask\nimage[mask] = 0\n\nThen, we display the masked image.\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f90c853f0>\n\n\n\n\n\n\n\nMasked monster truck\nI’ll now try to mask an image of 32 Degrees from my son’s monster truck collection!\n\n# Load and display the original image\nimage = iio.imread(uri=\"Images/monster_truck.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f90c86470>\n\n\n\n\n\n\n# Create the basic mask\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n\n# Draw filled rectangle on the mask image\nrr, cc = skimage.draw.rectangle(start=(900, 900), end=(2200, 3000))\nmask[rr, cc] = False\n\n# Display mask image\nfig, ax = plt.subplots()\nplt.imshow(mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f90c85810>\n\n\n\n\n\n\nimage_copy = image.copy()\n\n# Apply the mask\nimage_copy[mask] = 0\n\n\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f2f910be050>\n\n\n\n\n\n\n\nMasking - iterating through a co-ordinates file\nConsider this image of a 96-well plate that has been scanned on a flatbed scanner:\n\n# Load the image\nimage = iio.imread(uri=\"Images/wellplate-01.jpg\")\n\n# Display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f90fe3d90>\n\n\n\n\n\nSuppose that we are interested in the colours of the solutions in each of the wells. We do not care about the colour of the rest of the image, i.e., the plastic that makes up the well plate itself.\nYour task is to write some code that will produce a mask that will mask out everything except for the wells. To help with this, you should use the text file data/centers.txt that contains the (cx, ry) coordinates of the centre of each of the 96 wells in this image. You may assume that each of the wells has a radius of 16 pixels. Your program should produce output that looks like this:\n\n\n# read in original image\nimage = iio.imread(uri=\"Images/wellplate-01.jpg\")\n\n# create the mask image\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n# open and iterate through the centers file...\nwith open(\"Images/centers.txt\", \"r\") as center_file:\n    for line in center_file:\n        # ... getting the coordinates of each well...\n        coordinates = line.split()\n        cx = int(coordinates[0])\n        ry = int(coordinates[1])\n\n        # ... and drawing a circle on the mask\n        rr, cc = skimage.draw.disk(center=(ry, cx), radius=16, shape=image.shape[0:2])\n        mask[rr, cc] = False\n\n# apply the mask\nimage_copy = image.copy()\nimage_copy[mask] = 0\n\n# display the result\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f2f905eabc0>\n\n\n\n\n\n\n\nMasking - using nested for loops\n\n# read in original image\nimage = iio.imread(uri=\"Images/wellplate-01.jpg\")\n\n# create the mask image\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n# upper left well coordinates\ncx0 = 91\nry0 = 108\n\n# spaces between wells\ndeltaCX = 70\ndeltaRY = 72\n\ncx = cx0\nry = ry0\n\n# iterate each row and column\nfor row in range(12):\n    # reset cx to leftmost well in the row\n    cx = cx0\n    for col in range(8):\n\n        # ... and drawing a circle on the mask\n        rr, cc = skimage.draw.disk(center=(ry, cx), radius=16, shape=image.shape[0:2])\n        mask[rr, cc] = False\n        cx += deltaCX\n    # after one complete row, move to next row\n    ry += deltaRY\n\n# apply the mask\ncopy_image=image.copy()\ncopy_image[mask] = 0\n\n# display the result\nfig, ax = plt.subplots()\nplt.imshow(copy_image)\n\n<matplotlib.image.AxesImage at 0x7f2f905ea080>\n\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nwe can use the NumPy zeros() function to create a blank, black image\nwe can draw on skimage images with functions such as skimage.draw.rectangle(), skimage.draw.disk(), skimage.draw.line(), and more\nthe drawing functions return indices to pixels that can be set directly"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#creating-histograms",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#creating-histograms",
    "title": "Image Processing with Python",
    "section": "4. Creating Histograms",
    "text": "4. Creating Histograms\nIn this section, we will learn how to use skimage functions to create and display histograms for images.\n\nIntroduction to Histograms\nAs it pertains to images, a histogram is a graphical representation showing how frequently various colour values occur in the image. We saw in section 1 that we could use a histogram to visualise the differences in uncompressed and compressed image formats. If our project involves detecting colour changes between images, histograms will prove to be very useful, and histograms are also quite handy as a preparatory step before performing thresholding.\n\n\nGrayscale Histograms\nWe will start with grayscale images, and then move on to colour images. We will use an image of a plant seedling as an example. ere we load the image in grayscale instead of full colour, and display it.\nAgain, we use the iio.imread() function to load our image. The first argument to iio.imread() is the filename of the image. The second argument mode=“L” defines the type and depth of a pixel in the image (e.g., an 8-bit pixel has a range of 0-255). This argument is forwarded to the pillow backend, for which mode “L” means 8-bit pixels and single-channel (i.e., grayscale). pillow is a Python imaging library; which backend is used by iio.imread() may be specified (to use pillow, you would pass this argument: plugin=“pillow”); if unspecified, iio.imread() determines the backend to use based on the image type.\nThen, we convert the grayscale image of integer dtype, with 0-255 range, into a floating-point one with 0-1 range, by calling the function skimage.util.img_as_float. We will keep working with images in the value range 0 to 1 in this section.\n\nimport imageio.v3 as iio\nimport numpy as np\nimport skimage.color\nimport skimage.util\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# read the image of a plant seedling as grayscale from the outset\nimage = iio.imread(uri=\"Images/plant-seedling.jpg\", mode=\"L\")\n\n# convert the image to float dtype with a value range from 0 to 1\nimage = skimage.util.img_as_float(image)\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f90393a60>\n\n\n\n\n\nWe now use the function np.histogram to compute the histogram of our image which, after all, is a NumPy array.\nThe parameter bins determines the number of “bins” to use for the histogram. We pass in 256 because we want to see the pixel count for each of the 256 possible values in the grayscale image.\nThe parameter range is the range of values each of the pixels in the image can have. Here, we pass 0 and 1, which is the value range of our input image after transforming it to grayscale.\nThe first output of the np.histogram function is a one-dimensional NumPy array, with 256 rows and one column, representing the number of pixels with the intensity value corresponding to the index. I.e., the first number in the array is the number of pixels found with intensity value 0, and the final number in the array is the number of pixels found with intensity value 255.\nThe second output of np.histogram is an array with the bin edges and one column and 257 rows (one more than the histogram itself). There are no gaps between the bins, which means that the end of the first bin, is the start of the second and so on. For the last bin, the array also has to contain the stop, so it has one more element, than the histogram.\n\n# create the histogram\nhistogram, bin_edges = np.histogram(image, bins=256, range=(0, 1))\n\nNext, we turn our attention to displaying the histogram, by taking advantage of the plotting facilities of the matplotlib library. We create the plot with plt.figure(), then label the figure and the coordinate axes with plt.title(), plt.xlabel(), and plt.ylabel() functions. We then set the limits on the values on the x-axis with the plt.xlim([0.0, 1.0]) function call, before creating the histogram plot itself with plt.plot(bin_edges[0:-1], histogram).\nWe use the left bin edges as x-positions for the histogram values by indexing the bin_edges array to ignore the last value (the right edge of the last bin). When we run the program on this image of a plant seedling, it produces this histogram:\n\n# configure and draw the histogram figure\nplt.figure()\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixel count\")\nplt.xlim([0.0, 1.0])  # <- named arguments do not work here\n\nplt.plot(bin_edges[0:-1], histogram)  # <- or here\n\n\n\n\n\n\n\n\n\n\nNote that we cannot used named parameters for the plt.xlim() or plt.plot() functions.\n\n\n\nThis is because these functions are defined to take an arbitrary number of unnamed arguments. The designers wrote the functions this way because they are very versatile, and creating named parameters for all of the possible ways to use them would be complicated.\n\n\n\n\n\n\n\n\nMatplotlib provides a dedicated function to compute and display histograms: plt.hist().\n\n\n\nWe will not use it in this section in order to understand how to calculate histograms in more detail. In practice, it is a good idea to use this function, because it visualises histograms more appropriately than plt.plot(). Here, we could use it by calling:\n**plt.hist(image.flatten(), bins=256, range=(0, 1))** \ninstead of np.histogram() and plt.plot()\n.flatten() is a numpy function that converts our two-dimensional image into a one-dimensional array.This is because these functions are defined to take an arbitrary number of unnamed arguments. The designers wrote the functions this way because they are very versatile, and creating named parameters for all of the possible ways to use them would be complicated.\n\n\nLooking at the histogram above, you will notice that there is a large number of very dark pixels, as indicated in the chart by the spike around the grayscale value 0.12. That is not so surprising, since the original image is mostly black background. What if we want to focus more closely on the leaf of the seedling? That is where a mask enters the picture!\nLet’s hover over the plant seedling image with your mouse to determine the (x, y) coordinates of a bounding box around the leaf of the seedling. Then, using techniques from section 3, we can create a mask with a white rectangle covering that bounding box. After we have created the mask, we can apply it to the input image before passing it to the np.histogram function.\n\n# Load and display the original image\nimage = iio.imread(uri=\"Images/plant-seedling.jpg\", mode = \"L\")\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f8ba21de0>\n\n\n\n\n\n\ncopy_image = image.copy()\n\n# create mask here, using np.zeros() and skimage.draw.rectangle()\nmask = np.zeros(shape=copy_image.shape, dtype=\"bool\")\n\n# Draw filled rectangle on the mask image\n# co-ordinates are (row, column)\n# first co-ord is top-left, second co-ord is bottom-right\nrr, cc = skimage.draw.rectangle(start=(199,410), end=(384,485))\nmask[rr, cc] = True\n\n\n# Display the mask\nfig, ax = plt.subplots()\nplt.imshow(mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f8b83b700>\n\n\n\n\n\n\n# mask the image and create the new histogram\nhistogram, bin_edges = np.histogram(copy_image[mask], bins=256, range=(0.0, 1.0))\n\n# configure and draw the histogram figure\nplt.figure()\n\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixel count\")\nplt.xlim([0.0, 1.0])\nplt.plot(bin_edges[0:-1], histogram)\n\nYour histogram of the masked area should look something like this:\n\n\n\nColour Histograms\nWe can also create histograms for full colour images, in addition to grayscale histograms. We have seen colour histograms before, in the first section. A program to create colour histograms starts in a familiar way. We read the original image, now in full colour, and display it:\n\n# read original image, in full color\nimage = iio.imread(uri=\"Images/plant-seedling.jpg\")\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f8b772980>\n\n\n\n\n\nNext, we create the histogram, by calling the np.histogram function three times, once for each of the channels. We obtain the individual channels, by slicing the image along the last axis. For example, we can obtain the red colour channel by calling:\nr_chan = image[:, :, 0]\nWe will draw the histogram line for each channel in a different colour, and so we create a tuple of the colours to use for the three lines with the:\ncolors = (\"red\", \"green\", \"blue\")\nline of code. Then, we limit the range of the x-axis with the plt.xlim() function call.\nNext, we use the for control structure to iterate through the three channels, plotting an appropriately-coloured histogram line for each. This may be new Python syntax for you, so we will take a moment to discuss what is happening in the for statement.\nThe Python built-in enumerate() function takes a list and returns an iterator of tuples, where the first element of the tuple is the index and the second element is the element of the list.\n\n# tuple to select colors of each channel line\ncolors = (\"red\", \"green\", \"blue\")\n\n# create the histogram plot, with three lines, one for\n# each color\nplt.figure()\nplt.xlim([0, 256])\n\n# using a tuple, (channel_id, color), as the for variable\n\nfor channel_id, color in enumerate(colors):\n    histogram, bin_edges = np.histogram(\n        image[:, :, channel_id], bins=256, range=(0, 256)\n    )\n    plt.plot(bin_edges[0:-1], histogram, color=color)\n    \n# label our axes and display the histogram\nplt.title(\"Color Histogram\")\nplt.xlabel(\"Color value\")\nplt.ylabel(\"Pixel count\")\n\nText(0, 0.5, 'Pixel count')\n\n\n\n\n\nIn our colour histogram program, we are using a tuple, (channel_id, color), as the for variable. The first time through the loop, the channel_id variable takes the value 0, referring to the position of the red colour channel, and the color variable contains the string “red”. The second time through the loop the values are the green channels index 1 and “green”, and the third time they are the blue channel index 2 and “blue”.\nInside the for loop, our code looks much like it did for the grayscale example. We calculate the histogram for the current channel with the:\nhistogram, bin_edges = np.histogram(image[:, :, channel_id], bins=256, range=(0, 256))\nfunction call, and then add a histogram line of the correct colour to the plot with the:\nplt.plot(bin_edges[0:-1], histogram, color=color)\nfunction call.\n\n\n\n\n\n\nIterators, tuples, and enumerate()\n\n\n\nIn Python, an iterator, or an iterable object, is something that can be iterated over with the for control structure. A tuple is a sequence of objects, just like a list. However, a tuple cannot be changed, and a tuple is indicated by (parentheses) instead of [square brackets]. The enumerate() function takes an iterable object, and returns an iterator of tuples consisting of the 0-based index and the corresponding object.\n\n\nFor example, consider this small Python program:\n\nlist = (\"a\", \"b\", \"c\", \"d\", \"e\")\n\nfor x in enumerate(list):\n    print(x)\n\n(0, 'a')\n(1, 'b')\n(2, 'c')\n(3, 'd')\n(4, 'e')\n\n\n\n\nColour histogram with a mask\nWe can also apply a mask to the images we apply the colour histogram process to, in the same way we did for grayscale histograms. Consider this image of a well plate, where various chemical sensors have been applied to water and various concentrations of hydrochloric acid and sodium hydroxide:\n\nSuppose we are interested in the colour histogram of one of the sensors in the well plate image, specifically, the seventh well from the left in the topmost row, which shows Erythrosin B reacting with water.Hover over the image with your mouse to find the centre of that well and the radius (in pixels) of the well. Then create a circular mask to select only the desired well. Then, use that mask to apply the colour histogram operation to that well.\nYour masked image should look like this:\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nin many cases, we can load images in grayscale by passing the mode=“L” argument to the iio.imread() function.\nwe can create histograms of images with the np.histogram function.\nwe can separate the RGB channels of an image using slicing operations.\nwe can display histograms using the matplotlib pyplot figure(), title(), xlabel(), ylabel(), xlim(), plot(), and show() functions."
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#blurring-images",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#blurring-images",
    "title": "Image Processing with Python",
    "section": "5. Blurring Images",
    "text": "5. Blurring Images\nIn this section, we will learn how to use skimage functions to blur images.\nWhen processing an image, we are often interested in identifying objects represented within it so that we can perform some further analysis of these objects e.g. by counting them, measuring their sizes, etc. An important concept associated with the identification of objects in an image is that of edges: the lines that represent a transition from one group of similar pixels in the image to another different group. One example of an edge is the pixels that represent the boundaries of an object in an image, where the background of the image ends and the object begins.\nWhen we blur an image, we make the colour transition from one side of an edge in the image to another smooth rather than sudden. The effect is to average out rapid changes in pixel intensity. A blur is a very common operation we need to perform before other tasks such as thresholding.\nThere are several different blurring functions in the skimage.filters module, so we will focus on just one here, the Gaussian blur.\n\nFilters\nIn the day-to-day, macroscopic world, we have physical filters which separate out objects by size. A filter with small holes allows only small objects through, leaving larger objects behind. This is a good analogy for image filters. A high-pass filter will retain the smaller details in an image, filtering out the larger ones. A low-pass filter retains the larger features, analogous to what’s left behind by a physical filter mesh. High- and low-pass, here, refer to high and low spatial frequencies in the image. Details associated with high spatial frequencies are small, a lot of these features would fit across an image. Features associated with low spatial frequencies are large - maybe a couple of big features per image.\n\n\nBlurring\nBlurring is to make something less clear or distinct. This could be interpreted quite broadly in the context of image analysis - anything that reduces or distorts the detail of an image might apply. Applying a low pass filter, which removes detail occurring at high spatial frequencies, is perceived as a blurring effect. A Gaussian blur is a filter that makes use of a Gaussian kernel.\n\n\nKernels\nA kernel can be used to implement a filter on an image. A kernel, in this context, is a small matrix which is combined with the image using a mathematical technique: convolution. Different sizes, shapes and contents of kernel produce different effects. The kernel can be thought of as a little image in itself, and will favour features of a similar size and shape in the main image. On convolution with an image, a big, blobby kernel will retain big, blobby, low spatial frequency features.\n\n\nGaussian blur\nConsider this image of a cat, in particular the area of the image outlined by the white square.\n\n\n\ncat.jpg\n\n\nNow, zoom in on the area of the cat’s eye, as shown in the left-hand image below. When we apply a filter, we consider each pixel in the image, one at a time. In this example, the pixel we are currently working on is highlighted in red, as shown in the right-hand image.\n\n\n\ncat-eye-pixels.jpg\n\n\nWhen we apply a filter, we consider rectangular groups of pixels surrounding each pixel in the image, in turn. The kernel is another group of pixels (a separate matrix / small image), of the same dimensions as the rectangular group of pixels in the image, that moves along with the pixel being worked on by the filter. The width and height of the kernel must be an odd number, so that the pixel being worked on is always in its centre. In the example shown above, the kernel is square, with a dimension of seven pixels.\nTo apply the kernel to the current pixel, an average of the the colour values of the pixels surrounding it is calculated, weighted by the values in the kernel. In a Gaussian blur, the pixels nearest the centre of the kernel are given more weight than those far away from the centre. The rate at which this weight diminishes is determined by a Gaussian function, hence the name Gaussian blur.\nA Gaussian function maps random variables into a normal distribution or “Bell Curve”.\n\nhttps://en.wikipedia.org/wiki/Gaussian_function#/media/File:Normal_Distribution_PDF.svg\nThe shape of the function is described by a mean value μ, and a variance value σ². The mean determines the central point of the bell curve on the x axis, and the variance describes the spread of the curve. In fact, when using Gaussian functions in Gaussian blurring, we use a 2D Gaussian function to account for X and Y dimensions, but the same rules apply. The mean μ is always 0, and represents the middle of the 2D kernel. Increasing values of σ² in either dimension increases the amount of blurring in that dimension.\n\n\n\nGaussian_2D.png\n\n\nThe averaging is done on a channel-by-channel basis, and the average channel values become the new value for the pixel in the filtered image. Larger kernels have more values factored into the average, and this implies that a larger kernel will blur the image more than a smaller kernel. To get an idea of how this works, consider this plot of the two-dimensional Gaussian function:\n\n\n\ngaussian-kernel.png\n\n\nImagine that plot laid over the kernel for the Gaussian blur filter. The height of the plot corresponds to the weight given to the underlying pixel in the kernel. I.e., the pixels close to the centre become more important to the filtered pixel colour than the pixels close to the outer limits of the kernel. The shape of the Gaussian function is controlled via its standard deviation, or sigma:\n\na large sigma value results in a flatter shape, while\na smaller sigma value results in a more pronounced peak.\n\nThe mathematics involved in the Gaussian blur filter are not quite that simple, but this explanation gives you the basic idea. To illustrate the blur process, consider the blue channel colour values from the seven-by-seven region of the cat image above:\n\n\n\ncat-corner-blue.png\n\n\nThe filter is going to determine the new blue channel value for the centre pixel – the one that currently has the value 86. The filter calculates a weighted average of all the blue channel values in the kernel giving higher weight to the pixels near the centre of the kernel.\n\n\n\ncombination.png\n\n\nThis weighted average, the sum of the multiplications, becomes the new value for the centre pixel (3, 3). The same process would be used to determine the green and red channel values, and then the kernel would be moved over to apply the filter to the next pixel in the image.\n\n\nImage edges\nSomething different needs to happen for pixels near the outer limits of the image, since the kernel for the filter may be partially off the image. For example, what happens when the filter is applied to the upper-left pixel of the image? Here are the blue channel pixel values for the upper-left pixel of the cat image, again assuming a seven-by-seven kernel:\n\n\n\nedges.JPG\n\n\nThe upper-left pixel is the one with value 4. Since the pixel is at the upper-left corner, there are no pixels underneath much of the kernel; here, this is represented by x’s. So, what does the filter do in that situation? The default mode is to fill in the nearest pixel value from the image. For each of the missing x’s the image value closest to the x is used. If we fill in a few of the missing pixels, you will see how this works:\n\n\n\nedges_2.JPG\n\n\nAnother strategy to fill those missing values is to reflect the pixels that are in the image to fill in for the pixels that are missing from the kernel.\n\n\n\nreflect.JPG\n\n\nA similar process would be used to fill in all of the other missing pixels from the kernel. Other border modes are available; you can learn more about them in the skimage documentation. This animation shows how the blur kernel moves along in the original image in order to calculate the colour channel values for the blurred image.\n\n\n\nblur-demo.gif\n\n\nskimage has built-in functions to perform blurring for us, so we do not have to perform all of these mathematical operations ourselves. Let’s work through an example of blurring an image with the skimage Gaussian blur function.\nFirst, we load the image, and display it:\n\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\nimport skimage.filters\n\nimage = iio.imread(uri=\"Images/gaussian-original.png\")\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7fa014b526e0>\n\n\n\n\n\nNext, we apply the gaussian blur:\n\nsigma = 3.0\n\n# apply Gaussian blur, creating a new image\nblurred = skimage.filters.gaussian(\n    image, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n\nThe first two parameters to skimage.filters.gaussian() are the image to blur, image, and a tuple defining the sigma to use in ry- and cx-direction, (sigma, sigma). The third parameter truncate gives the radius of the kernel in terms of sigmas. A Gaussian function is defined from -infinity to +infinity, but our kernel (which must have a finite, smaller size) can only approximate the real function. Therefore, we must choose a certain distance from the centre of the function where we stop this approximation, and set the final size of our kernel. In the above example, we set truncate to 3.5, which means the kernel size will be 2 * sigma * 3.5. For example, for a sigma of 1.0 the resulting kernel size would be 7, while for a sigma of 2.0 the kernel size would be 14. The default value for truncate in scikit-image is 4.0.\nThe last parameter to skimage.filters.gaussian() tells skimage to interpret our image, that has three dimensions, as a multichannel colour image.\nFinally, we display the blurred image:\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n<matplotlib.image.AxesImage at 0x7f9fdadf2500>\n\n\n\n\n\nThe size and shape of the kernel used to blur an image can have a significant effect on the result of the blurring and any downstream analysis carried out on the blurred image. Let’s now experiment with the sigma values of the kernel, as this is a good way to develop our understanding of how the choice of kernel can influence the result of blurring.\n\n\n\n\n\n\nGenerally speaking, what effect does the sigma value have on the blurred image?\n\n\n\n\n\nGenerally speaking, the larger the sigma value, the more blurry the result. A larger sigma will tend to get rid of more noise in the image, which will help for other operations we will cover soon, such as thresholding. However, a larger sigma also tends to eliminate some of the detail from the image. So, we must strike a balance with the sigma value used for blur filters.\n\n\n\n\nsigma = 1.0\n\n# apply Gaussian blur, creating a new image\nblurred = skimage.filters.gaussian(\n    image, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n<matplotlib.image.AxesImage at 0x7f9fda93f7f0>\n\n\n\n\n\n\nsigma = 5.0\n\n# apply Gaussian blur, creating a new image\nblurred = skimage.filters.gaussian(\n    image, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<matplotlib.image.AxesImage at 0x7f9fda9b6470>\n\n\n\n\n\n\n\n\n\n\n\nKernel shape - what is the effect of applying an asymmetric kernel to blurring an image?\n\n\n\n\n\nThese unequal sigma values produce a kernel that is rectangular instead of square. The result is an image that is much more blurred in the x direction than the y direction or vice versa. For most use cases, a uniform blurring effect is desirable and this kind of asymmetric blurring should be avoided. However, it can be helpful in specific circumstances e.g. when noise is present in your image in a particular pattern or orientation, such as vertical lines, or when you want to remove uniform noise without blurring edges present in the image in a particular orientation.\n\n\n\n\n# apply Gaussian blur, with a sigma of 1.0 in the ry direction, and 6.0 in the cx direction\nblurred = skimage.filters.gaussian(\n    image, sigma=(1.0, 6.0), truncate=3.5, multichannel=True\n)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n/tmp/ipykernel_2635/3991388852.py:2: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n  blurred = skimage.filters.gaussian(\n\n\n<matplotlib.image.AxesImage at 0x7f9fda82f160>\n\n\n\n\n\n\n# apply Gaussian blur, with a sigma of 6.0 in the ry direction, and 1.0 in the cx direction\nblurred = skimage.filters.gaussian(\n    image, sigma=(6.0, 1.0), truncate=3.5, multichannel=True\n)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n/tmp/ipykernel_2635/1775552283.py:2: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n  blurred = skimage.filters.gaussian(\n\n\n<matplotlib.image.AxesImage at 0x7f9fda8a3ca0>\n\n\n\n\n\n\n\nOther methods of blurring\nThe Gaussian blur is a way to apply a low-pass filter in skimage. It is often used to remove Gaussian (i. e., random) noise from the image. For other kinds of noise, e.g. “salt and pepper” or “static” noise, a median filter is typically used. See the skimage.filters documentation for a list of available filters.\n\n\n\n\n\n\nKey Points:\n\n\n\n\napplying a low-pass blurring filter smooths edges and removes noise from an image\nblurring is often used as a first step before we perform thresholding or edge detection\nthe Gaussian blur can be applied to an image with the skimage.filters.gaussian() function\nlarger sigma values may remove more noise, but they will also remove detail from an image"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#thresholding",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#thresholding",
    "title": "Image Processing with Python",
    "section": "6. Thresholding",
    "text": "6. Thresholding\nIn this section, we will learn how to use skimage functions to apply thresholding to an image. Thresholding is a type of image segmentation, where we change the pixels of an image to make the image easier to analyze. In thresholding, we convert an image from colour or grayscale into a binary image, i.e., one that is simply black and white. Most frequently, we use thresholding as a way to select areas of interest of an image, while ignoring the parts we are not concerned with. We have already done some simple thresholding, in section 2 Working with skimage. In that case, we used a simple NumPy array manipulation to separate the pixels belonging to the root system of a plant from the black background. In this section, we will learn how to use skimage functions to perform thresholding. Then, we will use the masks returned by these functions to select the parts of an image we are interested in.\n\nSimple thresholding\nConsider the image Images/shapes-01.jpg with a series of crudely cut shapes set against a white background.\n\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport skimage.color\nimport skimage.filters\n\n%matplotlib inline\n\n# load the image\nimage = iio.imread(uri=\"Images/shapes-01.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f9fdae61720>\n\n\n\n\n\nNow suppose we want to select only the shapes from the image. In other words, we want to leave the pixels belonging to the shapes “on,” while turning the rest of the pixels “off,” by setting their colour channel values to zeros. The skimage library has several different methods of thresholding. We will start with the simplest version, which involves an important step of human input. Specifically, in this simple, fixed-level thresholding, we have to provide a threshold value t.\nThe process works like this. First, we will load the original image, convert it to grayscale, and de-noise it as in the Blurring Images section:\n\n# convert the image to grayscale\ngray_image = skimage.color.rgb2gray(image)\n\n# blur the image to denoise\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\n\nfig, ax = plt.subplots()\nplt.imshow(blurred_image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9c5f850>\n\n\n\n\n\nNext, we would like to apply the threshold t such that pixels with grayscale values on one side of t will be turned “on”, while pixels with grayscale values on the other side will be turned “off”. How might we do that? Remember that grayscale images contain pixel values in the range from 0 to 1, so we are looking for a threshold t in the closed range [0.0, 1.0]. We see in the image that the geometric shapes are “darker” than the white background but there is also some light gray noise on the background. One way to determine a “good” value for t is to look at the grayscale histogram of the image and try to identify what grayscale ranges correspond to the shapes in the image or the background.\nThe histogram for the shapes image shown above can be produced as in the Creating Histograms section.\n\n# create a histogram of the blurred grayscale image\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\n\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixels\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nSince the image has a white background, most of the pixels in the image are white. This corresponds nicely to what we see in the histogram: there is a peak near the value of 1.0. If we want to select the shapes and not the background, we want to turn off the white background pixels, while leaving the pixels for the shapes turned on. So, we should choose a value of t somewhere before the large peak and turn pixels above that value “off”. Let us choose t=0.8.\nTo apply the threshold t, we can use the numpy comparison operators to create a mask. Here, we want to turn “on” all pixels which have values smaller than the threshold, so we use the less operator < to compare the blurred_image to the threshold t. The operator returns a mask, that we capture in the variable binary_mask. It has only one channel, and each of its values is either 0 or 1. The binary mask created by the thresholding operation can be shown with plt.imshow, where the False entries are shown as black pixels (0-valued) and the True entries are shown as white pixels (1-valued).\n\n# create a mask based on the threshold\nt = 0.8\nbinary_mask = blurred_image < t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fda7add20>\n\n\n\n\n\nYou can see that the areas where the shapes were in the original area are now white, while the rest of the mask image is black.\n\n\nWhat makes a good threshold?\nAs is often the case, the answer to this question is “it depends”. In the example above, we could have just switched off all the white background pixels by choosing t=1.0, but this would leave us with some background noise in the mask image. On the other hand, if we choose too low a value for the threshold, we could lose some of the shapes that are too bright. We can experiment with the threshold by re-running the above code lines with different values for t.\nIn practice, it is a matter of domain knowledge and experience to interpret the peaks in the histogram so to determine an appropriate threshold. The process often involves trial and error, which is a drawback of the simple thresholding method. Below we will introduce automatic thresholding, which uses a quantitative, mathematical definition for a good threshold that allows us to determine the value of t automatically. It is worth noting that the principle for simple and automatic thresholding can also be used for images with pixel ranges other than [0.0, 1.0]. For example, we could perform thresholding on pixel intensity values in the range [0, 255] as we have already seen in the Image Representation in skimage section.\nWe can now apply the binary_mask to the original coloured image as we learned in the Drawing and Bitwise Operations section. What we are left with is only the coloured shapes from the original.\n\n# use the binary_mask to select the \"interesting\" part of the image\nselection = image.copy()\nselection[~binary_mask] = 0\n\nfig, ax = plt.subplots()\nplt.imshow(selection)\n\n<matplotlib.image.AxesImage at 0x7f9fd99cffa0>\n\n\n\n\n\nSuppose we want to use simple thresholding to select only the coloured shapes (in this particular case we consider grayish to be a colour, too) from the image data/shapes-02.jpg:\n\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport skimage.color\nimport skimage.filters\n\n%matplotlib inline\n\n# load the image\nimage = iio.imread(uri=\"Images/shapes-02.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f9fd9a56770>\n\n\n\n\n\n\n# convert the image to grayscale\ngray_image = skimage.color.rgb2gray(image)\n\n# blur the image to denoise\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\n\nfig, ax = plt.subplots()\nplt.imshow(blurred_image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9b598a0>\n\n\n\n\n\n\n# create a histogram of the blurred grayscale image\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\n\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixels\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nWe can see a large spike around 0.3, and a smaller spike around 0.7. The spike near 0.3 represents the darker background, so it seems like a value close to t=0.5 would be a good choice.\n\n\n\n\n\n\nNote that unlike the image with a white background we used above, here the peak for the background colour is at a lower gray level than the shapes. Therefore, change the comparison operator less < to greater > to create the appropriate mask. Then apply the mask to the image and view the thresholded image. If everything works as it should, our output should show only the coloured shapes on a black background.\n\n\n\n\n\n\n\n# create a mask based on the threshold\nt = 0.5\nbinary_mask = blurred_image > t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9845300>\n\n\n\n\n\nAnd here are the commands to apply the mask and view the thresholded image:\n\nimage = iio.imread(uri=\"Images/shapes-02.jpg\")\nselection = image.copy()\nselection[~binary_mask] = 0\n\nfig, ax = plt.subplots()\nplt.imshow(selection)\n\n<matplotlib.image.AxesImage at 0x7f9fd971ad40>\n\n\n\n\n\n\n\nAutomatic thresholding\nThe downside of the simple thresholding technique is that we have to make an educated guess about the threshold t by inspecting the histogram. There are also automatic thresholding methods that can determine the threshold automatically for us. One such method is **Otsu’s method. It is particularly useful for situations where the grayscale histogram of an image has two peaks that correspond to background and objects of interest.\n\n\nDenoising an image before thresholding\nIn practice, it is often necessary to denoise the image before thresholding, which can be done with one of the methods from the Blurring Images section. Consider the image data/maize-root-cluster.jpg of a maize root system which we saw before in the Image Representation in skimage section:\n\nimage = iio.imread(uri=\"Images/maize_cluster.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f9fda4de470>\n\n\n\n\n\nWe use Gaussian blur with a sigma of 1.0 to denoise the root image. Let us look at the grayscale histogram of the denoised image:\n\n# convert the image to grayscale\ngray_image = skimage.color.rgb2gray(image)\n\n# blur the image to denoise\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\n\n# show the histogram of the blurred image\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Graylevel histogram\")\nplt.xlabel(\"gray value\")\nplt.ylabel(\"pixel count\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nThe histogram has a significant peak around 0.2, and a second, smaller peak very near 1.0. Thus, this image is a good candidate for thresholding with Otsu’s method. The mathematical details of how this works are complicated (see the skimage documentation if you are interested), but the outcome is that Otsu’s method finds a threshold value between the two peaks of a grayscale histogram.\nThe skimage.filters.threshold_otsu() function can be used to determine the threshold automatically via Otsu’s method. Then numpy comparison operators can be used to apply it as before. Here are the Python commands to determine the threshold t with Otsu’s method:\n\n# perform automatic thresholding\nt = skimage.filters.threshold_otsu(blurred_image)\nprint(\"Found automatic threshold t = {}.\".format(t))\n\nFound automatic threshold t = 0.43393258215217667.\n\n\nFor this root image and a Gaussian blur with the chosen sigma of 1.0, the computed threshold value is 0.43. No we can create a binary mask with the comparison operator >. As we have seen before, pixels above the threshold value will be turned on, those below the threshold will be turned off.\n\n# create a binary mask with the threshold found by Otsu's method\nbinary_mask = blurred_image > t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9b58cd0>\n\n\n\n\n\nFinally, we use the mask to select the foreground:\n\n# apply the binary mask to select the foreground\nselection = image.copy()\nselection[~binary_mask] = 0\n\nfig, ax = plt.subplots()\nplt.imshow(selection)\n\n<matplotlib.image.AxesImage at 0x7f9fcfa9b550>\n\n\n\n\n\n\n\nApplication: measuring root mass\nLet us now turn to an application where we can apply thresholding and other techniques we have learned to this point. Consider these four maize root system images, which we can find in the files data/trial-016.jpg, data/trial-020.jpg, data/trial-216.jpg, and data/trial-293.jpg.\n\n\n\nfour-maize-roots.jpg\n\n\nSuppose we are interested in the amount of plant material in each image, and in particular how that amount changes from image to image. Perhaps the images represent the growth of the plant over time, or perhaps the images show four different maize varieties at the same phase of their growth. The question we would like to answer is, “how much root mass is in each image?”\nWe will first construct a Python program to measure this value for a single image. Our strategy will be this:\n\nRead the image, converting it to grayscale as it is read. For this application we do not need the colour image.\nBlur the image.\nUse Otsu’s method of thresholding to create a binary image, where the pixels that were part of the maize plant are white, and everything else is black.\nSave the binary image so it can be examined later.\nCount the white pixels in the binary image, and divide by the number of pixels in the image. This ratio will be a measure of the root mass of the plant in the image.\nOutput the name of the image processed and the root mass ratio.\n\nOur intent is to perform these steps and produce the numeric result - a measure of the root mass in the image - without human intervention. Implementing the steps within a Python function will enable us to call this function for different images.\nHere is a Python function that implements this root-mass-measuring strategy. Since the function is intended to produce numeric output without human interaction, it does not display any of the images. Almost all of the commands should be familiar, and in fact, it may seem simpler than the code we have worked on thus far, because we are not displaying any of the images.\n\ndef measure_root_mass(filename, sigma=1.0):\n\n    # read the original image, converting to grayscale on the fly\n    image = iio.imread(uri=filename, mode=\"L\")\n\n    # blur before thresholding\n    blurred_image = skimage.filters.gaussian(image, sigma=sigma)\n\n    # perform automatic thresholding to produce a binary image\n    t = skimage.filters.threshold_otsu(blurred_image)\n    binary_mask = blurred_image > t\n\n    # determine root mass ratio\n    rootPixels = np.count_nonzero(binary_mask)\n    w = binary_mask.shape[1]\n    h = binary_mask.shape[0]\n    density = rootPixels / (w * h)\n\n    return density\n\nThe function begins with reading the original image from the file filename. We use iio.imread() with the optional argument mode=“L” to automatically convert it to grayscale. Next, the grayscale image is blurred with a Gaussian filter with the value of sigma that is passed to the function. Then we determine the threshold t with Otsu’s method and create a binary mask just as we did in the previous section. Up to this point, everything should be familiar.\nThe final part of the function determines the root mass ratio in the image. Recall that in the binary_mask, every pixel has either a value of zero (black/background) or one (white/foreground). We want to count the number of white pixels, which can be accomplished with a call to the numpy function np.count_nonzero. Then we determine the width and height of the image by using the elements of binary_mask.shape (that is, the dimensions of the numpy array that stores the image). Finally, the density ratio is calculated by dividing the number of white pixels by the total number of pixels wh* in the image. The function returns then root density of the image.\nWe can call this function with any filename and provide a sigma value for the blurring. If no sigma value is provided, the default value 1.0 will be used. For example, for the file data/trial-016.jpg and a sigma value of 1.5, we would call the function like this:\n\nmeasure_root_mass(filename=\"Images/trial-016.jpg\", sigma=1.5)\n\n0.04907413563829787\n\n\nNow we can use the function to process the series of four images shown above. In a real-world scientific situation, there might be dozens, hundreds, or even thousands of images to process. To save us the tedium of calling the function for each image by hand, we can write a loop that processes all files automatically. The following code block assumes that the files are located in the same directory and the filenames all start with the trial- prefix and end with the .jpg suffix.\n\nall_files = glob.glob(\"Images/trial-*.jpg\")\nfor filename in all_files:\n    density = measure_root_mass(filename=filename, sigma=1.5)\n    # output in format suitable for .csv\n    print(filename, density, sep=\",\")\n\nImages/trial-216.jpg,0.1420516954787234\nImages/trial-016.jpg,0.04907413563829787\nImages/trial-293.jpg,0.13665458776595746\nImages/trial-020.jpg,0.06381349734042553\n\n\n\n\nIgnoring more of the images\nLet us take a closer look at the binary masks produced by the measure_root_mass function:\n\n\n\nfour-maize-roots-binary.jpg\n\n\nYou may have noticed in the section on automatic thresholding that the thresholded image does include regions of the image aside of the plant root: the numbered labels and the white circles in each image are preserved during the thresholding, because their grayscale values are above the threshold. Therefore, our calculated root mass ratios include the white pixels of the label and white circle that are not part of the plant root. Those extra pixels affect how accurate the root mass calculation is!\n\n\n\n\n\n\nHow might we remove the labels and circles before calculating the ratio, so that our results are more accurate? Think about some options given what we have learned so far.\n\n\n\n\n\nOne approach we might take is to try to completely mask out a region from each image, particularly, the area containing the white circle and the numbered label. If we had coordinates for a rectangular area on the image that contained the circle and the label, we could mask the area out easily by using techniques we learned in the Drawing and Bitwise Operations section.\nHowever, a closer inspection of the binary images raises some issues with that approach. Since the roots are not always constrained to a certain area in the image, and since the circles and labels are in different locations each time, we would have difficulties coming up with a single rectangle that would work for every image. We could create a different masking rectangle for each image, but that is not a practicable approach if we have hundreds or thousands of images to process.\nAnother approach we could take is to apply two thresholding steps to the image. Look at the graylevel histogram of the file data/trial-016.jpg shown above again. Notice the peak near 1.0? Recall that a grayscale value of 1.0 corresponds to white pixels: the peak corresponds to the white label and circle. So, we could use simple binary thresholding to mask the white circle and label from the image, and then we could use Otsu’s method to select the pixels in the plant portion of the image.\nNote that most of this extra work in processing the image could have been avoided during the experimental design stage, with some careful consideration of how the resulting images would be used. For example, all of the following measures could have made the images easier to process, by helping us predict and/or detect where the label is in the image and subsequently mask it from further processing:\n\nusing labels with a consistent size and shape\nplacing all the labels in the same position, relative to the sample\nusing a non-white label, with non-black writing\n\n\n\n\nLet’s now Implement an enhanced version of the function measure_root_mass that applies simple binary thresholding to remove the white circle and label from the image before applying Otsu’s method. We can apply a simple binary thresholding with a threshold t=0.95 to remove the label and circle from the image. We use the binary mask to set the pixels in the blurred image to zero (black):\n\ndef enhanced_root_mass(filename, sigma):\n\n    # read the original image, converting to grayscale on the fly\n    image = iio.imread(uri=filename, mode=\"L\")\n\n    # blur before thresholding\n    blurred_image = skimage.filters.gaussian(image, sigma=sigma)\n\n    # perform binary thresholding to mask the white label and circle\n    binary_mask = blurred_image < 0.95\n    # use the mask to remove the circle and label from the blurred image\n    blurred_image[~binary_mask] = 0\n\n    # perform automatic thresholding to produce a binary image\n    t = skimage.filters.threshold_otsu(blurred_image)\n    binary_mask = blurred_image > t\n\n    # determine root mass ratio\n    rootPixels = np.count_nonzero(binary_mask)\n    w = binary_mask.shape[1]\n    h = binary_mask.shape[0]\n    density = rootPixels / (w * h)\n\n    return density\n\nall_files = glob.glob(\"Images/trial-*.jpg\")\nfor filename in all_files:\n    density = enhanced_root_mass(filename=filename, sigma=1.5)\n    # output in format suitable for .csv\n    print(filename, density, sep=\",\")\n\nImages/trial-216.jpg,0.13761419547872342\nImages/trial-016.jpg,0.04632878989361702\nImages/trial-293.jpg,0.1323479055851064\nImages/trial-020.jpg,0.05924468085106383\n\n\nThe output of the improved program does illustrate that the white circles and labels were skewing our root mass ratios. The values generated by the enhanced function are lower.\nHere are the binary images produced by the additional thresholding. Note that we have not completely removed the offending white pixels. Outlines still remain. However, we have reduced the number of extraneous pixels, which should make the output more accurate.\n\n\n\nfour-maize-roots-binary-improved.jpg\n\n\n\n\nThresholding a bacteria colony image\nIn the images directory Images/, you will find an image named colonies-01.tif:\n\n\n\ncolonies-01.jpg\n\n\nThis is one of the images we will be working with in the morphometric challenge at the end of this blog. Let’s first plot and inspect the grayscale histogram of the image to determine a good threshold value for the image:\n\nimage = iio.imread(uri=\"Images/colonies-01.tif\")\ngray_image = skimage.color.rgb2gray(image)\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Graylevel histogram\")\nplt.xlabel(\"gray value\")\nplt.ylabel(\"pixel count\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nThe peak near one corresponds to the white image background, and the broader peak around 0.5 corresponds to the yellow/brown culture medium in the dish. The small peak near zero is what we are after: the dark bacteria colonies. A reasonable choice thus might be to leave pixels below t=0.2 on.\nNow let’s create the binary mask that leaves the pixels in the bacteria colonies “on” while turning the rest of the pixels in the image “off”. Here is the code to create and show the binarized image using the < operator with a threshold t=0.2:\n\nt = 0.2\nbinary_mask = blurred_image < t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd2f0dae0>\n\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nthresholding produces a binary image, where all pixels with intensities above (or below) a threshold value are turned on, while all other pixels are turned off\nthe binary images produced by thresholding are held in two-dimensional NumPy arrays, since they have only one colour value channel. They are boolean, hence they contain the values 0 (off) and 1 (on)\nthresholding can be used to create masks that select only the interesting parts of an image, or as the first step before edge detection or finding contours"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#connected-component-analysis",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#connected-component-analysis",
    "title": "Image Processing with Python",
    "section": "7. Connected Component Analysis",
    "text": "7. Connected Component Analysis\nIn the Thresholding section we covered dividing an image into foreground and background pixels. In the shapes example image, we considered the coloured shapes as foreground objects on a white background:\n\n\n\nshapes-01.jpg\n\n\nIn thresholding we went from the original image to this version:\n\n\n\nshapes-01-mask.png\n\n\nHere, we created a mask that only highlights the parts of the image that we find interesting, the objects. All objects have pixel value of True while the background pixels are False.\nBy looking at the mask image, we can count the objects that are present in the image (7). But how did we actually do that, how did we decide which lump of pixels constitutes a single object?\n\nPixel Neighborhoods\nIn order to decide which pixels belong to the same object, one can exploit their neighborhood: pixels that are directly next to each other and belong to the foreground class can be considered to belong to the same object.\nLet’s discuss the concept of pixel neighborhoods in more detail. Consider the following mask “image” with 8 rows, and 8 columns. For the purpose of illustration, the digit 0 is used to represent background pixels, and the letter X is used to represent object pixels foreground).\n0 0 0 0 0 0 0 0\n0 X X 0 0 0 0 0\n0 X X 0 0 0 0 0\n0 0 0 X X X 0 0\n0 0 0 X X X X 0\n0 0 0 0 0 0 0 0\nThe pixels are organised in a rectangular grid. In order to understand pixel neighborhoods we will introduce the concept of “jumps” between pixels. The jumps follow two rules:\n\nOnly one jump is allowed along the column, or the row. Diagonal jumps are not allowed. So, from a centre pixel, denoted with o, only the pixels indicated with a 1 are reachable\n\n\n\n\none_jump.JPG\n\n\nThe pixels on the diagonal (from o) are not reachable with a single jump, which is denoted by the -. The pixels reachable with a single jump form the 1-jump neighborhood.\n\nIn a sequence of jumps, one may only jump in row and column direction once -> they have to be orthogonal. An example of a sequence of orthogonal jumps is shown below. Starting from o the first jump goes along the row to the right. The second jump then goes along the column direction up. After this, the sequence cannot be continued as a jump has already been made in both row and column direction:\n\n\n\n\ntwo_jump.JPG\n\n\nAll pixels reachable with one, or two jumps form the 2-jump neighborhood. The grid below illustrates the pixels reachable from the centre pixel o with a single jump, highlighted with a 1, and the pixels reachable with 2 jumps with a 2.\n2 1 2\n1 o 1\n2 1 2\nIn the 1-jump version, only pixels that have direct neighbors along rows or columns are considered connected. Diagonal connections are not included in the 1-jump neighborhood. With two jumps, however, we only get a single object A because pixels are also considered connected along the diagonals.\n0 0 0 0 0 0 0 0\n0 A A 0 0 0 0 0\n0 A A 0 0 0 0 0\n0 0 0 A A A 0 0\n0 0 0 A A A A 0\n0 0 0 0 0 0 0 0\n\n\nObject counting\n\n\n\n\n\n\nConsider the mask below. How many objects with 1 orthogonal jump?\n\n\n\n\n\nFive.\n\n\n\n0 0 0 0 0 0 0 0\n0 X 0 0 0 X X 0\n0 0 X 0 0 0 0 0\n0 X 0 X X X 0 0\n0 X 0 X X 0 0 0\n0 0 0 0 0 0 0 0\n\n\n\n\n\n\nConsider the mask above. How many objects with 2 orthogonal jump?\n\n\n\n\n\nTwo.\n\n\n\n\n\nJumps and neighborhoods\nWe have just introduced how you can reach different neighboring pixels by performing one or more orthogonal jumps. We have used the terms 1-jump and 2-jump neighborhood. There is also a different way of referring to these neighborhoods: the 4- and 8-neighborhood.\nWith a single jump you can reach four pixels from a given starting pixel. Hence, the 1-jump neighborhood corresponds to the 4-neighborhood. When two orthogonal jumps are allowed, eight pixels can be reached, so the 2-jump neighborhood corresponds to the 8-neighborhood.\n\n\nConnected Component Analysis\nIn order to find the objects in an image, we want to employ an operation that is called Connected Component Analysis (CCA). This operation takes a binary image as an input. Usually, the False value in this image is associated with background pixels, and the True value indicates foreground, or object pixels. Such an image can be produced, e.g., with thresholding. Given a thresholded image, the connected component analysis produces a new labeled image with integer pixel values. Pixels with the same value, belong to the same object. Skimage provides connected component analysis in the function skimage.measure.label(). Let us add this function to the already familiar steps of thresholding an image. Here we define a reusable Python function connected_components:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport skimage.color\nimport skimage.filters\nimport skimage.measure\n\ndef connected_components(filename, sigma=1.0, t=0.5, connectivity=2):\n    # load the image\n    image = iio.imread(filename)\n    # convert the image to grayscale\n    gray_image = skimage.color.rgb2gray(image)\n    # denoise the image with a Gaussian filter\n    blurred_image = skimage.filters.gaussian(gray_image, sigma=sigma)\n    # mask the image according to threshold\n    binary_mask = blurred_image < t\n    # perform connected component analysis\n    labeled_image, count = skimage.measure.label(binary_mask,\n                                                 connectivity=connectivity, return_num=True)\n    return labeled_image, count\n\nNote the new import of skimage.measure in order to use the skimage.measure.label function that performs the CCA. The first four lines of code are familiar from the Thresholding section.\nThen we call the skimage.measure.label function. This function has one positional argument where we pass the binary_mask, i.e., the binary image to work on. With the optional argument connectivity, we specify the neighborhood in units of orthogonal jumps. For example, by setting connectivity=2 we will consider the 2-jump neighborhood introduced above. The function returns a labeled_image where each pixel has a unique value corresponding to the object it belongs to. In addition, we pass the optional parameter return_num=True to return the maximum label index as count.\n\n\nOptional parameters and return values\nThe optional parameter return_num changes the data type that is returned by the function skimage.measure.label. The number of labels is only returned if return_num is True. Otherwise, the function only returns the labeled image. This means that we have to pay attention when assigning the return value to a variable. If we omit the optional parameter return_num or pass return_num=False, we can call the function as:\n\nlabeled_image = skimage.measure.label(binary_mask)\n\nIf we pass return_num=True, the function returns a tuple and we can assign it as:\n\nlabeled_image, count = skimage.measure.label(binary_mask, return_num=True)\n\nIf we used the same assignment as in the first case, the variable labeled_image would become a tuple, in which labeled_image[0] is the image and labeled_image[1] is the number of labels. This could cause confusion if we assume that labeled_image only contains the image and pass it to other functions. If you get an AttributeError: ‘tuple’ object has no attribute ‘shape’ or similar, check if you have assigned the return values consistently with the optional parameters.\nWe can call the above function connected_components and display the labeled image like so:\n\nlabeled_image, count = connected_components(filename=\"Images/shapes-01.jpg\", sigma=2.0, t=0.9, connectivity=2)\n\nfig, ax = plt.subplots()\nplt.imshow(labeled_image)\nplt.axis(\"off\");\n\n\n\n\n\n\n\n\n\n\nColor mappings\n\n\n\nHere you might get a warning UserWarning: Low image data range; displaying image with stretched contrast. or just see an all black image (Note: this behavior might change in future versions or not occur with a different image viewer).\nWhat went wrong? When we hover over the black image, the pixel values are shown as numbers in the lower corner of the viewer. We an see that some pixels have values different from 0, so they are not actually pure black.\n\n\nLet’s find out more by examining labeled_image. Properties that might be interesting in this context are dtype, the minimum and maximum value. We can print them with the following lines:\n\nprint(\"dtype:\", labeled_image.dtype)\nprint(\"min:\", np.min(labeled_image))\nprint(\"max:\", np.max(labeled_image))\n\ndtype: int32\nmin: 0\nmax: 11\n\n\n\n\n\n\n\n\nint64\n\n\n\nIf the dtype of labeled_image is int64, this means that values in this image range from -2 ** 63 to 2 ** 63 - 1. Those are really big numbers. From this available space we only use the range from 0 to 11. When showing this image in the viewer, it squeezes the complete range into 256 gray values. Therefore, the range of our numbers will not produce any visible change.\n\n\nFortunately, the skimage library has tools to cope with this situation.\nWe can use the function skimage.color.label2rgb() to convert the colours in the image (recall that we already used the skimage.color.rgb2gray() function to convert to grayscale). With skimage.color.label2rgb(), all objects are coloured according to a list of colours that can be customised. We can use the following commands to convert and show the image:\n\n# convert the label image to color image\ncolored_label_image = skimage.color.label2rgb(labeled_image, bg_label=0)\n\nfig, ax = plt.subplots()\nplt.imshow(colored_label_image)\nplt.axis(\"off\");\n\n\n\n\n\n\nHow many objects are in that image?\nIt looks fairly obvious right? Seven. But, let’s practice. Using the function connected_components, we can find two ways of printing out the number of objects found in the image. As you might have guessed, the return value count already contains the number of found images. So it can simply be printed with:\n\nprint(\"Found\", count, \"objects in the image.\")\n\nFound 11 objects in the image.\n\n\nBut there is also a way to obtain the number of found objects from the labeled image itself. Recall that all pixels that belong to a single object are assigned the same integer value. The connected component algorithm produces consecutive numbers. The background gets the value 0, the first object gets the value 1, the second object the value 2, and so on. This means that by finding the object with the maximum value, we also know how many objects there are in the image. We can thus use the np.max function from Numpy to find the maximum value that equals the number of found objects:\n\nnum_objects = np.max(labeled_image)\nprint(\"Found\", num_objects, \"objects in the image.\")\n\nFound 11 objects in the image.\n\n\nInvoking the function with sigma=2.0, and threshold=0.9, both methods will print Found 11 objects in the image.\nYou might wonder why the connected component analysis with sigma=2.0, and threshold=0.9 finds 11 objects, whereas we would expect only 7 objects. Where are the four additional objects? With a bit of detective work, we can spot some small objects in the image, for example, near the left border:\n\n\n\nshapes-01-cca-detail.png\n\n\nFor us it is clear that these small spots are artifacts and not objects we are interested in. But how can we tell the computer? One way to calibrate the algorithm is to adjust the parameters for blurring (sigma) and thresholding (t), but you may have noticed during the above exercise that it is quite hard to find a combination that produces the right output number. In some cases, background noise gets picked up as an object. And with other parameters, some of the foreground objects get broken up or disappear completely. Therefore, we need other criteria to describe desired properties of the objects that are found.\n\n\nMorphometrics - Describe object features with numbers\nMorphometrics is concerned with the quantitative analysis of objects and considers properties such as size and shape. For the example of the images with the shapes, our intuition tells us that the objects should be of a certain size or area. So we could use a minimum area as a criterion for when an object should be detected. To apply such a criterion, we need a way to calculate the area of objects found by connected components. Recall how we determined the root mass in the Thresholding section by counting the pixels in the binary mask. But here we want to calculate the area of several objects in the labeled image. The skimage library provides the function skimage.measure.regionprops to measure the properties of labeled regions. It returns a list of RegionProperties that describe each connected region in the images. The properties can be accessed using the attributes of the RegionProperties data type. Here we will use the properties “area” and “label”. You can explore the skimage documentation to learn about other properties available.\nWe can get a list of areas of the labeled objects as follows:\n\n# compute object features and extract object areas\nobject_features = skimage.measure.regionprops(labeled_image)\nobject_areas = [objf[\"area\"] for objf in object_features]\nobject_areas\n\n[318542, 1, 523204, 496613, 517331, 143, 256215, 1, 68, 338784, 265755]\n\n\n\n\nPlot a histogram of the object area distribution\nit is often helpful to inspect the histogram of an object property. For example, we want to look at the distribution of the object areas.\n\nCreate and examine a histogram of the object areas obtained with skimage.measure.regionprops\nWhat does the histogram tell us about the objects?\n\n\n# plot the histogram\nfig, ax = plt.subplots()\nplt.hist(object_areas)\nplt.xlabel(\"Area (pixels)\")\nplt.ylabel(\"Number of objects\");\n\n\n\n\nThe histogram shows the number of objects (vertical axis) whose area is within a certain range (horizontal axis). The height of the bars in the histogram indicates the prevalence of objects with a certain area. The whole histogram tells us about the distribution of object sizes in the image. It is often possible to identify gaps between groups of bars (or peaks if we draw the histogram as a continuous curve) that tell us about certain groups in the image.\nIn this example, we can see that there are four small objects that contain less than 50000 pixels. Then there is a group of four (1+1+2) objects in the range between 200000 and 400000, and three objects with a size around 500000. For our object count, we might want to disregard the small objects as artifacts, i.e, we want to ignore the leftmost bar of the histogram. We could use a threshold of 50000 as the minimum area to count. In fact, the object_areas list already tells us that there are fewer than 200 pixels in these objects. Therefore, it is reasonable to require a minimum area of at least 200 pixels for a detected object. In practice, finding the “right” threshold can be tricky and usually involves an educated guess based on domain knowledge.\n\n\nFilter objects by area\nNow we would like to use a minimum area criterion to obtain a more accurate count of the objects in the image.\n\nOne way to count only objects above a certain area is to first create a list of those objects, and then take the length of that list as the object count. This can be done as follows:\n\n\nmin_area = 200\nlarge_objects = []\nfor objf in object_features:\n    if objf[\"area\"] > min_area:\n        large_objects.append(objf[\"label\"])\nprint(\"Found\", len(large_objects), \"objects!\")\n\nFound 7 objects!\n\n\n\nAnother option is to use Numpy arrays to create the list of large objects. We first create an array object_areas containing the object areas, and an array object_labels containing the object labels. The labels of the objects are also returned by skimage.measure.regionprops. We have already seen that we can create boolean arrays using comparison operators. Here we can use object_areas > min_area to produce an array that has the same dimension as object_labels. It can then used to select the labels of objects whose area is greater than min_area by indexing:\n\n\nobject_areas = np.array([objf[\"area\"] for objf in object_features])\nobject_labels = np.array([objf[\"label\"] for objf in object_features])\nlarge_objects = object_labels[object_areas > min_area]\nprint(\"Found\", len(large_objects), \"objects!\")\n\nFound 7 objects!\n\n\n\n\n\n\n\n\nNumPy vs for loops and if statements\n\n\n\nThe advantage of using Numpy arrays is that for loops and if statements in Python can be slow, and in practice the first approach may not be feasible if the image contains a large number of objects. In that case, Numpy array functions turn out to be very useful because they are much faster.\n\n\n\nIn this example, we can also use the np.count_nonzero function that we saw earlier together with the > operator to count the objects whose area is above min_area:\n\n\nn = np.count_nonzero(object_areas > min_area)\nprint(\"Found\", n, \"objects!\")\n\nFound 7 objects!\n\n\nFor all three alternatives, the output is the same and gives the expected count of 7 objects.\n\n\n\n\n\n\nUsing functions from Numpy and other Python packages\n\n\n\nFunctions from Python packages such as Numpy are often more efficient and require less code to write. It is a good idea to browse the reference pages of NumPy and skimage to look for an availabe function that can solve a given task.\n\n\n\n\nRemove small objects\nWe might also want to exclude (mask) the small objects when plotting the labeled image. Enhance the connected_components function such that it automatically removes objects that are below a certain area that is passed to the function as an optional parameter.\n\nTo remove the small objects from the labeled image, we change the value of all pixels that belong to the small objects to the background label 0. One way to do this is to loop over all objects and set the pixels that match the label of the object to :\n\n\nfor object_id, objf in enumerate(object_features, start=1):\n    if objf[\"area\"] < min_area:\n        labeled_image[labeled_image == objf[\"label\"]] = 0\n\n\nHere NumPy functions can also be used to eliminate for loops and if statements. Like above, we can create an array of the small object labels with the comparison object_areas < min_area. We can use another Numpy function, np.isin, to set the pixels of all small objects to 0. np.isin takes two arrays and returns a boolean array with values True if the entry of the first array is found in the second array, and False otherwise. This array can then be used to index the labeled_image and set the entries that belong to small objects to 0 :\n\n\nobject_areas = np.array([objf[\"area\"] for objf in object_features])\nobject_labels = np.array([objf[\"label\"] for objf in object_features])\nsmall_objects = object_labels[object_areas < min_area]\nlabeled_image[np.isin(labeled_image,small_objects)] = 0\n\n\nAn even more elegant way to remove small objects from the image is to leverage the skimage.morphology module. It provides a function skimage.morphology.remove_small_objects that does exactly what we are looking for. It can be applied to a binary image and returns a mask in which all objects smaller than min_area are excluded, i.e, their pixel values are set to False. We can then apply skimage.measure.label to the masked image:\n\n\nobject_mask = skimage.morphology.remove_small_objects(binary_mask,min_area)\nlabeled_image, n = skimage.measure.label(object_mask,\n                                         connectivity=2, return_num=True)\n\nUsing the skimage features, we can implement the enhanced_connected_component as follows:\n\ndef enhanced_connected_components(filename, sigma=1.0, t=0.5, connectivity=2, min_area=0):\n    image = iio.imread(filename)\n    gray_image = skimage.color.rgb2gray(image)\n    blurred_image = skimage.filters.gaussian(gray_image, sigma=sigma)\n    binary_mask = blurred_image < t\n    object_mask = skimage.morphology.remove_small_objects(binary_mask,min_area)\n    labeled_image, count = skimage.measure.label(object_mask,\n                                                 connectivity=connectivity, return_num=True)\n    return labeled_image, count\n\nWe can now call the function with a chosen min_area and display the resulting labeled image:\n\nlabeled_image, count = enhanced_connected_components(filename=\"Images/shapes-01.jpg\", sigma=2.0, t=0.9,\n                                                     connectivity=2, min_area=min_area)\ncolored_label_image = skimage.color.label2rgb(labeled_image, bg_label=0)\n\nfig, ax = plt.subplots()\nplt.imshow(colored_label_image)\nplt.axis(\"off\");\n\nprint(\"Found\", count, \"objects in the image.\")\n\nFound 7 objects in the image.\n\n\n\n\n\nNote that the small objects are “gone” and we obtain the correct number of 7 objects in the image.\n\n\nColour objects by area\nFinally, we would like to display the image with the objects coloured according to the magnitude of their area. In practice, this can be used with other properties to give visual cues of the object properties.\nWe already know how to get the areas of the objects from the regionprops. We just need to insert a zero area value for the background (to colour it like a zero size object). The background is also labeled 0 in the labeled_image, so we insert the zero area value in front of the first element of object_areas with np.insert. Then we can create a colored_area_image where we assign each pixel value the area by indexing the object_areas with the label values in labeled_image.\n\nobject_areas = np.array([objf[\"area\"] for objf in skimage.measure.regionprops(labeled_image)])\nobject_areas = np.insert(0,1,object_areas)\ncolored_area_image = object_areas[labeled_image]\n\nfig, ax = plt.subplots()\nim = plt.imshow(colored_area_image)\ncbar = fig.colorbar(im, ax=ax, shrink=0.85)\ncbar.ax.set_title(\"Area\")\nplt.axis(\"off\");\n\n\n\n\nYou may have noticed that in the solution, we have used the labeled_image to index the array object_areas. This is an example of advanced indexing in NumPy. The result is an array of the same shape as the labeled_image whose pixel values are selected from object_areas according to the object label. Hence the objects will be colored by area when the result is displayed. Note that advanced indexing with an integer array works slightly different than the indexing with a Boolean array that we have used for masking. While Boolean array indexing returns only the entries corresponding to the True values of the index, integer array indexing returns an array with the same shape as the index.\n\n\n\n\n\n\nKey Points:\n\n\n\n\nwe can use skimage.measure.label to find and label connected objects in an image\nwe can use skimage.measure.regionprops to measure properties of labeled objects\nwe can use skimage.morphology.remove_small_objects to mask small objects and remove artifacts from an image\nwe can display the labeled image to view the objects coloured by label"
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html",
    "title": "Audio Feature Extraction",
    "section": "",
    "text": "In this blog, I’ll be sharing how we can extract some prominent features from an audio file for further processing and analysis using Python, in particular the librosa, pydub and wave libraries."
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#audio-files-and-concepts",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#audio-files-and-concepts",
    "title": "Audio Feature Extraction",
    "section": "Audio files and concepts",
    "text": "Audio files and concepts\nIn audio data analysis, we process and transform audio signals captured by digital devices. Depending on how they’re captured, they can come in many different formats such as wav, mp3, m4a, aiff, and flac.\nQuoting Izotope.com, Waveform (wav) is one of the most popular digital audio formats. It is a lossless file format — which means it captures the closest mathematical representation of the original audio with no noticeable audio quality loss. In mp3 or m4a (Apple’s mp3 format) the data is compressed in such a way so it can be more easily distributed although in lower quality. In audio data analytics, most libraries support wav file processing.\nAs a form of a wave, sound/audio signal has the generic properties of:\n\nFrequency: occurrences of vibrations per unit of time\nAmplitude: maximum displacement or distance moved by a point on a wave measured from its equilibrium position; impacting the sound intensity\nSpeed of sound: distance traveled per unit of time by a soundwave\n\nThe information to be extracted from audio files are just transformations of the main properties above.\n\nExploratory analysis on audio files\nFor this analysis, I’m going to compare two demo tracks that our band Thirteen-Seven produced.\nThe files will be analyzed mainly with these Python packages:\n\nlibrosa for audio signal extraction and visualization\npydub for audio file manipulation\nwave for reading wav files"
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#general-audio-parameters",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#general-audio-parameters",
    "title": "Audio Feature Extraction",
    "section": "General audio parameters",
    "text": "General audio parameters\nJust like how we usually start evaluating tabular data by getting the statistical summary of the data (i.e using “Dataframe.describe” method), in the audio analysis we can start by getting the audio metadata summary. We can do so by utilizing the audiosegment module in pydub.\nBelow are some generic features that can be extracted:\n\nChannels: number of channels; 1 for mono, 2 for stereo audio\nSample width: number of bytes per sample; 1 means 8-bit, 2 means 16-bit, 3 means 24-bit, 4 means 32-bit\nFrame rate(sample rate): frequency of samples used (in Hertz)\nFrame width: Number of bytes for each “frame”. One frame contains a sample for each channel.\nLength: audio file length (in milliseconds)\nFrame count: the number of frames from the sample\nIntensity: loudness in dBFS (dB relative to the maximum possible loudness)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pydub import AudioSegment\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\n\n\n# Load in the track and create widget to listen\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\nipd.Audio(all_the_excuses, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Load files\nall_the_excuses = AudioSegment.from_file('Audio/all_the_excuses.wav')\n\n# Print attributes\nprint(f\"***All The Excuses - metadata***\")\nprint(f\"Channels:  {all_the_excuses.channels}\")\nprint(f\"Sample width: {all_the_excuses.sample_width}\")\nprint(f\"Frame rate (sample rate): {all_the_excuses.frame_rate}\")\nprint(f\"Frame width:  {all_the_excuses.frame_width}\")\nprint(f\"Length (ms): {len(all_the_excuses)}\")\nprint(f\"Frame count:  {all_the_excuses.frame_count()}\")\nprint(f\"Intensity: {all_the_excuses.dBFS}\")\n\n***All The Excuses - metadata***\nChannels:  2\nSample width: 2\nFrame rate (sample rate): 44100\nFrame width:  4\nLength (ms): 252891\nFrame count:  11152512.0\nIntensity: -10.902991191150802\n\n\n\n# Load in the track and create widget to listen\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\nipd.Audio(all_or_nothing, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nall_or_nothing = AudioSegment.from_file('Audio/all_or_nothing.wav')\n\n# Print attributes\nprint(f\"***All or Nothing - metadata***\")\nprint(f\"Channels:  {all_or_nothing.channels}\")\nprint(f\"Sample width: {all_or_nothing.sample_width}\")\nprint(f\"Frame rate (sample rate): {all_or_nothing.frame_rate}\")\nprint(f\"Frame width:  {all_or_nothing.frame_width}\")\nprint(f\"Length (ms): {len(all_or_nothing)}\")\nprint(f\"Frame count:  {all_or_nothing.frame_count()}\")\nprint(f\"Intensity: {all_or_nothing.dBFS}\")\n\n***All or Nothing - metadata***\nChannels:  2\nSample width: 2\nFrame rate (sample rate): 44100\nFrame width:  4\nLength (ms): 239099\nFrame count:  10544256.0\nIntensity: -10.614852809540894"
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#feature-extraction",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#feature-extraction",
    "title": "Audio Feature Extraction",
    "section": "Feature extraction",
    "text": "Feature extraction\nNumerous advanced features can be extracted and visualized using librosa to analyze audio characteristics.\n\nAmplitude envelope\nWe can visualize the amplitude over time of an audio file to get an idea of the wave movement using librosa:\n\n# Import required module\nimport librosa.display\nimport matplotlib.pyplot as plt\n\n\n# Load in our track\nall_the_excuses = 'Audio/all_the_excuses.wav'\nx , sr = librosa.load(all_the_excuses, sr=None)\n    \n# Plot the signal\nplt.figure(figsize=(15, 3))\nplt.title(\"Thirteen-Seven | All The Excuses - waveplot\")\nlibrosa.display.waveshow(x, sr=sr)\n\n<librosa.display.AdaptiveWaveplot at 0x7fbc37a9a260>\n\n\n\n\n\n\n# Load in our track\nall_or_nothing = 'Audio/all_or_nothing.wav'\nx , sr = librosa.load(all_or_nothing, sr=None)\n    \n# Import required module\nimport librosa.display\n\n# Plot the signal\nplt.figure(figsize=(15, 3))\nplt.title(\"Thirteen-Seven | All or Nothing - waveplot\")\nlibrosa.display.waveshow(x, sr=sr)\n\n<librosa.display.AdaptiveWaveplot at 0x7fbc2b8dcd60>\n\n\n\n\n\n\n\nSpectrogram\nThe extracted audio features can be visualized on a spectrogram. Quoting Wikipedia, a spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. It is usually depicted as a heat map, with the intensity shown on varying color gradients.\n\nimport librosa.display\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\nX = librosa.stft(all_the_excuses)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(15, 3))\nplt.title('Thirteen-Seven | All The Excuses - spectrogram')\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fbc29d14c40>\n\n\n\n\n\n\nimport librosa.display\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\nX = librosa.stft(all_or_nothing)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(15, 3))\nplt.title('Thirteen-Seven | All or Nothing - spectrogram')\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fbc29c08dc0>\n\n\n\n\n\nThe vertical axis shows frequency, the horizontal axis shows the time of the clip, and the color variation shows the intensity of the audio wave.\n\n\nRoot-mean-square (RMS)\nThe root-mean-square here refers to the total magnitude of the signal, which in layman terms can be interpreted as the loudness or energy parameter of the audio file.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\n# Get RMS value from each frame's magnitude value\nS, phase = librosa.magphase(librosa.stft(all_the_excuses))\nrms = librosa.feature.rms(S=S)\n\n\n# Plot the RMS energy\nfig, ax = plt.subplots(figsize=(15, 6), nrows=2, sharex=True)\ntimes = librosa.times_like(rms)\nax[0].semilogy(times, rms[0], label='RMS Energy')\nax[0].set(xticks=[])\nax[0].legend()\nax[0].label_outer()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[1])\nax[1].set(title='Thirteen-Seven | All The Excuses - log Power spectrogram')\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All The Excuses - log Power spectrogram')]\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\n# Get RMS value from each frame's magnitude value\nS, phase = librosa.magphase(librosa.stft(all_or_nothing))\nrms = librosa.feature.rms(S=S)\n\n\n# Plot the RMS energy\nfig, ax = plt.subplots(figsize=(15, 6), nrows=2, sharex=True)\ntimes = librosa.times_like(rms)\nax[0].semilogy(times, rms[0], label='RMS Energy')\nax[0].set(xticks=[])\nax[0].legend()\nax[0].label_outer()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[1])\nax[1].set(title='Thirteen-Seven | All or Nothing - log Power spectrogram')\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All or Nothing - log Power spectrogram')]\n\n\n\n\n\nHere we can see the RMS values are consistently high (until the very end of the tracks) as this rock music is loud and intense throughout.\n\n\nZero crossing rate\nQuoting Wikipedia, zero-crossing rate (ZCR) is the rate at which a signal changes from positive to zero to negative or from negative to zero to positive. Its value has been widely used in both speech recognition and music information retrieval, being a key feature to classify percussive sounds. Highly percussive sounds like rock, metal, emo, or punk music tend to have higher zero-crossing rate values.\nWe can get this data manually by zooming into a certain frame in the amplitude time series, counting the times it passes zero value in the y-axis and extrapolating for the whole audio. Alternatively, there is a function in librosa that we can use to get the zero-crossing state and rate.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\nzcrs = librosa.feature.zero_crossing_rate(all_the_excuses)\n                     \nprint(f\"Zero crossing rate: {sum(librosa.zero_crossings(all_the_excuses))}\")\nplt.figure(figsize=(15, 3))\nplt.plot(zcrs[0])\nplt.title('Thirteen-Seven | All The Excuses - zero-crossing rate (ZCR)')\n\nZero crossing rate: 706615\n\n\nText(0.5, 1.0, 'Thirteen-Seven | All The Excuses - zero-crossing rate (ZCR)')\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\nzcrs = librosa.feature.zero_crossing_rate(all_or_nothing)\n                     \nprint(f\"Zero crossing rate: {sum(librosa.zero_crossings(all_or_nothing))}\")\nplt.figure(figsize=(15, 3))\nplt.plot(zcrs[0])\nplt.title('Thirteen-Seven | All or Nothing - zero-crossing rate (ZCR)')\n\nZero crossing rate: 679083\n\n\nText(0.5, 1.0, 'Thirteen-Seven | All or Nothing - zero-crossing rate (ZCR)')\n\n\n\n\n\nAbove is the zero crossing value and rate for the track. Here we can see the zero-crossing rate is high as it is a highly percussive rock song.\n\n\nMel-Frequency Cepstral Coefficients (MFCCs)\nQuoting Analytics Vidhya, humans do not perceive frequencies on a linear scale. We are better at detecting differences in lower frequencies than higher frequencies, even if the gap is the same (i.e 50 and 1,000 Hz vs 10,000 and 10,500 Hz). In Mel-scale, equal distances in pitch sounded equally distant to the listener.\nMel-Frequency Cepstral Coefficients (MFCCs) is a representation of the short-term power spectrum of a sound, based on some transformation in a Mel-scale. It is commonly used in speech recognition as people’s voices are usually on a certain range of frequency and different from one to another. Getting and displaying MFCCs is quite straightforward in Librosa.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\nmfccs = librosa.feature.mfcc(all_the_excuses, sr=sr)\n\n#Displaying  the MFCCs:\nfig,ax = plt.subplots(figsize=(15, 3))\nimg = librosa.display.specshow(mfccs, sr=sr, x_axis='time')\nfig.colorbar(img, ax=ax)\n                     \nax.set(title='Thirteen-Seven | All The Excuses - Mel-Frequency Cepstral Coefficients (MFCCs')\n\n/tmp/ipykernel_64/1341239866.py:2: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  mfccs = librosa.feature.mfcc(all_the_excuses, sr=sr)\n\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All The Excuses - Mel-Frequency Cepstral Coefficients (MFCCs')]\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\nmfccs = librosa.feature.mfcc(all_or_nothing, sr=sr)\n\n#Displaying  the MFCCs:\nfig,ax = plt.subplots(figsize=(15, 3))\nimg = librosa.display.specshow(mfccs, sr=sr, x_axis='time')\nfig.colorbar(img, ax=ax)\n                     \nax.set(title='Thirteen-Seven | All or Nothing - Mel-Frequency Cepstral Coefficients (MFCCs')\n\n/tmp/ipykernel_64/2838893102.py:2: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  mfccs = librosa.feature.mfcc(all_or_nothing, sr=sr)\n\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All or Nothing - Mel-Frequency Cepstral Coefficients (MFCCs')]\n\n\n\n\n\n\n\nChroma\nWe can use Chroma feature visualization to know how dominant the characteristics of a certain pitch {C, C♯, D, D♯, E, F, F♯, G, G♯, A, A♯, B} is present in the sampled frame.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\nhop_length = 512\n\nchromagram = librosa.feature.chroma_stft(all_the_excuses, sr=sr, hop_length=hop_length)\n\nplt.figure(figsize=(15, 5))\nplt.title('Thirteen-Seven | All The Excuses - chromagram')\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\n\n/tmp/ipykernel_64/4136034188.py:5: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  chromagram = librosa.feature.chroma_stft(all_the_excuses, sr=sr, hop_length=hop_length)\n\n\n<matplotlib.collections.QuadMesh at 0x7fbc29847760>\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\nhop_length = 512\n\nchromagram = librosa.feature.chroma_stft(all_or_nothing, sr=sr, hop_length=hop_length)\n\nplt.figure(figsize=(15, 5))\nplt.title('Thirteen-Seven | All or Nothing - chromagram')\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\n\n/tmp/ipykernel_64/626002655.py:5: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  chromagram = librosa.feature.chroma_stft(all_or_nothing, sr=sr, hop_length=hop_length)\n\n\n<matplotlib.collections.QuadMesh at 0x7fbc298ddfc0>\n\n\n\n\n\n\n\nTempogram\nTempo refers to the rate of the musical beat and is given by the reciprocal of the beat period. Tempo is often defined in units of beats per minute (BPM). Tempo can vary locally within a piece. Therefore, we introduce the tempogram (FMP, p. 317) as a feature matrix which indicates the prevalence of certain tempo at each moment in time.\n\n# Estimate the tempo:\ntempo = librosa.beat.tempo(all_the_excuses, sr=sr)\ntempo\n\n/tmp/ipykernel_64/3447807976.py:2: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.15874578 -0.19955122\n -0.38175374] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempo = librosa.beat.tempo(all_the_excuses, sr=sr)\n\n\narray([172.265625])\n\n\n\n# Visualize the tempo estimate on top of the input signal\nT = len(all_the_excuses)/float(sr)\nseconds_per_beat = 60.0/tempo[0]\nbeat_times = np.arange(0, T, seconds_per_beat)\n\nlibrosa.display.waveshow(all_the_excuses)\nplt.vlines(beat_times, -1, 1, color='r')\nplt.title(\"Thirteen-Seven | All The Excuses - estimated tempo plot\")\n\nText(0.5, 1.0, 'Thirteen-Seven | All The Excuses - estimated tempo plot')\n\n\n\n\n\n\n# Listen to the input signal with a click track using the tempo estimate:\nclicks = librosa.clicks(beat_times, sr, length=len(all_the_excuses))\nipd.Audio(all_the_excuses + clicks, rate=sr)\n\n/tmp/ipykernel_64/625036389.py:1: FutureWarning: Pass times=[ 0.          0.53405896  1.06811791  1.60217687  2.13623583  2.67029478\n  3.20435374  3.7384127   4.27247166  4.80653061  5.34058957  5.87464853\n  6.40870748  6.94276644  7.4768254   8.01088435  8.54494331  9.07900227\n  9.61306122 10.14712018 10.68117914 11.2152381  11.74929705 12.28335601\n 12.81741497 13.35147392 13.88553288 14.41959184 14.95365079 15.48770975\n 16.02176871 16.55582766 17.08988662 17.62394558 18.15800454 18.69206349\n 19.22612245 19.76018141 20.29424036 20.82829932 21.36235828 21.89641723\n 22.43047619 22.96453515 23.4985941  24.03265306 24.56671202 25.10077098\n 25.63482993 26.16888889 26.70294785 27.2370068  27.77106576 28.30512472\n 28.83918367 29.37324263 29.90730159], frames=22050 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  clicks = librosa.clicks(beat_times, sr, length=len(all_the_excuses))\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Estimate the tempo:\ntempo = librosa.beat.tempo(all_or_nothing, sr=sr)\ntempo\n\n/tmp/ipykernel_64/3362745988.py:2: FutureWarning: Pass y=[0.         0.         0.         ... 0.3365013  0.3631341  0.37732217] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempo = librosa.beat.tempo(all_or_nothing, sr=sr)\n\n\narray([112.34714674])\n\n\n\n# Visualize the tempo estimate on top of the input signal\nT = len(all_or_nothing)/float(sr)\nseconds_per_beat = 60.0/tempo[0]\nbeat_times = np.arange(0, T, seconds_per_beat)\n\nlibrosa.display.waveshow(all_or_nothing)\nplt.vlines(beat_times, -1, 1, color='r')\nplt.title(\"Thirteen-Seven | All or Nothing - estimated tempo plot\")\n\nText(0.5, 1.0, 'Thirteen-Seven | All or Nothing - estimated tempo plot')\n\n\n\n\n\n\n# Listen to the input signal with a click track using the tempo estimate:\nclicks = librosa.clicks(beat_times, sr, length=len(all_or_nothing))\nipd.Audio(all_or_nothing + clicks, rate=sr)\n\n/tmp/ipykernel_64/1680592788.py:2: FutureWarning: Pass times=[ 0.          0.53405896  1.06811791  1.60217687  2.13623583  2.67029478\n  3.20435374  3.7384127   4.27247166  4.80653061  5.34058957  5.87464853\n  6.40870748  6.94276644  7.4768254   8.01088435  8.54494331  9.07900227\n  9.61306122 10.14712018 10.68117914 11.2152381  11.74929705 12.28335601\n 12.81741497 13.35147392 13.88553288 14.41959184 14.95365079 15.48770975\n 16.02176871 16.55582766 17.08988662 17.62394558 18.15800454 18.69206349\n 19.22612245 19.76018141 20.29424036 20.82829932 21.36235828 21.89641723\n 22.43047619 22.96453515 23.4985941  24.03265306 24.56671202 25.10077098\n 25.63482993 26.16888889 26.70294785 27.2370068  27.77106576 28.30512472\n 28.83918367 29.37324263 29.90730159], frames=22050 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  clicks = librosa.clicks(beat_times, sr, length=len(all_or_nothing))\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "",
    "text": "This blog has been produced after working through the Introduction to Geospatial Raster and Vector data with Python lesson provided by Data Carpentry."
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-data",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-data",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "1. Raster data",
    "text": "1. Raster data\nRaster data is any pixelated (or gridded) data where each pixel is associated with a specific geographic location. The value of a pixel can be continuous (e.g. elevation) or categorical (e.g. land use). If this sounds familiar, it is because this data structure is very common: it’s how we represent any digital image. A geospatial raster is only different from a digital photo in that it is accompanied by spatial information that connects the data to a particular location. This includes the raster’s extent and cell size, the number of rows and columns, and its coordinate reference system (or CRS).\n\nSome examples of continuous rasters include:\n\nPrecipitation maps\nMaps of tree height derived from LiDAR data\nElevation values for a region\n\nA map of elevation for Harvard Forest derived from the NEON AOP LiDAR sensor is below. Elevation is represented as a continuous numeric variable in this map. The legend shows the continuous range of values in the data from around 300 to 420 meters:\n\nSome rasters contain categorical data where each pixel represents a discrete class such as a landcover type (e.g., “forest” or “grassland”) rather than a continuous value such as elevation or temperature. Some examples of classified maps include:\n\nLandcover / land-use maps\nTree height maps classified as short, medium, and tall trees\nElevation maps classified as low, medium, and high elevation\n\n\nThe map above shows the contiguous United States with landcover as categorical data. Each color is a different landcover category. (Source: Homer, C.G., et al., 2015, Completion of the 2011 National Land Cover Database for the conterminous United States-Representing a decade of land cover change information. Photogrammetric Engineering and Remote Sensing, v. 81, no. 5, p. 345-354)\n\n\n\n\n\n\nCan you think of potential advantages and disadvantages of storing data in raster format?\n\n\n\n\n\nRaster data has some important advantages:\n\nrepresentation of continuous surfaces\npotentially very high levels of detail\ndata is ‘unweighted’ across its extent - the geometry doesn’t implicitly highlight features\ncell-by-cell calculations can be very fast and efficient\n\nThe downsides of raster data are:\n\nvery large file sizes as cell size gets smaller\ncurrently popular formats don’t embed metadata well (more on this later!)\ncan be difficult to represent complex information\n\n\n\n\n\nExtent\nThe spatial extent is the geographic area that the raster data covers. The spatial extent of an object represents the geographic edge or location that is the furthest north, south, east and west. In other words, extent represents the overall geographic coverage of the spatial object.\n\n\n\n\n\n\n\nIn the image above, the dashed boxes around each set of objects seems to imply that the three objects have the same extent. Is this accurate? If not, which object(s) have a different extent?\n\n\n\n\n\nThe lines and polygon objects have the same extent. The extent for the points object is smaller in the vertical direction than the other two because there are no points on the line at y = 8.\n\n\n\n\n\nResolution\nA resolution of a raster represents the area on the ground that each pixel of the raster covers. The image below illustrates the effect of changes in resolution:\n\n\n\nRaster data format\nRaster data can come in many different formats. In this blog, we will use the GeoTIFF format which has the extension .tif. A .tif file stores metadata or attributes about the file as embedded tif tags. For instance, your camera might store a tag that describes the make and model of the camera or the date the photo was taken when it saves a .tif. A GeoTIFF is a standard .tif image format with additional spatial (georeferencing) information embedded in the file as tags. These tags should include the following raster metadata:\n\nextent\nresolution\nCoordinate Reference System (CRS) - this concept will be introduced later\nvalues that represent missing data (NoDataValue) - this concept will be introduced later\n\n\n\n\n\n\n\nMore resources on the .tif format\n\n\n\n\nGeoTIFF on Wikipedia\n\n\n\n\n\nMulti-band Raster data\nA raster can contain one or more bands. One type of multi-band raster dataset that is familiar to many of us is a colour image. A basic colour image consists of three bands: red, green, and blue. Each band represents light reflected from the red, green or blue portions of the electromagnetic spectrum. The pixel brightness for each band, when composited creates the colours that we see in an image.\n\nWe can plot each band of a multi-band image individually. Or we can composite all three bands together to make a colour image. In a multi-band dataset, the rasters will always have the same extent, resolution, and CRS.\n\n\nOther Types of Multi-band Raster Data\nMulti-band raster data might also contain:\n\nTime series: the same variable, over the same area, over time\nMultispectral imagery: image rasters that have 4 or more bands\nhyperspectral imagery: image rasters with more than 10-15 bands\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- raster data is pixelated data where each pixel is associated with a specific location\n- raster data always has an extent and a resolution\n- the extent is the geographical area covered by a raster\n- the resolution is the area covered by each pixel of a raster"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "2. Vector data",
    "text": "2. Vector data\nVector data structures represent specific features on the Earth’s surface, and assign attributes to those features. Vectors are composed of discrete geometric locations (x, y values) known as vertices that define the shape of the spatial object. The organization of the vertices determines the type of vector that we are working with:\n\npoint;\nline; or\npolygon\n\nVector datasets are in use in many industries besides geospatial fields. For instance, computer graphics are largely vector-based, although the data structures in use tend to join points using arcs and complex curves rather than straight lines. Computer-aided design (CAD) is also vector- based. The difference is that geospatial datasets are accompanied by information tying their features to real-world locations.\n\n\nPoints\nEach point is defined by a single x, y coordinate. There can be many points in a vector point file. Examples of point data include: sampling locations, the location of individual trees, or the location of survey plots.\n\n\nLines\nLines are composed of many (at least 2) points that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has a defined x, y location.\n\n\nPolygons\nA polygon consists of 3 or more vertices that are connected and closed. The outlines of survey plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\n\n\n\nData tip\n\n\n\nSometimes, boundary layers such as states and countries, are stored as lines rather than polygons. However, these boundaries, when represented as a line, will not create a closed object with a defined area that can be filled.\n\n\n\n\n\n\n\n\nThe plot below includes examples of two of the three types of vector objects. Use the definitions above to identify which features are represented by which vector type.\n\n\n\n\n\nState boundaries are polygons. The Fisher Tower location is a point. There are no line features shown.\n\n\n\n\n\n\nAdvantages of vector data\n\nthe geometry itself contains information about what the dataset creator thought was important\nthe geometry structures hold information in themselves - why choose point over polygon, for instance?\neach geometry feature can carry multiple attributes instead of just one, e.g. a database of cities can have attributes for name, country, population, etc\ndata storage can be very efficient compared to rasters\n\n\n\nDisadvantages of vector data\n\npotential loss of detail compared to raster\npotential bias in datasets - what didn’t get recorded?\ncalculations involving multiple vector layers need to do math on the geometry as well as the attributes, so can be slow compared to raster math\n\n\n\nVector data format\nLike raster data, vector data can also come in many different formats. For this blog, we will use the Shapefile format. A Shapefile format consists of multiple files in the same directory, of which .shp, .shx, and .dbf files are mandatory. Other non-mandatory but very important files are .prj and shp.xml files.\n\nthe .shp file stores the feature geometry itself\n.shx is a positional index of the feature geometry to allow quickly searching forwards and backwards the geographic coordinates of each vertex in the vector\n.dbf contains the tabular attributes for each shape.\n.prj file indicates the Coordinate reference system (CRS)\n.shp.xml contains the Shapefile metadata.\n\nTogether, the Shapefile includes the following information:\n\nextent - the spatial extent of the shapefile (i.e. geographic area that the shapefile covers). The spatial extent for a shapefile represents the combined extent for all spatial objects in the shapefile.\nobject type - whether the shapefile includes points, lines, or polygons.\nCoordinate reference system (CRS)\nother attributes - for example, a line shapefile that contains the locations of streams, might contain the name of each stream.\n\nBecause the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\n\n\n\n\n\n\nMore resources on Shapefiles\n\n\n\nMore about shapefiles can be found on Wikipedia. Shapefiles are often publicly available from government services, such as this page from the US Census Bureau or this one from Australia’s Data.gov.au website.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- vector data structures represent specific features on the Earth’s surface along with attributes of those features\n- vector objects are either points, lines, or polygons"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#coordinate-reference-systems",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#coordinate-reference-systems",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "3. Coordinate Reference Systems",
    "text": "3. Coordinate Reference Systems\nA data structure cannot be considered geospatial unless it is accompanied by coordinate reference system (CRS) information, in a format that geospatial applications can use to display and manipulate the data correctly. CRS information connects data to the Earth’s surface using a mathematical model. The CRS associated with a dataset tells your mapping software (for example Python) where the raster is located in geographic space. It also tells the mapping software what method should be used to flatten or project the raster in geographic space.\nCRS and SRS (spatial reference system) are synonyms and are commonly interchanged. We will use only the term CRS throughout this blog.\n\nThe above image shows maps of the United States in different projections. Notice the differences in shape associated with each projection. These differences are a direct result of the calculations used to flatten the data onto a 2-dimensional map. (Source: opennews.org)\nThere are lots of great resources that describe coordinate reference systems and projections in greater detail.\n\n\n\n\n\n\nData from the same location but saved in different projections will not line up in any Geographic Information System(GIS) or other program.\n\n\n\nIt’s important when working with spatial data to identify the coordinate reference system applied to the data and retain it throughout data processing and analysis.\n\n\n\nComponents of a CRS\nCRS information has three components:\n\nDatum: A model of the shape of the earth. It has angular units (i.e. degrees) and defines the starting point (i.e. where is [0,0]?) so the angles reference a meaningful spot on the earth. Common global datums are WGS84 and NAD83. Datums can also be local - fit to a particular area of the globe, but ill-fitting outside the area of intended use. In this blog, we will use the WGS84 datum.\nProjection: A mathematical transformation of the angular measurements on a round earth to a flat surface (i.e. paper or a computer screen). The units associated with a given projection are usually linear (feet, meters, etc.). In this workshop, we will see data in two different projections.\nAdditional Parameters: Additional parameters are often necessary to create the full coordinate reference system. One common additional parameter is a definition of the center of the map. The number of required additional parameters depends on what is needed by each specific projection.\n\n\n\n\n\n\n\nOrange peel analogy\n\n\n\nA common analogy employed to teach projections is the orange peel analogy. If you imagine that the Earth is an orange, how you peel it and then flatten the peel is similar to how projections get made.\n\n\n\n\n\nWhich projection should we use?\nTo decide if a projection is right for our data, answer these questions:\n\nWhat is the area of minimal distortion?\nWhat aspect of the data does it preserve?\n\nPeter Dana from the University of Colorado at Boulder and the Department of Geo-Information Processing has a good discussion of these aspects of projections. Online tools like Projection Wizard can also help discover projections that might be a good fit for our data.\n\n\nDescribing Coordinate Reference Systems\nThere are several common systems in use for storing and transmitting CRS information, as well as translating among different CRSs. These systems generally comply with ISO 19111. Common systems for describing CRSs include EPSG, OGC WKT, and PROJ strings.\n\n\nEPSG\nThe EPSG system is a database of CRS information maintained by the International Association of Oil and Gas Producers. The dataset contains both CRS definitions and information on how to safely convert data from one CRS to another. Using EPSG is easy as every CRS has an integer identifier, e.g. WGS84 is EPSG:4326. The downside is that you can only use the CRSs defined by EPSG and cannot customise them (some datasets do not have EPSG codes). epsg.io is an excellent website for finding suitable projections by location or for finding information about a particular EPSG code.\n\n\nWell-Known Text\nThe Open Geospatial Consortium WKT standard is used by a number of important geospatial apps and software libraries. WKT is a nested list of geodetic parameters. The structure of the information is defined on their website. WKT is valuable in that the CRS information is more transparent than in EPSG, but can be more difficult to read and compare than PROJ since it is meant to necessarily represent more complex CRS information. Additionally, the WKT standard is implemented inconsistently across various software platforms, and the spec itself has some known issues.\n\n\nPROJ\nPROJ is an open-source library for storing, representing and transforming CRS information.\n\n\n\n\n\n\nPROJ strings continue to be used, but the format is deprecated by the PROJ C maintainers due to inaccuracies when converting to the WKT format.\n\n\n\nCRS information can still be represented with EPSG, WKT, or PROJ strings without consequence, but it is best to only use PROJ strings as a format for viewing CRS information, not for reprojecting data.\n\n\nThe data and python libraries we will be working with in this blog use different underlying representations of CRSs under the hood for reprojecting. PROJ represents CRS information as a text string of key-value pairs, which makes it easy to read and interpret.\nA PROJ4 string includes the following information:\n\nproj: the projection of the data\nzone: the zone of the data (this is specific to the UTM projection)\ndatum: the datum used\nunits: the units for the coordinates of the data\nellps: the ellipsoid (how the earth’s roundness is calculated) for the data\n\nNote that the zone is unique to the UTM projection. Not all CRSs will have a zone.\n Image source: Chrismurf at English Wikipedia, via Wikimedia Commons (CC-BY).\nHere is a PROJ4 string for one of the datasets we will use in this blog:\n+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0\n\n\n\n\n\n\nWhat projection, zone, datum, and ellipsoid are used for this data? What are the units of the data? Using the map above, what part of the United States was this data collected from?\n\n\n\n\n\n\nProjection is UTM, zone 18, datum is WGS84, ellipsoid is WGS84.\nThe data is in meters.\nThe data comes from the eastern US seaboard.\n\n\n\n\n\n\nFormat interoperability\nMany existing file formats were invented by GIS software developers, often in a closed-source environment. This led to the large number of formats on offer today, and considerable problems transferring data between software environments. The Geospatial Data Abstraction Library (GDAL) is an open-source answer to this issue.\nGDAL is a set of software tools that translate between almost any geospatial format in common use today (and some not so common ones). GDAL also contains tools for editing and manipulating both raster and vector files, including reprojecting data to different CRSs. GDAL can be used as a standalone command-line tool, or built in to other GIS software. Several open-source GIS programs use GDAL for all file import/export operations.\n\n\nMetadata\nSpatial data is useless without metadata. Essential metadata includes the CRS information, but proper spatial metadata encompasses more than that. History and provenance of a dataset (how it was made), who is in charge of maintaining it, and appropriate (and inappropriate!) use cases should also be documented in metadata. This information should accompany a spatial dataset wherever it goes. In practice this can be difficult, as many spatial data formats don’t have a built-in place to hold this kind of information. Metadata often has to be stored in a companion file, and generated and maintained manually.\n\n\n\n\n\n\nMore Resources on CRS\n\n\n\n\nspatialreference.org - A comprehensive online library of CRS information.\nQGIS Documentation - CRS Overview.\nChoosing the Right Map Projection.\nVideo highlighting how map projections can make continents seems proportionally larger or smaller than they actually are.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- All geospatial datasets (raster and vector) are associated with a specific coordinate reference system\n- A coordinate reference system includes datum, projection, and additional parameters specific to the dataset"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#the-geospatial-landscape",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#the-geospatial-landscape",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "4. The Geospatial Landscape",
    "text": "4. The Geospatial Landscape\nMost traditional GIS work is carried out in standalone applications that aim to provide end-to-end geospatial solutions. These applications are available under a wide range of licenses and price points. Some of the most common are listed below.\n\nOpen-source software\nThe Open Source Geospatial Foundation (OSGEO) supports several actively managed GIS platforms:\n\nQGIS is a professional GIS application that is built on top of and proud to be itself Free and Open Source Software (FOSS). QGIS is written in Python, has a python console interface, and has several interfaces written in R including RQGIS.\nGRASS GIS, commonly referred to as GRASS (Geographic Resources Analysis Support System), is a FOSS-GIS software suite used for geospatial data management and analysis, image processing, graphics and maps production, spatial modeling, and visualization. GRASS GIS is currently used in academic and commercial settings around the world, as well as by many governmental agencies and environmental consulting companies. It is a founding member of the Open Source Geospatial Foundation (OSGeo). GRASS GIS can be installed along with and made accessible within QGIS 3.\nGDAL is a multiplatform set of tools for translating between geospatial data formats. It can also handle reprojection and a variety of geoprocessing tasks. GDAL is built in to many applications both FOSS and commercial, including GRASS and QGIS.\nSAGA-GIS, or System for Automated Geoscientific Analyses, is a FOSS-GIS application developed by a small team of researchers from the Dept. of Physical Geography, Göttingen, and the Dept. of Physical Geography, Hamburg. SAGA has been designed for an easy and effective implementation of spatial algorithms, offers a comprehensive, growing set of geoscientific methods, provides an easily approachable user interface with many visualisation options, and runs under Windows and Linux operating systems. Like GRASS GIS, it can also be installed and made accessible in QGIS3.\nPostGIS is a geospatial extension to the PostGreSQL relational database.\n\n\n\nOnline + Cloud computing\n\nPANGEO is a community organization dedicated to open and reproducible data science with python. They focus on the Pangeo software ecosystem for working with big data in the geosciences. This community organization also supports python libraries like xarray, iris, dask, jupyter, and many other packages.\nGoogle has created Google Earth Engine which combines a multi-petabyte catalog of satellite imagery and geospatial datasets with planetary-scale analysis capabilities and makes it available for scientists, researchers, and developers to detect changes, map trends, and quantify differences on the Earth’s surface. Earth Engine API runs in both Python and JavaScript.\nArcGIS Online provides access to thousands of maps and base layers.\nKepler.gl is Uber’s toolkit for handling large datasets (i.e. Uber’s data archive).\nSepal.io by FAO Open Foris utilizing EOS satellite imagery and cloud resources for global forest monitoring.\n\n\n\nGUI vs CLI\nThe earliest computer systems operated without a graphical user interface (GUI), relying only on the command-line interface (CLI). Since mapping and spatial analysis are strongly visual tasks, GIS applications benefited greatly from the emergence of GUIs and quickly came to rely heavily on them. Most modern GIS applications have very complex GUIs, with all common tools and procedures accessed via buttons and menus.\nBenefits of using a GUI include:\n\nTools are all laid out in front of you\nComplex commands are easy to build\nDon’t need to learn a coding language\nCartography and visualisation is more intuitive and flexible\n\nDownsides of using a GUI include:\n\nLow reproducibility - you can’t record your actions and replay\nMost are not designed for batch-processing files\nLimited ability to customise functions or write your own\nIntimidating interface for new users - so many buttons!\n\nIn scientific computing, the lack of reproducibility in point-and-click software has come to be viewed as a critical weakness. As such, scripted CLI-style workflows are again becoming popular, which leads us to another approach to doing GIS — via a programming language. This is the approach we will be using throughout this blog.\n\n\nGIS in programming languages\nA number of powerful geospatial processing libraries exist for general-purpose programming languages like Java and C++. However, the learning curve for these languages is steep and the effort required is excessive for users who only need a subset of their functionality.\nHigher-level scripting languages like Python and R are easier to learn and use. Both now have their own packages that wrap up those geospatial processing libraries and make them easy to access and use safely. A key example is the Java Topology Suite (JTS), which is implemented in C++ as GEOS. GEOS is accessible in Python via the shapely package (and geopandas, which makes use of shapely) and in R via sf. R and Python also have interface packages for GDAL, and for specific GIS apps.\nThis last point is a huge advantage for GIS-by-programming; these interface packages give you the ability to access functions unique to particular programs, but have your entire workflow recorded in a central document - a document that can be re-run at will. Below are lists of some of the key spatial packages for Python, which we will be using in the remainder of this workshop.\n\ngeopandas and geocube for working with vector data\nrasterio and rioxarray for working with raster data\n\nThese packages along with the matplotlib package are all we need for spatial data visualisation. Python also has many fundamental scientific packages that are relevant in the geospatial domain. Below is a list of particularly fundamental packages:\n\nNumPy\nscipy\nscikit-image\n\nThese are all excellent options for working with rasters, as arrays. An overview of these and other Python spatial packages can be accessed here.\nAs a programming language, Python can be a CLI tool. However, using Python together with an Integrated Development Environment (IDE) application allows some GUI features to become part of your workflow. IDEs allow the best of both worlds. They provide a place to visually examine data and other software objects, interact with your file system, and draw plots and maps, but your activities are still command-driven: recordable and reproducible. There are several IDEs available for Python. JupyterLab is well-developed and the most widely used option for data science in Python. VSCode and Spyder are other popular options for data science.\nTraditional GIS apps are also moving back towards providing a scripting environment for users, further blurring the CLI/GUI divide. ESRI have adopted Python into their software, and QGIS is both Python and R-friendly.\n\n\nGIS File Types\nThere are a variety of file types that are used in GIS analysis. Depending on the program you choose to use some file types can be used while others are not readable. Below is a brief table describing some of the most common vector and raster file types.\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- Many software packages exist for working with geospatial data\n- Command-line programs allow you to automate and reproduce your work\n- JupyterLab provides a user-friendly interface for working with Python"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#access-satellite-imagery-using-python",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#access-satellite-imagery-using-python",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "5. Access satellite imagery using Python",
    "text": "5. Access satellite imagery using Python\nA number of satellites take snapshots of the Earth’s surface from space. The images recorded by these remote sensors represent a very precious data source for any activity that involves monitoring changes on Earth. Satellite imagery is typically provided in the form of geospatial raster data, with the measurements in each grid cell (“pixel”) being associated to accurate geographic coordinate information.\nIn this section we will explore how to access open satellite data using Python. In particular, we will consider the Sentinel-2 data collection that is hosted on AWS. This dataset consists of multi-band optical images acquired by the two satellites of the Sentinel-2 mission and it is continuously updated with new images.\n\n\n\n\n\n\nSatellite data sources\n\n\n\n\nLandsat\nSentinel\n\n\n\n\nSearch for satellite imagery - Sentinel\nCurrent sensor resolutions and satellite revisit periods are such that terabytes of data products are added daily to the corresponding collections. Such datasets cannot be made accessible to users via full-catalog download. Space agencies and other data providers often offer access to their data catalogs through interactive Graphical User Interfaces (GUIs), see for instance the Copernicus Open Access Hub portal for the Sentinel missions. Accessing data via a GUI is a nice way to explore a catalog and get familiar with its content, but it represents a heavy and error-prone task that should be avoided if carried out systematically to retrieve data.\nA service that offers programmatic access to the data enables users to reach the desired data in a more reliable, scalable and reproducible manner. An important element in the software interface exposed to the users, which is generally called the Application Programming Interface (API), is the use of standards. Standards, in fact, can significantly facilitate the reusability of tools and scripts across datasets and applications.\nThe SpatioTemporal Asset Catalog (STAC) specification is an emerging standard for describing geospatial data. By organizing metadata in a form that adheres to the STAC specifications, data providers make it possible for users to access data from different missions, instruments and collections using the same set of tools.\n\n\nSearch a STAC catalog\nThe STAC browser is a good starting point to discover available datasets, as it provides an up-to-date list of existing STAC catalogs. From the list, let’s click on the “Earth Search” catalog, i.e. the access point to search the archive of Sentinel-2 images hosted on AWS.\nLet’s take a moment to explore the Earth Search STAC catalog, which is the catalog indexing the Sentinel-2 collection that is hosted on AWS. We can interactively browse this catalog using the STAC browser at this link :\n\nOpen the link in your web browser. Which (sub-) catalogs are available?\n\nFour subcatalogs are available, including both Sentinel 2 and Landsat 8 images\n\n\n\ncatalogs.PNG\n\n\n\nOpen the Sentinel-2 L2A COGs collection, and select one item from the list. Each item corresponds to a satellite “scene”, i.e. a portion of the footage recorded by the satellite at a given time. Have a look at the metadata fields and the list of assets. What kind of data do the assets represent?\n\n\nWhen you select the Sentinel-2 L2A COGs collection, and randomly choose one of the items from the list, you should find yourself on a page similar to the screenshot above. On the left side you will find a list of the available assets: overview images (thumbnail and true color images), metadata files and the “real” satellite images, one for each band captured by the Multispectral Instrument on board Sentinel-2.\nWhen opening a catalog with the STAC browser, you can access the API URL by clicking on the “Source” button on the top right of the page. By using this URL, we have access to the catalog content and, if supported by the catalog, to the functionality of searching its items. For the Earth Search STAC catalog the API URL is:\n\napi_url = \"https://earth-search.aws.element84.com/v0\"\n\nWe can query a STAC API endpoint from Python using the pystac_client library:\n\nfrom pystac_client import Client\nclient = Client.open(api_url)\n\nIn the following, we ask for scenes belonging to the sentinel-s2-l2a-cogs collection. This dataset includes Sentinel-2 data products pre-processed at level 2A (bottom-of-atmosphere reflectance) and saved in Cloud Optimized GeoTIFF (COG) format:\n\n# Sentinel-2, Level 2A, COGs\ncollection = \"sentinel-s2-l2a-cogs\" \n\nhttps://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs\n\n\nCloud Optimized GeoTIFFs\nCloud Optimized GeoTIFFs (COGs) are regular GeoTIFF files with some additional features that make them ideal to be employed in the context of cloud computing and other web-based services. This format builds on the widely-employed GeoTIFF format, introduced in section 1: Raster Data.\nIn essence, COGs are regular GeoTIFF files with a special internal structure. One of the features of COGs is that data is organized in “blocks” that can be accessed remotely via independent HTTP requests. Data users can thus access the only blocks of a GeoTIFF that are relevant for their analysis, without having to download the full file. In addition, COGs typically include multiple lower-resolution versions of the original image, called “overviews”, which can also be accessed independently. By providing this “pyramidal” structure, users that are not interested in the details provided by a high-resolution raster can directly access the lower-resolution versions of the same image, significantly saving on the downloading time. More information on the COG format can be found here.\nWe also ask for scenes intersecting a geometry defined using the shapely library (in this case, a point):\n\n# AMS coordinates\nfrom shapely.geometry import Point\npoint = Point(4.89, 52.37)  \n\nNote: at this stage, we are only dealing with metadata, so no image is going to be downloaded yet. But even metadata can be quite bulky if a large number of scenes match our search! For this reason, we limit the search result to 10 items:\n\n# Search for scenes which include the point(4.89, 52.37)\nsearch = client.search(\n    collections=[collection],\n    intersects=point,\n    max_items=10,\n)\n\nWe submit the query and find out how many scenes match our search criteria (please note that this output can be different as more data is added to the catalog):\n\nprint(search.matched())\n\n736\n\n\nFinally, we retrieve the metadata of the search results:\n\nitems = search.get_all_items()\nprint(len(items))\n\n10\n\n\nThis is consistent with the maximum number of items that we set in the search criteria. We can iterate over the returned items and print these to show their IDs:\n\n# Iterate over our 10 items to show unique IDs\nfor item in items:\n    print(item)\n\n<Item id=S2B_31UFU_20221211_0_L2A>\n<Item id=S2B_31UFU_20221208_0_L2A>\n<Item id=S2A_31UFU_20221206_0_L2A>\n<Item id=S2A_31UFU_20221203_0_L2A>\n<Item id=S2B_31UFU_20221201_0_L2A>\n<Item id=S2B_31UFU_20221128_0_L2A>\n<Item id=S2A_31UFU_20221126_0_L2A>\n<Item id=S2A_31UFU_20221123_0_L2A>\n<Item id=S2B_31UFU_20221121_0_L2A>\n<Item id=S2B_31UFU_20221118_0_L2A>\n\n\nEach of the items contains information about the scene geometry, its acquisition time, and other metadata that can be accessed as a dictionary from the properties attribute. Let’s inspect the metadata associated with the first item of the search results:\n\n# extract some metadata from our first search item\nitem = items[0]\nprint(item.datetime)\nprint(item.geometry)\nprint(item.properties)\n\n2022-12-11 10:56:20+00:00\n{'type': 'Polygon', 'coordinates': [[[6.071664488869862, 52.22257539160586], [4.464995307918359, 52.25346561204129], [4.498475093400055, 53.24019917467795], [6.1417542968794585, 53.20819279121764], [6.071664488869862, 52.22257539160586]]]}\n{'datetime': '2022-12-11T10:56:20Z', 'platform': 'sentinel-2b', 'constellation': 'sentinel-2', 'instruments': ['msi'], 'gsd': 10, 'view:off_nadir': 0, 'proj:epsg': 32631, 'sentinel:utm_zone': 31, 'sentinel:latitude_band': 'U', 'sentinel:grid_square': 'FU', 'sentinel:sequence': '0', 'sentinel:product_id': 'S2B_MSIL2A_20221211T105339_N0509_R051_T31UFU_20221211T122517', 'sentinel:data_coverage': 100, 'eo:cloud_cover': 53.32, 'sentinel:valid_cloud_cover': True, 'sentinel:processing_baseline': '05.09', 'sentinel:boa_offset_applied': True, 'created': '2022-12-11T19:02:22.484Z', 'updated': '2022-12-11T19:02:22.484Z'}\n\n\nThe above metadata e.g. datetime, eo:cloud_cover are the arguments which can be fed into our search to acess specific information.\nLet’s try another scene search from the sentinel-s2-l2a-cogs collection using different criteria:\n-intersect a provided bounding box (use ±0.01 deg in lat/lon from the previously defined point);\n-have been recorded between 20 March 2020 and 30 March 2020;\n-have a cloud coverage smaller than 10% (hint: use the query input argument of client.search).\n\nbound_box = point.buffer(0.01).bounds\n\n\n# Search for scenes which intersect bounding box +- 0.01 deg in lat/lon from (4.89, 52.37) as prev defined\n# between 20/3/20 and 30/3/20 where cloud cover < 10%\nsearch = client.search(\n    collections=[collection],\n    bbox=bound_box,\n    datetime=\"2020-03-20/2020-03-30\",\n    query=[\"eo:cloud_cover<10\"]\n)\n\nprint(search.matched())\n\n4\n\n\n\n# Grab search items and save as a JSON file\nitems = search.get_all_items()\nitems.save_object(\"search.json\")\n\nAn extract of the JSON file is included below. As we can see the file contains a lot of information for each of our 4 search items, which are indexed 0 to 3, such as properties, geometry, links etc:\n\n\n\njson.PNG\n\n\n\n\nAccess the images(assets)\nSo far we have only discussed metadata - but how can one get to the actual images of a satellite scene (the “assets” in the STAC nomenclature)? These can be reached via links that are made available through the item’s attribute assets. The JSON file extract is included below, followed by how to access this info using Python.\n\n\n\njson_image_thumbnail.PNG\n\n\n\n# first item's asset dictionary\nassets = items[0].assets  \nprint(assets.keys())\n\ndict_keys(['thumbnail', 'overview', 'info', 'metadata', 'visual', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'AOT', 'WVP', 'SCL'])\n\n\nAs we can see these dictionary keys match the headings included in the JSON image above.\nWe can print a minimal description of the available assets:\n\nfor key, asset in assets.items():\n    print(f\"{key}: {asset.title}\")\n\nthumbnail: Thumbnail\noverview: True color image\ninfo: Original JSON metadata\nmetadata: Original XML metadata\nvisual: True color image\nB01: Band 1 (coastal)\nB02: Band 2 (blue)\nB03: Band 3 (green)\nB04: Band 4 (red)\nB05: Band 5\nB06: Band 6\nB07: Band 7\nB08: Band 8 (nir)\nB8A: Band 8A\nB09: Band 9\nB11: Band 11 (swir16)\nB12: Band 12 (swir22)\nAOT: Aerosol Optical Thickness (AOT)\nWVP: Water Vapour (WVP)\nSCL: Scene Classification Map (SCL)\n\n\nAmong the others, assets include multiple raster data files B01 through B12 (one per optical band, as acquired by the multi-spectral instrument), a thumbnail, a true-color image (“visual”), instrument metadata and scene-classification information (“SCL”).\n\n# Let’s get the URL links to the actual image:\nprint(assets[\"thumbnail\"].href)\n\nhttps://roda.sentinel-hub.com/sentinel-s2-l1c/tiles/31/U/FU/2020/3/28/0/preview.jpg\n\n\n\n\n\nsentinel_image_download.jpg\n\n\nRemote raster data can be directly opened via the rioxarray library. We will learn more about this library in the next sections.\n\n# Open raster B01 Band 1 (coastal)\nimport rioxarray\nb01_href = assets[\"B01\"].href\nb01 = rioxarray.open_rasterio(b01_href)\nprint(b01)\n\n<xarray.DataArray (band: 1, y: 1830, x: 1830)>\n[3348900 values with dtype=uint16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6.001e+05 6.002e+05 ... 7.097e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0\n\n\nWe can then save the data to disk:\n\n# save image to disk\nb01.rio.to_raster(\"B01.tif\")\n\n\n\nSearch for satellite imagery - Landsat 8\nLet’s now put into practice all the skills we have learned in this section to retrieve images from a different mission: Landsat 8. In particular, we browse images from the Harmonized Landsat Sentinel-2 (HLS) project, which provides images from NASA’s Landsat 8 and ESA’s Sentinel-2 that have been made consistent with each other. The HLS catalog is indexed in the NASA Common Metadata Repository (CMR) and it can be accessed from the STAC API endpoint at the following URL: https://cmr.earthdata.nasa.gov/stac/LPCLOUD.\n\nusing pystac_client, search for all assets of the Landsat 8 collection (HLSL30.v2.0) from February to March 2021\nintersecting the point with longitude/latitute coordinates (-73.97, 40.78) deg.\nsort by cloud cover\n\nVisualize an item’s thumbnail (asset key “browse”).\n\n# Connect to the STAC endpoint\napi_url = \"https://cmr.earthdata.nasa.gov/stac/LPCLOUD\"\nclient = Client.open(api_url)\n\n\n# Search for scenes which include point (-73.97, 40.78) between Feb and March 2021\n# Note we can enter collections and co_ors directly into search argument\nsearch = client.search(\n    collections=[\"HLSL30.v2.0\"],\n    intersects=Point(-73.97, 40.78),  \n    datetime=\"2021-02-01/2021-03-31\"\n )\n\n# retrieve search results\nitems = search.get_all_items()\n\n# save as JSON file\nitems.save_object(\"landsat.search.json\")\nprint(len(items))\n\n5\n\n\n\n# sort by cloud cover and select first item\nitems_sorted = sorted(items, key=lambda x: x.properties[\"eo:cloud_cover\"]) \nitem = items_sorted[0]\nprint(item)\n\n<Item id=HLS.L30.T18TWL.2021039T153324.v2.0>\n\n\n\n# Let’s get the URL links to the actual image:\nprint(item.assets[\"browse\"].href)\n\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T18TWL.2021039T153324.v2.0/HLS.L30.T18TWL.2021039T153324.v2.0.jpg\n\n\n\n\n\n\n\n\n\nPublic catalogs, protected data\n\n\n\nPublicly accessible catalogs and STAC endpoints do not necessarily imply publicly accessible data. Data providers, in fact, may limit data access to specific infrastructures and/or require authentication. For instance, the NASA CMR STAC endpoint considered in the last exercise offers publicly accessible metadata for the HLS collection, but most of the linked assets are available only for registered users (the thumbnail is publicly accessible).\nThe authentication procedure for dataset with restricted access might differ depending on the data provider. For the NASA CMR, follow these steps in order to access data using Python:\n\ncreate a NASA Earthdata login account here;\nset up a netrc file with your credentials, e.g. by using this script;\ndefine the following environment variables:\n\n\n\nimport os\nos.environ[\"GDAL_HTTP_COOKIEFILE\"] = \"./cookies.txt\"\nos.environ[\"GDAL_HTTP_COOKIEJAR\"] = \"./cookies.txt\"\n\n\n\n\n\n\nKey Points\n\n\n\n\naccessing satellite images via the providers’ API enables a more reliable and scalable data retrieval\nSTAC catalogs can be browsed and searched using the same tools and scripts\nrioxarray allows you to open and download remote raster files"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#read-and-visualize-raster-data",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#read-and-visualize-raster-data",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "6. Read and visualize raster data",
    "text": "6. Read and visualize raster data\nRaster datasets were introduced in section 1. Here, we introduce the fundamental principles, packages and metadata/raster attributes for working with raster data in Python. We will also explore how Python handles missing and bad data values.\nrioxarray is the Python package we will use throughout this blog to work with raster data. It is based on the popular rasterio package for working with rasters and xarray for working with multi-dimensional arrays. rioxarray extends xarray by providing top-level functions (e.g. the open_rasterio function to open raster datasets) and by adding a set of methods to the main objects of the xarray package (the Dataset and the DataArray). These additional methods are made available via the rio accessor and become available from xarray objects after importing rioxarray.\nWe will also use the pystac package to load rasters from the search results we created in the previous section.\nWe’ll continue from the results of the satellite image search that we have carried out in the previous section. We will load data starting from the search.json file, using one scene from the search results as an example to demonstrate data loading and visualization. You can download the raster data using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started with this lesson.\n\nLoad a Raster and View Attributes\nIn the previous section, we searched for Sentinel-2 images, and then saved the search results to a file named search.json. This contains the information on where and how to access the target images from a remote repository. We can use the function pystac.ItemCollection.from_file() to load the search results as an Item list.\n\nimport pystac\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\nIn the search results, we have 2 Item type objects, corresponding to 4 Sentinel-2 scenes from March 26th and 28th in 2020. We will focus on the first scene S2A_31UFU_20200328_0_L2A, and load band B09 (central wavelength 945 nm). We can load this band using the function rioxarray.open_rasterio(), via the Hypertext Reference href (commonly referred to as a URL):\n\nimport rioxarray\n\n# load band B09 (central wavelength 945 nanometres(nm) - 1 nm = 10^-9m\nraster_ams_b9 = rioxarray.open_rasterio(items[0].assets[\"B09\"].href)\n\n# Call the variable name to get a quick look at the shape and attributes\nraster_ams_b9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1830, x: 1830)>\n[3348900 values with dtype=uint16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6.001e+05 6.002e+05 ... 7.097e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 1830x: 1830...[3348900 values with dtype=uint16]Coordinates: (4)band(band)int641array([1])x(x)float646e+05 6.001e+05 ... 7.098e+05array([600030., 600090., 600150., ..., 709650., 709710., 709770.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900010., 5899950., 5899890., ..., 5790390., 5790330., 5790270.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 60.0 0.0 5900040.0 0.0 -60.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600030.0, 600090.0, 600150.0, 600210.0, 600270.0, 600330.0,\n              600390.0, 600450.0, 600510.0, 600570.0,\n              ...\n              709230.0, 709290.0, 709350.0, 709410.0, 709470.0, 709530.0,\n              709590.0, 709650.0, 709710.0, 709770.0],\n             dtype='float64', name='x', length=1830))yPandasIndexPandasIndex(Float64Index([5900010.0, 5899950.0, 5899890.0, 5899830.0, 5899770.0, 5899710.0,\n              5899650.0, 5899590.0, 5899530.0, 5899470.0,\n              ...\n              5790810.0, 5790750.0, 5790690.0, 5790630.0, 5790570.0, 5790510.0,\n              5790450.0, 5790390.0, 5790330.0, 5790270.0],\n             dtype='float64', name='y', length=1830))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\nThe first call to rioxarray.open_rasterio() opens the file from remote or local storage, and then returns a xarray.DataArray object. The object is stored in a variable, i.e. raster_ams_b9. Reading in the data with xarray instead of rioxarray also returns a xarray.DataArray, but the output will not contain the geospatial metadata (such as projection information). We can use numpy functions or built-in Python math operators on a xarray.DataArray just like a numpy array. Calling the variable name of the DataArray also prints out all of its metadata information.\nThe output tells us that we are looking at an xarray.DataArray, with:\n\n1 band;\n1830 rows(y); and\n1830 columns(x)\n\nWe can also see the number of pixel values in the DataArray (1,830 x 1,830 = 3,348,900) and the type of those pixel values, which is unsigned integer (or uint16). The DataArray also stores different values for the coordinates of the DataArray. When using rioxarray, the term coordinates refers to spatial coordinates like x and y but also the band coordinate. Each of these sequences of values has its own data type, like float64 for the spatial coordinates and int64 for the band coordinate.\nThis DataArray object also has a couple of attributes that are accessed like .rio.crs, .rio.nodata, and .rio.bounds(), which contain the metadata for the file we opened.\n\n\n\n\n\n\nNote\n\n\n\nNote that many of the metadata are accessed as attributes using .attribute_name, but bounds() is a method (i.e. a function in an object) and needs parentheses.\n\n\n\n# Co_ordinate Reference System\nprint(raster_ams_b9.rio.crs)\n# Nodata value encoded as...\nprint(raster_ams_b9.rio.nodata)\n# Bounding box corners\nprint(raster_ams_b9.rio.bounds())\n# Width \nprint(raster_ams_b9.rio.width)\n# Height\nprint(raster_ams_b9.rio.height)\n\nEPSG:32631\n0\n(600000.0, 5790240.0, 709800.0, 5900040.0)\n1830\n1830\n\n\nThe Coordinate Reference System, or raster_ams_b9.rio.crs, is reported as the string EPSG:32631. The nodata value is encoded as 0 and the bounding box corners of our raster are represented by the output of .bounds() as a tuple (like a list but we can’t edit it). The height and width match what we saw when we printed the DataArray, but by using .rio.width and .rio.height we can access these values if we need them in calculations.\nWe will be exploring this data throughout this section. By the end of this section we will be able to understand and explain the metadata output.\n\n\nVisualize a Raster\nAfter viewing the attributes of our raster, we can examine the raw values of the array with .values:\n\nraster_ams_b9.values\n\narray([[[    0,     0,     0, ...,  8888,  9075,  8139],\n        [    0,     0,     0, ..., 10444, 10358,  8669],\n        [    0,     0,     0, ..., 10346, 10659,  9168],\n        ...,\n        [    0,     0,     0, ...,  4295,  4289,  4320],\n        [    0,     0,     0, ...,  4291,  4269,  4179],\n        [    0,     0,     0, ...,  3944,  3503,  3862]]], dtype=uint16)\n\n\nThis can give us a quick view of the values of our array, but only at the corners. Since our raster is loaded in python as a DataArray type, we can plot this in one line similar to a pandas DataFrame with DataArray.plot() :\n\nraster_ams_b9.plot()\n\n<matplotlib.collections.QuadMesh at 0x7fa254cc0d30>\n\n\n\n\n\nNice plot! Notice that rioxarray helpfully allows us to plot this raster with spatial coordinates on the x and y axis (this is not the default in many cases with other functions or libraries).\nThis plot shows the satellite measurement of the spectral band B09 for an area that covers part of the Netherlands. According to the Sentinel-2 documentaion, this is a band with the central wavelength of 945nm, which is sensitive to water vapour. It has a spatial resolution of 60m. Note that the band=1 in the image title refers to the ordering of all the bands in the DataArray, not the Sentinel-2 band number B09 that we saw in the pystac search results.\n\n\n\n\n\n\nWith a quick view of the image, we notice that half of the image is blank, no data is captured. We also see that the cloudy pixels at the top have high reflectance values, while the contrast of everything else is quite low. This is expected because this band is sensitive to the water vapour.\n\n\n\nTo obtain a better colour contrast, we can add the option robust=True, which displays values between the 2nd and 98th percentile.\n\n\n\n# restrict display to values within the 2nd and 98th quartile\nraster_ams_b9.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa255063370>\n\n\n\n\n\nNow that the colour limit is set in a way fitting most of the values in the image, we have a much better view of the ground pixels.\n\n\n\n\n\n\nThe option robust=True defaults to displaying values between the 2nd and 98th percentile.\n\n\n\nFor a customized displaying range, you can also manually specifying the keywords vmin and vmax.\n\n\n\nraster_ams_b9.plot(vmin=100, vmax=7000)\n\n<matplotlib.collections.QuadMesh at 0x7fa254f4e680>\n\n\n\n\n\n\n\nView Raster Coordinate Reference System (CRS) in Python\nAnother feature that we’re interested in is the CRS, and it can be accessed with .rio.crs. We introduced the concept of a CRS in an earlier section. Now we will see how features of the CRS appear in our data file and what meanings they have. We can view the CRS string associated with our DataArray’s rio object using the crs attribute.\n\n# view the co_ordinate reference system string\nprint(raster_ams_b9.rio.crs)\n\nEPSG:32631\n\n\nTo print the EPSG code number as an int, we use the .to_epsg() method:\n\n# print the ESPG code as an int\nraster_ams_b9.rio.crs.to_epsg()\n\n32631\n\n\nEPSG codes are great for succinctly representing a particular coordinate reference system. But what if we want to see more details about the CRS, like the units? For that, we can use pyproj, a library for representing and working with coordinate reference systems.\n\nfrom pyproj import CRS\nepsg = raster_ams_b9.rio.crs.to_epsg()\ncrs = CRS(epsg)\ncrs\n\n<Derived Projected CRS: EPSG:32631>\nName: WGS 84 / UTM zone 31N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 0°E and 6°E, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Andorra. Belgium. Benin. Burkina Faso. Denmark - North Sea. France. Germany - North Sea. Ghana. Luxembourg. Mali. Netherlands. Niger. Nigeria. Norway. Spain. Togo. United Kingdom (UK) - North Sea.\n- bounds: (0.0, 0.0, 6.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 31N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nUnderstanding pyproj CRS Summary\nLet’s break down the pieces of the pyproj CRS summary. The string contains all of the individual CRS elements that Python or another GIS might need, separated into distinct sections, and datum.\n\nName: of the projection is UTM zone 31N (UTM has 60 zones, each 6-degrees of longitude in width). The underlying datum is WGS84.\nAxis Info: the CRS shows a Cartesian system with two axes, easting and northing, in meter units.\nArea of Use: the projection is used for a particular range of longitudes 0°E to 6°E in the northern hemisphere (0.0°N to 84.0°N)\nCoordinate Operation: the operation to project the coordinates (if it is projected) onto a cartesian (x, y) plane. Transverse Mercator is accurate for areas with longitudinal widths of a few degrees, hence the distinct UTM zones.\nDatum: Details about the datum, or the reference point for coordinates. WGS 84 and NAD 1983 are common datums. NAD 1983 is set to be replaced in 2022.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the zone is unique to the UTM projection. Not all CRSs will have a zone.\n\n\nThe CRS class from the pyproj library allows us to create a CRS object with methods and attributes for accessing specific information about a CRS, or the detailed summary shown above. A particularly useful attribute is area_of_use, which shows the geographic bounds that the CRS is intended to be used.\n\ncrs.area_of_use\n\nAreaOfUse(west=0.0, south=0.0, east=6.0, north=84.0, name='Between 0°E and 6°E, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Andorra. Belgium. Benin. Burkina Faso. Denmark - North Sea. France. Germany - North Sea. Ghana. Luxembourg. Mali. Netherlands. Niger. Nigeria. Norway. Spain. Togo. United Kingdom (UK) - North Sea.')\n\n\n\n\n\n\n\n\nWhat units are our data in? See if you can find a method to examine this information using help(crs) or dir(crs)\n\n\n\n\n\ncrs.axis_info tells us that the CRS for our raster has two axis and both are in meters. We could also get this information from the attribute raster_ams_b9.rio.crs.linear_units.\n\n\n\n\ncrs.axis_info\n\n[Axis(name=Easting, abbrev=E, direction=east, unit_auth_code=EPSG, unit_code=9001, unit_name=metre),\n Axis(name=Northing, abbrev=N, direction=north, unit_auth_code=EPSG, unit_code=9001, unit_name=metre)]\n\n\n\nraster_ams_b9.rio.crs.linear_units\n\n'metre'\n\n\n\n\nCalculate Raster Statistics\nIt is useful to know the minimum or maximum values of a raster dataset. We can compute these and other descriptive statistics with min, max, mean, and std.\n\nprint(raster_ams_b9.min())\nprint(raster_ams_b9.max())\nprint(raster_ams_b9.mean())\nprint(raster_ams_b9.std())\n\n<xarray.DataArray ()>\narray(0, dtype=uint16)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(15497, dtype=uint16)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(1652.44009944)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(2049.16447495)\nCoordinates:\n    spatial_ref  int64 0\n\n\nThe information above includes a report of the min, max, mean, and standard deviation values, along with the data type. If we want to see specific quantiles, we can use xarray’s .quantile() method. For example for the 25% and 75% quantiles:\n\nprint(raster_ams_b9.quantile([0.25, 0.75]))\n\n<xarray.DataArray (quantile: 2)>\narray([   0., 2911.])\nCoordinates:\n  * quantile  (quantile) float64 0.25 0.75\n\n\nWe could also get each of these values one by one using NumPy:\n\nimport numpy\nprint(numpy.percentile(raster_ams_b9, 25))\nprint(numpy.percentile(raster_ams_b9, 75))\n\n0.0\n2911.0\n\n\n\n\n\n\n\n\nYou may notice that raster_ams_b9.quantile and numpy.percentile didn’t require an argument specifying the axis or dimension along which to compute the quantile. This is because axis=None is the default for most numpy functions, and therefore dim=None is the default for most xarray methods.\n\n\n\nit’s always good to check out the docs on a function to see what the default arguments are, particularly when working with multi-dimensional image data. To do so, we can use help(raster_ams_b9.quantile) or ?raster_ams_b9.quantile if you are using jupyter notebook or jupyter lab.\n\n\n\n\nDealing with Missing Data\nSo far, we have visualized a band of a Sentinel-2 scene and calculated its statistics. However, we need to take missing data into account. Raster data often has a “no data value” associated with it and for raster datasets read in by rioxarray. This value is referred to as nodata. This is a value assigned to pixels where data is missing or no data were collected. There can be different cases that cause missing data, and it’s common for other values in a raster to represent different cases. The most common example is missing data at the edges of rasters.\nBy default the shape of a raster is always rectangular. So if we have a dataset that has a shape that isn’t rectangular, some pixels at the edge of the raster will have no data values. This often happens when the data were collected by a sensor which only flew over some part of a defined region. As we have seen above, the nodata value of this dataset (raster_ams_b9.rio.nodata) is 0. When we have plotted the band data, or calculated statistics, the missing value was not distinguished from other values.\n\n\n\n\n\n\nMissing data may cause some unexpected results. For example, the 25th percentile we just calculated was 0, probably reflecting the presence of a lot of missing data in the raster.\n\n\n\nTo distinguish missing data from real data, one possible way is to use NaN to represent them. This can be done by specifying masked=True when loading the raster.\n\n\n\n# use NaN to represent missing data\nraster_ams_b9 = rioxarray.open_rasterio(items[0].assets[\"B09\"].href, masked=True)\n\nOr, we can also use the where function to select all the pixels which are different from the nodata value of the raster:\n\nraster_ams_b9.where(raster_ams_b9!=raster_ams_b9.rio.nodata)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1830, x: 1830)>\narray([[[   nan,    nan,    nan, ...,  8888.,  9075.,  8139.],\n        [   nan,    nan,    nan, ..., 10444., 10358.,  8669.],\n        [   nan,    nan,    nan, ..., 10346., 10659.,  9168.],\n        ...,\n        [   nan,    nan,    nan, ...,  4295.,  4289.,  4320.],\n        [   nan,    nan,    nan, ...,  4291.,  4269.,  4179.],\n        [   nan,    nan,    nan, ...,  3944.,  3503.,  3862.]]],\n      dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6.001e+05 6.002e+05 ... 7.097e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 1830x: 1830nan nan nan nan nan ... 3.996e+03 3.944e+03 3.503e+03 3.862e+03array([[[   nan,    nan,    nan, ...,  8888.,  9075.,  8139.],\n        [   nan,    nan,    nan, ..., 10444., 10358.,  8669.],\n        [   nan,    nan,    nan, ..., 10346., 10659.,  9168.],\n        ...,\n        [   nan,    nan,    nan, ...,  4295.,  4289.,  4320.],\n        [   nan,    nan,    nan, ...,  4291.,  4269.,  4179.],\n        [   nan,    nan,    nan, ...,  3944.,  3503.,  3862.]]],\n      dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float646e+05 6.001e+05 ... 7.098e+05array([600030., 600090., 600150., ..., 709650., 709710., 709770.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900010., 5899950., 5899890., ..., 5790390., 5790330., 5790270.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 60.0 0.0 5900040.0 0.0 -60.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600030.0, 600090.0, 600150.0, 600210.0, 600270.0, 600330.0,\n              600390.0, 600450.0, 600510.0, 600570.0,\n              ...\n              709230.0, 709290.0, 709350.0, 709410.0, 709470.0, 709530.0,\n              709590.0, 709650.0, 709710.0, 709770.0],\n             dtype='float64', name='x', length=1830))yPandasIndexPandasIndex(Float64Index([5900010.0, 5899950.0, 5899890.0, 5899830.0, 5899770.0, 5899710.0,\n              5899650.0, 5899590.0, 5899530.0, 5899470.0,\n              ...\n              5790810.0, 5790750.0, 5790690.0, 5790630.0, 5790570.0, 5790510.0,\n              5790450.0, 5790390.0, 5790330.0, 5790270.0],\n             dtype='float64', name='y', length=1830))Attributes: (4)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGEscale_factor :1.0add_offset :0.0\n\n\nEither way will change the nodata value from 0 to nan. Now if we compute the statistics again, the missing data will not be considered:\n\nprint(raster_ams_b9.min())\nprint(raster_ams_b9.max())\nprint(raster_ams_b9.mean())\nprint(raster_ams_b9.std())\n\n<xarray.DataArray ()>\narray(8., dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(15497., dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(2477.405, dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(2061.9539, dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n\n\nAnd if we plot the image, the nodata pixels are not shown because they are not 0 anymore:\n\nraster_ams_b9.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa254eac670>\n\n\n\n\n\n\n\n\n\n\n\nNotice that there is a side effect of using NaN instead of 0 to represent missing data: the data type of the DataArray was changed from integers to float\n\n\n\nThis need to be taken into consideration when the data type matters in our application.\n\n\n\n\nRaster Bands\nSo far we looked into a single band raster, i.e. the B09 band of a Sentinel-2 scene. However, to get an overview of the scene, we may also want to visualize the true-colour thumbnail of the region. This is provided as a multi-band raster – a raster dataset that contains more than one band.\n\nThe overview asset in the Sentinel-2 scene is a multiband asset. Similar to B09, we can load it by:\n\nraster_ams_overview = rioxarray.open_rasterio(items[0].assets['overview'].href)\nraster_ams_overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 3, y: 343, x: 343)>\n[352947 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 6.002e+05 6.005e+05 ... 7.093e+05 7.096e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.899e+06 ... 5.791e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 3y: 343x: 343...[352947 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float646.002e+05 6.005e+05 ... 7.096e+05array([600160., 600480., 600800., ..., 708960., 709280., 709600.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5899880., 5899560., 5899240., ..., 5791080., 5790760., 5790440.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 320.0 0.0 5900040.0 0.0 -320.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600160.0, 600480.0, 600800.0, 601120.0, 601440.0, 601760.0,\n              602080.0, 602400.0, 602720.0, 603040.0,\n              ...\n              706720.0, 707040.0, 707360.0, 707680.0, 708000.0, 708320.0,\n              708640.0, 708960.0, 709280.0, 709600.0],\n             dtype='float64', name='x', length=343))yPandasIndexPandasIndex(Float64Index([5899880.0, 5899560.0, 5899240.0, 5898920.0, 5898600.0, 5898280.0,\n              5897960.0, 5897640.0, 5897320.0, 5897000.0,\n              ...\n              5793320.0, 5793000.0, 5792680.0, 5792360.0, 5792040.0, 5791720.0,\n              5791400.0, 5791080.0, 5790760.0, 5790440.0],\n             dtype='float64', name='y', length=343))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\nThe band number comes first when GeoTiffs are read with the .open_rasterio() function. As we can see in the xarray.DataArray object, the shape is now (band: 3, y: 343, x: 343), with three bands in the band dimension. It’s always a good idea to examine the shape of the raster array you are working with and make sure it’s what you expect. Many functions, especially the ones that plot images, expect a raster array to have a particular shape. We can also check the shape using the .shape attribute:\n\nraster_ams_overview.shape\n\n(3, 343, 343)\n\n\nWe can visualize the multi-band data with the DataArray.plot.imshow() function:\n\nraster_ams_overview.plot.imshow()\n\n<matplotlib.image.AxesImage at 0x7fa254c55600>\n\n\n\n\n\nNote that the DataArray.plot.imshow() function makes assumptions about the shape of the input DataArray, that since it has three channels, the correct colormap for these channels is RGB. It does not work directly on image arrays with more than 3 channels. We can replace one of the RGB channels with another band, to make a false-colour image.\nAs seen in the figure above, the true-colour image is stretched. Let’s visualize it with the right aspect ratio. Since we know the height/width ratio is 1:1 (check the rio.height and rio.width attributes), we can set the aspect ratio to be 1. For example, we can choose the size to be 5 inches, and set aspect=1. Note that according to the documentation of DataArray.plot.imshow(), when specifying the aspect argument, size also needs to be provided.\n\nraster_ams_overview.plot.imshow(size=5, aspect=1)\n\n<matplotlib.image.AxesImage at 0x7fa2545752a0>\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- rioxarray and xarray are for working with multidimensional arrays like pandas is for working with tabular data\n- rioxarray stores CRS information as a CRS object that can be converted to an EPSG code or PROJ4 string\n- missing raster data are filled with nodata values, which should be handled with care for statistics and visualization"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data-in-python",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data-in-python",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "7. Vector data in Python",
    "text": "7. Vector data in Python\nAs covered in section 2, vector data represents specific features on the Earth’s surface using points, lines and polygons. These geographic elements can then have one or more attributes assigned to them, such as ‘name’ and ‘population’ for a city, or crop type for a field. Vector data can be much smaller in (file) size than raster data, while being very rich in terms of the information captured.\nIn this section, we will be moving from working with raster data to working with vector data. We will use Python to open and plot point, line and polygon vector data. In particular, we will make use of the geopandas package to open, manipulate and write vector datasets. geopandas extends the popular pandas library for data analysis to geospatial applications. The main pandas objects (the Series and the DataFrame) are expanded by including geometric types, represented in Python using the **shapely library, and by providing dedicated methods for spatial operations (union, intersection, etc.).\n\nIntroducing the vector data\nThe data we will work with comes from the Dutch government’s open geodata sets, obtained from the PDOK platform. It provides open data for various applications, e.g. real estate, infrastructure, agriculture, etc. In this episode we will use three data sets:\n\ncrop fields (polygons)\nwater ways (lines)\nground water monitoring wells (points)\n\nIn later sections, we will learn how to work with raster and vector data together and combine them into a single plot.\n\nimport geopandas as gpd\n\nWe will use the geopandas module to load the crop field vector data we downloaded at: data/brpgewaspercelen_definitief_2020_small.gpkg. This file contains data for the entirety of the European portion of the Netherlands, resulting in a very large number of crop field parcels. Directly loading the whole file to memory can be slow. Let’s consider as Area of Interest (AoI) northern Amsterdam, which is a small portion of the Netherlands. We only need to load this part.\nWe define a bounding box, and will only read the data within the extent of the bounding box:\n\n# Define bounding box\nxmin, xmax = (110_000, 140_000)\nymin, ymax = (470_000, 510_000)\nbbox = (xmin, ymin, xmax, ymax)\n\n\n\n\n\n\n\nHow should I define my bounding box?\n\n\n\nFor simplicity, here we assume the Coordinate Reference System (CRS) and extent of the vector file are known (for instance they are provided in the dataset documentation). Some Python tools, e.g. fiona(which is also the backend of geopandas), provides the file inspection functionality without actually the need to read the full data set into memory. An example can be found in the documentation of fiona.\n\n\n\n# Partially load data within the bounding box\ncropfield = gpd.read_file(\"Data/brpgewaspercelen_definitief_2020_small.gpkg\", bbox=bbox)\n\n\n\nVector Metadata & Attributes\nWhen we import the vector dataset to Python (as our cropfield object) it comes in as a DataFrame, specifically a GeoDataFrame. The read_file() function also automatically stores geospatial information about the data. We are particularly interested in describing the format, CRS, extent, and other components of the vector data, and the attributes which describe properties associated with each individual vector object.\n\n\nSpatial Metadata\nKey metadata includes:\n\nObject Type: the class of the imported object.\nCoordinate Reference System (CRS): the projection of the data.\nExtent: the spatial extent (i.e. geographic area that the data covers). Note that the spatial extent for a vector dataset represents the combined extent for all spatial objects in the dataset.\n\nEach GeoDataFrame has a “geometry” column that contains geometries. In the case of our cropfield object, this geometry is represented by a shapely.geometry.Polygon object. geopandas uses the shapely library to represent polygons, lines, and points, so the types are inherited from shapely.\nWe can view the metadata using the .crs, .bounds and .type attributes. First, let’s view the geometry type for our crop field dataset:\n\n# view the geometry type using the pandas method .type on the GeoDataFrame object, cropfield\ncropfield.type\n\n0        Polygon\n1        Polygon\n2        Polygon\n3        Polygon\n4        Polygon\n          ...   \n22026    Polygon\n22027    Polygon\n22028    Polygon\n22029    Polygon\n22030    Polygon\nLength: 22031, dtype: object\n\n\n\n# view the CRS metadata\ncropfield.crs\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\n- bounds: (3.2, 50.75, 7.22, 53.7)\nCoordinate Operation:\n- name: RD New\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\nOur data is in the CRS RD New.\n\n\n\n\n\n\nImportant\n\n\n\nThe CRS is critical to interpreting the object’s extent values as it specifies units. To find the extent of our dataset in the projected coordinates, we can use the .total_bounds attribute.\n\n\n\n# find the extent of our dataset in the projected coordinates\ncropfield.total_bounds\n\narray([109222.03325 , 469461.512625, 140295.122125, 510939.997875])\n\n\nThis array contains, in order, the values for the overall dataset:\n\nminx, miny, maxx, maxy\n\nThe spatial extent of a GeoDataFrame represents the geographic “edge” or location that is the furthest north, south, east, and west. Thus, it is represents the overall geographic coverage of the spatial object. We can convert these coordinates to a bounding box or acquire the index of the dataframe to access the geometry. Either of these polygons can be used to clip rasters (more on that later).\n\n\nSelecting spatial features\nSometimes, the loaded data can still be too large. We can cut it is to a even smaller extent using the .cx indexer (note the use of square brackets instead of round brackets, which are used instead with functions and methods):\n\n# Define a Boundingbox in RD\nxmin, xmax = (120_000, 135_000)\nymin, ymax = (485_000, 500_000)\ncropfield_crop = cropfield.cx[xmin:xmax, ymin:ymax]\n\nThis will cut out a smaller area, defined by a box in units of the projection, discarding the rest of the data. The resultant GeoDataframe, which includes all the features intersecting the box, is found in the cropfield_crop object. Note that the edge elements are not ‘cropped’ themselves. We can check the total bounds of this new data as before:\n\ncropfield_crop.total_bounds\n\narray([119594.384   , 484949.292625, 135375.77025 , 500782.531   ])\n\n\nWe can then save this cropped dataset for use in future, using the to_file() method of our GeoDataFrame object:\n\ncropfield_crop.to_file('cropped_field.shp')\n\nThis will write it to disk (in this case, in ‘shapefile’ format), containing only the data from our cropped area. It can be read in again at a later time using the read_file() method we have been using above. Note that this actually writes multiple files to disk (cropped_field.cpg, cropped_field.dbf, cropped_field.prj, cropped_field.shp, cropped_field.shx). All these files should ideally be present in order to re-read the dataset later, although only the .shp, .shx, and .dbf files are mandatory. See section 2 for more information.\n\n\nPlotting a vector dataset\nWe can now plot this data. Any GeoDataFrame can be plotted in CRS units to view the shape of the object with .plot().\n\ncropfield_crop.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nWe can customize our boundary plot by setting the figsize, edgecolor, and color. Making some polygons transparent will come in handy when we need to add multiple spatial datasets to a single plot.\n\ncropfield_crop.plot(figsize=(5,5), edgecolor=\"purple\", facecolor=\"None\")\n\n<AxesSubplot: >\n\n\n\n\n\nUnder the hood, geopandas is using matplotlib to generate this plot. In the next section we will see how we can add DataArrays and other vector datasets to this plot to start building an informative map of our area of interest.\n\n\nSpatial Data Attributes\nWe introduced the idea of spatial data attributes in an earlier section. Now we will explore how to use spatial data attributes stored in our data to plot different features.\n\nWaterways\n\n# load data \nwaterways_nl = gpd.read_file(\"Data/status_vaarweg.zip\")\n\n\n# Type of features\nwaterways_nl.type\n\n0     LineString\n1     LineString\n2     LineString\n3     LineString\n4     LineString\n         ...    \n86    LineString\n87    LineString\n88    LineString\n89    LineString\n90    LineString\nLength: 91, dtype: object\n\n\n\n# Co_ord reference system\nwaterways_nl.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Bounds\nwaterways_nl.total_bounds\n\narray([50.7916,  3.1626, 53.6161,  7.0121])\n\n\n\n# How many spatial features\nprint(len(waterways_nl))\n\n91\n\n\nOur waterways dataset includes 91 lines.\nNow let’s take a deeper look at the Dutch waterway lines: waterways_nl. Let’s visualize it with the plot function:\n\nwaterways_nl.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\n\n\n\nAxis ordering - we can tell that the latitude and longitude of the file are flipped.\n\n\n\nAccording to the standards, the axis ordering for a CRS should follow the definition provided by the competent authority. For the commonly used EPSG:4326 geographic coordinate system, the EPSG defines the ordering as first latitude then longitude. However, in the GIS world, it is custom to work with coordinate tuples where the first component is aligned with the east/west direction and the second component is aligned with the north/south direction. Multiple software packages thus implement this convention also when dealing with EPSG:4326. As a result, one can encounter vector files that implement either convention - keep this in mind and always check your datasets!\n\n\n\n\nGround water monitoring wells\n\n# load data \nwells_nl = gpd.read_file(\"Data/brogmwvolledigeset.zip\")\n\n\n# Type of features\nwells_nl.type\n\n0        Point\n1        Point\n2        Point\n3        Point\n4        Point\n         ...  \n54654    Point\n54655    Point\n54656    Point\n54657    Point\n54658    Point\nLength: 54659, dtype: object\n\n\n\n# Co_ord reference system\nwells_nl.crs\n\n<Geographic 2D CRS: EPSG:4258>\nName: ETRS89\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Moldova; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain; Sweden; Switzerland; United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\n- bounds: (-16.1, 32.88, 40.18, 84.73)\nDatum: European Terrestrial Reference System 1989 ensemble\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\n\n# Bounds\nwells_nl.total_bounds\n\narray([ 3.37982412, 50.75590464,  7.21010667, 53.49457587])\n\n\n\n# How many spatial features\nprint(len(wells_nl))\n\n54659\n\n\nOur wells dataset includes 54659 points.\n\nwells_nl.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\nModify the geometry of a GeoDataFrame\nSometimes we need to modify the geometry of a GeoDataFrame. For example, as we saw previously, the latitude and longitude are flipped in the vector data waterways_nl. This error needs to be fixed before performing further analysis. Let’s first take a look at what makes up the geometry column of waterways_nl:\n\nwaterways_nl['geometry']\n\n0     LINESTRING (52.41810 4.84060, 52.42070 4.84090...\n1     LINESTRING (52.11910 4.67450, 52.11930 4.67340...\n2     LINESTRING (52.10090 4.25730, 52.10390 4.25530...\n3     LINESTRING (53.47250 6.84550, 53.47740 6.83840...\n4     LINESTRING (52.32270 5.14300, 52.32100 5.14640...\n                            ...                        \n86    LINESTRING (51.49270 5.39100, 51.48050 5.39160...\n87    LINESTRING (52.15900 5.38510, 52.16010 5.38340...\n88    LINESTRING (51.97340 4.12420, 51.97110 4.12220...\n89    LINESTRING (52.11910 4.67450, 52.11850 4.67430...\n90    LINESTRING (51.88940 4.61900, 51.89040 4.61350...\nName: geometry, Length: 91, dtype: geometry\n\n\nEach row is a LINESTRING object. We can further zoom into one of the rows, for example, the 13th row:\n\nprint(waterways_nl['geometry'][12])\nprint(type(waterways_nl['geometry'][12]))\n\nLINESTRING (51.714200001 4.620299999, 51.7203 4.62279999999998, 51.7212 4.62319999900001, 51.73 4.62759999899998, 51.736000001 4.62959999999998, 51.7434 4.63139999999999, 51.7489 4.63139999999999, 51.753600001 4.63019999900001, 51.759799998 4.62700000000001, 51.764699999 4.62630000000001, 51.769200001 4.62680000099999, 51.771699999 4.62720000000002, 51.773699999 4.62740000000002, 51.775800001 4.62740000000002, 51.780099999 4.62740000000002, 51.782699998 4.62709999800001, 51.785700001 4.62640000099998, 51.791900001 4.62359999900002, 51.7962 4.62229999900001, 51.800000001 4.62130000100001)\n<class 'shapely.geometry.linestring.LineString'>\n\n\nAs we can see in the output, the LINESTRING object contains a list of coordinates of the vertices. In our situation, we would like to find a way to flip the x and y of every coordinates set. A good way to look for the solution is to use the documentation of the shapely package, since we are seeking to modify the LINESTRING object. Here we are going to use the shapely.ops.transform function, which applies a self-defined function to all coordinates of a geometry.\n\nimport shapely\n\n# Define a function flipping the x and y coordinate values\ndef flip(geometry):\n    return shapely.ops.transform(lambda x, y: (y, x), geometry)\n\n# Apply this function to all coordinates and all lines\ngeom_corrected = waterways_nl['geometry'].apply(flip)\n\nThen we can update the geometry column with the corrected geometry geom_corrected, and visualize it to check:\n\n# Update geometry\nwaterways_nl['geometry'] = geom_corrected\n\n# Visualization\nwaterways_nl.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nNow the waterways look good! We can save the vector data for later usage:\n\n# Save plot as .shp file\nwaterways_nl.to_file('waterways_nl_corrected.shp')\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- vector dataset metadata include geometry type, CRS, and extent\n- load spatial objects into Python with geopandas.read_file() function\n- spatial objects can be plotted directly with GeoDataFrame’s .plot() method"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#crop-raster-data-with-rioxarray-and-geopandas",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#crop-raster-data-with-rioxarray-and-geopandas",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "8. Crop raster data with rioxarray and geopandas",
    "text": "8. Crop raster data with rioxarray and geopandas\nIt is quite common that the raster data we have in hand is too large to process, or not all the pixels are relevant to our area of interest (AoI). In both situations, we should consider cropping our raster data before performing data analysis. In this section, we will introduce how to crop raster data into the desired area. We will use one Sentinel-2 image over Amsterdam as the example raster data, and introduce how to crop our data to different types of AoIs.\n\nIntroduce the Data\nWe’ll continue from the results of the satellite image search that we carried out in a previous section. We will load data starting from the search.json file.\nThe rasta data can be downloaded using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started. We also use the vector data that was introduced in the previous section.\n\n\nCrop raster data with a bounding box\nLet’s load a true colour image using pystac and rioxarray and check the shape of the raster:\n\nimport pystac\nimport rioxarray\n\n# Load image and inspect the shape\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\n# Select a true colour image\ntrue_color_image = rioxarray.open_rasterio(items[1].assets[\"visual\"].href) \nprint(true_color_image.shape)\n\n(3, 10980, 10980)\n\n\nThe large size of the raster data makes it time and memory consuming to visualise in its entirety. Instead, we can plot the “overview” asset, to investigate the coverage of the image:\n\n# Get the overview asset\noverview_image = rioxarray.open_rasterio(items[1].assets[\"overview\"].href)\nprint(overview_image.shape)\n\n# Visualize it\noverview_image.plot.imshow(figsize=(8,8))\n\n(3, 343, 343)\n\n\n<matplotlib.image.AxesImage at 0x7fa21341cc70>\n\n\n\n\n\nAs we can see, the overview image is much smaller compared to the original true colour image. Therefore the visualization is much faster. If we are interested in the crop fields, then we would like to know where these are located in the image. To compare its coverage with the raster data, we first check the coordinate systems of both raster and vector data.\nFor raster data, we use pyproj.CRS:\n\nfrom pyproj import CRS\n\n# Check the coordinate system\nCRS(true_color_image.rio.crs)\n\n<Derived Projected CRS: EPSG:32631>\nName: WGS 84 / UTM zone 31N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 31N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nTo open and check the coordinate system of vector data, we use geopandas:\n\nimport geopandas as gpd\n\n# Load the polygons of the crop fields\ncf_boundary_crop = gpd.read_file(\"cropped_field.shp\")\n\n# Check the coordinate system\ncf_boundary_crop.crs\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\n- bounds: (3.2, 50.75, 7.22, 53.7)\nCoordinate Operation:\n- name: RD New\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\n\n\n\n\n\n\nAs we can see, the coordinate systems differ.\n\n\n\nTo crop the raster using the shapefile, we first convert the coordinate system of cf_boundary_crop to the coordinate system of true_color_image, using .to_crs and then check the coverage:\n\n\n\nfrom matplotlib import pyplot as plt\n\n# Convert the coordinate system\ncf_boundary_crop = cf_boundary_crop.to_crs(true_color_image.rio.crs)\n\n# Plot\nfig, ax = plt.subplots()\nfig.set_size_inches((8,8))\n\n# Plot image\noverview_image.plot.imshow(ax=ax)\n\n# Plot crop fields\ncf_boundary_crop.plot(\n    ax=ax,\n    edgecolor=\"red\",\n)\n\n<AxesSubplot: title={'center': 'spatial_ref = 0'}, xlabel='x', ylabel='y'>\n\n\n\n\n\nSeeing from the location of the polygons, the crop fields (red) only takes a small part of the raster. Therefore, before actual processing, we can first crop the raster to our area of interest. The clip_box function allows us to crop a raster by the min/max of the x and y coordinates. Note that we are cropping the original image true_color_image now, and not the overview image overview_image.\n\n# Crop the raster with the bounding box\nraster_clip_box = true_color_image.rio.clip_box(*cf_boundary_crop.total_bounds)\nprint(raster_clip_box.shape)\n\n(3, 1574, 1584)\n\n\nWe successfully cropped the raster to a much smaller piece. We can visualize it now:\n\n# view the image\nraster_clip_box.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa213d2ab90>\n\n\n\n\n\nThis cropped image can be saved for later use:\n\n# save cropped image as .tif file\nraster_clip_box.rio.to_raster(\"raster_clip.tif\")\n\n\n\nCrop raster data with polygons\nWe have a cropped image around the fields. To further analyze the fields, we might want to crop the image to the exact field boundaries. This can be done with the clip function:\n\n# crop image to exact field boundaries\nraster_clip_fields = raster_clip_box.rio.clip(cf_boundary_crop['geometry'])\n\nAnd we can visualize the results:\n\n# view the image\nraster_clip_fields.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa2132929b0>\n\n\n\n\n\nWe can save this image for later use:\n\n# save image as .tif file\nraster_clip_fields.rio.to_raster(\"crop_fields.tif\")\n\n\n\nCrop raster data with a geometry buffer\nIt is not always the case that the AoI comes in polygon format. Sometimes we would like to perform analysis around a (set of) point(s), or polyline(s). For example, in our AoI, there are also some groundwater monitoring wells available as point vector data. We may also want to perform analysis around these wells. The location of the wells is stored in data/groundwater_monitoring_well.\nWe can first load the wells vector data, and select wells within the coverage of the image:\n\n# Load wells\nwells = gpd.read_file(\"Data/brogmwvolledigeset.zip\")\nwells = wells.to_crs(raster_clip_box.rio.crs)\n\n# Crop the wells to the image extent\nxmin, ymin, xmax, ymax = raster_clip_box.rio.bounds()\nwells = wells.cx[xmin:xmax, ymin:ymax]\n\nThen we can check the location of the wells:\n\n# Plot the wells over raster\nfig, ax = plt.subplots()\nfig.set_size_inches((8,8))\nraster_clip_box.plot.imshow(ax=ax)\nwells.plot(ax=ax, color='red', markersize=2)\n\n<AxesSubplot: title={'center': 'spatial_ref = 0'}, xlabel='x coordinate of projection\\n[metre]', ylabel='y coordinate of projection\\n[metre]'>\n\n\n\n\n\nTo select pixels around the geometries, we need to first define a region including the geometries. This region is called a “buffer” and it is defined in the units of the projection. The size of the buffer depends on the analysis in our research. A buffer is also a polygon, which can be used to crop the raster data. geopandas’ objects have a buffer method to generate buffer polygons.\n\n# Create 200m buffer around the wells\nwells_buffer = wells.buffer(200)\n\n\n# Visualize buffer on raster\nfig, ax = plt.subplots()\nfig.set_size_inches((8,8))\nraster_clip_box.plot.imshow(ax=ax)\nwells_buffer.plot(ax=ax, color='red')\n\n<AxesSubplot: title={'center': 'spatial_ref = 0'}, xlabel='x coordinate of projection\\n[metre]', ylabel='y coordinate of projection\\n[metre]'>\n\n\n\n\n\nThe red dots have grown larger indicating the conversion from points to buffer polygons.\n\n\nSelect the raster data around the wells\nNow we have the buffer polygons around the groudwater monitoring wells, i.e. wells_buffer. Let’s now crop the image raster_clip_box to the buffer polygons, and visualize the results of cropping:\n\n# Crop the image raster_clip_box to the buffer polygons\nraster_clip_wells = raster_clip_box.rio.clip(wells_buffer)\n\n# Visualize cropped buffer\nraster_clip_wells.plot.imshow()\n\nNameError: name 'raster_clip_box' is not defined\n\n\n\n\nSelect the raster data around the waterways\nLet’s now attempt to select all the raster data within 100m around the waterways, and visualize the results:\n\n# Load waterways\nwaterways_nl = gpd.read_file(\"Data/waterways_nl_corrected.shp\")\nwaterways_nl = waterways_nl.to_crs(raster_clip_box.rio.crs)\n\n# Crop the waterways to the image extent\nwaterways_nl = waterways_nl.cx[xmin:xmax, ymin:ymax]\n\n# Create 100m buffer around the waterways\nwaterways_nl_buffer = waterways_nl.buffer(100)\n\n# Crop\nraster_clip_waterways = raster_clip_box.rio.clip(waterways_nl_buffer)\n\n# Visualize\nraster_clip_waterways.plot.imshow(figsize=(8,8))\n\nNameError: name 'raster_clip_box' is not defined\n\n\n\n\nCrop raster data using another raster dataset\nSo far we have learned how to crop raster image with vector data. We can also crop a raster with another raster data. Let’s demonstrate how to crop the true_color_image image using the crop_fields.tif image. that was produced in the sub-section “Crop raster data with polygon”.\n\n# Read crop_fields\ncrop_fields = rioxarray.open_rasterio(\"Data/crop_fields.tif\")\n\n# Reproject to RD to make the CRS different from the \"true_color_image\"\ncrop_fields = crop_fields.rio.reproject(\"EPSG:28992\")\nCRS(crop_fields.rio.crs)\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: unnamed\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\nAnd let’s check again the CRS of true_color_image:\n\n# Get CRS of true_color_image\nCRS(true_color_image.rio.crs)\n\n<Derived Projected CRS: EPSG:32631>\nName: WGS 84 / UTM zone 31N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 31N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nNow the two images are in different coordinate systems. We can use rioxarray.reproject_match() function to crop true_color_image image. It will perform both the reprojection and the cropping operation. This might take a few minutes, because the true_color_image image is large:\n\n# Crop and reproject\ncropped_raster = true_color_image.rio.reproject_match(crop_fields)\n\n# Visualize\ncropped_raster.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa20b696b90>\n\n\n\n\n\n\n\n\n\n\n\nIn one line reproject_match does a lot of helpful things:\n\n\n\n\nit reprojects\nit matches the extent using nodata values or by clipping the data\nit sets nodata values. This means we can run calculations on those two images\n\n\n\n\n\n\n\n\n\nIf you want more control over how rasters are resampled, clipped, and/or reprojected\n\n\n\nUse the reproject() method and other rioxarray methods individually\n\n\nThis time let’s do it the other way around by cropping the crop_fields image using the true_color_image image:\n\n# Crop and reproject\ncropped_raster = crop_fields.rio.reproject_match(true_color_image)\n\n# Visualize\ncropped_raster.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa20b647c10>\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- use clip_box in DataArray.rio to crop a raster with a bounding box\n- use clip in DataArray.rio to crop a raster with a given polygon\n- use buffer in geopandas to make a buffer polygon of a (multi)point or a polyline. This polygon can be used to crop data\n- use reproject_match function in DataArray.rio to reproject and crop a raster data using another raster data"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-calculations-in-python",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-calculations-in-python",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "9. Raster Calculations in Python",
    "text": "9. Raster Calculations in Python\nWe often want to combine values of and perform calculations on rasters to create a new output raster. This episode covers how to perform basic math operations using raster datasets. It also illustrates how to match rasters with different resolutions so that they can be used in the same calculation. As an example, we will calculate a vegetation index over one of the satellite scenes.\n\nNormalized Difference Vegetation Index (NDVI)\nSuppose we are interested in monitoring vegetation fluctuations using satellite remote sensors. Scientists have defined a vegetation index to quantify the amount of green leaf vegetation using the light reflected in different wavelengths. This index, named Normalized Difference Vegetation Index (NDVI), exploits the fact that healthy green leaves strongly absorb red visible light while they mostly reflect light in the near infrared (NIR). The NDVI is computed as:\nNDVI = \\(\\frac{NIR - red}{NIR + red}\\)\nwhere NIR and red label the reflectance values of the corresponding wavelengths. NDVI values range from -1 to +1.\nValues close to one indicate high density of green leaves. Poorly vegetated areas typically have NDVI values close to zero. Negative NDVI values often indicate cloud and water bodies.\n\nSource: Wu C-D, McNeely E, Cedeño-Laurent JG, Pan W-C, Adamkiewicz G, Dominici F, et al. (2014) Linking Student Performance in Massachusetts Elementary Schools with the “Greenness” of School Surroundings Using Remote Sensing. PLoS ONE 9(10): e108548. https://doi.org/10.1371/journal.pone.0108548\nCheck out more on NDVI in the NASA Earth Observatory portal: Measuring Vegetation.\n\n\nLoad and crop the data\nWe’ll continue from the results of the satellite image search that we have carried out in Section 5 previously. We will load data starting from the search.json file. You can download the raster data using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started.\nLet’s load the results of our initial imagery search using pystac:\n\nimport pystac\n# load in our imagery search\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\nWe then select the second item, and extract the URIs of the red and NIR bands (“B04” and “B8A”, respectively):\n\n# extract URIs of second items of red and NIR bands\nred_uri = items[1].assets[\"B04\"].href\nnir_uri = items[1].assets[\"B8A\"].href\n\nLet’s load the rasters with open_rasterio using the argument masked=True.\n\nimport rioxarray\n# load in the rasters with argument masked=True which \nred = rioxarray.open_rasterio(red_uri, masked=True)\nnir = rioxarray.open_rasterio(nir_uri, masked=True)\n\nLet’s also restrict our analysis to the same crop field area defined in the previous section by clipping the rasters using a bounding box:\n\n# clip the rasters using a bounding box\nbbox = (629_000, 5_804_000, 639_000, 5_814_000)\nred_clip = red.rio.clip_box(*bbox)\nnir_clip = nir.rio.clip_box(*bbox)\n\nWe can now plot the two rasters. Using robust=True color values are stretched between the 2nd and 98th percentiles of the data, which results in clearer distinctions between high and low reflectances:\n\n# plot the red visible light wavelength raster\nred_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20b6031c0>\n\n\n\n\n\n\n\n\n\n\n\nThe crop fields (rectangular shapes in the central part of the figure) appear as dark spots in the red-visible wavelength, suggesting the presence of leafy crop at the time of observation (end of March).\n\n\n\nThe same fields would instead appear as bright spots in the off season.\n\n\n\n# plot the near infra red wavelength raster\nnir_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20ca6ff70>\n\n\n\n\n\n\n\n\n\n\n\nThe crop fields (rectangular shapes in the central part of the figure) appear as bright spots in the NIR wavelength, suggesting the presence of leafy crop at the time of observation (end of March).\n\n\n\nThe same fields would instead appear as dark spots in the off season.\n\n\nLet’s check this by grabbing scenes for an ‘off-season’ period\n\napi_url = \"https://earth-search.aws.element84.com/v0\"\n\nfrom pystac_client import Client\nclient = Client.open(api_url)\n\n# Sentinel-2, Level 2A, COGs\ncollection = \"sentinel-s2-l2a-cogs\" \n\n# AMS coordinates\nfrom shapely.geometry import Point\npoint = Point(4.89, 52.37)  \n\nbound_box = point.buffer(0.01).bounds\n\n# Search for scenes which intersect bounding box +- 0.01 deg in lat/lon from (4.89, 52.37) as prev defined\n# between 09/13/20 and 09/30/20 where cloud cover < 10%\nsearch = client.search(\n    collections=[collection],\n    bbox=bound_box,\n    datetime=\"2020-09-13/2020-09-30\",\n    query=[\"eo:cloud_cover<10\"]\n)\n\nprint(search.matched())\n\n# Grab search items and save as a JSON file\nitems_off_season = search.get_all_items()\nitems_off_season.save_object(\"search_off_season.json\")\n\n3\n\n\n\nimport pystac\nitems_off_season = pystac.ItemCollection.from_file(\"search_off_season.json\")\n\n# extract URIs of second items of red and NIR bands\nred_uri_off_season = items_off_season[1].assets[\"B04\"].href\nnir_uri_off_season = items_off_season[1].assets[\"B8A\"].href\n\nimport rioxarray\n\nred_off_season = rioxarray.open_rasterio(red_uri_off_season, masked=True)\nnir_off_season = rioxarray.open_rasterio(nir_uri_off_season, masked=True)\n\n# clip the rasters using a bounding box\nbbox = (629_000, 5_804_000, 639_000, 5_814_000)\nred_off_season_clip = red_off_season.rio.clip_box(*bbox)\nnir_off_season_clip = nir_off_season.rio.clip_box(*bbox)\n\n\n# plot the OFF SEASON red visible light wavelength raster\nred_off_season_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20c4d8130>\n\n\n\n\n\n\n# plot the OFF SEASON near infra red wavelength raster\nnir_off_season_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20c4787c0>\n\n\n\n\n\n\n\nRaster Math\nWe can perform raster calculations by subtracting (or adding, multiplying, etc.) two rasters. In the geospatial world, we call this “raster math”, and typically it refers to operations on rasters that have the same width and height (including nodata pixels). We can check the shapes of the two rasters in the following way:\n\nprint(red_clip.shape, nir_clip.shape)\n\n(1, 1000, 1000) (1, 500, 500)\n\n\nBoth rasters include a single band, but their width and height do not match. We can now use the reproject_match function, which both reprojects and clips a raster to the CRS and extent of another raster.\n\nred_clip_matched = red_clip.rio.reproject_match(nir_clip)\nprint(red_clip_matched.shape)\n\n(1, 500, 500)\n\n\nLet’s now compute the NDVI as a new raster using the formula\nNDVI = $\\frac{NIR - red}{NIR + red}$\nWe’ll use rioxarray objects so that we can easily plot our result and keep track of the metadata.\n\nndvi = (nir_clip - red_clip_matched)/ (nir_clip + red_clip_matched)\nprint(ndvi)\n\n<xarray.DataArray (band: 1, y: 500, x: 500)>\narray([[[ 0.7379576 ,  0.77153456,  0.54531944, ...,  0.39254385,\n          0.49227372,  0.4465174 ],\n        [ 0.7024894 ,  0.7074668 ,  0.3903298 , ...,  0.423283  ,\n          0.4706971 ,  0.45964912],\n        [ 0.6557818 ,  0.5610572 ,  0.46742022, ...,  0.4510345 ,\n          0.43815723,  0.6005133 ],\n        ...,\n        [ 0.02391171,  0.21843003,  0.02479339, ..., -0.50923485,\n         -0.53367877, -0.4955414 ],\n        [ 0.11376493,  0.17681159, -0.1673566 , ..., -0.5221932 ,\n         -0.5271318 , -0.4852753 ],\n        [ 0.45398772, -0.00518135,  0.03346133, ..., -0.5019455 ,\n         -0.4987013 , -0.49081364]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6.29e+05 6.29e+05 6.29e+05 ... 6.39e+05 6.39e+05\n  * y            (y) float64 5.814e+06 5.814e+06 ... 5.804e+06 5.804e+06\n    spatial_ref  int64 0\n\n\nWe can now plot the output NDVI:\n\nndvi.plot()\n\n<matplotlib.collections.QuadMesh at 0x7fa20b9cfdc0>\n\n\n\n\n\n\n\n\n\n\n\nNotice that the range of values for the output NDVI is between -1 and 1. This makes sense for the selected region?\n\n\n\n\n\nYes. Remember that NDVI values close to one indicate high density of green leaves, and negative NDVI values often indicate water bodies. This is consistent with our region.\n\n\n\nMaps are great, but it can also be informative to plot histograms of values to better understand the distribution. We can accomplish this using a built-in xarray method we have already been using: .plot\n\nndvi.plot.hist()\n\n(array([2.1000e+01, 1.2800e+02, 1.6076e+04, 1.3546e+04, 6.4030e+03,\n        1.5811e+04, 2.4608e+04, 4.0979e+04, 8.2569e+04, 4.9858e+04]),\n array([-9.98647749e-01, -7.98825085e-01, -5.99002421e-01, -3.99179786e-01,\n        -1.99357137e-01,  4.65512276e-04,  2.00288162e-01,  4.00110811e-01,\n         5.99933445e-01,  7.99756110e-01,  9.99578774e-01]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n\nExplore NDVI Raster Values\nIt’s often a good idea to explore the range of values in a raster dataset just like we might explore a dataset that we collected in the field. The histogram we just made is a good start but there’s more we can do to improve our understanding of the data.\n\n# What is the min and maximum value for the NDVI raster (ndvi) that we just created?\n# Are there missing values?\n\nprint(ndvi.min().values)\nprint(ndvi.max().values)\nprint(ndvi.isnull().sum().values)\n\n-0.99864775\n0.9995788\n1\n\n\n\n# Plot a histogram with 50 bins instead of 8. What do you notice that wasn’t clear before?\nndvi.plot.hist(bins=50)\n\n(array([1.2000e+01, 2.0000e+00, 2.0000e+00, 3.0000e+00, 2.0000e+00,\n        8.0000e+00, 7.0000e+00, 4.0000e+00, 2.9000e+01, 8.0000e+01,\n        5.5800e+02, 2.0930e+03, 3.1920e+03, 4.6700e+03, 5.5630e+03,\n        7.6140e+03, 2.9020e+03, 1.3340e+03, 9.5700e+02, 7.3900e+02,\n        8.1800e+02, 1.0210e+03, 1.2840e+03, 1.4820e+03, 1.7980e+03,\n        2.1740e+03, 2.8470e+03, 3.2340e+03, 3.7030e+03, 3.8530e+03,\n        4.2410e+03, 4.6100e+03, 4.8320e+03, 5.3200e+03, 5.6050e+03,\n        6.2230e+03, 6.9630e+03, 7.9370e+03, 9.0210e+03, 1.0835e+04,\n        1.2352e+04, 1.3883e+04, 1.5219e+04, 1.8287e+04, 2.2828e+04,\n        2.5534e+04, 1.9329e+04, 4.9850e+03, 8.0000e+00, 2.0000e+00]),\n array([-9.98647749e-01, -9.58683193e-01, -9.18718696e-01, -8.78754139e-01,\n        -8.38789642e-01, -7.98825085e-01, -7.58860588e-01, -7.18896031e-01,\n        -6.78931534e-01, -6.38966978e-01, -5.99002421e-01, -5.59037924e-01,\n        -5.19073367e-01, -4.79108840e-01, -4.39144313e-01, -3.99179786e-01,\n        -3.59215260e-01, -3.19250733e-01, -2.79286206e-01, -2.39321664e-01,\n        -1.99357137e-01, -1.59392610e-01, -1.19428076e-01, -7.94635490e-02,\n        -3.94990183e-02,  4.65512276e-04,  4.04300429e-02,  8.03945735e-02,\n         1.20359100e-01,  1.60323635e-01,  2.00288162e-01,  2.40252689e-01,\n         2.80217230e-01,  3.20181757e-01,  3.60146284e-01,  4.00110811e-01,\n         4.40075338e-01,  4.80039865e-01,  5.20004392e-01,  5.59968948e-01,\n         5.99933445e-01,  6.39898002e-01,  6.79862559e-01,  7.19827056e-01,\n         7.59791613e-01,  7.99756110e-01,  8.39720666e-01,  8.79685163e-01,\n         9.19649720e-01,  9.59614217e-01,  9.99578774e-01]),\n <BarContainer object of 50 artists>)\n\n\n\n\n\nIncreasing the number of bins gives us a much clearer view of the distribution. Also, there seem to be very few NDVI values larger than ~ 0.9.\n\n# Plot the ndvi raster using breaks that make sense for the data\nclass_bins = (-1, 0., 0.2, 0.7, 1)\nndvi.plot(levels=class_bins)\n\n<matplotlib.collections.QuadMesh at 0x7fa20b8f6e90>\n\n\n\n\n\nWe can discretize the colour bar by specifying the intervals via the level argument to plot(). Suppose we want to bin our data in the following intervals:\n\n-1 \\(\\le\\) NDVI \\(\\lt\\) 0 for water\n0 \\(\\le\\) NDVI \\(\\lt\\) 0.2 for no vegetation\n0.2 \\(\\le\\) NDVI \\(\\lt\\) 0.7 for sparse vegetation\n0.7 \\(\\le\\) NDVI \\(\\lt\\) 1 for dense vegetation\n\nMissing values can be interpolated from the values of neighbouring grid cells using the .interpolate_na method. We then save ndvi as a GeoTiff file:\n\nndvi_nonan = ndvi.interpolate_na(dim=\"x\")\nndvi_nonan.rio.to_raster(\"NDVI.tif\")\n\n\n\nClassifying Continuous Rasters in Python\nNow that we have a sense of the distribution of our NDVI raster, we can reduce the complexity of our map by classifying it. Classification involves assigning each pixel in the raster to a class based on its value. In Python, we can accomplish this using the numpy.digitize function.\nFirst, we define NDVI classes based on a list of values, as defined in the last exercise: [-1, 0., 0.2, 0.7, 1]. When bins are ordered from low to high, as here, numpy.digitize assigns classes like so:\n\n\n\nNDVI-classes.jpg\n\n\nSource: Image (license)\nNote that, by default, each class includes the left but not the right bound. This is not an issue here, since the computed range of NDVI values is fully contained in the open interval (-1; 1).\n\nimport numpy as np\nimport xarray\n\n# Defines the bins for pixel values\nclass_bins = (-1, 0., 0.2, 0.7, 1)\n\n# The numpy.digitize function returns an unlabeled array, in this case, a\n# classified array without any metadata. That doesn't work--we need the\n# coordinates and other spatial metadata. We can get around this using\n# xarray.apply_ufunc, which can run the function across the data array while\n# preserving metadata.\nndvi_classified = xarray.apply_ufunc(\n    np.digitize,\n    ndvi_nonan,\n    class_bins\n)\n\nLet’s now visualize the classified NDVI, customizing the plot with proper title and legend. We can use EarthPy to assist. We then export the figure in PNG format:\n\nimport earthpy.plot as ep\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.colors import ListedColormap\n\n# Define color map of the map legend\nndvi_colors = [\"blue\", \"gray\", \"green\", \"darkgreen\"]\nndvi_cmap = ListedColormap(ndvi_colors)\n\n# Define class names for the legend\ncategory_names = [\n    \"Water\",\n    \"No Vegetation\",\n    \"Sparse Vegetation\",\n    \"Dense Vegetation\"\n]\n\n# We need to know in what order the legend items should be arranged\ncategory_indices = list(range(len(category_names)))\n\n# Make the plot\nim = ndvi_classified.plot(cmap=ndvi_cmap, add_colorbar=False)\nplt.title(\"Classified NDVI\")\n\n# earthpy helps us by drawing a legend given an existing image plot and legend items, plus indices\nep.draw_legend(im_ax=im, classes=category_indices, titles=category_names)\n\n# Save the figure\nplt.savefig(\"NDVI_classified.png\", bbox_inches=\"tight\", dpi=300)\n\n\n\n\nWe can finally export the classified NDVI raster object to a GeoTiff file. The to_raster() function by default writes the output file to your working directory unless you specify a full file path.\n\nndvi_classified.rio.to_raster(\"NDVI_classified.tif\", dtype=\"int32\")\n\n\n\nCompute the NDVI for the Texel island\nData are often more interesting and powerful when we compare them across various locations. Let’s compare the computed NDVI map with the one of another region in the same Sentinel-2 scene: the Texel island, located in the North Sea. You should have the red- and the NIR-band rasters already loaded (red and nir variables, respectively).\n\n# Crop the two rasters using the following bounding box: (610000, 5870000, 630000, 5900000)\n# Don’t forget to check the shape of the data, and make sure the cropped areas have the same CRSs, heights and widths\n\nbbox_texel = (610_000, 5_870_000, 630_000, 5_900_000)\n\n# We crop the area of interest using clip_box\nred_texel = red.rio.clip_box(*bbox_texel)\nnir_texel = nir.rio.clip_box(*bbox_texel)\n\n\n# Reproject and clip one raster to the extent of the smaller raster using reproject_match. \n# The lines of code below assign a variable to the reprojected raster and calculate the NDVI\nred_texel_matched = red_texel.rio.reproject_match(nir_texel)\nndvi_texel = (nir_texel - red_texel_matched)/ (nir_texel + red_texel_matched)\n\n\n# Plot the NDVI and save the raster data as a GeoTIFF file.\nndvi_texel.plot()\nndvi_texel.rio.to_raster(\"NDVI_Texel.tif\")\n\n\n\n\n\n# Compute the NDVI histogram and compare it with the region that we have previously investigated. \nndvi_texel.plot.hist(bins=50)\n\n(array([1.00000e+00, 1.00000e+00, 2.00000e+00, 5.40000e+01, 4.69000e+02,\n        2.57300e+03, 1.62910e+04, 8.02330e+04, 1.78238e+05, 1.53825e+05,\n        1.01914e+05, 9.36200e+04, 9.87920e+04, 1.01029e+05, 9.09090e+04,\n        5.97100e+04, 2.73410e+04, 9.47800e+03, 3.04700e+03, 1.62600e+03,\n        1.55700e+03, 1.65400e+03, 2.29600e+03, 3.89600e+03, 5.28800e+03,\n        1.36810e+04, 4.09490e+04, 2.19440e+04, 1.86000e+04, 1.45780e+04,\n        1.44360e+04, 1.21830e+04, 1.17350e+04, 1.19370e+04, 1.27390e+04,\n        1.38190e+04, 1.53210e+04, 1.83800e+04, 2.38000e+04, 2.65840e+04,\n        2.62420e+04, 2.41790e+04, 2.32460e+04, 2.33390e+04, 2.34600e+04,\n        2.27600e+04, 2.10470e+04, 1.70200e+04, 1.20100e+04, 2.16700e+03]),\n array([-0.9191919 , -0.88229859, -0.84540534, -0.80851203, -0.77161872,\n        -0.73472542, -0.69783217, -0.66093886, -0.62404555, -0.58715224,\n        -0.55025899, -0.51336569, -0.47647238, -0.4395791 , -0.40268579,\n        -0.36579251, -0.3288992 , -0.29200593, -0.25511262, -0.21821934,\n        -0.18132605, -0.14443275, -0.10753946, -0.07064617, -0.03375287,\n         0.00314042,  0.04003371,  0.07692701,  0.1138203 ,  0.15071359,\n         0.18760689,  0.22450018,  0.26139346,  0.29828677,  0.33518004,\n         0.37207335,  0.40896663,  0.44585994,  0.48275322,  0.51964653,\n         0.55653983,  0.59343308,  0.63032639,  0.6672197 ,  0.70411301,\n         0.74100626,  0.77789956,  0.81479287,  0.85168618,  0.88857943,\n         0.92547274]),\n <BarContainer object of 50 artists>)\n\n\n\n\n\n\nmany more grid cells have negative NDVI values, since the area of interest includes much more water\nalso, NDVI values close to zero are more abundant, indicating the presence of bare ground (sand) regions\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- Python’s built-in math operators are fast and simple options for raster math\n- numpy.digitize can be used to classify raster values in order to generate a less complicated map"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#calculating-zonal-statistics-on-rasters",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#calculating-zonal-statistics-on-rasters",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "10. Calculating Zonal Statistics on Rasters",
    "text": "10. Calculating Zonal Statistics on Rasters\nStatistics on predefined zones of the raster data are commonly used for analysis and to better understand the data. These zones are often provided within a single vector dataset, identified by certain vector attributes. For example, in the previous sections, we used the crop field polygon dataset. The fields with the same crop type can be identified as a “zone”, resulting in multiple zones in one vector dataset. We might be interested in performing statistical analysis over these crop zones.\nIn this section, we will explore how to calculate zonal statistics based on the types of crops in *cropped_field.shp . To do this, we will first identify zones from the vector data, then rasterize these vector zones. Finally the zonal statistics for ndvi will be calculated over the rasterized zones.\n\nMaking vector and raster data compatible\nFirst, let’s load the NDVI.tif file saved in the previous episode to obtained our calculated raster ndvi data. We also use the squeeze() function in order to reduce our raster data ndvi dimension to 2D by removing the singular band dimension - this is necessary for use with the rasterize and zonal_stats functions:\n\nimport rioxarray\n\n# load in raster data from .tif file\nndvi = rioxarray.open_rasterio('Data/NDVI.tif')\n\n# reduce raster data dimension to 2D\nndvi_sq = ndvi.squeeze()\n\nLet’s also read the crop fields vector data from our saved cropped_field.shp file and view the CRS information.\n\nimport geopandas as gpd\n\n# read in the vector data from the .shp file\nfield = gpd.read_file('Data/cropped_field.shp')\nfield.crs\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\n- bounds: (3.2, 50.75, 7.22, 53.7)\nCoordinate Operation:\n- name: RD New\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\n\n\n\n\n\n\nLoading a .shp file\n\n\n\nAll the files which come with the .shp file MUST be in the same folder (.cp, .dbf, .prj, .shx)\n\n\nIn order to use the vector data as a classifier for our raster, we need to convert the vector data to the appropriate CRS. We can perform the CRS conversion from the vector CRS (EPSG:28992) to our raster ndvi CRS (EPSG:32631) and view the data with:\n\n# convert vector data to appropriate CRS\nfield_to_raster_crs = field.to_crs(ndvi.rio.crs)\nfield_to_raster_crs\n\n\n\n\n\n  \n    \n      \n      category\n      gewas\n      gewascode\n      jaar\n      status\n      geometry\n    \n  \n  \n    \n      0\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627394.386 5812746.538, 627393.681 5...\n    \n    \n      1\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627326.170 5813192.913, 627324.734 5...\n    \n    \n      2\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627093.646 5812863.661, 627090.972 5...\n    \n    \n      3\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627384.004 5813467.330, 627376.549 5...\n    \n    \n      4\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((627179.293 5812870.376, 627160.747 5...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4867\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640710.959 5809885.718, 640703.030 5...\n    \n    \n      4868\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640753.587 5810143.151, 640751.454 5...\n    \n    \n      4869\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((640890.832 5809998.743, 640892.271 5...\n    \n    \n      4870\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640770.878 5810005.390, 640766.767 5...\n    \n    \n      4871\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640704.702 5809791.384, 640698.357 5...\n    \n  \n\n4872 rows × 6 columns\n\n\n\n\n\nRasterizing our vector data\nBefore calculating zonal statistics, we first need to rasterize our field_to_raster_crs vector geodataframe with the rasterio.features.rasterize function. With this function, we aim to produce a grid with numerical values representing the types of crop as defined by the column gewascode from field_cropped. gewascode stands for the crop codes as defined by the Netherlands Enterprise Agency (RVO) for different types of crops or gewas (Grassland, permanent; Grassland, temporary; corn fields; etc.). This grid of values thus defines the zones for the xrspatial.zonal_stats function, where each pixel in the zone grid overlaps with a corresponding pixel in our NDVI raster.\nWe can generate the geometry, gewascode pairs for each vector feature to be used as the first argument to rasterio.features.rasterize as:\n\n# generate geometty,gewascode pairings\ngeom = field_to_raster_crs[['geometry', 'gewascode']].values.tolist()\n\n# Limit output to firat 10 items\ngeom[:10]\n\n[[<POLYGON ((627394.386 5812746.538, 627393.681 5812749.022, 627384.063 581279...>,\n  265],\n [<POLYGON ((627326.17 5813192.913, 627324.734 5813196.828, 627324.11 5813203....>,\n  265],\n [<POLYGON ((627093.646 5812863.661, 627090.972 5812870.702, 627076.762 581286...>,\n  265],\n [<POLYGON ((627384.004 5813467.33, 627376.549 5813470.117, 627385.233 5813495...>,\n  265],\n [<POLYGON ((627179.293 5812870.376, 627160.747 5812846.868, 627159.505 581284...>,\n  331],\n [<POLYGON ((627419.816 5812765.254, 627416.356 5812766.811, 627413.1 5812769....>,\n  265],\n [<POLYGON ((627358.683 5813335.376, 627362.871 5813352.853, 627370.175 581337...>,\n  265],\n [<POLYGON ((627423.06 5813550.358, 627421.409 5813550.392, 627427.293 5813568...>,\n  265],\n [<POLYGON ((627191.219 5813028.547, 627189.596 5813028.866, 627186.64 5813029...>,\n  265],\n [<POLYGON ((627349.601 5813329.836, 627343.37 5813330.159, 627341.775 5813330...>,\n  265]]\n\n\nWe can now rasterize our vector data using rasterio.features.rasterize:\n\nfrom rasterio import features\nfield_cropped_raster = features.rasterize(geom, out_shape=ndvi_sq.shape, fill=0, transform=ndvi.rio.transform())\n\nThe argument out_shape specifies the shape of the output grid in pixel units, while transform represents the projection from pixel space to the projected coordinate space. We also need to specify the fill value for pixels that are not contained within a polygon in our shapefile, which we do with fill = 0. It’s important to pick a fill value that is not the same as any value already defined in gewascode or else we won’t distinguish between this zone and the background.\nWe convert the output of the rasterio.features.rasterize function, which generates a numpy array np.ndarray, to xarray.DataArray which will be used further:\n\nimport xarray as xr\nfield_cropped_raster_xarr = xr.DataArray(field_cropped_raster)\n\n\n\nCalculate zonal statistics\nIn order to calculate the statistics for each crop zone, we call the function, xrspatial.zonal_stats. The xrspatial.zonal_stats function takes as input zones, a 2D xarray.DataArray, that defines different zones, and values, a 2D xarray.DataArray providing input values for calculating statistics.\nWe call the zonal_stats function with field_cropped_raster_xarr as our classifier and the 2D raster with our values of interest ndvi_sq to obtain the NDVI statistics for each crop type:\n\nfrom xrspatial import zonal_stats\nzonal_stats(field_cropped_raster_xarr, ndvi_sq)\n\n\n\n\n\n  \n    \n      \n      zone\n      mean\n      max\n      min\n      sum\n      std\n      var\n      count\n    \n  \n  \n    \n      0\n      0\n      0.266528\n      0.999579\n      -0.998648\n      38887.554688\n      0.409970\n      0.168075\n      145904.0\n    \n    \n      1\n      259\n      0.520282\n      0.885242\n      0.289196\n      449.003052\n      0.111205\n      0.012366\n      863.0\n    \n    \n      2\n      265\n      0.775609\n      0.925955\n      0.060755\n      66478.976562\n      0.091089\n      0.008297\n      85712.0\n    \n    \n      3\n      266\n      0.794128\n      0.918048\n      0.544686\n      1037.925781\n      0.074009\n      0.005477\n      1307.0\n    \n    \n      4\n      331\n      0.703056\n      0.905304\n      0.142226\n      10725.819336\n      0.102255\n      0.010456\n      15256.0\n    \n    \n      5\n      332\n      0.681699\n      0.849158\n      0.178113\n      321.080261\n      0.123633\n      0.015285\n      471.0\n    \n    \n      6\n      335\n      0.648063\n      0.865804\n      0.239661\n      313.662598\n      0.146582\n      0.021486\n      484.0\n    \n    \n      7\n      863\n      0.388575\n      0.510572\n      0.185987\n      1.165724\n      0.144245\n      0.020807\n      3.0\n    \n  \n\n\n\n\nThe zonal_stats function calculates the minimum, maximum, and sum for each zone along with statistical measures such as the mean, variance and standard deviation for each rasterized vector zone.\n\nIn our raster data-set zone = 0, corresponding to non-crop areas, has the highest count followed by zone = 265 which corresponds to ‘Grasland, blijvend’ (‘Grassland, permanent’).\nThe highest mean NDVI is observed for zone = 266 for ‘Grasslands, temporary’ with the lowest mean, aside from non-crop area, going to zone = 863 representing ‘Forest without replanting obligation’.\n\nThus, the zonal_stats function can be used to analyse and understand different sections of our raster data. The definition of the zones can be derived from vector data or from classified raster data as presented in the challenge below:\n\n\nCalculate zonal statistics for zones defined by ndvi_classified\nTo apply what we have just learned, let’s now calculate NDVI zonal statistics for the different zones as classified by ndvi_classified in the previous section.\nFirst we’ll load both raster data-sets and convert into 2D xarray.DataArray. Then, we’ll calculate zonal statistics for each class_bins, and inspect the output of the zonal_stats function.\n\n# load both raster data-sets and convert into 2D xarray.DataArray\nndvi = rioxarray.open_rasterio('Data/NDVI.tif')\nndvi_classified = rioxarray.open_rasterio('Data/NDVI_classified.tif')\n\n\n# reduce raster data dimension to 2D\nndvi_classified_sq = ndvi_classified.squeeze()\n\n# calculate zonal statistics for each class_bins\nzonal_stats(ndvi_classified_sq, ndvi_sq)\n\n\n\n\n\n  \n    \n      \n      zone\n      mean\n      max\n      min\n      sum\n      std\n      var\n      count\n    \n  \n  \n    \n      0\n      1\n      -0.355660\n      -0.000257\n      -0.998648\n      -12838.253906\n      0.145916\n      0.021291\n      36097.0\n    \n    \n      1\n      2\n      0.110731\n      0.199839\n      0.000000\n      1754.752441\n      0.055864\n      0.003121\n      15847.0\n    \n    \n      2\n      3\n      0.507998\n      0.700000\n      0.200000\n      50410.167969\n      0.140193\n      0.019654\n      99233.0\n    \n    \n      3\n      4\n      0.798281\n      0.999579\n      0.700025\n      78888.523438\n      0.051730\n      0.002676\n      98823.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- zones can be extracted by attribute columns of a vector dataset\n- zones can be rasterized using rasterio.features.rasterize\n- calculate zonal statistics with xrspatial.zonal_stats over the rasterized zones"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#parallel-raster-computations-using-dask",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#parallel-raster-computations-using-dask",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "11. Parallel raster computations using Dask",
    "text": "11. Parallel raster computations using Dask\nVery often raster computations involve applying the same operation to different pieces of data. Think, for instance, to the “pixel”-wise sum of two raster datasets, where the same sum operation is applied to all the matching grid-cells of the two rasters. This class of tasks can benefit from chunking the input raster(s) into smaller pieces: operations on different pieces can be run in parallel using multiple computing units (e.g., multi-core CPUs), thus potentially speeding up calculations. In addition, working on chunked data can lead to smaller memory footprints, since one may bypass the need to store the full dataset in memory by processing it chunk by chunk.\nIn this section, we will introduce the use of Dask in the context of raster calculations. Dask is a Python library for parallel and distributed computing. It provides a framework to work with different data structures, including chunked arrays (Dask Arrays). Dask is well integrated with (rio)xarray objects, which can use Dask arrays as underlying data structures.\nThis section shows how Dask can be used to parallelize operations on local CPUs. However, the same library can be configured to run tasks on large compute clusters. More resources on Dask:\nDask Array\nXarray with Dask\nIt is important to realize, however, that many details determine the extent to which using Dask’s chunked arrays instead of regular NumPy arrays leads to faster calculations (and lower memory requirements). The actual operations to carry out, the size of the dataset, and parameters such as the chunks’ shape and size, all affects the performance of our computations. Depending on the specifics of the calculations, serial calculations might actually turn out to be faster! Being able to profile the computational time is thus essential, and we will see how to do that in a Jupyter environment in the next section.\n\nTime profiling in Jupyter\n\nWe’ll continue from the results of the satellite image search that we carried out in the previous sections. We will load data starting from the search.json file. You can download the raster data using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started.\nLet’s set up a raster calculation using assets from our previous search of satellite scenes. We first load the item collection using the pystac library:\n\nimport pystac\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\nLet’s select the last scene, and extract the URLs of two assets: - the true-color image (“visual”) and - the scene classification layer (“SCL”). The latter is a mask where each grid cell is assigned a label that represents a specific class e.g. “4” for vegetation, “6” for water, etc. (all classes and labels are reported in the Sentinel-2 documentation.\n\n# last item's assets\nassets = items[-1].assets\n\n# true color image\nvisual_href = assets[\"visual\"].href  \n\n# scene classification layer\nscl_href = assets[\"SCL\"].href \n\nOpening the two assets with rioxarray shows that the true-color image is available as a raster file with 10 m resolution, while the scene classification layer has a lower resolution (20 m):\n\nimport rioxarray\nscl = rioxarray.open_rasterio(scl_href)\nvisual = rioxarray.open_rasterio(visual_href)\nprint(scl.rio.resolution(), visual.rio.resolution())\n\n(20.0, -20.0) (10.0, -10.0)\n\n\nIn order to match the image and the mask pixels, we take advantage of a feature of the cloud-optimized GeoTIFF (COG) format, which is used to store these raster files. COGs typically include multiple lower-resolution versions of the original image, called “overviews”, in the same file. This allows to avoid downloading high-resolution images when only quick previews are required.\nOverviews are often computed using powers of 2 as down-sampling (or zoom) factors (e.g. 2, 4, 8, 16). For the true-color image we thus open the first level overview (zoom factor 2) and check that the resolution is now also 20 m:\n\nvisual = rioxarray.open_rasterio(visual_href, overview_level=0)\nprint(visual.rio.resolution())\n\n(20.0, -20.0)\n\n\nWe can now time profile the first step of our raster calculation: the (down)loading of the rasters’ content. We do it by using the Jupyter magic %%time, which returns the time required to run the content of a cell:\n\n%%time\nscl = scl.load()\nvisual = visual.load()\n\nCPU times: user 614 ms, sys: 241 ms, total: 855 ms\nWall time: 9.31 s\n\n\n\nvisual.plot.imshow(figsize=(10,10))\nscl.squeeze().plot.imshow(levels=range(13), figsize=(12,10))\n\n<matplotlib.image.AxesImage at 0x7f655d6225c0>\n\n\n\n\n\n\n\n\nAfter loading the raster files into memory, we run the following steps:\n\nwe create a mask of the grid cells that are labeled as “cloud” in the scene classification layer (values “8” and “9”, standing for medium- and high-cloud probability, respectively)\nwe use this mask to set the corresponding grid cells in the true-color image to null values\nwe save the masked image to disk as in COG format\n\nAgain, we measure the cell execution time using %%time:\n\n%%time\nmask = scl.squeeze().isin([8, 9])\nvisual_masked = visual.where(~mask, other=visual.rio.nodata)\nvisual_masked.rio.to_raster(\"band_masked.tif\")\n\nCPU times: user 164 ms, sys: 154 ms, total: 318 ms\nWall time: 317 ms\n\n\nWe can inspect the masked image as:\n\nvisual_masked.plot.imshow(figsize=(10, 10))\n\n<matplotlib.image.AxesImage at 0x7f655d3e7640>\n\n\n\n\n\nIn the following sub-section we will see how to parallelize these raster calculations, and we will compare timings to the serial calculations that we just ran.\n\n\nDask-powered rasters\n\nChunked arrays\nAs we have mentioned, rioxarray supports the use of Dask’s chunked arrays as underlying data structure. When opening a raster file with open_rasterio and providing the chunks argument, Dask arrays are employed instead of regular Numpy arrays. chunks describes the shape of the blocks which the data will be split in. As an example, we open the blue band raster (“B02”) using a chunk shape of (1, 4000, 4000) (block size of 1 in the first dimension and of 4000 in the second and third dimensions):\n\nblue_band_href = assets[\"B02\"].href\nblue_band = rioxarray.open_rasterio(blue_band_href, chunks=(1, 4000, 4000))\n\nXarray and Dask also provide a graphical representation of the raster data array and of its blocked structure:\n\nblue_band\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 10980, x: 10980)>\ndask.array<open_rasterio-bb9b2ce30f7d0b7c18c953045067cb21<this-array>, shape=(1, 10980, 10980), dtype=uint16, chunksize=(1, 4000, 4000), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6e+05 6e+05 ... 7.098e+05 7.098e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 10980x: 10980dask.array<chunksize=(1, 4000, 4000), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         229.95 MiB \n                         30.52 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 10980, 10980) \n                         (1, 4000, 4000) \n                    \n                    \n                         Count \n                         2 Graph Layers \n                         9 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  10980\n  10980\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05array([600005., 600015., 600025., ..., 709775., 709785., 709795.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900035., 5900025., 5900015., ..., 5790265., 5790255., 5790245.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 10.0 0.0 5900040.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600005.0, 600015.0, 600025.0, 600035.0, 600045.0, 600055.0,\n              600065.0, 600075.0, 600085.0, 600095.0,\n              ...\n              709705.0, 709715.0, 709725.0, 709735.0, 709745.0, 709755.0,\n              709765.0, 709775.0, 709785.0, 709795.0],\n             dtype='float64', name='x', length=10980))yPandasIndexPandasIndex(Float64Index([5900035.0, 5900025.0, 5900015.0, 5900005.0, 5899995.0, 5899985.0,\n              5899975.0, 5899965.0, 5899955.0, 5899945.0,\n              ...\n              5790335.0, 5790325.0, 5790315.0, 5790305.0, 5790295.0, 5790285.0,\n              5790275.0, 5790265.0, 5790255.0, 5790245.0],\n             dtype='float64', name='y', length=10980))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\nWe have already seen how COGs are regular GeoTIFF files with a special internal structure. Another feature of COGs is that data is organized in “blocks” that can be accessed remotely via independent HTTP requests, enabling partial file readings. This is useful if you want to access only a portion of your raster file, but it also allows for efficient parallel reading. You can check the blocksize employed in a COG file with the following code snippet:\n\nimport rasterio\nwith rasterio.open(visual_href) as r:\n    if r.is_tiled:\n        print(f\"Chunk size: {r.block_shapes}\")\n\nChunk size: [(1024, 1024), (1024, 1024), (1024, 1024)]\n\n\nIn order to optimally access COGs it is best to align the blocksize of the file with the chunks employed when loading the file.\nLet’s open the blue-band asset (“B02”) of a Sentinel-2 scene as a chunked DataArray object using a suitable chunk size:\n\nimport rasterio\nwith rasterio.open(blue_band_href) as r:\n    if r.is_tiled:\n        print(f\"Chunk size: {r.block_shapes}\")\n\nChunk size: [(1024, 1024)]\n\n\ndeal chunk size values for this raster are thus multiples of 1024. An element to consider is the number of resulting chunks and their size. Chunks should not be too big nor too small (i.e. too many). As a rule of thumb, chunk sizes of 100 MB typically work well with Dask (see, e.g., this blog post. Also, the shape might be relevant, depending on the application! Here, we might select a chunks shape of (1, 6144, 6144):\n\nband = rioxarray.open_rasterio(blue_band_href, chunks=(1, 6144, 6144))\n\nwhich leads to chunks 72 MB large: ((1 x 6144 x 6144) x 2 bytes / 2^20 = 72 MB). Also, we can let rioxarray and Dask figure out appropriate chunk shapes by setting chunks=“auto”:\n\nband = rioxarray.open_rasterio(blue_band_href, chunks=\"auto\")\n\nwhich leads to (1, 8192, 8192) chunks (128 MB) as illustrated below:\n\nband\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 10980, x: 10980)>\ndask.array<open_rasterio-e0482a83b59aad29d7a36b8e8d497165<this-array>, shape=(1, 10980, 10980), dtype=uint16, chunksize=(1, 8192, 8192), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6e+05 6e+05 ... 7.098e+05 7.098e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 10980x: 10980dask.array<chunksize=(1, 8192, 8192), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         229.95 MiB \n                         128.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 10980, 10980) \n                         (1, 8192, 8192) \n                    \n                    \n                         Count \n                         2 Graph Layers \n                         4 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n\n  \n  \n  \n  \n\n  \n  \n\n  \n  10980\n  10980\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05array([600005., 600015., 600025., ..., 709775., 709785., 709795.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900035., 5900025., 5900015., ..., 5790265., 5790255., 5790245.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 10.0 0.0 5900040.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600005.0, 600015.0, 600025.0, 600035.0, 600045.0, 600055.0,\n              600065.0, 600075.0, 600085.0, 600095.0,\n              ...\n              709705.0, 709715.0, 709725.0, 709735.0, 709745.0, 709755.0,\n              709765.0, 709775.0, 709785.0, 709795.0],\n             dtype='float64', name='x', length=10980))yPandasIndexPandasIndex(Float64Index([5900035.0, 5900025.0, 5900015.0, 5900005.0, 5899995.0, 5899985.0,\n              5899975.0, 5899965.0, 5899955.0, 5899945.0,\n              ...\n              5790335.0, 5790325.0, 5790315.0, 5790305.0, 5790295.0, 5790285.0,\n              5790275.0, 5790265.0, 5790255.0, 5790245.0],\n             dtype='float64', name='y', length=10980))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\n\n\n\nParallel computations\nOperations performed on a DataArray that has been opened as a chunked Dask array are executed using Dask. Dask coordinates how the operations should be executed on the individual chunks of data, and runs these tasks in parallel as much as possible.\nLet’s now repeat the raster calculations that we have carried out in the previous section, but running calculations in parallel over a multi-core CPU. We first open the relevant rasters as chunked arrays:\n\nscl = rioxarray.open_rasterio(scl_href, lock=False, chunks=(1, 2048, 2048))\nvisual = rioxarray.open_rasterio(visual_href, overview_level=0, lock=False, chunks=(3, 2048, 2048))\n\nSetting lock=False tells rioxarray that the individual data chunks can be loaded simultaneously from the source by the Dask workers.\nAs the next step, we trigger the download of the data using the .persist() method, see below. This makes sure that the downloaded chunks are stored in the form of a chunked Dask array (calling .load() would instead merge the chunks in a single Numpy array).\nWe explicitly tell Dask to parallelize the required workload over 4 threads. Don’t forget to add the Jupyter magic to record the timing!\n\n%%time\nscl = scl.persist(scheduler=\"threads\", num_workers=4)\nvisual = visual.persist(scheduler=\"threads\", num_workers=4)\n\nCPU times: user 2.04 s, sys: 168 ms, total: 2.21 s\nWall time: 13.5 s\n\n\nSo downloading chunks of data using 4 workers was actually slower (13.5 s vs 9.31 s).\nLet’s now continue to the second step of the calculation. Note how the same syntax as for its serial version is employed for creating and applying the cloud mask. Only the raster saving includes additional arguments:\n\ntiled=True: write raster as a chunked GeoTIFF.\nlock=threading.Lock(): the threads which are splitting the workload must “synchronise” when writing to the same file (they might otherwise overwrite each other’s output).\ncompute=False: do not immediately run the calculation, more on this later.\n\n\nfrom threading import Lock\n\n\n%%time\nmask = scl.squeeze().isin([8, 9])\nvisual_masked = visual.where(~mask, other=0)\nvisual_store = visual_masked.rio.to_raster(\"band_masked.tif\", tiled=True, lock=Lock(), compute=False)\n\nCPU times: user 13.1 ms, sys: 12 ms, total: 25.1 ms\nWall time: 21.1 ms\n\n\nDid we just observe a 15x speed-up when comparing to the serial calculation (317 ms vs 21.1 ms)? Actually, no calculation has run yet. This is because operations performed on Dask arrays are executed “lazily”, i.e. they are not immediately run.\n\n\nDask graph\nThe sequence of operations to carry out is stored in a task graph, which can be visualized with:\n\nimport dask\ndask.visualize(visual_store)\n\n\n\n\nThe task graph gives Dask the complete “overview” of the calculation, thus enabling a better management of tasks and resources when dispatching calculations to be run in parallel.\nWhile most methods of DataArray’s run operations lazily when Dask arrays are employed, some methods by default trigger immediate calculations, like the method to_raster() (we have changed this behaviour by specifying compute=False). In order to trigger calculations, we can use the .compute() method. Again, we explicitly tell Dask to run tasks on 4 threads. Let’s time the cell execution:\n\n%%time\nvisual_store.compute(scheduler=\"threads\", num_workers=4)\n\nCPU times: user 292 ms, sys: 39.3 ms, total: 331 ms\nWall time: 176 ms\n\n\n[None, None, None, None, None, None, None, None, None]\n\n\nThe timing that we have recorded for this step is about half the speed of the one recorded for the serial calculation, despite the overhead that Dask introduces to manage the tasks in the Dask graph. This overhead, which is typically of the order of milliseconds per task, can sometimes be larger than the parallelization gain, and this is typically the case for calculations with small chunks (note that here we have used chunks that are only 4 to 32 MB large).\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- The %%time Jupyter magic command can be used to profile calculations\n- data ‘chunks’ are the unit of parallelization in raster calculations\n- (rio)xarray can open raster files as chunked arrays\n- the chunk shape and size can significantly affect the calculation performance\n- cloud-optimized GeoTIFFs have an internal structure that enables performant parallel read"
  },
  {
    "objectID": "posts/Hello World/HelloWorld.html",
    "href": "posts/Hello World/HelloWorld.html",
    "title": "Hello, World!",
    "section": "",
    "text": "A chartered accountant by trade, I now find myself transitioning into Data Science, and Machine Learning in particular. The voice in my head just wouldn’t be silenced and I had no option but to take its advice.\nI set about changing track in the ‘traditional’ way. I started building the foundations. I signed up for a Maths and Statistics degree with the Open University. It turns out my aptitude had not left me despite a 20-year gap, but I soon found that picking away at this in the evenings whilst working full time was not the quickest route. My progress has been ‘frozen’ at the half-way mark, and I may decide to pick this up again at some stage. I feel like I owe it to my high school maths teacher! In any event, the mathematical and statistical concepts covered certainly gives me a solid foothold for my new path.\nI also signed up for the Google Data Analytics certificate, which gave me an introduction to R, SQL and Tableau. The combination of the power and speed of computer programming languages and the beauty of the visualisation tools had me hooked! It reminded me of my very first computer, an Amstrad CPC 464. Seems like yesterday but it was during the times of the BASIC programming language! I can still remember staring proudly at the screen as a flashing purple circle winked back at me 😉\n\nI then quickly followed this up with some Python courses via Data Camp, and was in the middle of yet another course, Machine Learning Specialization taught by Andrew Ng, when I had the fortune to stumble upon Radek Osmulski’s book Meta Learning. He too, had followed a traditional model of learning but found that this wasn’t working out. That resonated with me as, despite the apparent ‘progress’ I was making via all these courses, it where was the tangible output? What did I have to show for all those hours of lectures?\nIt was Radek’s book that changed my life trajectory, by introducing me to Jeremy Howard and the fast.ai Practical Deep Learning for Coders. The ‘getting your hands dirty’ approach is the complete antithesis of my career path to date and is just what I needed to springboard my transition.\nThe initial purpose of this blog is to finally have a platform to showcase my achievements, immature and naïve as they are in these first steps. As I dive deeper hopefully I can start to give something back to the community and do some troubleshooting. The road is bumpy out there.\nCome join me… Into the Unknown!"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html",
    "title": "Numpy Tutorial",
    "section": "",
    "text": "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\nAt the core of the NumPy package, is the ndarray object. This encapsulates n-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance. There are several important differences between NumPy arrays and the standard Python sequences.\nSee the documentation for further information.\n\n\n\narrays.JPG"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#numpy-v-lists",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#numpy-v-lists",
    "title": "Numpy Tutorial",
    "section": "NumPy [ ] v Lists ( )",
    "text": "NumPy [ ] v Lists ( )\n\nWhy is NumPy faster?\nNumPy has a significant speed advantage over Lists.\n\n\n\nspeed.JPG\n\n\nLet’s find out why…\n\nLess memory is used to represent data\n\nSay for example the number 5. This is represented in binary form as follows:\n\nint8 which takes up 8 bits (or 1 byte) of memory and is represented in binary form as 00000101\n\nNumPy can cast this to:\n\nint16 which takes up 16 bits (or 2 bytes) of memory and is represented in binary form as 00000000 00000101\nint32 which takes up 32 bits (or 4 bytes) of memory and is represented in binary form as 00000000 00000000 00000000 00000101\nint64 which takes up 64 bits (or 8 bytes) of memory and is represented in binary form as 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101\n\nLists use a built in int type for Python which consists of four different things:\n\nobject value\nobject type\nreference count\nsize of value\n\neach of which use up memory:\n\nobject value: 8 bytes 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101\nobject type: 8 bytes 11001010 10111110 01100001 01000100 11111100 00000000 11001100 01011111\nreference count: 8 bytes 00000001 00111101 11111110 10111100 00011010 11011101 10100100 11011000\nsize of value: 4 bytes 00000000 00000000 00000000 00011100\n\n\nNo type checking when iterating through objects\nNumPy utilizes contiguous memory which enables Single Instruction Multiple Data (SIMD) Vector Processing to be harnessed, and effective cache utilization\n\n\n\nHow do Lists differ from NumPy?\n\n\n\ndifferences.JPG\n\n\nNumPy allows itemwise computation:\n\n\n\nmultiply.JPG"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#applications-of-numpy",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#applications-of-numpy",
    "title": "Numpy Tutorial",
    "section": "Applications of NumPy",
    "text": "Applications of NumPy\n\n\n\napplications.JPG\n\n\n\n\n\n\n\n\nSciPy\n\n\n\nNumPy can perform complex mathematical operations, although the SciPy library allows even more advanced computation.\n\n\n\n# load in \nimport numpy as np\n\n\n# which version?\nnp.__version__\n\n'1.22.4'"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#the-basics",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#the-basics",
    "title": "Numpy Tutorial",
    "section": "The Basics",
    "text": "The Basics\n\n# create an array of integers\na = np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\n# create an array of floating point numbers\nb = np.array([[9.0,8.0,7.0],[6.0,5.0,4.0]])\nb\n\narray([[9., 8., 7.],\n       [6., 5., 4.]])\n\n\n\nShape of a NumPy array\nEvery NumPy array can be queried for its shape. A shape is a tuple of the format (n_rows, n_cols).\n\n# Grab the array shape\na.shape\n\n(3,)\n\n\nSince there is no value after the comma, this is a one-dimensional array.\n\n# Grab the array dimension\na.ndim\n\n1\n\n\n\n# Grab the array shape\nb.shape\n\n(2, 3)\n\n\n(2, 3) means that the matrix has 2 rows and 3 columns.\n\n# Grab the array dimension\nb.ndim\n\n2\n\n\n\n# Get type\na.dtype\n\ndtype('int64')\n\n\nThis is the default size, but we can assign a lower int value to save memory:\n\na = np.array([1,2,3], dtype='int16')\na\n\narray([1, 2, 3], dtype=int16)\n\n\n\n# Get size - how many bytes?\na.itemsize\n\n2\n\n\n\n# Get size - how many bytes?\nb.itemsize\n\n8\n\n\n\n# Get total number of elements\na.size\n\n3\n\n\n\n# Get total size\na.size * a.itemsize\n\n6\n\n\n\n# Get total size\na.nbytes\n\n6"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---accessing-specific-elements-rows-columns-etc",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---accessing-specific-elements-rows-columns-etc",
    "title": "Numpy Tutorial",
    "section": "Indexing - accessing specific elements, rows, columns etc",
    "text": "Indexing - accessing specific elements, rows, columns etc\n\n\n\n\n\n\nIndexing\n\n\n\nRemember - the first index of a NumPy array is zero! and not one\n\n\n\nc = np.array([[1,2,3,4,5,6,7],[8,9,10,11,12,13,14]])\nprint(c)\n\n[[ 1  2  3  4  5  6  7]\n [ 8  9 10 11 12 13 14]]\n\n\n\nc.shape\n\n(2, 7)\n\n\n\n# Get a specific element from row, column (r,c)\nc[1,5]\n\n13\n\n\n\n# Get a specific element from row, column (r,c)\nc[1,-2]\n\n13\n\n\n\n# Get a specific ROW\nc[0, :]\n\narray([1, 2, 3, 4, 5, 6, 7])\n\n\n\n# Get a specific COLUMN\nc[:, 2]\n\narray([ 3, 10])\n\n\n\n# Getting a little more fancy [start_index : end_index : step_size]\n# Row 0, then columns 1 to 6 (excluding 6) in steps of 2\nc [0, 1:6:2]\n\narray([2, 4, 6])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---changing-specific-elements",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---changing-specific-elements",
    "title": "Numpy Tutorial",
    "section": "Indexing - changing specific elements",
    "text": "Indexing - changing specific elements\n\n# change row 1, column 5 value (13) to 20\nc[1,5] = 20\nprint(c)\n\n[[ 1  2  3  4  5  6  7]\n [ 8  9 10 11 12 20 14]]\n\n\n\n# change column 2 [3,10] to [21,28]\nc[:,2] = [21,28]\nprint(c)\n\n[[ 1  2 21  4  5  6  7]\n [ 8  9 28 11 12 20 14]]\n\n\n\n# 3-d example\nd = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\nprint(d)\n\n[[[1 2]\n  [3 4]]\n\n [[5 6]\n  [7 8]]]\n\n\n\n# Get specific example (work from outside in)\n\n# the first index reference [0] relates to the first grouping\n# [1, 2]\n# [3, 4]\n\n# the second index reference [1] relates to the second row within that grouping [3, 4]\n\n# the third index reference [1] specifies the second column from the second row of the grouping [3,4] i.e 4\n\nd[0,1,1]\n\n4\n\n\n\n# Get specific example (work from outside in)\n\n# the first index reference : means we are selecting from both groupings\n# [1 2]\n# [3 4]\n\n# [5 6]\n# [7 8]\n\n# the second index reference [1] relates to the second row from each of the groupings i.e. [3,4] and [7,8]\n\n# the third index reference [:] specifies all values within those rows \n\nd[:,1,:]\n\narray([[3, 4],\n       [7, 8]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#initializing-different-array-types",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#initializing-different-array-types",
    "title": "Numpy Tutorial",
    "section": "Initializing Different Array Types",
    "text": "Initializing Different Array Types\n\n# All 0s matrix specifying shape\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n\n# All 0s 2 x 3 matrix specifying shape\n\nnp.zeros((2,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# All 0s 2 x 3 x 4 matrix \n\n# first reference = number of groupings\n# second reference = number of rows\n# third reference = number of columns\nnp.zeros((2,3,4))\n\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])\n\n\n\n# All 1s matrix\n\nnp.ones((4,2,2), dtype='int16')\n\narray([[[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]]], dtype=int16)\n\n\n\n# Any other number .full\n\nnp.full((2,2), 137, dtype='float32')\n\narray([[137., 137.],\n       [137., 137.]], dtype=float32)\n\n\n\n# Any other number .full_like\n# creates a new array full of 4s matching the shape of the d array we previously created\n\nnp.full_like(d,4)\n\narray([[[4, 4],\n        [4, 4]],\n\n       [[4, 4],\n        [4, 4]]])\n\n\n\n# random decimals\n# using random.rand\n\nnp.random.rand(1,3,7)\n\narray([[[0.16735136, 0.86937755, 0.30866395, 0.05841447, 0.28817268,\n         0.55635487, 0.87087044],\n        [0.95692978, 0.45277212, 0.87002198, 0.59516086, 0.56308885,\n         0.71476549, 0.64600732],\n        [0.06479773, 0.83108022, 0.0321547 , 0.3054754 , 0.72857438,\n         0.56460774, 0.72935517]]])\n\n\n\n# random decimals following shape of previously defined array\n# using random.random_sample\n\nnp.random.random_sample(d.shape)\n\narray([[[0.04212123, 0.39748958],\n        [0.63778666, 0.3396428 ]],\n\n       [[0.33967012, 0.49291645],\n        [0.97990323, 0.45722717]]])\n\n\n\n# random integers\n# first argument is range of integers to sample from (upper limit is EXclusive)\n# second argument is shape - established by size=\n\nnp.random.randint(7,13, size=(3,3))\n\narray([[10, 10, 10],\n       [12, 12, 12],\n       [10,  7,  9]])\n\n\n\n# identity matrix\n# ones on main diagonal and zeros elsewhere\n# only one pararmeter as this is a square matrix\n\nnp.identity(7)\n\narray([[1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 1.]])\n\n\n\n# repeat a matrix using .repeat\n# first argument is array to be repeated\n# second argument is how many times to be repeated\n# axis 0 refers to rows\n\narr = np.array([[1,3,7]])\nr1 = np.repeat(arr,3,axis=0)\nprint(r1)\n\n[[1 3 7]\n [1 3 7]\n [1 3 7]]\n\n\n\n# repeat a matrix using .repeat\n# first argument is array to be repeated\n# second argument is how many times to be repeated\n# axis 1 refers to columns\n\narr = np.array([[1,3,7]])\nr1 = np.repeat(arr,3,axis=1)\nprint(r1)\n\n[[1 1 1 3 3 3 7 7 7]]\n\n\nHow might we go about initializing the matrix below?\n\n\n\nmatrix.JPG\n\n\n\n# create a 5 x 5 1s matrix for the outer layer\noutputs =np.ones((5,5))\noutputs\n\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\n\n\n# create a 3 x 3 zero matric for the middle layer\nz = np.zeros ((3,3))\nz\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# fil in middle element i.e row 1 column 1 with a 9\nz[1,1] = 9\nz\n\narray([[0., 0., 0.],\n       [0., 9., 0.],\n       [0., 0., 0.]])\n\n\n\n# replace outer layer with inner layers\noutputs[1:4,1:4] = z\noutputs\n\narray([[1., 1., 1., 1., 1.],\n       [1., 0., 0., 0., 1.],\n       [1., 0., 9., 0., 1.],\n       [1., 0., 0., 0., 1.],\n       [1., 1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#be-careful-when-copying-arrays",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#be-careful-when-copying-arrays",
    "title": "Numpy Tutorial",
    "section": "Be careful when copying arrays",
    "text": "Be careful when copying arrays\n\na = np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\n# if you equate two arrays then any changes impact BOTH\n# this is NOT making a copy!\nb = a\nb\n\narray([1, 2, 3])\n\n\n\n# To illustrate let's change the first element of b\nb[0] = 137\nb\n\narray([137,   2,   3])\n\n\n\n# note the first element in a has also changed!\na\n\narray([137,   2,   3])\n\n\nIn order to make a copy we have to use .copy:\n\na = np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\nb = a.copy()\nb\n\narray([1, 2, 3])\n\n\n\nb[0] = 137\nb\n\narray([137,   2,   3])\n\n\n\na\n\narray([1, 2, 3])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#mathematics",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#mathematics",
    "title": "Numpy Tutorial",
    "section": "Mathematics",
    "text": "Mathematics\n\nWe can carry out element wise arithmetic using NumPy\n\n\na = np.array([1,2,3,4])\na\n\narray([1, 2, 3, 4])\n\n\n\n# add 2 to every element within the array\na + 2\n\narray([3, 4, 5, 6])\n\n\n\n# deduct 2 from every element within the array\na - 2\n\narray([-1,  0,  1,  2])\n\n\n\n# multiply every element within the array by 2\na * 2\n\narray([2, 4, 6, 8])\n\n\n\n# divide every element within the array by 2\na / 2\n\narray([0.5, 1. , 1.5, 2. ])\n\n\n\nb = np.array([1,0,1,0])\na + b\n\narray([2, 2, 4, 4])\n\n\n\na ** 3\n\narray([ 1,  8, 27, 64])\n\n\n\n# Take the sin \nnp.sin(a)\n\narray([ 0.84147098,  0.90929743,  0.14112001, -0.7568025 ])\n\n\n\n# Take the cos\nnp.cos(a)\n\narray([ 0.54030231, -0.41614684, -0.9899925 , -0.65364362])\n\n\nFor a comprehensive outline of the mathematical operations possible using NumPy see the documentation.\n\nMatrix Multiplication\nNote that for matrix multiplication the number of rows of one of the matrices needs to match the number of columns of the other matrix:\n\n# 2 rows x 3 columns\na = np.ones ((2,3))\na\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\n# 3 rows x 2 columns\nb = np.full((3,2), 2)\nb\n\narray([[2, 2],\n       [2, 2],\n       [2, 2]])\n\n\n\n\n\n\n\n\nMatrix Multiplication\n\n\n\nMatrix multiplication can only be performed where the number of rows in Matrix A match the number of columns in Matrix B. For example we can multiply a 2 row x [3] column matrix by a [3] row x 2 column matrix. The resulting matrixc is a 2 x 2 matrix - the [3]s cancel out leaving the outer 2s.\n\n\n\n# Let's try to multiply together\na * b\n\nValueError: operands could not be broadcast together with shapes (2,3) (3,2) \n\n\nHold on - why is this not working?! Our matrix multiplication criteria is satisfied - the number of rows in matrix a = 2 which matches the number of columns in matrix b. Don’t panic! We just have to use the .matmul() function instead:\n\n# Let's try again using .matmul\nnp.matmul(a,b)\n\narray([[6., 6.],\n       [6., 6.]])\n\n\n\n\nDeterminant of a matrix\nThe determinant of a 2 × 2 matrix is:\n\nFor simplicity let’s use an identity matrix to illustrate. Recall that the identity matric has 1s on the leading diagonal and 0s elsewhere:\n\ni= np.identity(2)\ni \n\narray([[1., 0.],\n       [0., 1.]])\n\n\nAn identity matrix using the above formula should have a determinant of 1:\n\n# calc the determinant of identity matrix i\nnp.linalg.det(i)\n\n1.0\n\n\nThere are many other linear algebra operations that can be performed. See the documentation for more detail."
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#summary-statistics",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#summary-statistics",
    "title": "Numpy Tutorial",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nstats = np.array([[1,2,3],[4,5,6]])\nstats\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\nnp.min(stats)\n\n1\n\n\n\nnp.max(stats)\n\n6\n\n\n\nnp.mean(stats)\n\n3.5\n\n\n\nnp.std(stats)\n\n1.707825127659933\n\n\n\nnp.sum(stats)\n\n21\n\n\n\n# sum by row going downwards (axis = 0)\nnp.sum(stats, axis = 0)\n\narray([5, 7, 9])\n\n\n\n# sum by column going across (axis = 1)\nnp.sum(stats, axis = 1)\n\narray([ 6, 15])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#reorganizing-arrays",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#reorganizing-arrays",
    "title": "Numpy Tutorial",
    "section": "Reorganizing arrays",
    "text": "Reorganizing arrays\n\nbefore = np.array([[1,2,3,4],\n                   [5,6,7,8]])\nbefore\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nbefore.shape\n\n(2, 4)\n\n\nIn some cases we might want to change the shape of the array:\n\nafter = before.reshape(8,1)\nafter\n\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8]])\n\n\n\nafter = before.reshape(2,2,2)\nafter\n\narray([[[1, 2],\n        [3, 4]],\n\n       [[5, 6],\n        [7, 8]]])\n\n\n\nVertcally stacking vectors\n\nv1 = np.array([1,2,3,4])\nv2 = np.array([5,6,7,8])\n\n\nnp.vstack([v1,v2])\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\n# we can stack as much as we want any in any order\nnp.vstack([v1,v2,v2,v2,v1,v1,v2])\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8],\n       [5, 6, 7, 8],\n       [5, 6, 7, 8],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\n\nHorizontally stacking vectors\n\nh1 = np.ones((2,4))\nh2 = np.zeros((2,2))\n\nh1\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\n\nh2\n\narray([[0., 0.],\n       [0., 0.]])\n\n\n\nnp.hstack([h1,h2])\n\narray([[1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 0.]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#other-use-cases",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#other-use-cases",
    "title": "Numpy Tutorial",
    "section": "Other use cases",
    "text": "Other use cases\n\n# Load data from file using .genfromtxt\n# automatically cast as float type\n\nfiledata = np.genfromtxt('NumPy.txt', delimiter=',')\nfiledata\n\narray([[  1.,  13.,  21.,  11., 196.,  75.,   4.,   3.,  34.,   6.,   7.,\n          8.,   0.,   1.,   2.,   3.,   4.,   5.],\n       [  3.,  42.,  12.,  33., 766.,  75.,   4.,  55.,   6.,   4.,   3.,\n          4.,   5.,   6.,   7.,   0.,  11.,  12.],\n       [  1.,  22.,  33.,  11., 999.,  11.,   2.,   1.,  78.,   0.,   1.,\n          2.,   9.,   8.,   7.,   1.,  76.,  88.]])\n\n\n\n# cast the data as int32\nfiledata = filedata.astype('int32')\nfiledata\n\narray([[  1,  13,  21,  11, 196,  75,   4,   3,  34,   6,   7,   8,   0,\n          1,   2,   3,   4,   5],\n       [  3,  42,  12,  33, 766,  75,   4,  55,   6,   4,   3,   4,   5,\n          6,   7,   0,  11,  12],\n       [  1,  22,  33,  11, 999,  11,   2,   1,  78,   0,   1,   2,   9,\n          8,   7,   1,  76,  88]], dtype=int32)\n\n\n\nBoolean masking and advanced indexing\n\n# this returns a boolean for every vlaue based on our condition\nfiledata > 50\n\narray([[False, False, False, False,  True,  True, False, False, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False,  True,  True, False,  True, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False,  True, False, False, False,  True,\n        False, False, False, False, False, False, False,  True,  True]])\n\n\nThis is good but not very helpful. We really want to extract the values:\n\n# this is better as this returns the values that meet our condition\nfiledata[filedata > 50]\n\narray([196,  75, 766,  75,  55, 999,  78,  76,  88], dtype=int32)\n\n\n\n\nIndexing with a List\n\na = np.array([1,2,3,4,5,6,7,8,9])\n\n# Let's grab 2, 3 and 9\na[[1,2,-1]]\n\nNameError: name 'np' is not defined\n\n\n\nnp.any(filedata > 50, axis = 0)\n\narray([False, False, False, False,  True,  True, False,  True,  True,\n       False, False, False, False, False, False, False,  True,  True])\n\n\n\nnp.any(filedata > 50, axis = 1)\n\narray([ True,  True,  True])\n\n\n\nnp.all(filedata > 50, axis = 0)\n\narray([False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False, False, False])\n\n\n\nnp.all(filedata > 50, axis = 1)\n\narray([False, False, False])\n\n\n\n(filedata > 50) & (filedata < 100)\n\narray([[False, False, False, False, False,  True, False, False, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False, False,  True, False,  True, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False, False, False, False, False,  True,\n        False, False, False, False, False, False, False,  True,  True]])\n\n\n\n# ~ means NOT and negates the condition specified\n(~((filedata > 50) & (filedata < 100)))\n\narray([[ True,  True,  True,  True,  True, False,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True, False,  True, False,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True,  True,  True,  True, False,\n         True,  True,  True,  True,  True,  True,  True, False, False]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#wrap-up",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#wrap-up",
    "title": "Numpy Tutorial",
    "section": "Wrap up",
    "text": "Wrap up\nHow would we index the blue highlighted section of the matrix below:\n\n\n\nindexing_blue.JPG\n\n\nFirst we index the row range, and then the column range:\n\na[2:4, 0:2]\n\nHow would we index the green highlighted section of the matrix below:\n\n\n\nindexing_green.JPG\n\n\nWe can do this by using two different lists within our indexing. The first list contains the row indices and the second list contains the column indices:\n\na[ [0,1,2,3], [1,2,3,4] ]\n\nHow would we index the red highlighted section of the matrix below:\n\n\n\nindexing_red.JPG\n\n\nAgain, we can do this by using two different lists. The first list contains the required rows, and the second list contains the required column range:\n\na [[0,4,5], 3:]"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#acknowledgements",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#acknowledgements",
    "title": "Numpy Tutorial",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks once again to Santiago for signposting this video posted by Keith Galli. This blog was written after interactively working through it.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QUT1VHiLmmI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
  },
  {
    "objectID": "posts/Paddy_Doctor/Paddy_Doctor.html",
    "href": "posts/Paddy_Doctor/Paddy_Doctor.html",
    "title": "Paddy Doctor: Paddy Disease Classification",
    "section": "",
    "text": "Problem Statement - identify the type of disease present in paddy leaf images\nRice (Oryza sativa) is one of the staple foods worldwide. Paddy, the raw grain before removal of husk, is cultivated in tropical climates, mainly in Asian countries. Paddy cultivation requires consistent supervision because several diseases and pests might affect the paddy crops, leading to up to 70% yield loss. Expert supervision is usually necessary to mitigate these diseases and prevent crop loss. With the limited availability of crop protection experts, manual disease diagnosis is tedious and expensive. Thus, it is increasingly important to automate the disease identification process by leveraging computer vision-based techniques that achieved promising results in various domains.\n\n\nObjective\nThe main objective of the competition is to develop a machine or deep learning-based model to classify the given paddy leaf images accurately. A training dataset of 10,407 (75%) labeled images across ten classes (nine disease categories and normal leaf) is provided. Moreover, also provided is additional metadata for each image, such as the paddy variety and age. Our task is to classify each paddy image in the given test dataset of 3,469 (25%) images into one of the nine disease categories or a normal leaf.\n\n\nApproach\nIn Iterate Like a Grandmaster Jeremy Howard explained that when working on a Kaggle project:\n\n…the focus generally should be two things:\n\nCreating an effective validation set\nIterating rapidly to find changes which improve results on the validation set\n\n\nHere we’re going to go further, showing the process he used to tackle the Paddy Doctor competition, leading to four submissions in a row which all were (at the time of submission) in 1st place, each one more accurate than the last. You might be surprised to discover that the process of doing this was nearly entirely mechanistic and didn’t involve any consideration of the actual data or evaluation details at all.\nThis notebook shows every step of the process. At the start of this notebook we’ll make a basic submission; by the end we’ll see how he got to the top of the table!:\n\nAs a special extra, also included is a selection of “walkthru” videos that were prepared for the new fast.ai course, and cover this competition:\n\nWalkthru 8\nWalkthru 9\nWalkthru 10\nWalkthru 11\nWalkthru 12\nWalkthru 13\n\n\n\nGetting set up\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\nFirst, we’ll get the data. There’s a new library called fastkaggle which has a few handy features, including getting the data for a competition correctly regardless of whether we’re running on Kaggle or elsewhere. Note we’ll need to first accept the competition rules and join the competition, and we’ll need our kaggle API key file kaggle.json downloaded if you’re running this somewhere other than on Kaggle. setup_comp is the function we use in fastkaggle to grab the data, and install or upgrade our needed python modules when we’re running on Kaggle:\n::: {.cell _kg_hide-output=‘true’ execution_count=10}\ncomp = 'paddy-disease-classification'\n\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n:::\n\npath\n\nPath('paddy-disease-classification')\n\n\nNow we can import the stuff we’ll need from fastai, set a seed (for reproducibility – just for the purposes of making this notebook easier to write; It’s not recommended to do that in your own analysis however) and check what’s in the data:\n\nfrom fastai.vision.all import *\nset_seed(42)\n\npath.ls()\n\n(#4) [Path('paddy-disease-classification/sample_submission.csv'),Path('paddy-disease-classification/test_images'),Path('paddy-disease-classification/train_images'),Path('paddy-disease-classification/train.csv')]\n\n\n\n\nLooking at the data\nThe images are in train_images, so let’s grab a list of all of them:\n\ntrn_path = path/'train_images'\nfiles = get_image_files(trn_path)\n\n…and take a look at one:\n\nimg = PILImage.create(files[0])\nprint(img.size)\nimg.to_thumb(128)\n\n(480, 640)\n\n\n\n\n\nLooks like the images might be 480x640 – let’s check all their sizes. This is faster if we do it in parallel, so we’ll use fastcore’s parallel for this:\nWatch out! In the imaging world images are represented by (columns, rows) however in the array/tensor world images are represented as (rows, columns). Pytorch would say size is (640, 480)!!\n\nfrom fastcore.parallel import *\n\n# create function to create a PILLOW image and get its size\n# speed up process using parallel \ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files, n_workers=8)\npd.Series(sizes).value_counts()\n\n(480, 640)    10403\n(640, 480)        4\ndtype: int64\n\n\nThey’re nearly all the same size, except for a few. Because of those few, however, we’ll need to make sure we always resize each image to common dimensions first, otherwise fastai won’t be able to create batches. For now, we’ll just squish them to 480x480 images, and then once they’re in batches we do a random resized crop down to a smaller size, along with the other default fastai augmentations provided by aug_transforms. We’ll start out with small resized images, since we want to be able to iterate quickly:\n\n# create our dataloader\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize(480, method='squish'), # resize to a 480 x 480 square using squish - change aspect ratio\n    batch_tfms=aug_transforms(size=128, min_scale=0.75))\n\n#  show_batch allows us to see or hear our data\ndls.show_batch(max_n=6)\n\n\n\n\n\n\nOur first model\nLet’s create a model. To pick an architecture, we should look at the options in The best vision models for fine-tuning. resnet26d is the fastest resolution-independent model which gets into the top-15 lists there.\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.').to_fp16()\n\nDownloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26d-69e92c46.pth\" to /home/stephen137/.cache/torch/hub/checkpoints/resnet26d-69e92c46.pth\n\n\nLet’s see what the learning rate finder shows:\n\n# puts through one mini-batch at a time, starting at a very low learning rate\n# gradually increase learning rate, see improvement, then once lr gets bigger worsens\nlearn.lr_find(suggest_funcs=(valley, slide))\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083, slide=0.0030199517495930195)\n\n\n\n\n\nlr_find generally recommends rather conservative learning rates, to ensure that your model will train successfully. I generally like to push it a bit higher if I can. Let’s train a few epochs and see how it looks:\n\n# let's fine tune for 3 epochs with a selected learning rate of 0.01 (10 ^ -2)\nlearn.fine_tune(3, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.774996\n      1.171467\n      0.378664\n      03:57\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.074707\n      0.791964\n      0.265257\n      04:52\n    \n    \n      1\n      0.786653\n      0.482838\n      0.144161\n      04:59\n    \n    \n      2\n      0.534015\n      0.414971\n      0.129265\n      05:00\n    \n  \n\n\n\nWe’re now ready to build our first submission!!! Let’s take a look at the sample Kaggle provided to see what it needs to look like:\n\n\nSubmitting to Kaggle\n\n# lets's have a look at the sample Kaggle submisison file\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n    \n  \n  \n    \n      0\n      200001.jpg\n      NaN\n    \n    \n      1\n      200002.jpg\n      NaN\n    \n    \n      2\n      200003.jpg\n      NaN\n    \n    \n      3\n      200004.jpg\n      NaN\n    \n    \n      4\n      200005.jpg\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3464\n      203465.jpg\n      NaN\n    \n    \n      3465\n      203466.jpg\n      NaN\n    \n    \n      3466\n      203467.jpg\n      NaN\n    \n    \n      3467\n      203468.jpg\n      NaN\n    \n    \n      3468\n      203469.jpg\n      NaN\n    \n  \n\n3469 rows × 2 columns\n\n\n\nOK so we need a CSV containing all the test images, in alphabetical order, and the predicted label for each one. We can create the needed test set using fastai like so:\n\n# create our test set\ntst_files = get_image_files(path/'test_images').sorted()\n\n# create a dataloader pointing at the test set - use dls.test_dl\n# key difference from normal dataloader is that it does not have any labels\ntst_dl = dls.test_dl(tst_files)\n\nWe can now get the probabilities of each class, and the index of the most likely class, from this test set (the 2nd thing returned by get_preds are the targets, which are blank for a test set, so we discard them):\n\n# get our precitions and indexes from our learner\n# decoded means rather than just get probability will get indexes of 0 to 9\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n\nTensorBase([4, 3, 3,  ..., 4, 8, 3])\n\n\nThese need to be mapped to the names of each of these diseases, these names are stored by fastai automatically in the vocab:\n\n# grab names of the diseases from the index vocab\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\nWe can create an apply this mapping using pandas:\n\n# map disease name to indexes\nmapping = dict(enumerate(dls.vocab)) # create a dictionary of the indexes and vocab\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping) # looks up the dictionary and returns the indexes, and name of indexes. Passing .map to a dictionary (mapping) is much fasster than passing to a function\nresults\n\n0       brown_spot\n1            blast\n2            blast\n3            blast\n4            blast\n           ...    \n3464         blast\n3465         blast\n3466    brown_spot\n3467        normal\n3468         blast\nName: idxs, Length: 3469, dtype: object\n\n\nKaggle expects the submission as a CSV file, so let’s save it, and check the first few lines:\n\n# replace 'label' column with our results\nss['label'] = results\nss.to_csv('subm.csv', index=False) \n!head subm.csv\n\nimage_id,label\n200001.jpg,brown_spot\n200002.jpg,blast\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,normal\n200007.jpg,blast\n200008.jpg,blast\n200009.jpg,hispa\n\n\nLet’s submit this to kaggle. We can do it from the notebook if we’re running on Kaggle, otherwise we can use the API:\n\n# function to submit to Kaggle\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial rn26d 128px', comp)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 62.9k/62.9k [00:01<00:00, 41.0kB/s]\n\n\nSuccess! We successfully created a submission, although it’s not very good (top 80% - or bottoms 20%!) but it only took a short time to train. The important thing is that we have a good starting point to iterate from, and we can do rapid iterations. Every step from loading the data to creating the model to submitting to Kaggle is all automated and runs quickly. Therefore, we can now try lots of things quickly and easily and use those experiments to improve our results.\n\n\nGoing faster\nI have noticed often when using Kaggle that the “GPU” indicator in the top right is nearly empty, and the “CPU” one is full. This strongly suggests that Kaggle’s notebook is CPU bound by decoding and resizing the images. This is a common problem on machines with poor CPU performance.\nWe really need to fix this, since we need to be able to iterate much more quickly. What we can do is to simply resize all the images to 40% of their height and width – which reduces their number of pixels 6.25x. This should mean an around 6.25x increase in performance for training small models.\nLuckily, fastai has a function which does exactly this, whilst maintaining the folder structure of the data: resize_images.\n\ntrn_path = Path('sml')\n\n\nresize_images(path/'train_images', dest=trn_path, max_size=256, recurse=True)\n\nThis will give us 192x256px images. Let’s take a look:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize((256,192)))\n\ndls.show_batch(max_n=3)\n\n\n\n\nIn this section we’ll be experimenting with a few different architectures and image processing approaches (item and batch transforms). In order to make this easier, we’ll put our modeling steps together into a little function which we can pass the architecture, item transforms, and batch transforms to:\n\ndef train(arch, item, batch, epochs=5):\n    dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)\n    learn = vision_learner(dls, arch, metrics=error_rate)\n    learn.fine_tune(epochs, 0.01)\n    return learn\n\nOur item_tfms already resize our images to small sizes, so this shouldn’t impact the accuracy of our models much, if at all. Let’s re-run our resnet26d to test.\n\nlearn = train('resnet26d', item=Resize(192),\n              batch=aug_transforms(size=128, min_scale=0.75))\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.915986\n      1.551140\n      0.477174\n      03:13\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.242299\n      1.098648\n      0.353676\n      04:09\n    \n    \n      1\n      0.969338\n      0.703203\n      0.231619\n      04:05\n    \n    \n      2\n      0.744738\n      0.554062\n      0.181643\n      04:04\n    \n    \n      3\n      0.532851\n      0.422054\n      0.135031\n      04:15\n    \n    \n      4\n      0.423329\n      0.404017\n      0.123979\n      04:10\n    \n  \n\n\n\nThat’s a big improvement in speed, and the accuracy looks fine.\n\n\nPyTorch Image Models (timm)\nPyTorch Image Models (timm) is a wonderful library by Ross Wightman which provides state-of-the-art pre-trained computer vision models. It’s like Hugging Face Transformers, but for computer vision instead of NLP (and it’s not restricted to transformers-based models)!\nRoss regularly benchmarks new models as they are added to timm, and puts the results in a CSV in the project’s GitHub repo. To analyse the data, we’ll first clone the repo:\n\n! git clone --depth 1 https://github.com/rwightman/pytorch-image-models.git\n%cd pytorch-image-models/results\n\nCloning into 'pytorch-image-models'...\nremote: Enumerating objects: 532, done.\nremote: Counting objects: 100% (532/532), done.\nremote: Compressing objects: 100% (367/367), done.\nremote: Total 532 (delta 222), reused 340 (delta 156), pack-reused 0\nReceiving objects: 100% (532/532), 1.30 MiB | 1.21 MiB/s, done.\nResolving deltas: 100% (222/222), done.\n/home/stephen137/Kaggle_Comp/pytorch-image-models/results\n\n\nUsing Pandas, we can read the two CSV files we need, and merge them together:\n\nimport pandas as pd\ndf_results = pd.read_csv('results-imagenet.csv')\n\nWe’ll also add a “family” column that will allow us to group architectures into categories with similar characteristics. Ross told Jeremy Howard which models he’s found the most usable in practice, so we’ll limit the charts to just look at these. (Also include is VGG, not because it’s good, but as a comparison to show how far things have come in the last few years.)\n\ndef get_data(part, col):\n    df = pd.read_csv(f'benchmark-{part}-amp-nhwc-pt111-cu113-rtx3090.csv').merge(df_results, on='model')\n    df['secs'] = 1. / df[col]\n    df['family'] = df.model.str.extract('^([a-z]+?(?:v2)?)(?:\\d|_|$)')\n    df = df[~df.model.str.endswith('gn')]\n    df.loc[df.model.str.contains('in22'),'family'] = df.loc[df.model.str.contains('in22'),'family'] + '_in22'\n    df.loc[df.model.str.contains('resnet.*d'),'family'] = df.loc[df.model.str.contains('resnet.*d'),'family'] + 'd'\n    return df[df.family.str.contains('^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin')]\n\n\ndf = get_data('infer', 'infer_samples_per_sec')\n\n\nInference results\nHere’s the results for inference performance (see the last section for training performance). In this chart:\n\nthe x axis shows how many seconds it takes to process one image (note: it’s a log scale)\nthe y axis is the accuracy on Imagenet\nthe size of each bubble is proportional to the size of images used in testing\nthe color shows what “family” the architecture is from.\n\nHover your mouse over a marker to see details about the model. Double-click in the legend to display just one family. Single-click in the legend to show or hide a family.\nNote: on my screen, Kaggle cuts off the family selector and some plotly functionality – to see the whole thing, collapse the table of contents on the right by clicking the little arrow to the right of “Contents”.\n\nimport plotly.express as px\nw,h = 1000,800\n\ndef show_all(df, title, size):\n    return px.scatter(df, width=w, height=h, size=df[size]**2, title=title,\n        x='secs',  y='top1', log_x=True, color='family', hover_name='model', hover_data=[size])\n\n\nshow_all(df, 'Inference', 'infer_img_size')\n\n\n                                                \n\n\nI noticed that the GPU usage bar in Kaggle was still nearly empty, so we’re still CPU bound. That means we should be able to use a more capable model with little if any speed impact. convnext_small tops the performance/accuracy tradeoff score there, so let’s give it a go!\n\n\n\nConvNeXT\nThe ConvNeXT model was proposed in A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them.\nThe abstract from the paper is the following:\n\nThe “Roaring 20s” of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.\n\nLet’s take a look at one of them…\n\n# choose our vision model architecture\narch = 'convnext_small_in22k'\n\n\n# feed chosen model into our learner\nlearn = train(arch, item=Resize(192, method='squish'),\n              batch=aug_transforms(size=128, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.288349\n      0.913078\n      0.279673\n      05:54\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.646190\n      0.435760\n      0.138395\n      34:20\n    \n    \n      1\n      0.492490\n      0.374682\n      0.122057\n      34:16\n    \n    \n      2\n      0.316289\n      0.239387\n      0.075444\n      34:23\n    \n    \n      3\n      0.200733\n      0.164755\n      0.053340\n      34:13\n    \n    \n      4\n      0.134689\n      0.158538\n      0.050937\n      34:10\n    \n  \n\n\n\n\n# create our test set\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = learn.dls.test_dl(tst_files)\n\n\n# grab our predictions\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n\n\n\n\n\n\n\nTensorBase([7, 8, 3,  ..., 8, 1, 5])\n\n\n\n# grab disease names from vocab\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\n\n# map disease names to indexes\nmapping = dict(enumerate(dls.vocab)) # create a dictionary of the indexes and vocab\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping) # looks up the dictionary and returns the indexes, and name of indexes. Passing .map to a dictionary (mapping) is much fasster than passing to a function\nresults\n\n0                       hispa\n1                      normal\n2                       blast\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\n\n# lets's have a look at the sample Kaggle submisison file\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n    \n  \n  \n    \n      0\n      200001.jpg\n      NaN\n    \n    \n      1\n      200002.jpg\n      NaN\n    \n    \n      2\n      200003.jpg\n      NaN\n    \n    \n      3\n      200004.jpg\n      NaN\n    \n    \n      4\n      200005.jpg\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3464\n      203465.jpg\n      NaN\n    \n    \n      3465\n      203466.jpg\n      NaN\n    \n    \n      3466\n      203467.jpg\n      NaN\n    \n    \n      3467\n      203468.jpg\n      NaN\n    \n    \n      3468\n      203469.jpg\n      NaN\n    \n  \n\n3469 rows × 2 columns\n\n\n\n\n# replace 'label' column with our results\nss['label'] = results\nss.to_csv('subm.csv', index=False) \n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\n# function to submit to Kaggle\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial convnext small in22k', comp)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 70.5k/70.5k [00:01<00:00, 50.3kB/s]\n\n\nExcellent. This improved model achiveved a public score of 0.95617, comfortably mid table. But, we can do even better:\n\n\nPre-processing experiments\nSo, what shall we try first? One thing which can make a difference is whether we “squish” a rectangular image into a square shape by changing it’s aspect ratio, or randomly crop out a square from it, or whether we add black padding to the edges to make it a square. In the previous version we “squished”.\nWe can also try padding, which keeps all the original image without transforming it – here’s what that looks like:\n\n# data augmentation using padding\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize(192, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros))\ndls.show_batch(max_n=3)\n\n\n\n\n\n# feed our learner \nlearn = train(arch, item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n      batch=aug_transforms(size=(171,128), min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.263865\n      0.892569\n      0.281115\n      07:24\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.659388\n      0.440261\n      0.138395\n      44:51\n    \n    \n      1\n      0.513566\n      0.397354\n      0.131667\n      44:49\n    \n    \n      2\n      0.339301\n      0.231382\n      0.067756\n      44:36\n    \n    \n      3\n      0.204870\n      0.158647\n      0.047093\n      44:34\n    \n    \n      4\n      0.134242\n      0.140719\n      0.044690\n      44:33\n    \n  \n\n\n\nThat’s looking like a pretty good improvement - an error_rate of 0.044690 against 0.050937.\n\n\nTest time augmentation\nTo make the predictions even better, we can try test time augmentation (TTA), which our book defines as:\n\nDuring inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nBefore trying that out, we’ll first see how to check the predictions and error rate of our model without TTA:\n\nvalid = learn.dls.valid\npreds,targs = learn.get_preds(dl=valid)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n\n\nerror_rate(preds, targs)\n\nTensorBase(0.0509)\n\n\nThat’s the same error rate we saw at the end of training, above, so we know that we’re doing that correctly. Here’s what our data augmentation is doing – if you look carefully, you can see that each image is a bit lighter or darker, sometimes flipped, zoomed, rotated, warped, and/or zoomed:\n\nlearn.dls.train.show_batch(max_n=6, unique=True)\n\n\n\n\nIf we call tta() then we’ll get the average of predictions made for multiple different augmented versions of each image, along with the unaugmented original:\n\ntta_preds,_ = learn.tta(dl=valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nLet’s check the error rate of this:\n\nerror_rate(tta_preds, targs)\n\nTensorBase(0.0375)\n\n\nThat’s a huge improvement! We’re now ready to get our Kaggle submission sorted. First, we’ll grab the test set like we did in the last notebook:\n\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = learn.dls.test_dl(tst_files)\n\nNext, do TTA on that test set:\n\npreds,_ = learn.tta(dl=tst_dl)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nWe need to indices of the largest probability prediction in each row, since that’s the index of the predicted disease. argmax in PyTorch gives us exactly that:\n\nidxs = preds.argmax(dim=1)\n\nNow we need to look up those indices in the vocab. Last time we did that using pandas, although since then I realised there’s an even easier way!:\n\nvocab = np.array(learn.dls.vocab)\nresults = pd.Series(vocab[idxs], name=\"idxs\")\n\n\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\n# submit to Kaggle\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'convnext small 256x192 tta', comp)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 70.4k/70.4k [00:01<00:00, 44.9kB/s]\n\n\nThis submission scored 0.96309, improving on our previous submission score of 0.95617.\n\n\nIterative approach\nIt took a long time to train the ConvNeXT model (due to GPU constraints on my machine and also on Paperspace - it’s very rare for there to be any GPU’s available on the free subscription. I’ve since upgraded to ‘Pro’ which costs $8 pm at the time of writing). However you can see the significant improvements made by iterating, and the latest submission of 0.96309 would have been further improved by using larger images and more epochs.\n\n\n\nkaggle_submissions.JPG\n\n\n\n\nKey takeaways\nMost importantly, we have learned the importance of making an early submission to Kaggle, in order to obtain a baseline for rapid improvement through iterating. We’ve also learned some powerful data augmentation techniques, in particular test time augmentation (TTA), how to handle CPU bound environments by resizing images, and discovered the vision model playground that is timm."
  },
  {
    "objectID": "posts/Scaling up/Scaling_up.html",
    "href": "posts/Scaling up/Scaling_up.html",
    "title": "Scaling up",
    "section": "",
    "text": "Overview\nIn this analysis our goal will be to train an ensemble of larger models with larger inputs. The challenge when training such models is generally GPU memory. Kaggle GPUs have 16280MiB of memory available, as at the time of writing. I like to try out my notebooks on my home PC, then upload them – but we still need them to run OK on Kaggle (especially if it’s a code competition, where this is required). Just because it runs OK at home doesn’t mean it’ll run OK on Kaggle.\nI’m using PaperSpace (I recently upgraded to a ‘Pro’ subscription but capacity for GPUs is still quite limited.\nIt’s really helpful to be able to quickly try a few models and image sizes and find out what will run successfully. To make this quick, we can just grab a small subset of the data for running short epochs – the memory use will still be the same, but it’ll be much faster.\nOne easy way to do this is to simply pick a category with few files in it. Here’s our options:\nFirst we’ll repeat the steps we used last time to access the data and ensure all the latest libraries are installed, and we’ll also grab the files we’ll need for the test set:\n::: {.cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ tags=‘[]’}\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n!pip install timm\nimport timm\n:::\n\n!pip install -q kaggle\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n\nfrom fastai.vision.all import *\nset_seed(42)\n\ntst_files = get_image_files(path/'test_images').sorted()\n\n\n# load dataset\ndf = pd.read_csv(path/'train.csv')\ndf.label.value_counts()\n\nnormal                      1764\nblast                       1738\nhispa                       1594\ndead_heart                  1442\ntungro                      1088\nbrown_spot                   965\ndowny_mildew                 620\nbacterial_leaf_blight        479\nbacterial_leaf_streak        380\nbacterial_panicle_blight     337\nName: label, dtype: int64\n\n\n\n\nMemory and gradient accumulation\nGradient accumulation refers to a very simple trick: rather than updating the model weights after every batch based on that batch’s gradients, instead keep accumulating (adding up) the gradients for a few batches, and them update the model weights with those accumulated gradients. In fastai, the parameter you pass to GradientAccumulation defines how many batches of gradients are accumulated. Since we’re adding up the gradients over accum batches, we therefore need to divide the batch size by that same number. The resulting training loop is nearly mathematically identical to using the original batch size, but the amount of memory used is the same as using a batch size accum times smaller!\nFor instance, here’s a basic example of a single epoch of a training loop without gradient accumulation:\nfor x,y in dl:\n    calc_loss(coeffs, x, y).backward()\n    coeffs.data.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\nHere’s the same thing, but with gradient accumulation added (assuming a target effective batch size of 64):\ncount = 0            # track count of items seen since last weight update\nfor x,y in dl:\n    count += len(x)  # update count based on this minibatch size\n    calc_loss(coeffs, x, y).backward()\n    if count>64:     # count is greater than accumulation target, so do weight update\n        coeffs.data.sub_(coeffs.grad * lr)\n        coeffs.grad.zero_()\n        count=0      # reset count\nThe full implementation in fastai is only a few lines of code – here’s the source code. To see the impact of gradient accumulation, consider this small model:\nLet’s use bacterial_panicle_blight since it’s the smallest:\n\n# create a susbset to include only the smallest disease dataset \n# we don't really care about this model, we just want to know how much memory it uses\ntrn_path = path/'train_images'/'bacterial_panicle_blight'\n\nNow we’ll set up a train function which is very similar to the steps we used for training in the last notebook. But there’s a few significant differences…\nThe first is that I’m using a finetune argument to pick whether we are going to run the fine_tune() method, or the fit_one_cycle() method – the latter is faster since it doesn’t do an initial fine-tuning of the head. When we fine tune in this function I also have it calculate and return the TTA predictions on the test set, since later on we’ll be ensembling the TTA results of a number of models. Note also that we no longer have seed=42 in the ImageDataLoaders line – that means we’ll have different training and validation sets each time we call this. That’s what we’ll want for ensembling, since it means that each model will use slightly different data.\nThe more important change is that I’ve added an accum argument to implement gradient accumulation. As you’ll see in the code below, this does two things:\n\nDivide the batch size by accum\nAdd the GradientAccumulation callback, passing in accum.\n\n\n# create a function to train a model which includes a gradient accumulation (accum) argument, when set to 1 no impact on batch size\n# note no seed set so different training & validation sets each time we call it\ndef train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item,\n        batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)\n    cbs = GradientAccumulation(64) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    if finetune:\n        learn.fine_tune(epochs, 0.01)\n        return learn.tta(dl=dls.test_dl(tst_files))\n    else:\n        learn.unfreeze()\n        learn.fit_one_cycle(epochs, 0.01)\n\n\n# try out our new training function on a small model\ntrain('convnext_small_in22k', 128, epochs=1, accum=1, finetune=False)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      00:13\n    \n  \n\n\n\nLet’s create a function to find out how much memory it used, and also to then clear out the memory for the next run:\n\nimport gc\n# create a function that tells us how much memory model uses\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache() # clear out memory for next run\n\n\n!pip install pynvml\nreport_gpu()\n\nRequirement already satisfied: pynvml in /usr/local/lib/python3.9/dist-packages (11.4.1)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nGPU:0\nprocess      26004 uses     3768.625 MB GPU memory\n\n\nSo with accum=1 the GPU used around 3.8GB RAM. Let’s try accum=2:\n\n# try out our new training function on a model\ntrain('convnext_small_in22k', 128, epochs=1, accum=2, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      00:12\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     2750.625 MB GPU memory\n\n\nAs you see, the RAM usage has now gone down to 2.75GB. It’s not halved since there’s other overhead involved (for larger models this overhead is likely to be relatively lower).\nLet’s try 4:\n\n# try out our new training function on a model\ntrain('convnext_small_in22k', 128, epochs=1, accum=4, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      00:12\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     2216.625 MB GPU memory\n\n\nThe memory use is even lower!\n\n\nChecking memory use\nWe’ll now check the memory use for each of the architectures and sizes we’ll be training later, to ensure they all fit in 8GB RAM. For each of these, I tried accum=1 first, and then doubled it any time the resulting memory use was over 8GB. As it turns out, accum=32 covered most of what I needed. swin_large_patch4_window7_224 and vit_large_patch16_224 were too large.\nFirst, convnext_large:\n\n# number 1\ntrain('convnext_large_in22k', 224, epochs=1, accum=16, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      01:55\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     5168.625 MB GPU memory\n\n\n\n# number 2\ntrain('convnext_large_in22k', (320,240), epochs=1, accum=32, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      03:37\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     4996.625 MB GPU memory\n\n\nHere’s vit_large. Not able to run this one, even when setting accum to 64!\n\n# number 3\ntrain('vit_large_patch16_224', 224, epochs=1, accum=64, finetune=False)\nreport_gpu()\n\nThen finally our swinv2 and swin models:\n\n# number 4\ntrain('swinv2_large_window12_192_22k', 192, epochs=1, accum=32, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      01:03\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     7780.625 MB GPU memory\n\n\n\n#number 5\ntrain('swin_large_patch4_window7_224', 224, epochs=1, accum=64, finetune=False)\nreport_gpu()\n\n\n\nRunning the models\nIn my previous blog, we tried a bunch of different architectures and preprocessing approaches on small models, and picked a few which looked good. We can use a dict to list the preprocessing approaches we’ll use for each architecture of interest based on that analysis:\n\nres = 640,480\n\n\n# create a dictionary of model preprocessing\nmodels = {\n    'convnext_large_in22k': {\n        (Resize(res), (320,224)),\n    }, 'vit_large_patch16_224': {\n        (Resize(480, method='squish'), 224),\n        (Resize(res), 224),\n    }, 'swinv2_large_window12_192_22k': {\n        (Resize(480, method='squish'), 192),\n        (Resize(res), 192),\n    }, 'swin_large_patch4_window7_224': {\n        (Resize(res), 224),\n    }\n}\n\nWe’ll need to switch to using the full training set of course!\n\n# set training set\ntrn_path = path/'train_images'\n\nNow we’re ready to train all these models. Remember that each is using a different training and validation set, so the results aren’t directly comparable.\nWe’ll append each set of TTA predictions on the test set into a list called tta_res.\n\n# display for each model in our above dictionary\n# architecture, data augmentations, loss and error rates\ntta_res = []\n\nfor arch,details in models.items():\n    for item,size in details:\n        print('---',arch)\n        print(size)\n        print(item.name)\n        tta_res.append(train(arch, size, item=item, accum=32)) #, epochs=1))\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n\n\nconvnext_large_in22k.JPG\n\n\n\n\n\nvit_large_patch16_224.JPG\n\n\n\n\n\nvit_large_patch16_224_square.JPG\n\n\n\n\n\nswinv2_large_window12_192_22k_square.JPG\n\n\n\n\n\nswinv2_large_window12_192_22k.JPG\n\n\n\n\n\nswin_large_patch4_window7_224.JPG\n\n\nSince this has taken quite a while to run, let’s save the results, just in case something goes wrong!\n\n# pickle the results for future use\nsave_pickle('tta_res.pkl', tta_res)\n\n\n\nEnsembling\nAs you can see from the above, each of the individual models score well, but an ensemble (which simply refers to a model which is itself the result of combining a number of other models) can produce even better results. The simplest way to do ensembling is to take the average of the predictions of each model:\nLearner.tta returns predictions and targets for each rows. We just want the predictions:\n\ntta_prs = first(zip(*tta_res))\n\nOriginally I just used the above predictions, but later I realised in my experiments on smaller models that vit was a bit better than everything else, so I decided to give those double the weight in my ensemble. I did that by simply adding to the list a second time (we could also do this by using a weighted average):\n\ntta_prs += tta_prs[1:3]\n\n\n# calculate average predictions of our ensemble of models\navg_pr = torch.stack(tta_prs).mean(0)\navg_pr.shape\n\ntorch.Size([3469, 10])\nThat’s all that’s needed to create an ensemble! Finally, we copy the steps we used in the last notebook to create a submission file:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\nidxs = avg_pr.argmax(dim=1)\nvocab = np.array(dls.vocab)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = vocab[idxs]\nss.to_csv('subm.csv', index=False)\n\n\n\nSubmit to Kaggle\n\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'part 3 v2', comp)\n\nThat’s it – at the time of creating this analysis, that got easily to the top of the leaderboard! Here are the four submissions Jeremy entered, each of which was better than the last, and each of which was ranked #1:\n\nEdit: Actually the one that got to the top of the leaderboard timed out when Jeremy ran it on Kaggle Notebooks, so he had to remove four of the runs from the ensemble. There’s only a small difference in accuracy however.\nGoing from bottom to top, here’s what each one was:\n\nconvnext_small trained for 12 epochs, with TTA\nconvnext_large trained the same way\nThe ensemble in this notebook, with vit models not over-weighted\nThe ensemble in this notebook, with vit models over-weighted.\n\n\n\nConclusion\n\nRuntimeError: CUDA error: out of memory\n\nWe all know how frustrating the above error is, and the inevitable lament - “if only I had a better GPU spec :(”\nThe key takeaway from this blog is to remain calm when faced with the above error, and resist the temptation to go and splash out on an expensive new GPU card! It is often possible to scale up and train large models using a technique called gradient accumulation (despite apparent GPU constraints). We can then use a further technique called ensembling which involvees averaging the results of models with different architectures, and varying performance, to achieve an overall performance which is better than any of the models on their own."
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html",
    "href": "posts/Financial Forecasting/Financial Forecasting.html",
    "title": "Financial Forecasting in Python",
    "section": "",
    "text": "In Financial Forecasting in Python, we will step into the role of CFO and learn how to advise a board of directors on key metrics while building a financial forecast, the basics of income statements and Balance Sheets, and cleaning messy financial data. During this blog we will examine real-life datasets from Netflix, Tesla, and Ford, using the pandas package. Following this blog we will be able to calculate financial metrics, work with assumptions and variances, and build our own forecast in Python."
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#balance-sheet-and-forecast-ratios",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#balance-sheet-and-forecast-ratios",
    "title": "Financial Forecasting in Python",
    "section": "Balance Sheet and forecast ratios",
    "text": "Balance Sheet and forecast ratios\n\nCalculating accounts receivable (debtors)\nWhen we sell something on credit, the credit portion is in the balance sheet under ‘Accounts Receivable’ or ‘Debtors’. For example, if credit sales are made in January with a 60-day payback period, they would be recorded in our ‘Debtors’ account in January, but only be paid (released) in March, and so on.\nIn this exercise, we will create the following lists:\n\nThe credit sales in the month credits, which in this exercise is 60% of the sale value.\nThe total accounts receivable debtors, to be calculated as the credits for the current month, plus the credits of the month before, minus the credits of two months before (as we assume the credits from 2 months ago or 60 days, will be repaid by then).\n\nWe have set an index for the variable month. The month value is set at 0.\n\n# Create the list for sales, and empty lists for debtors and credits\nmonth = 0\nsales = [500, 350, 700]\ndebtors = [] \ncredits = []\n\n# Create the statement to append the calculated figures to the debtors and credits lists\nfor mvalue in sales: \n    credits.append(mvalue * 0.6)\n    if month > 0:\n        debtors.append(credits[month] + credits[month-1])\n    else:\n        debtors.append(credits[month]) \n    month += 1\n# Print the result\nprint(\"The ‘Debtors’ are {}.\".format(debtors))\n\nThe ‘Debtors’ are [300.0, 510.0, 630.0].\n\n\n\n\nBad debts\nWhen offering credit terms to customers, there is always a risk that the customer does not pay their debt. In the finance world, this is known as “bad debts”.\nAs we have already recorded sales, we need to record the loss of sales now, as we never received the payment.\nThis affects both the income statement and the balance sheet. In the income statement, we record a negative value in the sales for the month we write off the debt. In the balance sheet, we need to reduce our debtor’s asset.\nThe following variables have been defined for January: debtors_jan = 1500\nIn February, we received news that a customer has gone into liquidation. This customer currently owes 500 USD.\nWe expect to recover 70% of this amount; the rest has to be written off as bad debts.\n\ndebtors_jan = 1500\n\n# Calculate the bad debts for February\nbad_debts_feb = 500 * 0.3\n\n# Calculate the feb debtors amount\ndebtors_feb = (debtors_jan- bad_debts_feb)\n\n# Print the debtors for January and the bad debts and the debtors for February\nprint(\"The debtors are {} in January, {} in February. February's bad debts are {} USD.\".format(debtors_jan, debtors_feb, bad_debts_feb))\n\nThe debtors are 1500 in January, 1350.0 in February. February's bad debts are 150.0 USD.\n\n\nYou can see that our debtors amount is reduced by the amount of bad debts.\n\n\nCalculating accounts payable (creditors)\nNow we will look at a scenario where we are the ones being granted credit. This means that we can buy something, but only have to pay for this amount later.\nIn this exercise, T-Z needs to buy nuts and bolts to produce 1000 units in January and 1200 units in February. The cost of nuts and bolts per unit is 0.25 USD. The credit terms are 50% cash upfront and 50% in 30 days.\nTherefore, the creditors’ value, in this case, would be paid the month directly after. This means that the creditors’ value would only reflect the current month’s credit purchases.\n\n# Set the cost per unit\nunit_cost = 0.25\n\n# Create the list for production units and empty list for creditors\nproduction = [1000,1200]\ncreditors = []\n\n# Calculate the accounts payable for January and February\nfor mvalue in production: \n    creditors.append(mvalue * unit_cost * 0.5)\n    \n# Print the creditors balance for January and February\nprint(\"The creditors balance for January and February are {} and {} USD.\".format(creditors[0], creditors[1]))\n\nThe creditors balance for January and February are 125.0 and 150.0 USD.\n\n\nAs we can see, the Balance Sheet shows us what our real cash situation looks like, as just because we made a sale does not mean money in the bank, and incurring an expense also does not mean we have to pay it right away!\n\n\nDebtor days ratio\n\n\n\ndebtors_days_ratio.JPG\n\n\nThe first ratio we will look at is debtor days. This ratio looks at how many days it takes to receive our money from our debtors. It is usually calculated over a period of 1 financial year.\nThe following information is available to you:\n\nSales for the year: 12,500 USD\nEnding Debtors balance: 650\n\n\n# Create the variables\ndebtors_end = 650\nsales_tot = 12500\n\n# Calculate the debtor days variable\nddays_ratio = (debtors_end/sales_tot) * 365\n\n# Print the result\nprint(\"The debtor days ratio is {}.\".format(ddays_ratio))\n\nThe debtor days ratio is 18.98.\n\n\n\n\nDays payable outstanding\n\n\n\ncreditors_days_ratio.JPG\n\n\nWe will now have a look at our accounts payable, or creditors, and a ratio called the Days Payable Outstanding (DPO).\nThis ratio is an efficiency ratio that measures the average number of days a company takes to pay its suppliers.\nT-Z wants to know its days payable outstanding and has asked you to calculate it.\n\n# Get the variables\ncogs_tot = 4000\ncreditors_end = 650\n\n# Calculate the days payable outstanding\ndpo = (creditors_end/cogs_tot)*365\n\n# Print the days payable outstanding\nprint(\"The days payable outstanding is {}.\".format(dpo))\n\nThe days payable outstanding is 59.3125.\n\n\n\n\nDays in inventory\n\n\n\ndays_in_inventory.JPG\n\n\nIn this exercise, we will calculate the time it takes for a company to turn inventory into sales (days in inventory or DII ratio) based on the following information:\n\ncogs_total = 4000\nav_inv = 1900\nsales_tot = 10000\nob_assets = 2000\ncb_assets = 7000\n\n\n# Calculate the dii ratio \ndii_ratio = (av_inv/cogs_tot)*365\n\n# Print the result\nprint(\"The DII ratio is {}.\".format(dii_ratio))\n\nThe DII ratio is 173.375.\n\n\n\n\nAsset Turnover\n\n\n\nasset_turnover.JPG\n\n\nIn this exercise, we will calculate the efficiency of a company’s assets by seeing how the company uses its assets to generate sales (asset turnover ratio):\n\n# Calculate the Average Assets\nav_assets = (ob_assets + cb_assets)/2\n\n# Calculate the Asset Turnover Ratio\nat_ratio = sales_tot/av_assets\n\n# Print the Asset Turnover Ratio\nprint(\"The asset turnover ratio is {}.\".format(at_ratio))\n\nThe asset turnover ratio is 2.2222222222222223.\n\n\nLet’s test our understanding of Balance Sheet ratios:\n\n\n\nunderstanding_ratios.JPG\n\n\n\n\nCalculating Balance Sheet ratios for Ford\nNow we will look at a real life example, Ford Inc, a company producing motor vehicles. We will first upload a dataset: balance_sheet with the data for Ford Inc’s Balance Sheet as at 31 December 2017. The sales and cost of sales figures have been provided for 2017 within the Key_Figures_Memo dataset.\nWe are only interested in one line on the balance sheet, the Receivables (another name for Debtors), and therefore need to create a filter for this. In this exercise, we will use boolean indexing to filter our dataset for Receivables in the metric column. We will first specify our metric of interest ('Receivables'), and then check whether the column of interest has this value in each row. This will generate a boolean series of True and False values. With this series, we can then filter our existing dataset.\nOnce we have filtered our dataset, we can retrieve the receivables values from the most recent time period and calculate the debtor days ratio.\n\n# read in the Ford Balance Sheet data\nbalance_sheet = pd.read_csv('Data/F-Balance-Sheet.csv')\n\n\n# Create the filter metric for Receivables\nreceivables_metric = ['Receivables']\n\n# Create a boolean series with your metric\nreceivables_filter = balance_sheet.metric.isin(receivables_metric)\n\n# Use the series to filter the dataset\nfiltered_balance_sheet = balance_sheet[receivables_filter]\nfiltered_balance_sheet \n\n\n\n\n\n  \n    \n      \n      metric\n      2013-12\n      2014-12\n      2015-12\n      2016-12\n      2017-12\n    \n  \n  \n    \n      6\n      Receivables\n      87309.0\n      92819.0\n      101975.0\n      57368.0\n      62809.0\n    \n  \n\n\n\n\n\n# bring in values for Sales and Cost of Sales\nsales=156776\ncogs=131332 \n\n\n# From previous step\nreceivables_metric = ['Receivables']\nreceivables_filter = balance_sheet.metric.isin(receivables_metric)\nfiltered_balance_sheet = balance_sheet[receivables_filter]\n\n# Extract the zeroth value from the last time period (2017-12)\ndebtors_end = filtered_balance_sheet['2017-12'].iloc[0]\n\n# Calculate the debtor days ratio\nddays = (debtors_end/sales) * 365\n\n# Print the debtor days ratio\nprint(\"The debtor day ratio is {:.0f}. A higher debtors days ratio means it takes longer to collect cash from debtors.\".format(ddays))\n\nThe debtor day ratio is 146. A higher debtors days ratio means it takes longer to collect cash from debtors.\n\n\nNext, we will learn how to append this information into our forecast.\n\n\nForecasting the Balance Sheet for Ford\nNow that we have the ratios for Ford, the management wishes to improve them for the 2018 financial year.\nThere is one ratio in particular that management would like to be improved – the debtor days ratio, which is currently sitting at 146 days, while better than Nissan (244 days), is still much higher than Toyota, the industry leader (31 days).\nManagement would like to bring the debtor days down to below 100 days.\n\n# set target debtor days\nddays = 99\n\nSales are expected to increase by 10%. Calculate these forecasted sales, f_sales from the existing sales :\n\n# Calculate the forecasted sales \nf_sales = sales * 1.10\nf_sales\n\n172453.6\n\n\nWe can rearrange the debtor days formula to calculate the closing balance of debtors needed to achieve 99 debtor days :\n\n# Solve for the forecasted debtors' ending balance\nf_debtors_end = f_sales * ddays/ 365\n\nprint(\"If sales rise by 10% and the debtor days decrease to {:.0f} then the forecasted closing balance for debtors will be {:.0f}.\".format(ddays, f_debtors_end))\n\nIf sales rise by 10% and the debtor days decrease to 99 then the forecasted closing balance for debtors will be 46775.\n\n\nLet’s now append a column to include the forecasted debtors:\n\n# Get the number of columns in the filtered balance sheet\nn_cols = len(filtered_balance_sheet.columns)\n\n# Append a Forecast column of the forecasted debtors' end balance\nfiltered_balance_sheet.insert(n_cols, 'Forecast', f_debtors_end)\n\n# See the result\nfiltered_balance_sheet\n\n\n\n\n\n  \n    \n      \n      metric\n      2013-12\n      2014-12\n      2015-12\n      2016-12\n      2017-12\n      Forecast\n    \n  \n  \n    \n      6\n      Receivables\n      87309.0\n      92819.0\n      101975.0\n      57368.0\n      62809.0\n      46775.086027"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#financial-periods-and-how-to-work-with-them",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#financial-periods-and-how-to-work-with-them",
    "title": "Financial Forecasting in Python",
    "section": "Financial periods and how to work with them",
    "text": "Financial periods and how to work with them\n\nConverting quarters into months\nA company has a challenge in separating data into months. It has received the following data:\n\nQuarter 1 = 700\nQuarter 2 = 650\n\nThe split across the months within each quarter is equal. Our goal is to separate this data into a list format containing the amounts per month for the first two quarters.\n\n# Create a list for quarters and initialize an empty list qrtlist\nquarters = [700, 650]\nqrtlist = []\n\n# Create a for loop to split the quarters into months and add to qrtlist\nfor qrt in quarters:\n month = round(qrt / 3, 2)\n qrtlist = qrtlist + [month, month, month]\n \n# Print the result\nprint(\"The values per month for the first two quarters are {}.\".format(qrtlist))\n\nThe values per month for the first two quarters are [233.33, 233.33, 233.33, 216.67, 216.67, 216.67].\n\n\n\n\nMerging months into quarters\nNow we saw what to do when we wanted to split quarters into months for more detailed monthly information. But what happens when the opposite is true and we wish to combine data into fewer columns? This is typical when dealing with historical data, when monthly details may not be necessary, or when we need a highly consolidated version of the data for a report.\nThe key here is to create an index, and then only add to the quarter total quarter in cycles of 3, or until the length of the list. We can do this with the following code:\nif index % 3 == 0 or index == len(months):\nThis code checks whether the index divided by three yields a remainder of 0, or if the index is at the end of the list months. Thus, in a loop it will execute the specified code every three months or when it reaches the end of the list.\nThe monthly sales are already provided in the code as months, containing the sales from the first two quarters, as well as the first month of Q3. Our task is to generate a new list called quarters that contains the quarterly totals from the first three months (which includes the partial total of Q3).\n\n# Create a months list, as well as an index, and set the quarter to 0\nmonths = [100, 100, 150, 250, 300, 10, 20]\nquarter = 0\nquarters = []\nindex = 1\n\n# Create for loop for quarter, print result, and increment the index\nfor sales in months:\n    quarter += sales\n    if index % 3 == 0 or index == len(months):\n        quarters.append(quarter)\n        quarter = 0\n    index = index + 1\n    \nprint(\"The quarter totals are Q1: {}, Q2: {}, Q3: {}\".format(quarters[0], quarters[1], quarters[2]))\n\nThe quarter totals are Q1: 350, Q2: 560, Q3: 20\n\n\nLet’s have a look at a library that can help us more when working with dates.\n\n\nThe datetime library\nSales area A in Europe and Sales area B in Australia have different date formats.\n\nSale A: 4000 on 14/02/2018\nSale B: 3000 on 2 March 2018\n\nIf we want to consolidate or compare sales periods, we need to convert to the same date format. We can easily do this by using the datetime library and the datetime.strptime(date_string, format) method, using the following directives:\n\n\n\ndate_time.JPG\n\n\n\n# Import the datetime python library\nfrom datetime import datetime\n\n# Create a dt_object to convert the first date and print the month result\ndt_object1 = datetime.strptime('14/02/2018', '%d/%m/%Y')\nprint(dt_object1)\n\n# Create a dt_object to convert the second date and print the month result\ndt_object2 = datetime.strptime('2 March 2018', '%d %B %Y')\nprint(dt_object2)\n\n2018-02-14 00:00:00\n2018-03-02 00:00:00\n\n\n\nConverting date formats - explicit\nLet’s revisut one of the dates from the previous exercise.\n\nSale A: 4000 on 14/02/2018\n\nWe used the datetime library to identify the day d, month m, and year y which could help us to identify data from datasets with different date formats. However, what about a scenario where we want to convert date formats into a specific format?\nIn this exercise we will convert Sale A from the format 14/02/2018 to the same date format as Sale B (i.e. 14 February 2018).\nWe can do this easily with built-in Python functions. To split a string we can use the .split()method:\n\n\n\nsplit_().JPG\n\n\n\n# Set the variable for the datetime to convert\ndt = '14/02/2018'\n\n# Create the dictionary for the month values\nmm = {'01': 'January', '02': 'February', '03': 'March'}\n\n# Split the dt string into the different parts\nday, month, year = dt.split('/')\n\n# Print the concatenated date string\nprint(day + ' ' + mm['02'] + ' ' + year)\n\n14 February 2018"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#tips-and-tricks-when-working-with-datasets",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#tips-and-tricks-when-working-with-datasets",
    "title": "Financial Forecasting in Python",
    "section": "Tips and tricks when working with datasets",
    "text": "Tips and tricks when working with datasets\n\n\n\nchallenges.JPG\n\n\n\nWorking with datasets - month totals\nIn this exercise, we will be exploring a dataset that has multiple sales in one month. We will create a script that will enable us to identify dates within the same month, and combine them into a new month total, and append this to the table.\nWe will be using the dataset df, which represents data from one of our sales areas. Print it out in the console to have a look at the data. As you can see, there were two sales in March. We will combine these sales into a single month total. We can iterate over the dataset using the .iteritems() method.\n\n\n\niteritems().JPG\n\n\nWe will also be using the .split() method.\n\n\n\nsplit_().JPG\n\n\n\n# create a DataFrame to include our sales data\ndf = pd.DataFrame(columns=['Description','14-Feb', '19-Mar', '22-Mar'])\n\ndf.loc[0] = ['Sales', 3000, 1200, 1500]\ndf\n\n\n\n\n\n  \n    \n      \n      Description\n      14-Feb\n      19-Mar\n      22-Mar\n    \n  \n  \n    \n      0\n      Sales\n      3000\n      1200\n      1500\n    \n  \n\n\n\n\n\n# Set the index to start at 0\nindex = 0\n\n# Create the dictionary for the months\ntt = {'Jan': 0, 'Feb': 0, 'Mar': 0}\n\n\n# Create a for loop that will iterate the date and amount values in the dataset\nfor date, amount in df.iteritems():\n    # Create the if statement to split the day and month, then add it to the new tt variable\n    if index > 0: \n        day, month = date.split('-')\n        tt[month] +=float(amount[0])\n    index += 1\n\nprint(tt)\n\n{'Jan': 0, 'Feb': 3000.0, 'Mar': 2700.0}\n\n\n/tmp/ipykernel_127/3317898835.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for date, amount in df.iteritems():\n\n\n\n\nWorking with datasets - combining datasets\nIn this example, we will be working with two datasets, df1 and df2. You will notice that they contain different date formatting.\nMore specifically, df1 specifies the month by the name (e.g. 02-Feb-18), whereas df2 specifies the month numerically (e.g. 06/01/2018). Additionally, df1 uses a hyphen (-) as a separator, whereas df2 uses a forward slash (/) as a separator.\nWe will be combining these two datasets to form a consolidated forecast for the quarter. To do this, we will need to parse the different date formats of df1 and df2.\n\n# create a DataFrame to include our sales data\ndf1 = pd.DataFrame(columns=['02-Feb-18', '15-Mar-18'])\n\ndf1.loc[0] = [3000, 1200]\ndf1\n\n\n\n\n\n  \n    \n      \n      02-Feb-18\n      15-Mar-18\n    \n  \n  \n    \n      0\n      3000\n      1200\n    \n  \n\n\n\n\n\n# create an empty dictionary containing total sales for each month initialized to 0\ntotals = {'Jan': 0, 'Feb': 0, 'Mar': 0}\n\n# create a dictionary containing the months (Jan, Feb, Mar) and corresponding numbers\ncalendar = {'01': 'Jan', '02': 'Feb', '03': 'Mar'}\n\n\n# Create a for loop to iterate over the items in the first dataset df1\nfor date, amount in df1.iteritems():\n        day, month, year = date.split('-')\n        totals[month] +=float(amount[0]) \n\n/tmp/ipykernel_127/1296325721.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for date, amount in df1.iteritems():\n\n\n\n# create a DataFrame to include our sales data\ndf2 = pd.DataFrame(columns=['06/01/2018', '14/02/2018'])\n\ndf2.loc[0] = [1000, 1200]\ndf2\n\n\n\n\n\n  \n    \n      \n      06/01/2018\n      14/02/2018\n    \n  \n  \n    \n      0\n      1000\n      1200\n    \n  \n\n\n\n\n\n# Create a for loop to iterate over the items in the second dataset df2\n# This time month will yield a a numerical reference, so we will need to use our calendar dictionary to add the amount to our totals dictionary.\nfor date, amount in df2.iteritems():\n        day, month, year = date.split('/')\n        totals[calendar[month]] += float(amount[0])\n\nprint(totals)\n\n{'Jan': 1000.0, 'Feb': 4200.0, 'Mar': 1200.0}\n\n\n/tmp/ipykernel_127/4206160929.py:3: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for date, amount in df2.iteritems():"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#exporting-data",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#exporting-data",
    "title": "Financial Forecasting in Python",
    "section": "Exporting data",
    "text": "Exporting data\n\n\n\nexport.JPG"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#assumptions-and-variances-in-forecasts",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#assumptions-and-variances-in-forecasts",
    "title": "Financial Forecasting in Python",
    "section": "Assumptions and variances in forecasts",
    "text": "Assumptions and variances in forecasts\n\nBuilding sensitive forecast models\n\n\n\nforecasting_considerations.JPG\n\n\n\n\n\nassumptions_2.JPG\n\n\n\n\nWeighted probability\nTxs Tools, a company selling hardware tools, is looking to expand out of their home market A into Market B. They have done some market research, and have received the following numeric probabilities:\n\n\n\nweighted_prob.JPG\n\n\nTxs Tools will only be motivated to expand if they can have reasonable assurance that they will achieve sales of 400 or more. To manage the different forecast sales probabilities, Txs Tools have asked us to calculate the weighted probability.\n\n# Create the combined list for sales and probability\nsales_probability = ['0|0.05', '200|0.10', '300|0.40', '500|0.2', '800|0.25'] \nweighted_probability = 0\n\n# Create a for loop to calculate the weighted probability\nfor pair in sales_probability:\n    parts = pair.split('|')\n    weighted_probability += float(parts[0]) * float(parts[1]) # float converts to a floating point \n\n# Print the weighted probability result\nprint(\"The weighted probability is {}.\".format(weighted_probability))\n\nThe weighted probability is 440.0.\n\n\nHave a look at the calculated weighted probability. We can see it reflects a weighted value between the highest and lowest sales figures. The weighted probability is a technique to manage the uncertainty in Txs Tools sales forecasting, and can give a more balanced view on expected sales numbers as opposed to just going for the lowest or highest number.\n\n\nMarket sentiment\nTxs Tools has forecast sales of 500 in January, with an expected increase of 5% per month for the rest of the quarter.\nHowever, this is dependent on the market sentiment. Based on historical trends, the following information has been provided:\n\nIf the market sentiment drops below 0.6 then the sales will only be realized at an increase of 2% per month.\nIf market sentiment increases above 0.8. then sales are expected to increase by 7%.\n\n\n# Create the computevariance function\ndef computevariance(amount, sentiment):\n if (sentiment < 0.6):\n  res = amount + (amount * 0.02)\n elif (sentiment > 0.8):\n  res = amount + (amount * 0.07)\n else:\n  res = amount + (amount * 0.05)\n return res\n\n\n# Compute the variance for jan, feb and mar\njan = computevariance(500, 0.5)\nfeb = computevariance(500, 0.65)\nmar = computevariance(500, 0.85)\n\nprint(\"The forecast sales considering variance due to market sentiment is {} for Jan, {} for Feb, and {} for Mar.\".format(jan, feb, mar))\n\nThe forecast sales considering variance due to market sentiment is 510.0 for Jan, 525.0 for Feb, and 535.0 for Mar.\n\n\n\n\nDependencies and sensitivity\n\n\n\ndep_sens.JPG\n\n\n\nAssigning dependencies for sales and COGS\nTxs Tools have built a monthly forecast for their gross profit. This will rely on dependencies for Sales and COGS.\nSet the dependencies for sales and cogs based on the information below:\n\nSales dependency sales_dep: The sale price is the net price after 1 USD commission. Commissions paid increase from 1 USD per unit to 2 USD per unit for every unit above 350 units sold.\nCost dependency cost_dep: When sales per unit increase above 500 units, an additional production line needs to be used, causing an increase in the cost per unit above 500 of 2 USD per unit.\n\nThe baseline sale price per unit (base_sales_price) is 15 USD and the baseline cost per unit (base_cost_price) is 7 USD.\n\n# instantiate the base sales price\nbase_sales_price = 15\n\n# instantiate the sales \nsales = 750\n\n\n# Set the Sales Dependency\nif sales >= 350:\n    sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\nelse:\n    sales_dep = sales * base_sales_price\n\n# Print the results\nprint(\"The sales dependency is {} USD.\".format(sales_dep))\n\nThe sales dependency is 10850 USD.\n\n\n\n# instantiate the bases cost price \nbase_cost_price = 7\n\n\n# Set the Cost Dependency\nif sales >= 500:\n    cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\nelse:\n    cost_dep = sales * base_cost_price\n    \n# Print the results\nprint(\"The cost dependency is {} USD.\".format(cost_dep))\n\nThe cost dependency is 5750 USD.\n\n\n\n\nBuilding a sensitivity analysis for gross profit\nxs Tools is now ready to use these dependencies in the gross profit forecast.\nThe following forecast unit sales have been provided:\nJul = 700 Aug = 350 Sep = 650\nThe dependencies for sales and cogs are based on the following:\n\nSales dependency sales_dep: The sale price is the net price after 1 USD commission. Commissions paid increase from 1 USD per unit to 2 USD per unit for every unit above 350 units sold.\nCost dependency cost_dep: When sales per unit increase above 500 units, an additional production line needs to be used, causing an increase in the cost per unit above 500 of 2 USD per unit.\n\nThe basic cost price base_cost_price = 7 and basic sales price base_sales_price = 15\n\n# Create the sales_usd list\nsales_usd = [700, 350, 650]\n\n\n# Create the if statement to calculate the forecast_gross_profit\nfor sales in sales_usd:\n    if sales > 350:\n        sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n    else:\n        sales_dep = sales * base_sales_price\n    if sales > 500:\n        cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n    else:\n        cost_dep = sales * base_cost_price\n    forecast_gross_profit = sales_dep - cost_dep\n\n    # Print the result\n    print(\"The gross profit forecast for a sale unit value of {} is {} USD.\".format(sales, forecast_gross_profit))\n\nThe gross profit forecast for a sale unit value of 700 is 4850 USD.\nThe gross profit forecast for a sale unit value of 350 is 2800 USD.\nThe gross profit forecast for a sale unit value of 650 is 4600 USD.\n\n\n\n\nAssigning dependencies for expenses\nTxs Tools wants to assign a dependency for its operating expenses, particularly admin salaries.\nThe conditions are as follows:\n\nAdmin expenses increase in July and August (Jul and Aug) as temporary workers need to be hired to cover the summer holiday.\nThe increase is based on the number of employees taking holidays during that time. For the current year, the value for August is emp_leave = 6 (6 employees expected to take leave).\nThe cost is 80 USD per temp employee hired.\n\n\n# instantiate the emp leave value for August\nemp_leave = 6\n\n\n# Set the admin dependency\nif emp_leave > 0:\n    admin_dep = emp_leave * 80\n\n# Print the results\nprint(\"The admin dependency for August is {} USD.\".format(admin_dep))\n\nThe admin dependency for August is 480 USD.\n\n\n\n\nBuild a sensitivity analysis for the net profit\nTxs Tools has provided the following forecast admin cost in USD based on full-time employees:\nJul = 1500 Aug = 1500 Sep = 1500\nBuild the forecast net profit forecast_net_profit when emp_leave = [6, 6, 0] and the cost per temp employee is 80 USD.\n\n# instantiate our standing data\nadmin_usd = [1500, 1500, 1500]\nemp_leave = [6, 6, 0]\nforecast_gross_profit = [4850, 2800, 4600]\n\n\n# Create an index variable and initialize this index to 0\nindex = 0\n\n\n# Create the dependency by looping through the admin_usd list, using our index to access the correct month in our lists.\nfor admin in admin_usd:\n    temp = emp_leave[index]\n    if temp > 0:\n        admin_dep = temp * 80 + admin\n    else: \n         admin_dep = admin\n    forecast_net_profit = forecast_gross_profit[index] - admin_dep\n    print(forecast_net_profit)\n    index += 1\nprint(\"The forecast net profit is: {} USD.\".format(forecast_net_profit))\n\n2870\n820\n3100\nThe forecast net profit is: 3100 USD.\n\n\n\n\n\nWorking with variances in the forecast\nIdentifying, quantifying, and investigating the difference between an old forecast and the new forecast is often referred to as Gap Analysis.\n\n\n\ngap_analysis.JPG\n\n\n\nBuilding an alternate forecast\n\n\n\nalternative_forecasts.JPG\n\n\nWe will now build an alternative forecast for Txs Tools. The new quarter forecast is based off actual data for Jul - Aug as well as adjusted forecast data for September. The data (units sold) is as follows:\n\nJul = 700\nAug = 220\nSep = 520\n\nThe dependencies calculations have already been completed from the previous exercise. The following information applies:\n\nbase_cost_price = 7\nbase_sales_price = 15\n\n\n# create a dependencies() function for sales and costs with the arguments base_cost_price,base_sales_price, and sales_usd\n# Pass the arguments into the function in this order.\n\ndef dependencies(base_cost_price, base_sales_price, sales_usd):\n    res = []\n    for sales in sales_usd:\n        if sales >= 350:\n            sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n        else:\n            sales_dep = sales * base_sales_price\n        if sales >= 500:\n            cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n        else:\n            cost_dep = sales * base_cost_price\n        res.append(sales_dep - cost_dep)\n    return res\n\n\n# Create scenario forecast1 for the original forecast\nforecast1 = dependencies(7, 15, [700, 350, 650])\n\n# Create scenario forecast2 for the alternative forecast. \n# Use the data provided above to calculate the alternative forecast\nforecast2 = dependencies(7, 15, [700, 220, 520])\n\n\nprint(\"The original forecast scenario is {}:\".format(forecast1))\nprint(\"The alternative forecast scenario is {}:\".format(forecast2))\n\nThe original forecast scenario is [4850, 2800, 4600]:\nThe alternative forecast scenario is [4850, 1760, 3950]:\n\n\n\n\n\nBuilding a gap analysis between forecasts\nTxs Tools now has two forecasts, the original forecast forecast1 and the adjusted forecast forecast2.\nThe dependencies have already been defined as def dependencies(base_cost_price, base_sales_price, sales_usd), where base_cost_price = 7 and base_sales_price = 15, with forecast2 based off the following adjusted sales unit values:\n\nJul = 700\nAug = 220\nSep = 520\n\nIn this exercise, we will look at how to use a for loop to cycle between two different lists, forecast1 and forecast2 and calculate the difference (“gap”) using an incremented index. It is possible to do this simultaneously as both lists have the same length.\n\n# Set the two results\nforecast1 = dependencies(7, 15, [700, 350, 650])\nforecast2 = dependencies(7, 15, [700, 220, 520])\n\n# Create an index and the gap analysis for the forecast\nindex = 0\nfor value in forecast2:\n    print(\"The gap between forecasts is {}\".format(value - forecast1[index]))\n    index += 1\n\nThe gap between forecasts is 0\nThe gap between forecasts is -1040\nThe gap between forecasts is -650\n\n\nYou can see how easy it is to use a for loop to compare results across different lists.\nNote that the gap between forecasts is driven purely by the difference in sales volume - base sales and cost prices are unchanged.\n\nIn July forecast2 sales are as per forecast 1 - so no gap.\nIn August forecast2 sales are 220 units against 350 - resulting in a gap of 130 units x profit per unit of 8 (15 - 7) which is 1040\nIn September forecast2 sales are 520 units against 650 - resulting in a gap of 130 units x profit per unit of 8 (15 - 7) which is 1040, but we also have a saving of 3 per unit (sales commission 1 and additional production line cost 2) which reduces the gap by 130 x 3 = 390 to 650.\n\n\n\nSetting dependencies for Netflix\nNetflix compiled a forecast up to the 2019 financial year netflix_f_is, and has based the sales figures in 2019 on the following dependency:\n\nNumber of active subscriptions, which are based on the success of Netflix original shows.\n\nFor 2019, the success of original shows (critical and commercial acclaim) are estimated at 78%. The total amount of subscribers per percentage point is 500, and set to the variable n_subscribers_per_pp (i.e there is a calculated correlation between show success and number of subscribers).\nIn this exercise, we will calculate how dependent sales are on the number of subscribers in the forecast, which we will use in the next exercise.\n\n# instantiate subscribers per % point\nn_subscribers_per_pp = 500\n\n# load in Netflix financials\nnetflix_f_is = pd.read_csv('Data/Netflix.csv')\nnetflix_f_is\n\n\n\n\n\n  \n    \n      \n      metric\n      2014_act\n      2015_act\n      2016_act\n      2017_fc\n      2018_fc\n      2019_fc\n    \n  \n  \n    \n      0\n      Sales\n      5505\n      6780\n      8831\n      11688\n      14979\n      17994\n    \n    \n      1\n      EBITDA\n      528\n      493\n      611\n      1088\n      1899\n      2943\n    \n    \n      2\n      Operating profit (EBIT)\n      403\n      306\n      380\n      837\n      1660\n      2702\n    \n    \n      3\n      Net income\n      267\n      123\n      187\n      559\n      1024\n      1721\n    \n  \n\n\n\n\n\n# Create a filter to select the sales row from the netflix_f_is dataset\nsales_metric = ['Sales']\n\n# Filter for rows containing the Sales metric\nfiltered_netflix_f_is = netflix_f_is[netflix_f_is.metric.isin(sales_metric)]\n\n# Extract the 2019 Sales forecast value\nforecast1 = netflix_f_is['2019_fc'].iloc[0]\n\n# Print the resulting forecast\nprint(\"The sales forecast is {}.\".format(forecast1))\n\nThe sales forecast is 17994.\n\n\n\n# Set the success percentage to 78%\npct_success = 0.78\n\n# Calculate the dependency for the subscriber base\nn_subscribers = n_subscribers_per_pp * pct_success\n\n# See the result\nprint(\"The dependency for the subscriber base is {}.\".format(n_subscribers))\n\nThe dependency for the subscriber base is 390.0.\n\n\n\n# Calculate the ratio between forecast sales and subscribers\nsales_subs_ratio = forecast1 / n_subscribers\n\n# See the result\nprint(\"The ratio between subscribers and sales is 1 subscriber equals ${:.2f}.\".format(sales_subs_ratio))\n\nThe ratio between subscribers and sales is 1 subscriber equals $46.14.\n\n\n\n\nCalculating an alternative forecast for Netflix\nThe original assumptions are as follows: the total amount of subscribers at a 78% success rate results in 39,000 subscribers. We used this to build the forecast numbers.\nHowever, the success rate for 2019 has been recalculated to have a probability of 65%, and the management has asked us to make an adjusted forecast based on this value.\nThe ratio between the subscribers and sales is 1 subscriber to 0.46 USD sales, set to variable sales_subs_ratio.\n\n# instantiate sales subs ratio\nsales_subs_ratio = 0.46\n\n\n# Set the proportion of successes to 65%\npct_success2 = 65\n\n# Calculate the number of subscribers\nn_subscribers2 = n_subscribers_per_pp * pct_success2 \n\n# Calculate the new forecast\nforecast2 = n_subscribers2  * sales_subs_ratio\nforecast2\n\n14950.0\n\n\n\n# Insert a column named AltForecast, containing forecast2\nfiltered_netflix_f_is.insert(len(filtered_netflix_f_is.columns), 'AltForecast', forecast2)\n\n# Insert a column named Gap, containing the difference\nfiltered_netflix_f_is.insert(len(filtered_netflix_f_is.columns), 'Gap', forecast1 - forecast2)\n\n# See the result\nfiltered_netflix_f_is\n\n\n\n\n\n  \n    \n      \n      metric\n      2014_act\n      2015_act\n      2016_act\n      2017_fc\n      2018_fc\n      2019_fc\n      AltForecast\n      Gap\n    \n  \n  \n    \n      0\n      Sales\n      5505\n      6780\n      8831\n      11688\n      14979\n      17994\n      14950.0\n      3044.0\n    \n  \n\n\n\n\n\n\nKey takeaways\nWe learned how to harnass Python, and in particular the pandas library to wrangle raw financial data, and extract relevant information to calculate key metrics.\nWe also learned how to handle date inconsistencies using the datetime library, parse dates using the split() method, and how to automate our work by writing functions and using for loops and .iteritems.\nAutomating the financial forecasting process allows fast iterations over different scenarios, saving time and reducing the scope of manual error."
  },
  {
    "objectID": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html",
    "href": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "This is my follow up to the second part of Lesson 7: Practical Deep Learning for Coders 2022 in which Jeremy shows how to build a Collaborative Filtering model from scratch, within Excel, and also using PyTorch, and explains latent factors and emdedding"
  },
  {
    "objectID": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html#recommendation-systems",
    "href": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html#recommendation-systems",
    "title": "Collaborative Filtering",
    "section": "Recommendation Systems",
    "text": "Recommendation Systems\nOne very common problem to solve is when you have a number of users and a number of products, and you want to recommend which products are most likely to be useful for which users. There are many variations of this: for example, recommending movies (such as on Netflix), figuring out what to highlight for a user on a home page, deciding what stories to show in a social media feed, and so forth. There is a general solution to this problem, called collaborative filtering, which works like this: look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend other products that those users have used or liked.\nFor example, on Netflix you may have watched lots of movies that are science fiction, full of action, and were made in the 1970s. Netflix may not know these particular properties of the films you have watched, but it will be able to see that other people that have watched the same movies that you watched also tended to watch other movies that are science fiction, full of action, and were made in the 1970s. In other words, to use this approach we don’t necessarily need to know anything about the movies, except who like to watch them.\nThere is actually a more general class of problems that this approach can solve, not necessarily involving users and products. Indeed, for collaborative filtering we more commonly refer to items, rather than products. Items could be links that people click on, diagnoses that are selected for patients, and so forth.\nThe key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.\n\nThis is chapter 8 of the book Practical Deep Learning for Coders, provided courtesy of O’Reilly Media. The full book is available as Jupyter Notebooks. A free course that covers the book is available here.\n\nFor this chapter we are going to work on this movie recommendation problem. We’ll start by getting some data suitable for a collaborative filtering model.\n\nA First Look at the Data\n\n# load required packages and set seed for reproducibility\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\nset_seed(42)\n\nWe do not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can use, called MovieLens. This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If you’re interested, it would be a great learning project to try and replicate this approach on the full 25-million recommendation dataset, which you can get from their website.\nThe dataset is available through the usual fastai function:\n\n# download data\npath = untar_data(URLs.ML_100k)\n\nAccording to the README, the main table is in the file u.data. It is tab-separated and the columns are, respectively user, movie, rating, and timestamp. Since those names are not encoded, we need to indicate them when reading the file with Pandas. Here is a way to open this table and take a look:\n\n# load in table - specify colums names\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, # tab(t) separated file, instead of a comma(c) separated file\n                      names=['user','movie','rating','timestamp']) # need to specify columns as not encoded\n\n# look at the first 5 rows\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nAlthough this has all the information we need, it is not a particularly helpful way for humans to look at this data. Here is the same data cross-tabulated into a human-friendly table:\n\n\n\nimage.png\n\n\nWe have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\nIf we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and +1, with positive numbers indicating stronger matches and negative numbers weaker ones, and the categories are science-fiction, action, and old movies, then we could represent the movie The Last Skywalker as:\n\n# embed features of the movie The Last Skywalker by creating vector of values between -1 and +1\n# science fiction 0.98, action 0.9, old movies -0.9\nlast_skywalker = np.array([0.98,0.9,-0.9])\n\nHere, for instance, we are scoring very science-fiction as 0.98, very action as 0.9, and very not old as -0.9. We could represent a user who likes modern sci-fi action movies as:\n\n# embed the features of a user based on their movie preferences by creating vector of values between -1 and +1\n# science fiction 0.9, action 0.8, old movies -0.6\nuser1 = np.array([0.9,0.8,-0.6])\n\nand we can now calculate the match between this combination:\n\n# calculate the dot product of the two vectors to see whether LastSkywalker is a good match for user 1\n(user1*last_skywalker).sum()\n\n2.1420000000000003\n\n\nWhen we multiply two vectors together and add up the results, this is known as the dot product. It is used a lot in machine learning, and forms the basis of matrix multiplication. We will be looking a lot more at matrix multiplication and dot products later.\n\njargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result.\n\nOn the other hand, we might represent the movie Casablanca as:\n\n# embed features of the movie Casablanca by creating vector of values between -1 and +1\n# science fiction 0.98, action 0.9, old movies -0.9\ncasablanca = np.array([-0.99,-0.3,0.8])\n\nThe match between this combination is:\n\n# calculate the dot product of the two vectors to see whether Casabalance is a good match for user 1\n(user1*casablanca).sum()\n\n-1.611\n\n\nSince we don’t know what latent factors actually are, and we don’t know how to score them for each user and movie, we should learn them.\n\n\nCollaborative filtering - using Excel\nThe problem is we haven’t been given any information about the users, or the movies, and we might not even know what things about movies actually matter to users. But, not to worry, we can just use Stochastic Gradient Descent (SGD) to find them!\nThere is surprisingly little difference between specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general gradient descent approach.\n\nStep 1: randomly initialize some parameters\n\nThese parameters will be a set of latent factors for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes let’s use 5 for now. Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle.\nSo, the initialized latent factors for movieId 27 are 0.71, 0.81, 0.74, 0.04, 0.04 and the latent factors for userID 14 are 0.19, 0.63, 0.31, 0.44, 0.51. We then multiply these together using the MMULT matrix multiplication function within Excel to obtain our initial predictions.\nWe don’t know what these factors are, but for example we can interpret that userID 14 doesn’t feel very strongly, with a value of 0.19 about movieID factor 1 which has a value of 0.71\nThis is what it looks like in Microsoft Excel:\n\n\n\nlatent_factors.JPG\n\n\n\nStep 2: Calculate our predictions using Matrix Multiplication\n\nAs we’ve discussed, we can do this by simply taking the dot product of each movie with each user. If, for instance, the first latent user factor represents how much the user likes action movies, and the first latent movie factor represents if the movie has a lot of action or not, the product of those will be particularly high if either the user likes action movies and the movie has a lot of action in it or the user doesn't like action movies and the movie doesn't have any action in it. On the other hand, if we have a mismatch (a user loves action movies but the movie isn’t an action film, or the user doesn’t like action movies and it is one), the product will be very low.\n\n\n\ninitialized.JPG\n\n\n\nStep 3: calculate our loss\n\nWe can use any loss function that we wish; let’s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction.\n\n\n\ninitialized.JPG\n\n\n\nStep 4: optimize using Stochastic Gradient Descent(SGD) - the Solver function in Excel approximates this\n\nThat’s all we need. With this in place, we can optimize our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better.\n\n\n\noptimized.JPG\n\n\nThe above spreadsheet screenshot shows the updated predictions after applying Stohastic Gradient Descent using Excel’s inbuilt Solver function - note that the movie rating predictions are now much more in line with the actual ratings (with values betwen 0 and 5) and our loss function RMSE has reduced from 2.8 to 0.42.\n\n\nUsing PyTorch to do the same thing\nTo use the usual Learner.fit function we will need to get our data into a DataLoaders, so let’s focus on that now.\nWhen showing the data, we would rather see movie titles than their IDs. The table u.item contains the correspondence of IDs to titles:\n\n# load in movie titles table\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1', #\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nWe can merge this with our ratings table to get the user ratings by title:\n\n# merge ratings and movie tables\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nWe can now build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the titles instead of the IDs:\n\n# build a Collaborative Filtering DataLoaders from out ratings DataFrame\n# needs a user column and an item column - we have a user column called user so don't need to pass in\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64) # need to pass in item_name to get title\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      542\n      My Left Foot (1989)\n      4\n    \n    \n      1\n      422\n      Event Horizon (1997)\n      3\n    \n    \n      2\n      311\n      African Queen, The (1951)\n      4\n    \n    \n      3\n      595\n      Face/Off (1997)\n      4\n    \n    \n      4\n      617\n      Evil Dead II (1987)\n      1\n    \n    \n      5\n      158\n      Jurassic Park (1993)\n      5\n    \n    \n      6\n      836\n      Chasing Amy (1997)\n      3\n    \n    \n      7\n      474\n      Emma (1996)\n      3\n    \n    \n      8\n      466\n      Jackie Chan's First Strike (1996)\n      3\n    \n    \n      9\n      554\n      Scream (1996)\n      3\n    \n  \n\n\n\nTo represent collaborative filtering in PyTorch we can’t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\nn_users  = len(dls.classes['user']) # set number of users = number of rows of users\nn_movies = len(dls.classes['title']) # set number of movies = nuumber of rows of movies\nn_factors = 5 # set number of columns (latent factors) to whatever we want\n\n# create initial random weightings for user latent factors\n# user EMBEDDING matrix\nuser_factors = torch.randn(n_users, n_factors) # random tensors \n\n\n# create initial random weightings for movie latent factors\n# movie EMBEDDING matrix\nmovie_factors = torch.randn(n_movies, n_factors) # random tensors \n\nNote fast.ai has a built in formula for setting an appropriate number of latent factors\n\nuser_factors\n\ntensor([[-1.0827,  0.2138,  0.9310, -0.2739, -0.4359],\n        [-0.5195,  0.7613, -0.4365,  0.1365,  1.3300],\n        [-1.2804,  0.0705,  0.6489, -1.2110,  1.8266],\n        ...,\n        [ 0.8009, -0.4734, -0.8962, -0.7348, -0.0246],\n        [ 0.3354, -0.8262, -0.1541,  0.4699,  0.4873],\n        [ 2.4054, -0.2156, -1.4126, -0.2467,  1.0571]])\n\n\n\nmovie_factors\n\ntensor([[-0.3978,  0.4563,  1.2301,  0.3745,  0.9689],\n        [-1.1836, -0.5818, -0.5587, -0.4316,  0.2128],\n        [ 0.0420,  1.3201, -0.7999,  1.1123, -0.7585],\n        ...,\n        [ 2.4743,  1.3068,  0.4540,  0.6958,  0.5228],\n        [ 2.3970, -0.2559, -1.7196,  1.0440, -0.2662],\n        [ 0.2786, -0.6593,  0.5260, -0.3416, -1.3938]])\n\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:\nTaking the dot product of a one hot coded vector and something, is the same as looking up the index in an array.\n\n# create a one-hot encoded vector of length n_users, with 2nd element set to 1 and everything else set to 0\none_hot_2 = one_hot(2, n_users).float()\n\n\n# matrix multiplication - users\n# .t transposes cols and rows to enable matrix multiplication\n# @ is the symbol for matrix multipy\nuser_factors.t() @ one_hot_2\n\ntensor([-1.2804,  0.0705,  0.6489, -1.2110,  1.8266])\n\n\nIt gives us the same vector as the one at index 2 in the user_factor matrix as shown previously.\n\n# create a one-hot encoded vector of length n_users, with 1st element set to 1 and everything else set to 0\none_hot_1 = one_hot(1, n_movies).float()\n\n\n# matrix multiplication - movie\n# .t transposes cols and rows to enable matrix multiplication\n# @ is the symbol for matrix multipy\nmovie_factors.t() @ one_hot_1\n\ntensor([-1.1836, -0.5818, -0.5587, -0.4316,  0.2128])\n\n\nIt gives us the same vector as the one at index 1 in the movie_factors matrix as shown previously.\n\n\nEmbedding layer\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one — we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding.\n\njargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.\n\nIn computer vision, we have a very easy way to get all the information of a pixel through its RGB values: each pixel in a colored image is represented by three numbers. Those three numbers give us the redness, the greenness and the blueness, which is enough to get our model to work afterward (with values between 0 and 255).\nFor the problem at hand, we don’t have the same easy way to characterize a user or a movie. There are probably relations with genres: if a given user likes romance, they are likely to give higher scores to romance movies. Other factors might be whether the movie is more action-oriented versus heavy on dialogue, or the presence of a specific actor that a user might particularly like.\nHow do we determine numbers to characterize those? The answer is, we don’t. We will let our model learn them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not. This is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\nAt the beginning, those numbers don’t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data about the relations between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance, and so on.\nWe are now in a position that we can create our whole model from scratch.\n\n\nCreating a Collaborative Filtering model in PyTorch from Scratch\nBefore we can write a model in PyTorch, we first need to learn the basics of object-oriented programming and Python. If you haven’t done any object-oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and getting some practice before moving on.\nThe key idea in object-oriented programming is the class. A model is a class. We have been using classes throughout this book, such as DataLoader, string, and Learner. Python also makes it easy for us to create new classes. Here is an example of a simple class:\n\n# example of a simple class\nclass Example:\n    def __init__(self, a): self.a = a # __init__ any method surrounded in double underscores like this is considered special\n    def say(self,x): return f'Hello {self.a}, {x}.'\n\nThe most important piece of this is the special method called __init__ (pronounced dunder init). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behavior associated with this method name. In the case of __init__, this is the method Python will call when your new object is created. So, this is where you can set up any state that needs to be initialized upon object creation.\nAny parameters included when the user constructs an instance of your class will be passed to the __init__ method as parameters. Note that the first parameter to any method defined inside a class is self, so you can use this to set and get any attributes that you will need:\n\nex = Example('Sylvain') # so self.a now equals Sylvain\nex.say('nice to meet you') # x is now 'nice to meet you - we can access the say function within the Example class using .say\n\n'Hello Sylvain, nice to meet you.'\n\n\nAlso note that creating a new PyTorch module requires inheriting from Module. Inheritance is an important object-oriented concept that we will not discuss in detail here—in short, it means that we can add additional behavior to an existing class. PyTorch already provides a Module class, which provides some basic foundations that we want to build on. So, we add the name of this superclass after the name of the class that we are defining, as shown in the following example.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\n\n# create a class to define our dot product module\nclass DotProduct(Module): # putting something in parentheses after a class name creates a SUPERclass\n    def __init__(self, n_users, n_movies, n_factors): # specify number of users, movies, and latent factors\n        self.user_factors = Embedding(n_users, n_factors) # create Embedding matrix for users - we will cover how to create Embedding Class later\n        self.movie_factors = Embedding(n_movies, n_factors) # create Embedding matrix for movies - we will cover how to create Embedding Class later\n    \n    # calculation of our model has to be defined in a function called forward\n    def forward(self, x):  # pass the object itself and thing calculating on - user and movie for a batch\n                           # each row will be one user and movie combination, columns will be users and movies\n        users = self.user_factors(x[:,0]) # grab first column i.e every row, and look it up using our user Embedding matrix\n        movies = self.movie_factors(x[:,1]) # grab second column i.e every row, and look it up using our movie Embedding matrix\n        return (users * movies).sum(dim=1) # calculate the dot product - # dim = 1 because we are summing across COLUMNS for each row # dim = 0 would sum across ROWS\n\nIf you haven’t seen object-oriented programming before, then don’t worry, you won’t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax.\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\n# inputs to the model are 64 rows x 2 columns - column 0 user IDs and column 1 movie IDs\nx,y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\nNow that we have defined our architecture, and created our parameter matrices, we need to create a Learner to optimize our model. In the past we have used special functions, such as cnn_learner, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain Learner class:\n\n# define our Dot Product model\nmodel = DotProduct(n_users, n_movies, 50)\n\n# we can pass our Dot Product class to our learner\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nWe are now ready to fit our model:\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3) # 5 epochs, learning rate 5e^-3\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.385412\n      1.293633\n      00:04\n    \n    \n      1\n      1.061318\n      1.070560\n      00:04\n    \n    \n      2\n      0.968811\n      0.976037\n      00:04\n    \n    \n      3\n      0.862989\n      0.883624\n      00:04\n    \n    \n      4\n      0.797610\n      0.869864\n      00:04\n    \n  \n\n\n\n\n\nSqueezing our predictions using Sigmoid\nThe first thing we can do to make this model a little bit better is to force those predictions to be between 0 and 5. For this, we just need to use sigmoid_range. Sigmoid on its own squeezes values between 0 and 1 but if we multiply by 5 that wil ensure the values are between 0 and 5. One thing we discovered empirically is that it’s better to have the range go a little bit over 5, so we use (0, 5.5):\n\n# tweak our Dot Product Class to squeeze preds between 0 and 5\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): # set range for predictions between 0 and 5 (with a little bit extra for comfort) \n        self.user_factors = Embedding(n_users, n_factors) # create Embedding matrix for users - we will cover how to create Embedding Class later\n        self.movie_factors = Embedding(n_movies, n_factors) # create Embedding matrix for movies - we will cover how to create Embedding Class later\n        self.y_range = y_range # range of predictions specified\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0]) # grab first column i.e every row, and look it up using our user Embedding matrix\n        movies = self.movie_factors(x[:,1]) # grab second column i.e every row, and look it up using our movie Embedding matrix\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range) # force predictions to be between 0 and 5 using sigmoid function\n\n\n# redefine our Dot Product model\nmodel = DotProduct(n_users, n_movies, 50)\n\n# pass in our Dot Product class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3) # 5 epochs, learning rate 5e^-3\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.991383\n      0.971459\n      00:04\n    \n    \n      1\n      0.862119\n      0.888047\n      00:04\n    \n    \n      2\n      0.677498\n      0.857523\n      00:04\n    \n    \n      3\n      0.464585\n      0.863056\n      00:04\n    \n    \n      4\n      0.384263\n      0.867252\n      00:05\n    \n  \n\n\n\nThis is negligibly better, but we cann improve on this.\n\n\nIntroducing Bias into our model\nOne obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it.\nThat’s because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. Let’s first look at this in Excel - we simply initialize an additional randomized bias factor to add to our existing latent factors and then optimize as before. This results in an improvement - our RMSE drops from 0.42 to 0.35 - see spreadsheet screenshot below:\n\n\n\nbias.JPG\n\n\nLet’s jump back to Python and adjust our model architecture there to introduce bias into our model:\n\n# create new Class to include bias \nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): # set range for predictions between 0 and 5 (with a little bit extra for comfort as sigmoid won't return as high as 1)\n        self.user_factors = Embedding(n_users, n_factors) # create Embedding matrix for users - we will cover how to create Embedding Class later\n        self.user_bias = Embedding(n_users, 1) # account for user BIAS (other factors outside of our latent factors)\n        self.movie_factors = Embedding(n_movies, n_factors) # create Embedding matrix for movies - we will cover how to create Embedding Class later\n        self.movie_bias = Embedding(n_movies, 1) # account for movie BIAS (other factors outside of our latent factors)\n        self.y_range = y_range # range of predictions specified\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0]) # grab first column i.e every row, and look it up using our user Embedding matrix\n        movies = self.movie_factors(x[:,1]) # grab second column i.e every row, and look it up using our movie Embedding matrix\n        res = (users * movies).sum(dim=1, keepdim=True) # calculate the dot product - # dim = 1 because we are summing across COLUMNS for each row # dim = 0 would sum across ROWS\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) # update dor product results for BIAS\n        return sigmoid_range(res, *self.y_range) # force predictions to be between 0 and 5 using sigmoid function\n\nLet’s try training this and see how it goes:\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50)\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3) # 5 epochs, learning rate 5e^-3\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.951611\n      0.925811\n      00:05\n    \n    \n      1\n      0.819404\n      0.855196\n      00:05\n    \n    \n      2\n      0.616164\n      0.856704\n      00:05\n    \n    \n      3\n      0.403988\n      0.885035\n      00:05\n    \n    \n      4\n      0.294023\n      0.891860\n      00:05\n    \n  \n\n\n\nUnlike in Excel, instead of being better, in PyTorch our validation loss has actually gone up (at least by the end of training)! Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As we’ve seen, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularization technique. One way to help avoid overfitting is an approach called weight decay.\n\n\nWeight Decay (L2 regularization)\nWeight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is:\n\n# example illustrating imapct of using weight decay\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\nSo, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters):\nloss_with_wd = loss + wd * (parameters**2).sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing:\nparameters.grad += wd * 2 * parameters\nIn practice, since wd is a parameter that we choose, we can just make it twice as big, so we don’t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle:\nThe whole reason for calculating the loss is to then calculate the gradient of the loss, by taking the derivative. The derivative of parameters^2 is 2*parameters.\n\nWeight decay value 0.1\nA higher weight decay value forces the weights lower, reducing the capacity of our model to make good prediction, but reducing the risk of overfitting.\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50)\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3, wd=0.1) # 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.976209\n      0.929432\n      00:05\n    \n    \n      1\n      0.867723\n      0.859258\n      00:05\n    \n    \n      2\n      0.751625\n      0.823332\n      00:04\n    \n    \n      3\n      0.580325\n      0.811122\n      00:05\n    \n    \n      4\n      0.485529\n      0.811769\n      00:05\n    \n  \n\n\n\nThat’s much better! The key to regularization is to find the right balance of the magnitude of the weights of the coefficients - low enough so we don’t overfit, but high enough so that we can make useful predictions. We can’t reduce them too much (then we end up with underfitting) - but if the weights are increased too much then our model will start to overfit. If there are latent factors in our model that don’t have any influence on overall prediciton, it will just set the co-efficient for that latent factor to zero.\n\n\nWeight decay value 0.01\nA lower weight decay value keeps the weights higher, increasing the capacity of our model to make good predictions, but increasing the risk of overfitting.\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50) # set number of latent factors = 50\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3, wd=0.01) # 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.937280\n      0.919222\n      00:05\n    \n    \n      1\n      0.836111\n      0.858221\n      00:05\n    \n    \n      2\n      0.594563\n      0.858991\n      00:05\n    \n    \n      3\n      0.416554\n      0.887284\n      00:05\n    \n    \n      4\n      0.282974\n      0.894385\n      00:05\n    \n  \n\n\n\nAs we can see we start off with an improvement and then from epoch 2 performance gets worse, suggesting overfitting.\n\n\nWeight decay value 0.001\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50)\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3, wd=0.001) # 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.922303\n      0.922695\n      00:05\n    \n    \n      1\n      0.856747\n      0.854244\n      00:05\n    \n    \n      2\n      0.600128\n      0.864396\n      00:05\n    \n    \n      3\n      0.404001\n      0.894145\n      00:05\n    \n    \n      4\n      0.283558\n      0.902557\n      00:04\n    \n  \n\n\n\nAgain, we start off with an improvement but then from epoch 2 performance gets worse, suggesting overfitting So, our original weight decay factor of 0.1 looks pretty optimal.\n\n\n\nCreating Our Own Embedding Module\nIf the following section proves to be difficult to follow then it would be a useful exercise to revisit the Linear model and neural net from scratch NoteBook.\nIn that Notebook we created functions to set initital weights, added layers, including bias, and created a further function to update the gradients i.e. perform gradient descent by calculating the layer gradients usind layer.grad * learning_rate. When using PyTorch a lot of this functionality is taken care of - PyTorch looks inside our Module and keeps track of anything that looks like a neural network parameter.\nSo far, we’ve used Embedding without thinking about how it really works. Let’s re-create DotProductBias without using this class. We’ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall that optimizers require that they can get all the parameters of a module from the module’s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\n# create a simple module which only includes a tensor\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3) # add a tensor as an attribute to our Module\n\nL(T().parameters()) # T() instantiates our Module, capital L in Fastcore returns a list of items\n\n(#0) []\n\n\nNote that the tensor is not included in parameters. To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn’t actually add any functionality (other than automatically calling requires_grad_ for us). It’s only used as a “marker” to show what to include in parameters:\n\n# create a simple module which only includes a tensor\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3)) # for PyTorch to recognise the parameters, we need to include the nn.Parameter wrapper\n\nL(T().parameters()) # T() instantiates our Module, capital L in Fastcore returns a list of the parameters\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nNow that we have included the tensor in an nn.Parameter wrapper, PyTorch can read the parameters and we can return these using Fastcore’s L.\nAll PyTorch modules use nn.Parameter for any trainable parameters, which is why we haven’t needed to explicitly use this wrapper up until now:\n\n# create a simple module which only includes a tensor\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False) # we can create our tensor as before but use nn.Linear which flags that parameters are included \n                                                             # no bias term, nn.Linear returns randomly initialized tensor values, size as defined, 1 x 3 \n\nt = T()\nL(t.parameters())  # T() instantiates our Module, capital L in Fastcore returns a list of the parameters\n\n(#1) [Parameter containing:\ntensor([[ 0.7645],\n        [ 0.8300],\n        [-0.2343]], requires_grad=True)]\n\n\nNow that we have included the tensor in an nn.Linear wrapper, PyTorch can read the parameters and we can return these using Fastcore’s L.\n\n# find out what the attribute a is\nt.a\n\nLinear(in_features=1, out_features=3, bias=False)\n\n\n\n# find out what type the attribute a is \ntype(t.a)\n\ntorch.nn.modules.linear.Linear\n\n\n\n# find out what type the attribute a is \nt.a.weight\n\nParameter containing:\ntensor([[ 0.7645],\n        [ 0.8300],\n        [-0.2343]], requires_grad=True)\n\n\nWe can create a tensor as a parameter, with random initialization, like so:\n\n# create params function - poss in size (in case below n_users x n_factors)\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) # creates a tensor of zeros of requested size, then Gaussian distribution with mean=0 and Std Dev = 0.01\n# normal_ modifies eplaces inline with the values specified in brackets \n\nLet’s use this to create DotProductBias again, but without Embedding i.e let’s create PyTorch’s Embedding Matrix from scratch:\n\n# create PyTorch's embedding matrix from scratch\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors]) # create our user Embedding matrix of normally randomized values of size n_users x n_factors\n        self.user_bias = create_params([n_users]) # build user bias into our model - vector of size n_users\n        self.movie_factors = create_params([n_movies, n_factors]) # create our movie Embedding matrix of normally randomized values of size n_users x n_factors\n        self.movie_bias = create_params([n_movies]) # build movie bias into our model - vector of size n_movies\n        self.y_range = y_range # range of predictions as set above, between 0 and 5.5\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]] # user latent factors - note we can index into it\n        movies = self.movie_factors[x[:,1]] # movie latent factors - note we can index into it\n        res = (users*movies).sum(dim=1) # matrix multiplication\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] # add bias - note we ca \n        return sigmoid_range(res, *self.y_range) # force predictions to be between 0 and 5 using sigmoid function\n\nThen let’s train it again to check we get around the same results we saw in the previous section:\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50) # latent factors set to 50\n\n# # pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# train for 5 epochs, lr = 5e^-3, weight decay factor = 0.1\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.960358\n      0.956795\n      00:04\n    \n    \n      1\n      0.869042\n      0.874685\n      00:05\n    \n    \n      2\n      0.737840\n      0.839419\n      00:05\n    \n    \n      3\n      0.589841\n      0.823726\n      00:05\n    \n    \n      4\n      0.472334\n      0.824282\n      00:05\n    \n  \n\n\n\nNow, let’s take a look at what our model has learned.\n\n# what's inside movie bias?\nprint(model.movie_bias,len(model.movie_bias))\n\nParameter containing:\ntensor([-0.0010, -0.1098, -0.0022,  ..., -0.0443,  0.0685,  0.0255],\n       requires_grad=True) 1665\n\n\nMovie bias parameters that have been trained - 1,665 being the number of movies we have.\n\n# what is the shape of our movie bias vector?\nmodel.movie_bias.shape\n\ntorch.Size([1665])\n\n\n\n# what's inside movie factors?\nprint(model.movie_factors,len(model.movie_factors))\n\nParameter containing:\ntensor([[-0.0039, -0.0022,  0.0021,  ...,  0.0041, -0.0011,  0.0016],\n        [-0.1175, -0.1778, -0.0984,  ...,  0.0191,  0.0929,  0.0216],\n        [ 0.0109,  0.0653,  0.0031,  ..., -0.0156,  0.0204,  0.0313],\n        ...,\n        [-0.1234, -0.0363, -0.0474,  ..., -0.0825, -0.0893, -0.1314],\n        [ 0.0995,  0.1521,  0.0754,  ...,  0.0901,  0.1230,  0.1518],\n        [ 0.0164, -0.0041,  0.0183,  ..., -0.0054,  0.0122, -0.0150]],\n       requires_grad=True) 1665\n\n\n\n# what is the shape of our movie factors Embedding matrix?\nmodel.movie_factors.shape\n\ntorch.Size([1665, 50])\n\n\n1,665 movies, and 50 latent factors.\n\n# what's inside user factors?\nprint(model.user_factors,len(model.user_factors))\n\nParameter containing:\ntensor([[ 1.2866e-03,  7.8120e-04, -7.0611e-04,  ...,  8.2220e-06,\n         -3.2568e-03,  2.7836e-03],\n        [ 1.6745e-01,  9.3676e-02, -5.2638e-03,  ..., -2.9528e-02,\n         -1.1926e-01,  3.1058e-01],\n        [ 4.6036e-02, -4.4877e-03,  1.5233e-01,  ...,  9.4287e-02,\n          1.1350e-01,  1.4557e-01],\n        ...,\n        [ 6.7316e-02,  1.0262e-01,  2.9921e-01,  ...,  1.2235e-01,\n          4.4754e-02,  2.5394e-01],\n        [-8.0669e-03,  1.0943e-01,  2.0522e-01,  ...,  1.6869e-02,\n          1.7104e-01,  1.5911e-01],\n        [ 7.9618e-02,  2.9292e-01,  2.3172e-01,  ...,  1.1354e-01,\n          1.2088e-01,  9.0374e-02]], requires_grad=True) 944\n\n\nA bunch of user parameters (weights) that have been trained - 944 being the number of users we have.\n\n# what is the shape of our user factors Embedding matrix?\nmodel.user_factors.shape\n\ntorch.Size([944, 50])\n\n\n944 users, and 50 latent factors.\n\n\nInterpreting Embeddings and Biases\nOur model is already useful, in that it can provide us with movie recommendations for our users — but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector:\n\n# get movie_bias values\nmovie_bias = learn.model.movie_bias.squeeze() # \n\n# find out which movie id's have the lowest bias parameters\nidxs = movie_bias.argsort()[:5] # argsort sorts in ascending order by default - let's grab first 5 \n\n# look inside our DataLoaders to grab the names of those movies from the indexes\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Robocop 3 (1993)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Amityville 3-D (1983)',\n 'Mortal Kombat: Annihilation (1997)']\n\n\nThink about what this means. What it’s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don’t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias:\n\n# sort indexes by descending will give us movies with highest bias values\n# i.e movies that are popular even amongst users who don't normally like that kind of movie\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n\n\nSo, for instance, even if you don’t normally enjoy detective movies, you might enjoy LA Confidential!\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). If you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. Here’s what our movies look like based on two of the strongest PCA components:\n\ng = ratings.groupby('title')['rating'].count()\ng\n\ntitle\n'Til There Was You (1997)                  9\n1-900 (1994)                               5\n101 Dalmatians (1996)                    109\n12 Angry Men (1957)                      125\n187 (1997)                                41\n                                        ... \nYoung Guns II (1990)                      44\nYoung Poisoner's Handbook, The (1995)     41\nZeus and Roxanne (1997)                    6\nunknown                                    9\nÁ köldum klaka (Cold Fever) (1994)         1\nName: rating, Length: 1664, dtype: int64\n\n\n\n# group movies by title and rating \ng = ratings.groupby('title')['rating'].count()\n\n# sort top movies by rating - top 1000\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\n\n# get the indexes of the sorted top movies using: object to index (o2i)\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\n\n# \nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\n\n# Compress our 50 latent factors into just 3 most important factors\nmovie_pca = movie_w.pca(3)\n\n# draw a chart of these features - Visualized Embeddings\nfac0,fac1,fac2 = movie_pca.t() # .t transposes the array\nidxs = list(range(50)) # restrict number of movies plotted to 50\n\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\nWe can see here that the model seems to have discovered a concept of classic versus pop culture movies, or perhaps it is critically acclaimed that is represented here.\n\nj: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!\n\nWe defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. We’ll look at how to do that next.\n\n\nUsing fastai.collab\nWe can create and train a collaborative filtering model using the exact structure shown earlier by using fastai’s collab_learner. Let’s have a peek under the hood and see what is going on inside:\n\n# let's take a look at what's going on under the hood\ncollab_learner??\n\n\nSignature:\ncollab_learner(\n    dls,\n    n_factors=50,\n    use_nn=False,\n    emb_szs=None,\n    layers=None,\n    config=None,\n    y_range=None,\n    loss_func=None,\n    *,\n    opt_func=<function Adam at 0x7f6f614f4700>,\n    lr=0.001,\n    splitter: 'callable' = <function trainable_params at 0x7f6f63949870>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    default_cbs: 'bool' = True,\n)\nSource:   \n@delegates(Learner.__init__)\ndef collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, **kwargs):\n    \"Create a Learner for collaborative filtering on `dls`.\"\n    emb_szs = get_emb_sz(dls, ifnone(emb_szs, {}))\n    if loss_func is None: loss_func = MSELossFlat()\n    if config is None: config = tabular_config()\n    if y_range is not None: config['y_range'] = y_range\n    if layers is None: layers = [n_factors]\n    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, **config)\n    else:      model = EmbeddingDotBias.from_classes(n_factors, dls.classes, y_range=y_range)\n    return Learner(dls, model, loss_func=loss_func, **kwargs)\nFile:      ~/mambaforge/lib/python3.10/site-packages/fastai/collab.py\nType:      function\n\n\n\n\n\n# let's take a look at what's going on under the hood\nEmbeddingDotBias??\n\n\nInit signature: EmbeddingDotBias(n_factors, n_users, n_items, y_range=None)\nSource:        \nclass EmbeddingDotBias(Module):\n    \"Base dot model for collaborative filtering.\"\n    def __init__(self, n_factors, n_users, n_items, y_range=None):\n        self.y_range = y_range\n        (self.u_weight, self.i_weight, self.u_bias, self.i_bias) = [Embedding(*o) for o in [\n            (n_users, n_factors), (n_items, n_factors), (n_users,1), (n_items,1)\n        ]]\n    def forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\n    @classmethod\n    def from_classes(cls, n_factors, classes, user=None, item=None, y_range=None):\n        \"Build a model with `n_factors` by inferring `n_users` and  `n_items` from `classes`\"\n        if user is None: user = list(classes.keys())[0]\n        if item is None: item = list(classes.keys())[1]\n        res = cls(n_factors, len(classes[user]), len(classes[item]), y_range=y_range)\n        res.classes,res.user,res.item = classes,user,item\n        return res\n    def _get_idx(self, arr, is_item=True):\n        \"Fetch item or user (based on `is_item`) for all in `arr`\"\n        assert hasattr(self, 'classes'), \"Build your model with `EmbeddingDotBias.from_classes` to use this functionality.\"\n        classes = self.classes[self.item] if is_item else self.classes[self.user]\n        c2i = {v:k for k,v in enumerate(classes)}\n        try: return tensor([c2i[o] for o in arr])\n        except KeyError as e:\n            message = f\"You're trying to access {'an item' if is_item else 'a user'} that isn't in the training data. If it was in your original data, it may have been split such that it's only in the validation set now.\"\n            raise modify_exception(e, message, replace=True)\n    def bias(self, arr, is_item=True):\n        \"Bias for item or user (based on `is_item`) for all in `arr`\"\n        idx = self._get_idx(arr, is_item)\n        layer = (self.i_bias if is_item else self.u_bias).eval().cpu()\n        return to_detach(layer(idx).squeeze(),gather=False)\n    def weight(self, arr, is_item=True):\n        \"Weight for item or user (based on `is_item`) for all in `arr`\"\n        idx = self._get_idx(arr, is_item)\n        layer = (self.i_weight if is_item else self.u_weight).eval().cpu()\n        return to_detach(layer(idx),gather=False)\nFile:           ~/mambaforge/lib/python3.10/site-packages/fastai/collab.py\nType:           PrePostInitMeta\nSubclasses:     \n\n\n\n\nOK, let’s now reproduce what we did from scratch earlier using the fast.ai functionality with just a few lines of code:\n\n# create a collaborative filtering model using fastai\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) # latebt factors =50, predictions between 0 and 5.5\n\n\n# train for 5 epochs, learning rate = 5e^-3, weight decay = 0.1\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.940161\n      0.954125\n      00:05\n    \n    \n      1\n      0.845409\n      0.871870\n      00:04\n    \n    \n      2\n      0.732785\n      0.837964\n      00:05\n    \n    \n      3\n      0.581802\n      0.822925\n      00:05\n    \n    \n      4\n      0.483456\n      0.823324\n      00:04\n    \n  \n\n\n\nThe names of the layers can be seen by printing the model:\n\n# let's look at the layers of our model\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nNote the slight difference in terminology. u = users, and i=items. So, we have the user Embedding layer (u_weight), and the movie Embedding layer (i_weight) and our bias layers.\nWe can use these to replicate any of the analyses we did in the previous section — for instance:\n\n# we can look at the movie bias and grab the weights\nmovie_bias = learn.model.i_bias.weight.squeeze()\n\n# get indexes of top 5 movies by bias factor\nidxs = movie_bias.argsort(descending=True)[:5]\n\n# get title of top 5 movies by bias factor\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n \"Schindler's List (1993)\",\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Silence of the Lambs, The (1991)']\n\n\nWe get much the same results as before, that is LA Confidential is watched even by those that don’t normally watch that kind of movie.\nAnother interesting thing we can do with these learned embeddings is to look atdistance.\n\n\nEmbedding Distance\nOn a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: \\(\\sqrt{x^{2}+y^{2}}\\) (assuming that x and y are the distances between the coordinates on each axis). For a 50-dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.\nIf there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two movies’ embedding vectors can define that similarity. We can use this to find the most similar movie to Silence of the Lambs:\n\nmovie_factors = learn.model.i_weight.weight\n\n# convert Silence of the Lambs into its class ID using 'object to index' (o2i)\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\n\n# calculate the 'distance' betweeen the Silence of the Lambs and other movie vectors\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) # Cosine Similarity normalizes the angle between the vectors\n\n# sort distances from closes\nidx = distances.argsort(descending=True)[1]\n\n# attach movie titles to the movie indexes\ndls.classes['title'][idx]\n\n\"One Flew Over the Cuckoo's Nest (1975)\"\n\n\nNow that we have succesfully trained a model, let’s see how to deal with the situation where we have no data for a user. How can we make recommendations to new users?\n\n\nBootstrapping a Collaborative Filtering Model\nThe biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\nBut even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of use your common sense. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste.\nBetter still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\nOne thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don’t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of best ever movies lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\nSuch a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\nIn a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It’s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.\nOur dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization (PMF). Another approach, which generally works similarly well given the same data, is deep learning.\n\n\nDeep Learning for Collaborative Filtering - from scratch\nTo turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way. Since we’ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:\n\n# use fast.ai recommended embedding sizes\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nSo the suggested number of latent factors for our 944 users is 74, and the suggested number of latent factors for our 1,665 movies is 102.\nLet’s implement this class:\n\n# build a Collaborative Filtering neural net from scratch\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act), # \n            nn.ReLU(), # Rectified Linear Unit\n            nn.Linear(n_act, 1)) # Linear layer at the end to create a single output\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1)) # concatenate user and item embeddings together\n        return sigmoid_range(x, *self.y_range)\n\nAnd use it to create a model:\n\n# instantiate our model \nmodel = CollabNN(*embs)\n\nCollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. self.layers is identical to the mini-neural net we created in the chapter for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply sigmoid_range as we have in previous models.\nLet’s see if it trains:\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# train for 5 epochs, learning rate 5e^-3, weight decay = 0.01\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.960975\n      0.944082\n      00:06\n    \n    \n      1\n      0.899966\n      0.908818\n      00:06\n    \n    \n      2\n      0.877111\n      0.890931\n      00:06\n    \n    \n      3\n      0.791085\n      0.869468\n      00:06\n    \n    \n      4\n      0.771323\n      0.869940\n      00:06\n    \n  \n\n\n\nfastai provides this model in fastai.collab if you pass use_nn=True in your call to collab_learner (including calling get_emb_sz for you), and it lets you easily create more layers. For instance, here we’re creating two hidden layers, of size 100 and 50, respectively:\n\n\nDeep Learning for Collaborative Filtering - using fast.ai\n\n# create our Collab Filtering learner, define neural net layesr\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) # use_nn = True allows us to create a neural network, with 2 hidden layers\n\n# train for 5 epochs, learning rate 5e^-3, weight decay = 0.01\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.957651\n      0.987930\n      00:07\n    \n    \n      1\n      0.894093\n      0.919895\n      00:07\n    \n    \n      2\n      0.907125\n      0.892506\n      00:06\n    \n    \n      3\n      0.863961\n      0.864401\n      00:06\n    \n    \n      4\n      0.766643\n      0.866521\n      00:06\n    \n  \n\n\n\nDeep learning models really come into play when we have a lot of metadata e.g. information about our users, where are they from, when did they sign up, what sex are they etc and for our movies e.g. when was it released, what genre is it etc. In our scenario here where we don’t have this information to hand, the deep learning model scores a bit worse than our dot product model, which is taking advantage of our understanding of the problem domain. In practice we often create a model which has a dot product component and a neural net component.\nlearn.model is an object of type EmbeddingNN. Let’s take a look at fastai’s code for this class:\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) # n_cont=0 means number of continuous variables is zero\n\nWow, that’s not a lot of code! This class inherits from TabularModel, which is where it gets all its functionality from. In __init__ it calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received.\n\nTabularModel??\n\n\nInit signature:\nTabularModel(\n    emb_szs: 'list',\n    n_cont: 'int',\n    out_sz: 'int',\n    layers: 'list',\n    ps: 'float | list' = None,\n    embed_p: 'float' = 0.0,\n    y_range=None,\n    use_bn: 'bool' = True,\n    bn_final: 'bool' = False,\n    bn_cont: 'bool' = True,\n    act_cls=ReLU(inplace=True),\n    lin_first: 'bool' = True,\n)\nSource:        \nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, \n        emb_szs:list, # Sequence of (num_embeddings, embedding_dim) for each categorical variable\n        n_cont:int, # Number of continuous variables\n        out_sz:int, # Number of outputs for final `LinBnDrop` layer\n        layers:list, # Sequence of ints used to specify the input and output size of each `LinBnDrop` layer\n        ps:float|list=None, # Sequence of dropout probabilities for `LinBnDrop`\n        embed_p:float=0., # Dropout probability for `Embedding` layer\n        y_range=None, # Low and high for `SigmoidRange` activation \n        use_bn:bool=True, # Use `BatchNorm1d` in `LinBnDrop` layers\n        bn_final:bool=False, # Use `BatchNorm1d` on final layer\n        bn_cont:bool=True, # Use `BatchNorm1d` on continuous variables\n        act_cls=nn.ReLU(inplace=True), # Activation type for `LinBnDrop` layers\n        lin_first:bool=True # Linear layer is first or last in `LinBnDrop` layers\n    ):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\nFile:           ~/mambaforge/lib/python3.10/site-packages/fastai/tabular/model.py\nType:           PrePostInitMeta\nSubclasses:     EmbeddingNN, EmbeddingNN\n\n\n\n\n\n\nkwargs and Delegates\n\nEmbeddingNN includes **kwargs as a parameter to __init__. In Python **kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs. And **kwargs in an argument list means”insert all key/value pairs in the kwargs dict as named arguments here”. This approach is used in many popular libraries, such as matplotlib, in which the main plot function simply has the signature plot(*args, **kwargs). The plot documentation says “The kwargs are Line2D properties” and then lists those properties.\n\n\nWe’re using **kwargs in EmbeddingNN to avoid having to write all the arguments to TabularModel a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesn’t know what parameters are available. Consequently things like tab completion of parameter names and pop-up lists of signatures won’t work.\n\n\nfastai resolves this by providing a special @delegates decorator, which automatically changes the signature of the class or function (EmbeddingNN in this case) to insert all of its keyword arguments into the signature.\n\nAlthough the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That’s exactly what TabularModel does. In fact, we’ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we’d better spend some time learning about TabularModel, and how to use it to get great results! We’ll do that in the next chapter.\n\n\nNatural Language Processing (NLP)\nIt’s possible you may have heard about Embeddings before in the context of Natural Language Processing (NLP). We can turn words into integers using an embedding matrix. Let’s use the poem [Green Eggs and Ham] to illustrate:\n\n\n\nembedding_NLP.JPG\n\n\nFrom the spreadhseet screenshot above, we can see that each word that appears in the poem is given an index which can be arbitrary (in this case alphabetical) and then given 4 randomly initialized latent factors, and a bias factor. This allows the conversion from text to integers in the form of an Embedding matrix, which allows our neural net to interpret the text.\n\n\nKey takeaways\nThis blog has explored Collaborative Filtering and we have seen how to:\n\nbuild a Collaborative Filtering model from scratch\ncreate Embedding matrices from scratch\nreplicate the from-scratch model using PyTorch\nreplicate the from-scratch model using Fast.ai\n\nWe have also learned how to build a Collaborative Filtering Model using deep learning again, doing this from scratch, using PyTorch’s functionality, and also using the Fast.ai methodology. We saw how gradient descent can learn intrinsic factors or biases about items from a history of ratings, which can then give us information about the data, which can be used to provide e.g. tailored movie recommendations."
  },
  {
    "objectID": "posts/Kaggle/Kaggle.html",
    "href": "posts/Kaggle/Kaggle.html",
    "title": "Kaggle",
    "section": "",
    "text": "In order to download datasets from Kaggle when working outwith the Kaggle environment you will need to make use of a Kaggle API. You can get this by clicking on Account below your profile name, and then selecting Create New API Token\n\n\n\nKaggle_1.JPG\n\n\n\n\n\nKaggle_2.JPG\n\n\nInitially the file will be saved in your local environment - in a directory named .kaggle. We need to move this file into a .kaggle directory within your PaperSpace environment. I found this easiest to do by:\n\nOpen in JupyterLab\n\n\n\n\nJupyterLab.JPG\n\n\n\nLaunch Classic NoteBook\n\n\n\n\nClassic_NoteBook.JPG\n\n\n\nCreate a storage directory to store Kaggle API within PaperSpace - and add the Kaggle API json file from your local environment\n\n\n\n\nStorage.JPG\n\n\n\nMove the json file into a .kaggle directory within Paperspace. You can enter the following commands within the Terminal:\n\nmv kaggle.json ~/.kaggle\nThe .kaggle directory should already exist but if not you can create one by typing following command within the Terminal:\nmkdir ~/.kaggle\n\n\n\nTerminal.JPG\n\n\n\nTo prevent other users from using our API token, we can type the following command in the Terminal:\n\nchmod 600 /root/.kaggle/kaggle.json\nOK. That’s us now all set up and ready to access Kaggle datsets. If you are looking for a specific dataset, you can now type the following command within the Terminal:\nkaggle datasets list\n\n\n\ndatasets.JPG\n\n\nAs you can see, there’s plenty to keep us occupied!! If you wanted to download data from this API, just enter the command in the Terminal, following the format below :\n\nkaggle datasets download [ref]\n\nFor example if we wanted the (TOP 50)List if most expensive films dataset we would type:\nkaggle datasets download devrimtuner/top-50list-of-most-expensive-films"
  },
  {
    "objectID": "posts/Multi_Label_Classification/Multi_label_classification.html",
    "href": "posts/Multi_Label_Classification/Multi_label_classification.html",
    "title": "Multi-label classification",
    "section": "",
    "text": "Perhaps… but in some cases the opposite to be true, especially when training for quite a few epochs. By giving the model more signal about what is present in a picture, it may be able to use this information to find more interesting features that predict our target of interest. For instance, perhaps some of the features of disease change between varieties.\n\nSet up\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\nFirst we’ll repeat the steps we used last time to access the data and ensure all the latest libraries are installed:\n::: {.cell _kg_hide-output=‘true’ tags=‘[]’}\n!pip install fastai\n\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n\nfrom fastai.vision.all import *\nset_seed(42)\n\nfrom fastcore.parallel import *\ntrn_path = path/'train_images'\n:::\nHere’s the CSV that Kaggle provides, showing the variety of rice contained in each image – we’ll make image_id the index of our data frame so that we can look up images directly to grab their variety:\n\n# load in our training dataset - set index as image_id column\ndf = pd.read_csv(path/'train.csv', index_col='image_id')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      label\n      variety\n      age\n    \n    \n      image_id\n      \n      \n      \n    \n  \n  \n    \n      100330.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100365.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100382.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100632.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      101918.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n  \n\n\n\n\nPandas uses the loc attribute to look up rows by index. Here’s how we can get the variety of image 100330.jpg, for instance:\n\ndf.loc['100330.jpg', 'variety']\n\n'ADT45'\n\n\nOur DataBlock will be using get_image_files to get the list of training images, which returns Path objects. Therefore, to look up an item to get its variety, we’ll need to pass its name. Here’s a function which does just that:\n\n# create a function that looks up an item and gets its variety\ndef get_variety(p): return df.loc[p.name, 'variety']\n\nWe’re now ready to create our DataLoaders. To do this, we’ll use the DataBlock API, which is a flexible and convenient way to plug pieces of a data processing pipeline together:\n\n# Create our DataLoaders\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock), # these are inputs and outputs - specify which on line below\n    n_inp=1, # specify number of inputs included above - so first argument above is ImageBlock which is our inputs, and the 2 outputs are CategoryBlocks - disease and variety\n    get_items=get_image_files, # grab input images\n    get_y = [parent_label,get_variety], # grab labels - parent_label is disease, get_variety from function we defined earlier\n    splitter=RandomSplitter(0.2, seed=42), # split training set 80% validation 20%\n    item_tfms=Resize(192, method='squish'), # image augmentation\n    batch_tfms=aug_transforms(size=128, min_scale=0.75) # batch augmentation\n).dataloaders(trn_path)\n\nHere’s an explanation of each line:\nblocks=(ImageBlock,CategoryBlock,CategoryBlock),\nThe DataBlock will create 3 things from each file: an image (the contents of the file), and 2 categorical variables (the disease and the variety).\nn_inp=1,\nThere is 1 input (the image) – and therefore the other two variables (the two categories) are outputs.\nget_items=get_image_files,\nUse get_image_files to get a list of inputs.\nget_y = [parent_label,get_variety],\nTo create the two outputs for each file, call two functions: parent_label (from fastai) and get_variety (defined above).\nsplitter=RandomSplitter(0.2, seed=42),\nRandomly split the input into 80% train and 20% validation sets.\nitem_tfms=Resize(192, method='squish'),\nbatch_tfms=aug_transforms(size=128, min_scale=0.75)\nThese are the same item and batch transforms we’ve used in previous notebooks.\nLet’s take a look at part of a batch of this data:\n\ndls.show_batch(max_n=6)\n\n\n\n\nWe can see that fastai has created both the image input and two categorical outputs that we requested!\n\n\nReplicating the disease model\nNow we’ll replicate the same disease model we’ve made before, but have it work with this new data.\nThe key difference is that our metrics and loss will now receive three things instead of two: the model outputs (i.e. the metric and loss function inputs), and the two targets (disease and variety). Therefore, we need to define slight variations of our metric (error_rate) and loss function (cross_entropy) to pass on just the disease target:\n\n# modify our error function to accomodate two targets\ndef disease_err(inp,disease,variety): return error_rate(inp,disease)\n\n# modify our loss function to accomodate two targets\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp,disease) # cross entropy function is what fastai picked for us when we just had a single outout category\n\n\n\nCross-Entropy\nNote that all of the loss functions in PyTorch have two versions. There is a class which you can instantiate passing in various tweaks, and there is also a version that is a function but everyone, including PyTorch official docs refers to this by F.\nLet’s take some time out to firm up on Cross-Entropy. To illustrate, let’s use a 5 class classification task where an image is classified as either a cat, dog, plane, fish or building.\nThe first step is:\n\nconvert raw outputs of our model (which at this stage are just numbers based on inital random weights applied) to probabilities using the SOFTMAX function.\n\nWe do this by first taking our raw outputs(z) and calculating e to the power of (z) for each prediction i. We then convert to probabilities by pro-rating the results between 0 and 1 - to give us our probabilities which sum to 1 - as illustrated below:\n\n\n\nSoftmax.JPG\n\n\nThe second step is:\n\ncalculate Cross-Entropy loss\n\nFor the purposes of this example, the rather terrifying looking equation below, can effectively be reduced to simply taking the log of output probabilities:\n\n\n\nX_Entropy.JPG\n\n\nThe mathematical image included above in my screenshotted spreadsheet are taken from Things that confused me about cross-entropy by Chris Said.\nWe’re now ready to create our learner. There’s just one wrinkle to be aware of. Now that our DataLoaders is returning multiple targets, fastai doesn’t know how many outputs our model will need. Therefore we have to pass n_out when we create our Learner – we need 10 outputs, one for each possible disease:\n\n!pip3 install --upgrade fastai\nfrom fastai.vision.all import vision_learner\n\n!pip install timm\nimport timm\n\n# replicate our disease model\narch = 'convnext_small_in22k'\nlearn = vision_learner(dls, arch, loss_func=disease_loss, metrics=disease_err, n_out=10).to_fp16() # note we now have to specify which loss_func to use and number of outputs n_out\nlr = 0.01\n\nWhen we train this model we should get similar results to what we’ve seen with similar models before:\n\n# train our model\nlearn.fine_tune(5, lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      time\n    \n  \n  \n    \n      0\n      1.234077\n      0.826925\n      0.270062\n      03:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      time\n    \n  \n  \n    \n      0\n      0.603252\n      0.421769\n      0.135031\n      05:13\n    \n    \n      1\n      0.470395\n      0.415115\n      0.125420\n      05:12\n    \n    \n      2\n      0.303026\n      0.212930\n      0.071120\n      05:13\n    \n    \n      3\n      0.179699\n      0.146253\n      0.042287\n      05:13\n    \n    \n      4\n      0.142097\n      0.138253\n      0.041326\n      05:13\n    \n  \n\n\n\n\n\nMulti-label classification\nIn order to predict both the probability of each disease, and of each variety, we’ll now need the model to output a tensor of length 20, since there are 10 possible diseases, and 10 possible varieties. We can do this by setting n_out=20:\n\n# set model outputs to 20 - 10 diseases and 10 varieties\nlearn = vision_learner(dls, arch, n_out=20).to_fp16()\n\nWe can define disease_loss just like we did previously, but with one important change: the input tensor is now length 20, not 10, so it doesn’t match the number of possible diseases. We can pick whatever part of the input we want to be used to predict disease. Let’s use the first 10 values:\n\n# we need to specify which part of inputs are for use in disease loss function \ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease)\n\nThat means we can do the same thing for predicting variety, but use the last 10 values of the input, and set the target to variety instead of disease:\n\n# we need to specify which part of inputs are for use in variety loss function \ndef variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety)\n\nOur overall loss will then be the sum of these two losses:\n\n# overall loss - just add together loss functions for disease and variety\ndef combine_loss(inp,disease,variety): return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety)\n\nIt would be useful to view the error rate for each of the outputs too, so let’s do the same thing for out metrics:\n\n# function to include the error_rate for disease\ndef disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease)\n\n# function to include the error_rate for disease\ndef variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety)\n\n# combine disease error and variety error within variable err_metrics\nerr_metrics = (disease_err,variety_err)\n\nIt’s useful to see the loss for each of the outputs too, so we’ll add those as metrics:\n\n# combine error metrics and loss metrics within variable all_metrics \nall_metrics = err_metrics+(disease_loss,variety_loss)\n\nWe’re now ready to create and train our Learner:\n\n# pulling it all together into our Learner\nlearn = vision_learner(dls, arch, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16()\n\n\n# train the model\nlearn.fine_tune(5, lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      2.286528\n      1.215683\n      0.265257\n      0.113407\n      0.845696\n      0.369987\n      03:09\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      1.015679\n      0.607585\n      0.133109\n      0.062951\n      0.421834\n      0.185751\n      05:13\n    \n    \n      1\n      0.745607\n      0.412463\n      0.087938\n      0.043729\n      0.286902\n      0.125561\n      05:14\n    \n    \n      2\n      0.483214\n      0.263229\n      0.058626\n      0.025949\n      0.179259\n      0.083970\n      05:14\n    \n    \n      3\n      0.282286\n      0.204188\n      0.047093\n      0.017299\n      0.154198\n      0.049990\n      05:13\n    \n    \n      4\n      0.202148\n      0.174338\n      0.043248\n      0.013455\n      0.133468\n      0.040870\n      05:13\n    \n  \n\n\n\n\n\nKey takeaways\nSo, is this useful?\nWell… if we actually want a model that predicts multiple things, then yes, definitely! But as to whether it’s going to help us better predict rice disease, that is unknown. I haven’t come across any research that tackles this important question: when can a multi-target model improve the accuracy of the individual targets compared to a single target model? (That doesn’t mean it doesn’t exist of course – perhaps it does and I haven’t found it yet…)\nJeremy found that in previous projects there are cases where improvements to single targets can be made by using a multi-target model. It’ll be most useful when we’re having problems with overfitting and so try doing this with more epochs."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html",
    "href": "posts/Krakow_Football/Krakow_Football.html",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "",
    "text": "I arrived in Kraków a few months back and during my wanderings I noticed two football stadiums on either side of Błonia separated by just 700 metres. I didn’t know anything about either of these clubs but was told that the rivalry between them is intense. The so-called Holy War between Wisła Kraków and KS Cracovia has even featured in documentaries by Ross Kemp and Danny Dyer.\nI thought I would try to use Structured Query Language more commonly known as SQL to find out a bit more. To get the ball rolling I first needed to find a data source."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#accessing-sqlite-databases-using-python-and-pandas",
    "href": "posts/Krakow_Football/Krakow_Football.html#accessing-sqlite-databases-using-python-and-pandas",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Accessing SQLite Databases Using Python and Pandas",
    "text": "Accessing SQLite Databases Using Python and Pandas\nAfter downloading the database file which is in .sqlite format. my first question was:\n\nis it even possible to work with database data using Python within Jupyter Notebooks?\n\nThe good news is, yes it is possible :) After quite a bit of digging I found this article which gave me a platform to make a start on this project idea.\n\n# Install the required packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sqlite3\n\n\nThe sqlite3 module\nSQLite is a C library that provides a lightweight disk-based database that doesn’t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It’s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle.\nThe sqlite3 module was written by Gerhard Häring and provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249, and requires SQLite 3.7.15 or newer.\n\n# Create a SQL connection to our downloaded SQLite database file\ncon = sqlite3.connect(\"Data/database.sqlite\")\n\nBasically you can run SQL queries just as you would within a database, and then convert the results to a pandas DataFrame for further analysis and presentation. Let’s have a look at each of the 7 tables in turn to establish what sort of information is included:"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-1---country",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-1---country",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 1 - Country",
    "text": "Table 1 - Country\n\n# Create a pandas DataFRame of the Country table\ncountry = pd.read_sql_query('''SELECT * \n                            FROM Country'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(country)\n\n       id         name\n0       1      Belgium\n1    1729      England\n2    4769       France\n3    7809      Germany\n4   10257        Italy\n5   13274  Netherlands\n6   15722       Poland\n7   17642     Portugal\n8   19694     Scotland\n9   21518        Spain\n10  24558  Switzerland\n\n\nSo, the database covers 11 countries and the country id for Poland is 15722."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-2---league",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-2---league",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 2 - League",
    "text": "Table 2 - League\n\n# Create a pandas DataFrame of the League table\nleagues = pd.read_sql_query('''SELECT * \n                            FROM League'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(leagues)\n\n       id  country_id                      name\n0       1           1    Belgium Jupiler League\n1    1729        1729    England Premier League\n2    4769        4769            France Ligue 1\n3    7809        7809     Germany 1. Bundesliga\n4   10257       10257             Italy Serie A\n5   13274       13274    Netherlands Eredivisie\n6   15722       15722        Poland Ekstraklasa\n7   17642       17642  Portugal Liga ZON Sagres\n8   19694       19694   Scotland Premier League\n9   21518       21518           Spain LIGA BBVA\n10  24558       24558  Switzerland Super League\n\n\nAnd, the id for the League name is the same as the country_id - for the Polish Ekstraklasa this is 15722."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-3---match",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-3---match",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 3 - Match",
    "text": "Table 3 - Match\n\n# Create a pandas DataFrame of the Match table\nmatches = pd.read_sql_query('''SELECT * \n                            FROM Match'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nmatches.head()\n\n\n\n\n\n  \n    \n      \n      id\n      country_id\n      league_id\n      season\n      stage\n      date\n      match_api_id\n      home_team_api_id\n      away_team_api_id\n      home_team_goal\n      ...\n      SJA\n      VCH\n      VCD\n      VCA\n      GBH\n      GBD\n      GBA\n      BSH\n      BSD\n      BSA\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2008/2009\n      1\n      2008-08-17 00:00:00\n      492473\n      9987\n      9993\n      1\n      ...\n      4.00\n      1.65\n      3.40\n      4.50\n      1.78\n      3.25\n      4.00\n      1.73\n      3.40\n      4.20\n    \n    \n      1\n      2\n      1\n      1\n      2008/2009\n      1\n      2008-08-16 00:00:00\n      492474\n      10000\n      9994\n      0\n      ...\n      3.80\n      2.00\n      3.25\n      3.25\n      1.85\n      3.25\n      3.75\n      1.91\n      3.25\n      3.60\n    \n    \n      2\n      3\n      1\n      1\n      2008/2009\n      1\n      2008-08-16 00:00:00\n      492475\n      9984\n      8635\n      0\n      ...\n      2.50\n      2.35\n      3.25\n      2.65\n      2.50\n      3.20\n      2.50\n      2.30\n      3.20\n      2.75\n    \n    \n      3\n      4\n      1\n      1\n      2008/2009\n      1\n      2008-08-17 00:00:00\n      492476\n      9991\n      9998\n      5\n      ...\n      7.50\n      1.45\n      3.75\n      6.50\n      1.50\n      3.75\n      5.50\n      1.44\n      3.75\n      6.50\n    \n    \n      4\n      5\n      1\n      1\n      2008/2009\n      1\n      2008-08-16 00:00:00\n      492477\n      7947\n      9985\n      1\n      ...\n      1.73\n      4.50\n      3.40\n      1.65\n      4.50\n      3.50\n      1.65\n      4.75\n      3.30\n      1.67\n    \n  \n\n5 rows × 115 columns\n\n\n\nAs you can see we have a LOT of data here - 115 columns! Let’s focus on the season column for the moment. From the first 5 rows we can see that we have information starting from the 2008/2009 season. Let’s check how many seasons we have data for:\n\nmatches.season.unique()\n\narray(['2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013',\n       '2013/2014', '2014/2015', '2015/2016'], dtype=object)"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-4---player",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-4---player",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 4 - Player",
    "text": "Table 4 - Player\n\nplayers = pd.read_sql_query('''SELECT * \n                            FROM Player'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(players)\n\n          id  player_api_id          player_name  player_fifa_api_id  \\\n0          1         505942   Aaron Appindangoye              218353   \n1          2         155782      Aaron Cresswell              189615   \n2          3         162549          Aaron Doran              186170   \n3          4          30572        Aaron Galindo              140161   \n4          5          23780         Aaron Hughes               17725   \n...      ...            ...                  ...                 ...   \n11055  11071          26357       Zoumana Camara                2488   \n11056  11072         111182         Zsolt Laczko              164680   \n11057  11073          36491            Zsolt Low              111191   \n11058  11074          35506  Zurab Khizanishvili               47058   \n11059  11075          39902   Zvjezdan Misimovic              102359   \n\n                  birthday  height  weight  \n0      1992-02-29 00:00:00  182.88     187  \n1      1989-12-15 00:00:00  170.18     146  \n2      1991-05-13 00:00:00  170.18     163  \n3      1982-05-08 00:00:00  182.88     198  \n4      1979-11-08 00:00:00  182.88     154  \n...                    ...     ...     ...  \n11055  1979-04-03 00:00:00  182.88     168  \n11056  1986-12-18 00:00:00  182.88     176  \n11057  1979-04-29 00:00:00  180.34     154  \n11058  1981-10-06 00:00:00  185.42     172  \n11059  1982-06-05 00:00:00  180.34     176  \n\n[11060 rows x 7 columns]"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-5---player-attributes",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-5---player-attributes",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 5 - Player attributes",
    "text": "Table 5 - Player attributes\n\nplayer_attributes = pd.read_sql_query('''SELECT * \n                                      FROM Player_Attributes'''\n                                      ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nplayer_attributes.head()\n\n\n\n\n\n  \n    \n      \n      id\n      player_fifa_api_id\n      player_api_id\n      date\n      overall_rating\n      potential\n      preferred_foot\n      attacking_work_rate\n      defensive_work_rate\n      crossing\n      ...\n      vision\n      penalties\n      marking\n      standing_tackle\n      sliding_tackle\n      gk_diving\n      gk_handling\n      gk_kicking\n      gk_positioning\n      gk_reflexes\n    \n  \n  \n    \n      0\n      1\n      218353\n      505942\n      2016-02-18 00:00:00\n      67.0\n      71.0\n      right\n      medium\n      medium\n      49.0\n      ...\n      54.0\n      48.0\n      65.0\n      69.0\n      69.0\n      6.0\n      11.0\n      10.0\n      8.0\n      8.0\n    \n    \n      1\n      2\n      218353\n      505942\n      2015-11-19 00:00:00\n      67.0\n      71.0\n      right\n      medium\n      medium\n      49.0\n      ...\n      54.0\n      48.0\n      65.0\n      69.0\n      69.0\n      6.0\n      11.0\n      10.0\n      8.0\n      8.0\n    \n    \n      2\n      3\n      218353\n      505942\n      2015-09-21 00:00:00\n      62.0\n      66.0\n      right\n      medium\n      medium\n      49.0\n      ...\n      54.0\n      48.0\n      65.0\n      66.0\n      69.0\n      6.0\n      11.0\n      10.0\n      8.0\n      8.0\n    \n    \n      3\n      4\n      218353\n      505942\n      2015-03-20 00:00:00\n      61.0\n      65.0\n      right\n      medium\n      medium\n      48.0\n      ...\n      53.0\n      47.0\n      62.0\n      63.0\n      66.0\n      5.0\n      10.0\n      9.0\n      7.0\n      7.0\n    \n    \n      4\n      5\n      218353\n      505942\n      2007-02-22 00:00:00\n      61.0\n      65.0\n      right\n      medium\n      medium\n      48.0\n      ...\n      53.0\n      47.0\n      62.0\n      63.0\n      66.0\n      5.0\n      10.0\n      9.0\n      7.0\n      7.0\n    \n  \n\n5 rows × 42 columns"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-6---teams",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-6---teams",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 6 - Teams",
    "text": "Table 6 - Teams\n\n# Create a pandas DataFRame of the Team table\nteams = pd.read_sql_query('''SELECT * \n                          FROM Team'''\n                          ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(teams)\n\n        id  team_api_id  team_fifa_api_id      team_long_name team_short_name\n0        1         9987             673.0            KRC Genk             GEN\n1        2         9993             675.0        Beerschot AC             BAC\n2        3        10000           15005.0    SV Zulte-Waregem             ZUL\n3        4         9994            2007.0    Sporting Lokeren             LOK\n4        5         9984            1750.0   KSV Cercle Brugge             CEB\n..     ...          ...               ...                 ...             ...\n294  49479        10190             898.0       FC St. Gallen             GAL\n295  49837        10191            1715.0             FC Thun             THU\n296  50201         9777             324.0         Servette FC             SER\n297  50204         7730            1862.0  FC Lausanne-Sports             LAU\n298  51606         7896               NaN              Lugano             LUG\n\n[299 rows x 5 columns]\n\n\nSo we can see that the Team table has information about 299 teams. It would be useful to see the breakdown of the number of teams from each country league. It’s not possibe to do this using the information contained in the Team table alone. We will need to join two tables together to obtain this information:\n\n# Join the Team table to the Match table using an INNER JOIN and create a pandas DataFrame\nteam_league = pd.read_sql_query('''SELECT DISTINCT t.team_api_id AS team_id, t.team_long_name AS team_name, l.name AS league \n                                FROM Team AS t \n                                JOIN Match AS m ON t.team_api_id = m.home_team_api_id \n                                JOIN League AS l ON l.country_id = m.country_id''',con)\n\n# Verify that result of SQL query is stored in the dataframe\nteam_league.head(10)\n\n\n\n\n\n  \n    \n      \n      team_id\n      team_name\n      league\n    \n  \n  \n    \n      0\n      1601\n      Ruch Chorzów\n      Poland Ekstraklasa\n    \n    \n      1\n      1773\n      Oud-Heverlee Leuven\n      Belgium Jupiler League\n    \n    \n      2\n      1957\n      Jagiellonia Białystok\n      Poland Ekstraklasa\n    \n    \n      3\n      2033\n      S.C. Olhanense\n      Portugal Liga ZON Sagres\n    \n    \n      4\n      2182\n      Lech Poznań\n      Poland Ekstraklasa\n    \n    \n      5\n      2183\n      P. Warszawa\n      Poland Ekstraklasa\n    \n    \n      6\n      2186\n      Cracovia\n      Poland Ekstraklasa\n    \n    \n      7\n      4049\n      Tubize\n      Belgium Jupiler League\n    \n    \n      8\n      4064\n      Feirense\n      Portugal Liga ZON Sagres\n    \n    \n      9\n      4087\n      Évian Thonon Gaillard FC\n      France Ligue 1\n    \n  \n\n\n\n\nGreat, now we have the required information to show the number of teams split across the 11 distinct leagues. Let’s use pandas to sort and group this data and then illustrate graphically using matplotlib:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"European Soccer Database \"\nsubtitle_string = \"Number of teams per country\"\n\nx = team_league.groupby(\"league\")[\"team_name\"].count().sort_values(ascending=True)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Number of teams per country')\n\n\n\n\n\n\nteam_league.groupby(\"league\")[\"team_name\"].count().sort_values(ascending=False)\n\nleague\nFrance Ligue 1              35\nEngland Premier League      34\nSpain LIGA BBVA             33\nItaly Serie A               32\nGermany 1. Bundesliga       30\nPortugal Liga ZON Sagres    29\nBelgium Jupiler League      25\nNetherlands Eredivisie      25\nPoland Ekstraklasa          24\nScotland Premier League     17\nSwitzerland Super League    15\nName: team_name, dtype: int64\n\n\nThere is quite a disparity between the various leagues. The French Ligue 1 has 20 teams (we have data for 35 teams because the dataset covers 8 seasons and accounts for promotions from the lower leagues), whereas the Swiss Super League only has 10 teams."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-7---team-attributes",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-7---team-attributes",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 7 - Team attributes",
    "text": "Table 7 - Team attributes\n\n# Create a pandas DataFRame of the Team table\nteam_attributes= pd.read_sql_query('''SELECT * \n                                   FROM Team_Attributes'''\n                                   ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nteam_attributes.head()\n\n\n\n\n\n  \n    \n      \n      id\n      team_fifa_api_id\n      team_api_id\n      date\n      buildUpPlaySpeed\n      buildUpPlaySpeedClass\n      buildUpPlayDribbling\n      buildUpPlayDribblingClass\n      buildUpPlayPassing\n      buildUpPlayPassingClass\n      ...\n      chanceCreationShooting\n      chanceCreationShootingClass\n      chanceCreationPositioningClass\n      defencePressure\n      defencePressureClass\n      defenceAggression\n      defenceAggressionClass\n      defenceTeamWidth\n      defenceTeamWidthClass\n      defenceDefenderLineClass\n    \n  \n  \n    \n      0\n      1\n      434\n      9930\n      2010-02-22 00:00:00\n      60\n      Balanced\n      NaN\n      Little\n      50\n      Mixed\n      ...\n      55\n      Normal\n      Organised\n      50\n      Medium\n      55\n      Press\n      45\n      Normal\n      Cover\n    \n    \n      1\n      2\n      434\n      9930\n      2014-09-19 00:00:00\n      52\n      Balanced\n      48.0\n      Normal\n      56\n      Mixed\n      ...\n      64\n      Normal\n      Organised\n      47\n      Medium\n      44\n      Press\n      54\n      Normal\n      Cover\n    \n    \n      2\n      3\n      434\n      9930\n      2015-09-10 00:00:00\n      47\n      Balanced\n      41.0\n      Normal\n      54\n      Mixed\n      ...\n      64\n      Normal\n      Organised\n      47\n      Medium\n      44\n      Press\n      54\n      Normal\n      Cover\n    \n    \n      3\n      4\n      77\n      8485\n      2010-02-22 00:00:00\n      70\n      Fast\n      NaN\n      Little\n      70\n      Long\n      ...\n      70\n      Lots\n      Organised\n      60\n      Medium\n      70\n      Double\n      70\n      Wide\n      Cover\n    \n    \n      4\n      5\n      77\n      8485\n      2011-02-22 00:00:00\n      47\n      Balanced\n      NaN\n      Little\n      52\n      Mixed\n      ...\n      52\n      Normal\n      Organised\n      47\n      Medium\n      47\n      Press\n      52\n      Normal\n      Cover\n    \n  \n\n5 rows × 25 columns"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#which-polish-teams-are-included-in-the-database",
    "href": "posts/Krakow_Football/Krakow_Football.html#which-polish-teams-are-included-in-the-database",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Which Polish teams are included in the database?",
    "text": "Which Polish teams are included in the database?\n\n\n\npoland.jpg\n\n\n\n# Subquery filtering list with IN\npoland_league = pd.read_sql_query ('''SELECT team_long_name, \n                                   team_short_name AS abbr, \n                                   team_api_id AS team_id \n                                   FROM Team WHERE team_api_id \n                                   IN (SELECT home_team_api_id FROM Match WHERE country_id = 15722)'''\n                                   ,con)\npoland_league\n\n\n\n\n\n  \n    \n      \n      team_long_name\n      abbr\n      team_id\n    \n  \n  \n    \n      0\n      Ruch Chorzów\n      CHO\n      1601\n    \n    \n      1\n      Jagiellonia Białystok\n      BIA\n      1957\n    \n    \n      2\n      Lech Poznań\n      POZ\n      2182\n    \n    \n      3\n      P. Warszawa\n      PWA\n      2183\n    \n    \n      4\n      Cracovia\n      CKR\n      2186\n    \n    \n      5\n      Górnik Łęczna\n      LEC\n      8019\n    \n    \n      6\n      Polonia Bytom\n      GOR\n      8020\n    \n    \n      7\n      Zagłębie Lubin\n      ZAG\n      8021\n    \n    \n      8\n      Pogoń Szczecin\n      POG\n      8023\n    \n    \n      9\n      Widzew Łódź\n      WID\n      8024\n    \n    \n      10\n      Śląsk Wrocław\n      SLA\n      8025\n    \n    \n      11\n      Zawisza Bydgoszcz\n      ZAW\n      8027\n    \n    \n      12\n      Piast Gliwice\n      PIG\n      8028\n    \n    \n      13\n      Lechia Gdańsk\n      LGD\n      8030\n    \n    \n      14\n      Polonia Bytom\n      POB\n      8031\n    \n    \n      15\n      Podbeskidzie Bielsko-Biała\n      POD\n      8033\n    \n    \n      16\n      Odra Wodzisław\n      ODR\n      8242\n    \n    \n      17\n      Widzew Łódź\n      LOD\n      8244\n    \n    \n      18\n      Korona Kielce\n      KKI\n      8245\n    \n    \n      19\n      Arka Gdynia\n      ARK\n      8322\n    \n    \n      20\n      GKS Bełchatów\n      BEL\n      8569\n    \n    \n      21\n      Legia Warszawa\n      LEG\n      8673\n    \n    \n      22\n      Wisła Kraków\n      WIS\n      10265\n    \n    \n      23\n      Termalica Bruk-Bet Nieciecza\n      TBN\n      177361\n    \n  \n\n\n\n\nOne or two familiar names there, Lech Poznań, Legia Warszawa. Having recently relocated to Kraków the focus for the rest of this project will be on the two Kraków teams:\n\nWisła Kraków\nKS Cracovia\n\nThe history of both clubs appears to be colourful. Cracovia received a 5 point deduction in 2020/2021 for alleged match-rigging dating back to 2003/2004, and this article on the history of Wisła Kraków is an entertaining read!"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#the-holy-war---święta-wojna",
    "href": "posts/Krakow_Football/Krakow_Football.html#the-holy-war---święta-wojna",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "The Holy War - Święta Wojna",
    "text": "The Holy War - Święta Wojna\nThe term “Holy War” refers to the intense rivalry between the two Kraków-based teams; Wisła Kraków and KS Cracovia. In 1906, the establishment of the two first Polish football clubs, Cracovia and Wisła, created a rivalry that now dates back more than 100 years. The term “Holy War” was first used to describe the rivalry of Kraków’s Jewish teams, Makkabi and Jutrzenka. A Jutrzenka defender, Ludwik Gintel, who later joined the Cracovia side referred to the derby match against Wisła as the “Holy War”. The phrase was incorporated into a song and has since been popular amongst both Wisła and Cracovia fans.\nThe first recorded Kraków Derby was contested on 20 September 1908, a 1–1 draw.\nWisła Kraków were formed in 1906 and play their football at the Stadion Miejski im. Henryka Reymana which has a capacity of 33,130.\nThey have 14 league titles in their trophy cabinet, the most recent win was 2010/2011.\n \nKS Cracovia also formed in 1906, are widely considered to be the oldest club in Kraków. They play their football at the Stadion Cracovii im. Józefa Piłsudskiego which has a capacity of 15,016.\nThey have 5 league titles in their trophy cabinet, however they are getting a bit dusty. The last time they won the title was 1948.\n \nSo, what happens when these clubs go toe to toe?! Let’s find out by looking at the results for the matches between the bitter rivals over the period of study:\n\n# Use multiple CASE WHEN THEN queries to obtain outcomes from all matches between Wisła Kraków and Cracovia\nderbies = pd.read_sql_query('''SELECT date, season, home_team_api_id, away_team_api_id, home_team_goal,away_team_goal,\n                                 CASE WHEN home_team_api_ID = 2186 AND home_team_goal > away_team_goal \n                                 THEN 'KS Cracovia win!'\n                                WHEN home_team_api_ID = 2186 AND home_team_goal < away_team_goal \n                                THEN 'Wisła Kraków win!'\n                                WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal\n                                THEN 'Wisła Kraków win!'\n                                 WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal\n                                THEN 'KS Cracovia win!'\n                                ELSE 'Draw' END AS outcome \n                                FROM Match\n                                WHERE home_team_api_id = 2186 AND away_team_api_id = 10265\n                                OR home_team_api_id = 10265 AND away_team_api_id = 2186'''                                \n                            ,con)\n\n\n# Create a piechart showing the derby outcomes\nderbies['outcome'].value_counts().plot(kind='pie',autopct='%.2f')\n\n<AxesSubplot: ylabel='outcome'>\n\n\n\n\n\n\nderbies\n\n\n\n\n\n  \n    \n      \n      date\n      season\n      home_team_api_id\n      away_team_api_id\n      home_team_goal\n      away_team_goal\n      outcome\n    \n  \n  \n    \n      0\n      2009-03-22 00:00:00\n      2008/2009\n      10265\n      2186\n      4\n      1\n      Wisła Kraków win!\n    \n    \n      1\n      2008-08-31 00:00:00\n      2008/2009\n      2186\n      10265\n      1\n      1\n      Draw\n    \n    \n      2\n      2009-11-22 00:00:00\n      2009/2010\n      10265\n      2186\n      0\n      1\n      Cracovia win!\n    \n    \n      3\n      2010-05-11 00:00:00\n      2009/2010\n      2186\n      10265\n      1\n      1\n      Draw\n    \n    \n      4\n      2010-11-05 00:00:00\n      2010/2011\n      2186\n      10265\n      0\n      1\n      Wisła Kraków win!\n    \n    \n      5\n      2011-05-15 00:00:00\n      2010/2011\n      10265\n      2186\n      1\n      0\n      Wisła Kraków win!\n    \n    \n      6\n      2011-11-06 00:00:00\n      2011/2012\n      2186\n      10265\n      1\n      0\n      Cracovia win!\n    \n    \n      7\n      2012-04-30 00:00:00\n      2011/2012\n      10265\n      2186\n      1\n      0\n      Wisła Kraków win!\n    \n    \n      8\n      2014-02-23 00:00:00\n      2013/2014\n      10265\n      2186\n      3\n      1\n      Wisła Kraków win!\n    \n    \n      9\n      2013-09-21 00:00:00\n      2013/2014\n      2186\n      10265\n      1\n      1\n      Draw\n    \n    \n      10\n      2014-09-28 00:00:00\n      2014/2015\n      2186\n      10265\n      1\n      0\n      Cracovia win!\n    \n    \n      11\n      2015-03-21 00:00:00\n      2014/2015\n      10265\n      2186\n      2\n      1\n      Wisła Kraków win!\n    \n    \n      12\n      2015-11-29 00:00:00\n      2015/2016\n      10265\n      2186\n      1\n      2\n      Cracovia win!\n    \n    \n      13\n      2015-07-24 00:00:00\n      2015/2016\n      2186\n      10265\n      1\n      1\n      Draw\n    \n  \n\n\n\n\nSo, Wisła Kraków have the bragging rights for the period of study. The clubs met 14 times (no derby in 2012/2013 due to KS Cracovia being relegated from the top league in 2011/2012) between 2008/2009 and 2015/2016 :\n\nWisła Kraków wins = 6\nKS Cracovia wins = 4\nDraws = 4\n\nBut of course it’s not all about the derby matches. Let’s now take a look at how the two Kraków rivals performed overall."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#wisła-kraków",
    "href": "posts/Krakow_Football/Krakow_Football.html#wisła-kraków",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Wisła Kraków",
    "text": "Wisła Kraków\n\n\n2008/2009 to 2015/2016\n\nGooooooooooool !\n\n# Use a SQL CASE WHEN with SUM query to display a table summarizing goals scored and conceded for each season\n# Multiply goals conceded by -1 to allow bi-directional plotting\n\nwisla_krakow_goals = pd.read_sql_query('''SELECT season, SUM(CASE WHEN home_team_api_id = 10265 THEN home_team_goal END) AS home_goals_scored, \n                                       SUM(CASE WHEN away_team_api_id = 10265 THEN away_team_goal END) AS away_goals_scored,\n                                       SUM(CASE WHEN home_team_api_id = 10265 THEN away_team_goal END)*-1 AS home_goals_conceded, \n                                       SUM(CASE WHEN away_team_api_id = 10265 THEN home_team_goal END)*-1 AS away_goals_conceded \n                                       FROM Match \n                                       GROUP BY season'''\n                                       ,con)\n\n\n# Plot a bi-directional stacked bar plot showing home and away goals scored and conceded each season\nwisla_krakow_goals.plot.bar(x='season', stacked=True, title ='Wisła Kraków - goals for and against')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_goals\n\n\n\n\n\n  \n    \n      \n      season\n      home_goals_scored\n      away_goals_scored\n      home_goals_conceded\n      away_goals_conceded\n    \n  \n  \n    \n      0\n      2008/2009\n      31\n      22\n      -9\n      -12\n    \n    \n      1\n      2009/2010\n      18\n      30\n      -9\n      -11\n    \n    \n      2\n      2010/2011\n      27\n      17\n      -11\n      -18\n    \n    \n      3\n      2011/2012\n      14\n      15\n      -11\n      -15\n    \n    \n      4\n      2012/2013\n      15\n      13\n      -14\n      -21\n    \n    \n      5\n      2013/2014\n      23\n      15\n      -6\n      -24\n    \n    \n      6\n      2014/2015\n      20\n      27\n      -18\n      -21\n    \n    \n      7\n      2015/2016\n      20\n      25\n      -17\n      -18\n    \n  \n\n\n\n\nWe have data for all 8 seasons which confirms that they were not relegated from the top flight during the period of study. Generally speaking, for the period of study Wisła Kraków comfortably score more goals than they concede, the exception being season 2012/13 where they scored just 28 goals (only four more than relegated GKS Bełchatów) they finished 7th that year. We can represent the goal ‘difference’ graphically:\n\n\nGoal difference\n\n# Add a goal difference column to the wisla_krakow_goals DataFrame\nwisla_krakow_goals[\"goal_diff\"] = wisla_krakow_goals[\"home_goals_scored\"] + wisla_krakow_goals[\"away_goals_scored\"] + wisla_krakow_goals[\"home_goals_conceded\"] + wisla_krakow_goals[\"away_goals_conceded\"] \n\n# Select the season and goal difference columns and plot\nwisla_krakow_goal_diff = wisla_krakow_goals[[\"season\", \"goal_diff\"]]\nwisla_krakow_goal_diff.plot.bar(x='season', stacked=False, title ='Wisła Kraków - goal difference')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - goal difference'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_goal_diff\n\n\n\n\n\n  \n    \n      \n      season\n      goal_diff\n    \n  \n  \n    \n      0\n      2008/2009\n      32\n    \n    \n      1\n      2009/2010\n      28\n    \n    \n      2\n      2010/2011\n      15\n    \n    \n      3\n      2011/2012\n      3\n    \n    \n      4\n      2012/2013\n      -7\n    \n    \n      5\n      2013/2014\n      8\n    \n    \n      6\n      2014/2015\n      8\n    \n    \n      7\n      2015/2016\n      10\n    \n  \n\n\n\n\nFor the period of study Wisła Kraków scored more goals then they conceded, except for the 2012/2013 season.\n\n\nYou win some, you lose some\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won and lost\nwisla_krakow_wins_losses =  pd.read_sql_query('''SELECT season, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                              FROM Match \n                                              GROUP BY season'''\n                                              ,con)\n\n\n# Convert defeats to a negative value - multiply by -1 to enable bi-directional plotting\nwisla_krakow_wins_losses.home_defeats = wisla_krakow_wins_losses.home_defeats * -1\nwisla_krakow_wins_losses.away_defeats = wisla_krakow_wins_losses.away_defeats * -1\n\n# Plot a stacked bar plot showing home and away wins and defeats for each season\nwisla_krakow_wins_losses.plot.bar(x='season', stacked=True, title ='Wisła Kraków - games won and lost')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - games won and lost'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_wins_losses\n\n\n\n\n\n  \n    \n      \n      season\n      home_wins\n      away_wins\n      home_defeats\n      away_defeats\n    \n  \n  \n    \n      0\n      2008/2009\n      13\n      6\n      -1\n      -3\n    \n    \n      1\n      2009/2010\n      8\n      11\n      -4\n      -2\n    \n    \n      2\n      2010/2011\n      11\n      6\n      -2\n      -6\n    \n    \n      3\n      2011/2012\n      7\n      5\n      -6\n      -5\n    \n    \n      4\n      2012/2013\n      6\n      4\n      -6\n      -6\n    \n    \n      5\n      2013/2014\n      10\n      2\n      -3\n      -6\n    \n    \n      6\n      2014/2015\n      6\n      5\n      -4\n      -5\n    \n    \n      7\n      2015/2016\n      3\n      5\n      -4\n      -5\n    \n  \n\n\n\n\nFrom the above stacked bar chart we can see a downward trend. Wisła Kraków only won 8 matches in 2015/2016, finishing the season in 11th place.\n\n\nHome advantage?\n\n# Run an SQl query using COUNT CASE WHEN to extract % of home and away games won\nwisla_krakow_win_pct = pd.read_sql_query('''SELECT season, \n                                         ROUND(AVG(CASE WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal THEN 1 \n                                                   WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal THEN 0 END)*100,2) AS pct_home_wins, \n                                         ROUND(AVG(CASE WHEN away_team_api_id = 10265 AND away_team_goal > home_team_goal THEN 1 \n                                                   WHEN away_team_api_id = 10265 AND away_team_goal < home_team_goal THEN 0 END)*100,2) AS pct_away_wins \n                                         FROM Match \n                                         GROUP BY season'''\n                                         ,con)\n\n# Plot a bar plot showing % of home and away wins for each season\nwisla_krakow_win_pct.plot.bar(x='season', stacked=False, title ='Wisła Kraków - win percentages')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - win percentages'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_win_pct\n\n\n\n\n\n  \n    \n      \n      season\n      pct_home_wins\n      pct_away_wins\n    \n  \n  \n    \n      0\n      2008/2009\n      92.86\n      66.67\n    \n    \n      1\n      2009/2010\n      66.67\n      84.62\n    \n    \n      2\n      2010/2011\n      84.62\n      50.00\n    \n    \n      3\n      2011/2012\n      53.85\n      50.00\n    \n    \n      4\n      2012/2013\n      50.00\n      40.00\n    \n    \n      5\n      2013/2014\n      76.92\n      25.00\n    \n    \n      6\n      2014/2015\n      60.00\n      50.00\n    \n    \n      7\n      2015/2016\n      42.86\n      50.00\n    \n  \n\n\n\n\nIn the early part of the period of study, Wisła Kraków had a reputation of being a tough nut to crack at home. They won the league in 2008/2009, winning just under 93% of their home matches! They also had a high degreee of success on the road, winning two thirds of their away matches. They won the league again 2010/2011 with a lower percentage of wins.\n\n\nPoints make prizes\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won,lost, and drawn\nwisla_krakow_points =  pd.read_sql_query('''SELECT season, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal = away_team_goal THEN id END) AS home_draws, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal = home_team_goal THEN id END) AS away_draws, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                              FROM Match \n                                              GROUP BY season'''\n                                              ,con)\n\n# Calculate points - 3 for a win, 1 for a draw, 0 for a loss\nwisla_krakow_points[\"points\"] = wisla_krakow_points['home_wins'] * 3 +  wisla_krakow_points['away_wins'] * 3 + wisla_krakow_points['home_draws'] + wisla_krakow_points['away_draws']\nwisla_krakow_points = wisla_krakow_points[[\"season\",\"points\"]]\n\n# Plot a bar plot showing total points for each season\nwisla_krakow_points.plot.bar(x='season', stacked=False, title ='Wisła Kraków - total points')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - total points'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2008/2009\n      64\n    \n    \n      1\n      2009/2010\n      62\n    \n    \n      2\n      2010/2011\n      56\n    \n    \n      3\n      2011/2012\n      43\n    \n    \n      4\n      2012/2013\n      38\n    \n    \n      5\n      2013/2014\n      45\n    \n    \n      6\n      2014/2015\n      43\n    \n    \n      7\n      2015/2016\n      37\n    \n  \n\n\n\n\nWisła Kraków won the league in 2008/2009 with 64 points but 62 points the following year was only good enough for second place (behind a Robert Lewandowski inspired Lech Poznań). Only 56 points were needed to win the league in 2010/2011.\n\n\nFinal standings\nLet’s add an extra column for league position data:\n\n# Create an extra column and insert into our original points DataFrame\nwisla_krakow_points.insert(0,\"Position\",[1,2,1,7,7,5,5,11],True)\n\n\n# Plot a horizontal bar plot of league position\nwisla_krakow_points.plot.barh(x='season', y='Position', title ='Wisła Kraków - final league position')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - final league position'}, ylabel='season'>\n\n\n\n\n\n\nwisla_krakow_position = wisla_krakow_points[[\"season\",\"points\"]]\n\n\n\n\n\n  \n    \n      \n      Position\n      season\n      points\n    \n  \n  \n    \n      0\n      1\n      2008/2009\n      64\n    \n    \n      1\n      2\n      2009/2010\n      62\n    \n    \n      2\n      1\n      2010/2011\n      56\n    \n    \n      3\n      7\n      2011/2012\n      43\n    \n    \n      4\n      7\n      2012/2013\n      38\n    \n    \n      5\n      5\n      2013/2014\n      45\n    \n    \n      6\n      5\n      2014/2015\n      43\n    \n    \n      7\n      11\n      2015/2016\n      37\n    \n  \n\n\n\n\nThe general trend for Wisła Kraków is one of decline. They have struggled since the heights of 2008/2009 to 2010/2011 where they were placed 1, 2, 1, finishing 11th in 2015/2016.\n\n\n\n2016/2017 to 2021/2022\nThe European Soccer Database is now somewhat out of date, so I decided to gather data for seasons 2016/2017 to 2021/2022 and create dictionaries for conversion to DataFrames.\n\nGooooooooooool !\n\n# Create a dictionary for the most recent years\nwisla_recent = {'season': ['2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021','2021/2022'],\n        'goals_scored': [45, 41, 55, 37, 39, 37],\n        'goals_conceded': [46, 36, 48, 47,42, 54],\n        'games_won': [13, 12, 12, 10, 8, 7],\n        'games_lost': [12, 10, 12, 15, 13, 17],\n        'games_drawn': [5, 8, 6, 5, 9, 10],\n         'points':[44, 44, 42, 35, 33, 31],\n         'position':[5, 7, 9, 13, 13, 17]\n        } \n\n# Convert dictionary to a DataFrame\nwisla_recent = pd.DataFrame.from_dict(wisla_recent)  \n\n\n# Filter for columns to be plotted\nwisla_recent_goals = wisla_recent[['season','goals_scored','goals_conceded',]]\n\n# Plot a bar plot showing goals scored and conceded each season\nwisla_recent_goals.plot.bar(x='season', stacked=False, title ='Wisła Kraków - goals for and against')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\nwisla_recent_goals\n\n\n\n\n\n  \n    \n      \n      season\n      goals_scored\n      goals_conceded\n    \n  \n  \n    \n      0\n      2016/2017\n      45\n      46\n    \n    \n      1\n      2017/2018\n      41\n      36\n    \n    \n      2\n      2018/2019\n      55\n      48\n    \n    \n      3\n      2019/2020\n      37\n      47\n    \n    \n      4\n      2020/2021\n      39\n      42\n    \n    \n      5\n      2021/2022\n      37\n      54\n    \n  \n\n\n\n\nWisła Kraków leaked 54 goals in the 2021/2022 season which was enough to see them relegated to the 2nd tier of Polish football.\n\n\nWin, lose or draw\n\n# Filter for columns to be plotted\nwisla_recent_wins = wisla_recent[['season','games_won','games_drawn','games_lost']]\n\n# Plot a bar plot showing goals won, lost, and drawn each season\nwisla_recent_wins.plot.bar(x='season', stacked=False, title ='Wisła Kraków - win, draw, lose')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - win, draw, lose'}, xlabel='season'>\n\n\n\n\n\n\nwisla_recent_wins\n\n\n\n\n\n  \n    \n      \n      season\n      games_won\n      games_drawn\n      games_lost\n    \n  \n  \n    \n      0\n      2016/2017\n      13\n      5\n      12\n    \n    \n      1\n      2017/2018\n      12\n      8\n      10\n    \n    \n      2\n      2018/2019\n      12\n      6\n      12\n    \n    \n      3\n      2019/2020\n      10\n      5\n      15\n    \n    \n      4\n      2020/2021\n      8\n      9\n      13\n    \n    \n      5\n      2021/2022\n      7\n      10\n      17\n    \n  \n\n\n\n\nWisła Kraków lost 17 out of 34 games in the 2021/2022 season which contributed to their relegation to the 2nd tier of Polish football.\n\n\nPoints make prizes\n\n# Filter for columns to be plotted\nwisla_recent_points = wisla_recent[['season','points']]\n\n# Plot a bar plot showing total points for each season\nwisla_recent_points.plot.bar(x='season', stacked=False, title ='Wisła Kraków - total points')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - total points'}, xlabel='season'>\n\n\n\n\n\n\nwisla_recent_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2016/2017\n      44\n    \n    \n      1\n      2017/2018\n      44\n    \n    \n      2\n      2018/2019\n      42\n    \n    \n      3\n      2019/2020\n      35\n    \n    \n      4\n      2020/2021\n      33\n    \n    \n      5\n      2021/2022\n      31\n    \n  \n\n\n\n\nAs we can see. there was a steady decline in point totals from 2018/2019 onwards. Wisła Kraków accumulated just 31 points from their 34 matches in the 2021/2022 season which was not enough to retain their top flight status.\n\n\nFinal standings\n\n# Plot a horizontal bar plot of league position\nwisla_recent.plot.barh(x='season', y='position', title ='Wisła Kraków - final league position')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - final league position'}, ylabel='season'>\n\n\n\n\n\n\nwisla_position = wisla_recent[[\"season\",\"position\"]]\nwisla_position \n\n\n\n\n\n  \n    \n      \n      season\n      position\n    \n  \n  \n    \n      0\n      2016/2017\n      5\n    \n    \n      1\n      2017/2018\n      7\n    \n    \n      2\n      2018/2019\n      9\n    \n    \n      3\n      2019/2020\n      13\n    \n    \n      4\n      2020/2021\n      13\n    \n    \n      5\n      2021/2022\n      17\n    \n  \n\n\n\n\nThe decline in points totals is mirrored by a declining league position. Wisła Kraków accumulated just 31 points from their 34 matches in the 2021/2022 season and were relegated.\n\n\n\n2022/2023\nThe second tier I Liga table for 2022/2023 at the time of writing is shown below. The season is on hold as the FIFA World Cup - Qatar 2022 progresses. Wisła Kraków are languishing mid table. A long way away from the dizzy heights of the Ekstraklasa championship winning side of 2010/2011.\n\n\n\nLiga_1_2022_2-23.PNG"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#ks-cracovia",
    "href": "posts/Krakow_Football/Krakow_Football.html#ks-cracovia",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "KS Cracovia",
    "text": "KS Cracovia\n\n\n2008/2009 to 2015/2016\n\nGooooooooooool !\n\n# Now that we already have the query structure in place, we can simply replace the home_team_api_id with that for Cracovia - 2186\ncracovia_goals = pd.read_sql_query('''SELECT season, \n                                   ROUND(SUM(CASE WHEN home_team_api_id = 2186 THEN home_team_goal END),2) AS home_goals_scored, \n                                   ROUND(SUM(CASE WHEN away_team_api_id = 2186 THEN away_team_goal END),2) AS away_goals_scored, \n                                   SUM(CASE WHEN home_team_api_id = 2186 THEN away_team_goal END)*-1 AS home_goals_conceded, \n                                   SUM(CASE WHEN away_team_api_id = 2186 THEN home_team_goal END)*-1 AS away_goals_conceded \n                                   FROM Match \n                                   GROUP BY season'''\n                                   ,con)\n\n# Plot a stacked bar plot showing home and away goals scored and conceded each season\ncracovia_goals.plot.bar(x='season', stacked=True, title ='KS Cracovia - goals for and against')\n\n<AxesSubplot: title={'center': 'KS Cracovia - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_goals \n\n\n\n\n\n  \n    \n      \n      season\n      home_goals_scored\n      away_goals_scored\n      home_goals_conceded\n      away_goals_conceded\n    \n  \n  \n    \n      0\n      2008/2009\n      12.0\n      12.0\n      -8.0\n      -32.0\n    \n    \n      1\n      2009/2010\n      18.0\n      7.0\n      -23.0\n      -16.0\n    \n    \n      2\n      2010/2011\n      30.0\n      7.0\n      -21.0\n      -26.0\n    \n    \n      3\n      2011/2012\n      10.0\n      10.0\n      -20.0\n      -21.0\n    \n    \n      4\n      2012/2013\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      2013/2014\n      19.0\n      18.0\n      -22.0\n      -21.0\n    \n    \n      6\n      2014/2015\n      21.0\n      14.0\n      -17.0\n      -24.0\n    \n    \n      7\n      2015/2016\n      34.0\n      23.0\n      -19.0\n      -23.0\n    \n  \n\n\n\n\nInterestingly, we have a Not a Number (NaN) reference for season 2012/2013. On follow up it transpires that KS Cracovia finished bottom of the Ekstraklasa in 2011/2012 (as we can see above they only scored 20 goals in total that season) and were relegated to the I liga, the second tier of Polish football. They made an immediate return and the 37 goals scored in 2013/2014 was enough to secure them a repectable 9th spot out of 16. They scored 57 goals in 2015/2016 (finishing 5th), a goal tally only bettered by champions Legia Warszawa.\n\n\nGoal difference\n\n# Add a goal difference column to the cracovia_goals DataFrame\ncracovia_goals[\"goal_diff\"] = cracovia_goals[\"home_goals_scored\"] + cracovia_goals[\"away_goals_scored\"] + cracovia_goals[\"home_goals_conceded\"] + cracovia_goals[\"away_goals_conceded\"] \n\n# Select the season and goal difference columns and plot\ncracovia_goal_diff = cracovia_goals[[\"season\", \"goal_diff\"]]\ncracovia_goal_diff.plot.bar(x='season', stacked=False, title ='KS Cracovia - goal difference')\n\n<AxesSubplot: title={'center': 'KS Cracovia - goal difference'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_goal_diff\n\n\n\n\n\n  \n    \n      \n      season\n      goal_diff\n    \n  \n  \n    \n      0\n      2008/2009\n      -16.0\n    \n    \n      1\n      2009/2010\n      -14.0\n    \n    \n      2\n      2010/2011\n      -10.0\n    \n    \n      3\n      2011/2012\n      -21.0\n    \n    \n      4\n      2012/2013\n      NaN\n    \n    \n      5\n      2013/2014\n      -6.0\n    \n    \n      6\n      2014/2015\n      -6.0\n    \n    \n      7\n      2015/2016\n      15.0\n    \n  \n\n\n\n\nAs noted before, we have no data for 2012/2013 as KS Cracovia were relegated in 2011/2012. A goal difference of -21 highlights the poor performance that year. For all the years of study (with the exception of 2015/2016 where they finished 5th) KS Cracovia conceded more goals than they scored.\n\n\nYou win some, you lose some\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won and lost\ncracovia_wins_losses =  pd.read_sql_query('''SELECT season, \n                                          COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                          COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                          COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                          COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                          FROM Match \n                                          GROUP BY season'''\n                                          ,con)\n\n\n# Convert defeats to a negative value - multiply by -1 to allow bi-directional plotting\ncracovia_wins_losses.home_defeats = cracovia_wins_losses.home_defeats * -1\ncracovia_wins_losses.away_defeats = cracovia_wins_losses.away_defeats * -1\n\n# Plot a stacked bar plot showing home and away wins and defeats each season\ncracovia_wins_losses.plot.bar(x='season', stacked=True, title ='KS Cracovia - games won and lost')\n\n<AxesSubplot: title={'center': 'KS Cracovia - games won and lost'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_wins_losses\n\n\n\n\n\n  \n    \n      \n      season\n      home_wins\n      away_wins\n      home_defeats\n      away_defeats\n    \n  \n  \n    \n      0\n      2008/2009\n      6\n      1\n      -3\n      -11\n    \n    \n      1\n      2009/2010\n      5\n      4\n      -7\n      -7\n    \n    \n      2\n      2010/2011\n      7\n      1\n      -6\n      -11\n    \n    \n      3\n      2011/2012\n      3\n      1\n      -8\n      -8\n    \n    \n      4\n      2012/2013\n      0\n      0\n      0\n      0\n    \n    \n      5\n      2013/2014\n      6\n      5\n      -7\n      -6\n    \n    \n      6\n      2014/2015\n      8\n      2\n      -4\n      -9\n    \n    \n      7\n      2015/2016\n      7\n      5\n      -4\n      -5\n    \n  \n\n\n\n\nFrom the above stacked bar chart we can see a downward trend from 2009/2010 culminating in KS Cracovia being relegated in 2011/2012, with only 4 wins all season. After their return in 2013/2014 they seem to have consolidated. During the period of study KS Cracovia generally lost more games than they won, with the exception of 2015/2016 where they finished the season in a very respectable 5th place.\n\n\nHome advantage?\n\n# Run an SQl query using AVG CASE WHEN to extract % of home and away games won\ncracovia_win_pct = pd.read_sql_query('''SELECT season, \n                                     ROUND(AVG(CASE WHEN home_team_api_id = 2186 AND home_team_goal > away_team_goal THEN 1 \n                                               WHEN home_team_api_id = 2186 AND home_team_goal < away_team_goal THEN 0 END)*100,2) AS pct_home_wins, \n                                     ROUND(AVG(CASE WHEN away_team_api_id = 2186 AND away_team_goal > home_team_goal THEN 1 \n                                               WHEN away_team_api_id = 2186 AND away_team_goal < home_team_goal THEN 0 END)*100,2) AS pct_away_wins \n                                     FROM Match \n                                     GROUP BY season'''\n                                     ,con)\n\n# Plot a bar plot showing % of home and away wins for each season\ncracovia_win_pct.plot.bar(x='season', stacked=False, title ='KS Cracovia - win percentages')\n\n<AxesSubplot: title={'center': 'KS Cracovia - win percentages'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_win_pct\n\n\n\n\n\n  \n    \n      \n      season\n      pct_home_wins\n      pct_away_wins\n    \n  \n  \n    \n      0\n      2008/2009\n      66.67\n      8.33\n    \n    \n      1\n      2009/2010\n      41.67\n      36.36\n    \n    \n      2\n      2010/2011\n      53.85\n      8.33\n    \n    \n      3\n      2011/2012\n      27.27\n      11.11\n    \n    \n      4\n      2012/2013\n      NaN\n      NaN\n    \n    \n      5\n      2013/2014\n      46.15\n      45.45\n    \n    \n      6\n      2014/2015\n      66.67\n      18.18\n    \n    \n      7\n      2015/2016\n      63.64\n      50.00\n    \n  \n\n\n\n\nIn the case of KS Cracovia, home advantage certainly seems to be a factor. For an in-depth study of home advantage in European Football Leagues see this article.\n\n\nPoints make prizes\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won,lost, and drawn\ncracovia_points =  pd.read_sql_query('''SELECT season, \n                                              COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                              COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                              COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal = away_team_goal THEN id END) AS home_draws, \n                                              COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal = home_team_goal THEN id END) AS away_draws, \n                                              COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                              COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                              FROM Match \n                                              GROUP BY season'''\n                                              ,con)\n\n# Calculate points total for each season - 3 for a win, 1 for a draw, 0 for a loss\ncracovia_points[\"points\"] = cracovia_points['home_wins'] * 3 +  cracovia_points['away_wins'] * 3 + cracovia_points['home_draws'] + cracovia_points['away_draws']\n\n# Filter for the seasom and points columns for plotting\ncracovia_points = cracovia_points[[\"season\",\"points\"]]\n\n# Plot a bar plot showing total points for each season\ncracovia_points.plot.bar(x='season', stacked=False, title ='KS Cracovia - total points')\n\n<AxesSubplot: title={'center': 'KS Cracovia - total points'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2008/2009\n      30\n    \n    \n      1\n      2009/2010\n      34\n    \n    \n      2\n      2010/2011\n      29\n    \n    \n      3\n      2011/2012\n      22\n    \n    \n      4\n      2012/2013\n      0\n    \n    \n      5\n      2013/2014\n      39\n    \n    \n      6\n      2014/2015\n      37\n    \n    \n      7\n      2015/2016\n      45\n    \n  \n\n\n\n\nCracovia were relegated in 2011/2012 with 22 points.\n\n\nFinal standings\n\n# Create an extra column and insert into our original points DataFrame\ncracovia_points.insert(0,\"Position\",[14,12,14,16,0,9,12,5],True)\n\n\n# Plot a horizontal bar plot of league position\ncracovia_points.plot.barh(x='season', y='Position', title ='KS Cracovia - final league position')\n\n<AxesSubplot: title={'center': 'KS Cracovia - final league position'}, ylabel='season'>\n\n\n\n\n\n\ncracovia_position = cracovia_points\ncracovia_position \n\n\n\n\n\n  \n    \n      \n      Position\n      season\n      points\n    \n  \n  \n    \n      0\n      14\n      2008/2009\n      30\n    \n    \n      1\n      12\n      2009/2010\n      34\n    \n    \n      2\n      14\n      2010/2011\n      29\n    \n    \n      3\n      16\n      2011/2012\n      22\n    \n    \n      4\n      0\n      2012/2013\n      0\n    \n    \n      5\n      9\n      2013/2014\n      39\n    \n    \n      6\n      12\n      2014/2015\n      37\n    \n    \n      7\n      5\n      2015/2016\n      45\n    \n  \n\n\n\n\nAs we can see the trend for KS Cracovia is better since they returned to the Ekstraklasa in 2013/2014. They finished a respectable 9th on their first season back in the top league, and finished in 5th place in 2015/2016.\n\n\n\n2016/2017 to 2021/2022\nThe European Soccer Database is now somewhat out of date, so I decided to gather data for seasons 2016/2017 to 2021/2022 and create dictionaries for conversion to DataFrames.\n\nGooooooooooool !\n\n# Create a dictionary for the most recent years\ncracovia_recent = {'season': ['2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021','2021/2022'],\n        'goals_scored': [38, 40, 39, 39,28,40],\n        'goals_conceded': [43, 40, 34, 29,32,42],\n        'games_won': [6, 10, 14, 14,8,12],\n        'games_lost': [11, 11, 10, 12,9,12],\n        'games_drawn': [13,9,6,4,13,10],\n        'points':[31,39,48,46,32,46],\n        'position':[13,10,4,5,14,9]\n        } \n\n# Convert dictionary to a DataFrame\ncracovia_recent = pd.DataFrame.from_dict(cracovia_recent)\n\n\n# Filter for columns to be plotted\ncracovia_recent_goals = cracovia_recent[['season','goals_scored','goals_conceded',]]\n\n# Plot a bar plot showing goals scored and conceded each season\ncracovia_recent_goals.plot.bar(x='season', stacked=False, title ='KS Cracovia - goals for and against')\n\n<AxesSubplot: title={'center': 'KS Cracovia - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_recent_goals\n\n\n\n\n\n  \n    \n      \n      season\n      goals_scored\n      goals_conceded\n    \n  \n  \n    \n      0\n      2016/2017\n      38\n      43\n    \n    \n      1\n      2017/2018\n      40\n      40\n    \n    \n      2\n      2018/2019\n      39\n      34\n    \n    \n      3\n      2019/2020\n      39\n      29\n    \n    \n      4\n      2020/2021\n      28\n      32\n    \n    \n      5\n      2021/2022\n      40\n      42\n    \n  \n\n\n\n\nIn the last couple of years KS Cracova have conceded more than they have scored, although they seem to have added extra fire power up front in 2021/2022 with 40 goals scored compared to only 28 in 2020/2021.\n\n\nWin, lose or draw\n\n# Filter for columns to be plotted\ncracovia_recent_wins = cracovia_recent[['season','games_won','games_drawn','games_lost']]\n\n# Plot a bar plot showing goals won, lost, and drawn each season\ncracovia_recent_wins.plot.bar(x='season', stacked=False, title ='KS Cracovia - win, draw, lose')\n\n<AxesSubplot: title={'center': 'KS Cracovia - win, draw, lose'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_recent_wins\n\n\n\n\n\n  \n    \n      \n      season\n      games_won\n      games_drawn\n      games_lost\n    \n  \n  \n    \n      0\n      2016/2017\n      6\n      13\n      11\n    \n    \n      1\n      2017/2018\n      10\n      9\n      11\n    \n    \n      2\n      2018/2019\n      14\n      6\n      10\n    \n    \n      3\n      2019/2020\n      14\n      4\n      12\n    \n    \n      4\n      2020/2021\n      8\n      13\n      9\n    \n    \n      5\n      2021/2022\n      12\n      10\n      12\n    \n  \n\n\n\n\nKS Cracovia performed very well in 2018/2019 and 2019/2020. Season 2021/2022 was a year of consolidation, with an almost even split of wins, draws, and losses.\n\n\nPoints make prizes\n\n# Filter for columns to be plotted\ncracovia_recent_points = cracovia_recent[['season','points']]\n\n\n# Plot a bar plot showing total points for each season\ncracovia_recent_points.plot.bar(x='season', stacked=False, title ='KS Cracovia - total points')\n\n<AxesSubplot: title={'center': 'KS Cracovia - total points'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_recent_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2016/2017\n      31\n    \n    \n      1\n      2017/2018\n      39\n    \n    \n      2\n      2018/2019\n      48\n    \n    \n      3\n      2019/2020\n      46\n    \n    \n      4\n      2020/2021\n      32\n    \n    \n      5\n      2021/2022\n      46\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe points total of 32 for season 2020/2021 is net of a 5 point deduction imposed for corrupt activity backdated from the 2003/2004 season\nThe number of league games played increased to 34 for season 2021/2022 due to league reconstruction\n\n\n\n\n\nFinal standings\n\n# Plot a horizontal bar plot of league position\ncracovia_recent.plot.barh(x='season', y='position', title ='KS Cracovia - final league position')\n\n<AxesSubplot: title={'center': 'KS Cracovia - final league position'}, ylabel='season'>\n\n\n\n\n\n\ncracovia_position = cracovia_recent[[\"season\",\"position\"]]\ncracovia_position \n\n\n\n\n\n  \n    \n      \n      season\n      position\n    \n  \n  \n    \n      0\n      2016/2017\n      13\n    \n    \n      1\n      2017/2018\n      10\n    \n    \n      2\n      2018/2019\n      4\n    \n    \n      3\n      2019/2020\n      5\n    \n    \n      4\n      2020/2021\n      14\n    \n    \n      5\n      2021/2022\n      9\n    \n  \n\n\n\n\nMixed fortunes for KS Cracovia, an improvement from 2016/2017 and a 4th place finish in 2018/2019. Snce then things have tailed off somewhat.\n\n\n\n2022/2023\nThe Ekstraklasa league table for 2022/2023 at the time of writing is shown below. The season is on hold as the FIFA World Cup - Qatar 2022 progresses. KS Cracovia are sitting in 8th place, just four points behind Widzew and Pogon Szczecin in 3rd and 4th place respectively. Raków Częstochowa have a 9 point lead at the top of the table over nearest challengers Legia Warsaw.\n!"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#key-takeaways",
    "href": "posts/Krakow_Football/Krakow_Football.html#key-takeaways",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Key takeaways",
    "text": "Key takeaways\nWhen I first had the idea for this project I did not know if it was even possible to run SQL queries using Python within JupyterLab. This was a very satisfying project as it opens up the possibility of further project ideas using data stored within a database. The combined power of SQL and Python is formidable, and allows data which is stored in different locations and different formats to be pulled together, cleaned, manipulated, filtered and presented graphically in a way that produces meaningful insights, and facilitates data-driven decision making."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html",
    "title": "Name that Genre",
    "section": "",
    "text": "Using a dataset comprised of songs of two music genres (Hip-Hop and Rock), we will train a classifier to distinguish between the two genres based only on track information derived from Echonest (now part of Spotify). We will first make use of pandas packages in Python for subsetting the data, aggregating information, and creating plots when exploring the data for obvious trends or factors we should be aware of when doing machine learning.\nNext, we will use the scikit-learn package to predict whether we can correctly classify a song’s genre based on features such as danceability, energy, acousticness, tempo, etc. We will go over implementations of common algorithms such as PCA, logistic regression, decision trees, and so forth.\n \n\n\n\nPreparing our dataset\nPairwise relationships between continuous variables\nSplitting our data\nNormalizing the feature data\nPrincipal Component Analysis on our scaled data\nFurther visualization of PCA\nProjecting on to our features\nTrain a decision tree to classify genre\nCompare our decision tree to a logistic regression\nBalance our data for greater performance\nDoes balancing our dataset improve model bias?\nUsing cross-validation to evaluate our models"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#preparing-our-dataset",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#preparing-our-dataset",
    "title": "Name that Genre",
    "section": "1. Preparing our dataset",
    "text": "1. Preparing our dataset\nOver the past few years, streaming services with huge catalogs have become the primary means through which most people listen to their favorite music. But at the same time, the sheer amount of music on offer can mean users might be a bit overwhelmed when trying to look for newer music that suits their tastes.\nFor this reason, streaming services have looked into means of categorizing music to allow for personalized recommendations. One method involves direct analysis of the raw audio information in a given song, scoring the raw data on a variety of metrics. Today, we’ll be examining data compiled by a research group known as The Echo Nest. Our goal is to look through this dataset and classify songs as being either ‘Hip-Hop’ or ‘Rock’ - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression.\nTo begin with, let’s load the metadata about our tracks alongside the track metrics compiled by The Echo Nest. A song is about more than its title, artist, and number of listens. We have another dataset that has musical features of each track such as danceability and acousticness on a scale from -1 to 1. These exist in two different files, which are in different formats - CSV and JSON. While CSV is a popular file format for denoting tabular data, JSON (JavaScript Object Notation) is another common file format in which databases often return the results of a given query.\nLet’s start by creating two pandas DataFrames out of these files that we can merge so we have features and labels (often also referred to as X and y) for the classification later on.\n\nimport pandas as pd\nimport numpy as np\n\n# Read in the file with the track metadata \ntracks = pd.read_csv('Data/fma-rock-vs-hiphop.csv')\n\n# Read in JSON file with track metrics with the track acoustic metrics\nechonest_metrics = pd.read_json('Data/echonest-metrics.json',precise_float=True)\n\n# Merge the DataFrames on matching track_id values \n# Only retain genre_top columns of tracks (in addition to the key column that we are matching on\necho_tracks = echonest_metrics.merge(tracks[['genre_top','track_id']],on='track_id')\n\n# Inspect the resultant dataframe\necho_tracks.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4802 entries, 0 to 4801\nData columns (total 10 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   track_id          4802 non-null   int64  \n 1   acousticness      4802 non-null   float64\n 2   danceability      4802 non-null   float64\n 3   energy            4802 non-null   float64\n 4   instrumentalness  4802 non-null   float64\n 5   liveness          4802 non-null   float64\n 6   speechiness       4802 non-null   float64\n 7   tempo             4802 non-null   float64\n 8   valence           4802 non-null   float64\n 9   genre_top         4802 non-null   object \ndtypes: float64(8), int64(1), object(1)\nmemory usage: 412.7+ KB\n\n\n\n\n\n\n\n\nPandas documentation\n\n\n\nread_csv\nread_json\npd.merge\n\n\nSo our dependent variable or target is genre_top and our 8 independent variables or features are:\n- acousticness\n- danceability\n- energy\n- instrumentalness\n- liveness\n- speechiness\n- tempo\n- valence\nThe track_id is not really a feature and so we will drop that in due course."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#pairwise-relationships-between-continuous-variables",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#pairwise-relationships-between-continuous-variables",
    "title": "Name that Genre",
    "section": "2. Pairwise relationships between continuous variables",
    "text": "2. Pairwise relationships between continuous variables\nWe typically want to avoid using variables that have strong correlations with each other – hence avoiding feature redundancy – for a few reasons:\n\nTo keep the model simple and improve interpretability (with many features, we run the risk of overfitting).\nWhen our datasets are very large, using fewer features can drastically speed up our computation time.\n\nWe can visually inspect the correlation between the features by creating a correlation matrix using pandas.\n\ncorr_metrics = echo_tracks.corr()\ncorr_metrics.style.background_gradient()\n\n/tmp/ipykernel_116/408970472.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_metrics = echo_tracks.corr()\n\n\n\n\n\n  \n    \n       \n      track_id\n      acousticness\n      danceability\n      energy\n      instrumentalness\n      liveness\n      speechiness\n      tempo\n      valence\n    \n  \n  \n    \n      track_id\n      1.000000\n      -0.372282\n      0.049454\n      0.140703\n      -0.275623\n      0.048231\n      -0.026995\n      -0.025392\n      0.010070\n    \n    \n      acousticness\n      -0.372282\n      1.000000\n      -0.028954\n      -0.281619\n      0.194780\n      -0.019991\n      0.072204\n      -0.026310\n      -0.013841\n    \n    \n      danceability\n      0.049454\n      -0.028954\n      1.000000\n      -0.242032\n      -0.255217\n      -0.106584\n      0.276206\n      -0.242089\n      0.473165\n    \n    \n      energy\n      0.140703\n      -0.281619\n      -0.242032\n      1.000000\n      0.028238\n      0.113331\n      -0.109983\n      0.195227\n      0.038603\n    \n    \n      instrumentalness\n      -0.275623\n      0.194780\n      -0.255217\n      0.028238\n      1.000000\n      -0.091022\n      -0.366762\n      0.022215\n      -0.219967\n    \n    \n      liveness\n      0.048231\n      -0.019991\n      -0.106584\n      0.113331\n      -0.091022\n      1.000000\n      0.041173\n      0.002732\n      -0.045093\n    \n    \n      speechiness\n      -0.026995\n      0.072204\n      0.276206\n      -0.109983\n      -0.366762\n      0.041173\n      1.000000\n      0.008241\n      0.149894\n    \n    \n      tempo\n      -0.025392\n      -0.026310\n      -0.242089\n      0.195227\n      0.022215\n      0.002732\n      0.008241\n      1.000000\n      0.052221\n    \n    \n      valence\n      0.010070\n      -0.013841\n      0.473165\n      0.038603\n      -0.219967\n      -0.045093\n      0.149894\n      0.052221\n      1.000000\n    \n  \n\n\n\n\n\n\n\n\n\nPandas documentation\n\n\n\nDataFrame.corr"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#splitting-our-data",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#splitting-our-data",
    "title": "Name that Genre",
    "section": "3. Splitting our data",
    "text": "3. Splitting our data\nAs mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn’t find any particularly strong correlations between our features, we can now split our data into an array containing our features, and another containing the labels - the genre of the track.\nOnce we have split the data into these arrays, we will perform some preprocessing steps to optimize our model development.\n\n# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Create features by storing all values of the echo_tracks DataFrame except for the \"genre_top\" and \"track_id\" columns.\nfeatures = echo_tracks.drop([\"genre_top\",\"track_id\"],axis=1).values\n\n# Create labels\nlabels = echo_tracks[\"genre_top\"].values\n\n# Split our data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=10)\n\n\n\n\n\n\n\nsci-kit learn documentation\n\n\n\ntrain_test_split\n\n\nLet’s take a look at the shape of our features:\n\ntrain_features.shape\n\n(3601, 8)\n\n\nSo we have 3,601 rows which represent the individual tracks, and 8 columns which are our features. Let’s have a look at a few of the values for our training features and associated labels:\n\n# Show the first 5 train_features\ntrain_features[:5]\n\narray([[9.48677459e-01, 1.37199242e-01, 2.47403596e-02, 3.06704890e-03,\n        1.20902996e-01, 4.94376494e-02, 2.11283000e+02, 3.81535017e-02],\n       [1.02727000e-05, 1.18115308e-01, 7.50281301e-01, 4.93739963e-01,\n        1.57670577e-01, 4.45210657e-02, 9.51060000e+01, 3.59604392e-01],\n       [3.12262570e-02, 6.28698289e-01, 8.31143855e-01, 1.11037480e-03,\n        1.82296940e-01, 6.79014582e-02, 9.70140000e+01, 6.53131979e-01],\n       [1.67591241e-02, 3.61955450e-01, 8.72379042e-01, 9.22775365e-01,\n        1.51773090e-01, 3.86009300e-02, 1.46546000e+02, 5.32632464e-01],\n       [7.52748738e-01, 4.53939425e-01, 4.90570217e-01, 9.66552918e-01,\n        8.74566097e-02, 2.58750898e-02, 1.50057000e+02, 3.09709828e-01]])\n\n\n\n# Show the first 5 train_lables\ntrain_labels[:5]\n\narray(['Rock', 'Rock', 'Hip-Hop', 'Rock', 'Rock'], dtype=object)\n\n\nLet’s check the mean and standard deviation of our training features dataset:\n\nnp.mean(train_features),np.std(train_features)\n\n(16.185663138852977, 43.42267453551983)\n\n\n\n\n\n\n\n\nOur training features have a large standard deviation (broad range of values). The danger here is that the larger value features will dominate our model and smaller, but potentially influential features may be disregarded.\n\n\n\nWe can address this through normalizing which in simple terms squeezes the values so that they are more centred around zero. The normalized data will have a mean of zero and a standard deviation of one."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#normalizing-the-feature-data",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#normalizing-the-feature-data",
    "title": "Name that Genre",
    "section": "4. Normalizing the feature data",
    "text": "4. Normalizing the feature data\nAs mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn’t find any particular strong correlations between our features, we can instead use a common approach to reduce the number of features called principal component analysis (PCA). A comprehensive coverage of PCA can be found in this article by Matt Brems.\nIt is possible that the variance between genres can be explained by just a few features in the dataset. PCA rotates the data along the axis of highest variance, thus allowing us to determine the relative contribution of each feature of our data towards the variance between classes. However, since PCA uses the absolute variance of a feature to rotate the data, a feature with a broader range of values will overpower and bias the algorithm relative to the other features. To avoid this, we must first normalize our train and test features. There are a few methods to do this, but a common way is through standardization, such that all features have a mean = 0 and standard deviation = 1 (the resultant is a z-score).\n\n# Import the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Instantiate the Scaler without passign any agruments\nscaler = StandardScaler()\n\n# Use the fit_transform method to scale train_features and test_features\nscaled_train_features = scaler.fit_transform(train_features)\nscaled_test_features = scaler.transform(test_features)\n\n\n\n\n\n\n\nsci-kit learn documentation\n\n\n\nStandardScaler\n\n\n\n\n\n\n\n\nTo transform test features test it is important to use StandardScaler’s tranform method, after it has been fit to the training features (and potentially transformed them), rather than using fit_transform directly on the test features.\n\n\n\n\n\n\nNow that we have normalized our data, let’s take a look and see the impact on the feature values:\n\nscaled_train_features[:5]\n\narray([[ 1.24994743, -1.64927931, -2.45820448, -1.60032105, -0.44360329,\n        -0.37842314,  2.49377868, -1.55894527],\n       [-1.33744998, -1.75457157,  0.5134387 , -0.29343727, -0.19705986,\n        -0.41242335, -0.9269891 , -0.34245846],\n       [-1.25231142,  1.06248053,  0.8446325 , -1.60553256, -0.03192884,\n        -0.25073824, -0.87080909,  0.76835644],\n       [-1.29176911, -0.40922633,  1.01352202,  0.84927796, -0.23660521,\n        -0.45336354,  0.58763337,  0.31234255],\n       [ 0.71557091,  0.09827913, -0.55027614,  0.96587738, -0.6678766 ,\n        -0.54136801,  0.69101283, -0.53127759]])\n\n\nLet’s check the mean and standard deviation of our normalized data:\n\nnp.mean(scaled_train_features),np.std(scaled_train_features)\n\n(-6.010804800914205e-16, 1.0)\n\n\n\n\n\n\n\n\nThat’s much better. Our normalized feature values are all centred around zero (mean is basically zero, and standard deviation is one), thus reducing the risk of a particular feature dominating our model."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#principal-component-analysis-on-our-scaled-data",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#principal-component-analysis-on-our-scaled-data",
    "title": "Name that Genre",
    "section": "5. Principal Component Analysis on our scaled data",
    "text": "5. Principal Component Analysis on our scaled data\n\nNow that we have preprocessed our data, we are ready to use PCA to determine by how much we can reduce the dimensionality of our data. We can use scree-plots and cumulative explained ratio plots to find the number of components to use in further analyses.\nScree-plots display the number of components against the variance explained by each component, sorted in descending order of variance. Scree-plots help us get a better sense of which components explain a sufficient amount of variance in our data. When using scree plots, an ‘elbow’ (a steep drop from one data point to the next) in the plot is typically used to decide on an appropriate cutoff.\n\n# This is just to make plots appear in the notebook\n%matplotlib inline\n\n# Import our plotting module, and PCA class\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Instantiate our PCA class\npca = PCA()\n\n# Fit the model on our scaled_train_features\npca.fit(scaled_train_features)\n\n# Get the number of components\ncomponents = pca.n_components_\n\n# Retrieve the explained variance ratio\nexp_variance = pca.explained_variance_ratio_\n\n# plot the explained variance using a barplot\nfig, ax = plt.subplots()\nax.bar(range(components), exp_variance)\nax.set_xlabel('Principal Component #')\n\nText(0.5, 0, 'Principal Component #')\n\n\n\n\n\n\n\n\n\n\n\nsci-kit learn documentation\n\n\n\nPrincipal component analysis\n\n\n\n\n\n\n\n\n\nmatplotlib documentation\n\n\n\nbar plot"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#further-visualization-of-pca",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#further-visualization-of-pca",
    "title": "Name that Genre",
    "section": "6. Further visualization of PCA",
    "text": "6. Further visualization of PCA\nUnfortunately, there does not appear to be a clear elbow in this scree plot, which means it is not straightforward to find the number of intrinsic dimensions using this method.\nBut all is not lost! Instead, we can also look at the cumulative explained variance plot to determine how many features are required to explain, say, about 85% of the variance (cutoffs are somewhat arbitrary here, and usually decided upon by ‘rules of thumb’). Once we determine the appropriate number of components, we can perform PCA with that many components, ideally reducing the dimensionality of our data.\n\n# Import numpy\nimport numpy as np\n\n# Calculate the cumulative sums of our explained variance\ncum_exp_variance = np.cumsum(exp_variance)\n\n# Plot the cumulative explained variances and look for the no. of components at which we can account for >85% of our variance\nfig, ax = plt.subplots()\nax.plot(range(components), cum_exp_variance)\n\n# Draw a dashed line at 0.85\nax.axhline(y=0.85, linestyle='--')\n\n<matplotlib.lines.Line2D at 0x7f59f340c220>\n\n\n\n\n\n\n\n\n\n\n\nnumpy documentation\n\n\n\ncumsum()\n\n\n\n\n\n\n\n\nsklean documentation\n\n\n\nPCA"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#projecting-on-to-our-features",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#projecting-on-to-our-features",
    "title": "Name that Genre",
    "section": "7. Projecting on to our features",
    "text": "7. Projecting on to our features\nWe saw from the plot that 6 features (remember indexing starts at 0) can explain 85% of the variance! Therefore, we can use 6 components to perform PCA and reduce the dimensionality of our train and test features.\n\n# Perform PCA with the optimal no. of components from our cumulative explained variance plot\npca = PCA(n_components=6,random_state=10)\n\n# Fit and transform the scaled training features using pca\ntrain_pca = pca.fit_transform(scaled_train_features)\n\n# Fit and transform the scaled test features using pca\ntest_pca = pca.fit_transform(scaled_test_features)\n\n\ntrain_pca.shape\n\n(3601, 6)\n\n\n\n# Show the first 5 train_pca\ntrain_pca[:5]\n\narray([[-0.94683521, -0.98726024,  2.09691406,  1.82221026, -1.88745714,\n         2.63086297],\n       [-1.16474503,  0.81643176, -0.29364819, -1.2819193 , -0.85195312,\n        -0.22963785],\n       [ 1.52161734,  1.4246482 , -0.92071409, -1.25676649, -0.10178816,\n        -0.07969312],\n       [-0.97593229,  1.23672376, -1.10458784,  0.1487983 ,  0.02900411,\n        -0.0715869 ],\n       [-0.78621628, -1.10767364, -0.34725279,  0.89060792, -0.1259162 ,\n         0.3696838 ]])\n\n\n\n\n\n\n\n\nFollowing our Principal Component Analysis and dimensionality reduction, the train and test datasets now only include values for 6 features, reduced from our original 8."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#train-a-decision-tree-to-classify-genre",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#train-a-decision-tree-to-classify-genre",
    "title": "Name that Genre",
    "section": "8. Train a decision tree to classify genre",
    "text": "8. Train a decision tree to classify genre\nNow we can use the lower dimensional PCA projection of the data to classify songs into genres.\nHere, we will be using a simple algorithm known as a decision tree. Decision trees are rule-based classifiers that take in features and follow a ‘tree structure’ of binary decisions to ultimately classify a data point into one of two or more categories. In addition to being easy to both use and interpret, decision trees allow us to visualize the ‘logic flowchart’ that the model generates from the training data.\nHere is an example of a decision tree that demonstrates the process by which an input image (in this case, of a shape) might be classified based on the number of sides it has and whether it is rotated.\n\n\n# Import Decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate our DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=10)\n                              \n# Fit our DecisionTreeClassifier to the training data\ntree.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_tree = tree.predict(test_pca)\n\n# Show the first 10 labels\npred_labels_tree[:10]\n\narray(['Rock', 'Hip-Hop', 'Rock', 'Hip-Hop', 'Hip-Hop', 'Rock', 'Hip-Hop',\n       'Rock', 'Hip-Hop', 'Rock'], dtype=object)\n\n\n\n\n\n\n\n\nscikit-learn documentation\n\n\n\nDecisionTreeClassifier"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#compare-our-decision-tree-to-a-logistic-regression",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#compare-our-decision-tree-to-a-logistic-regression",
    "title": "Name that Genre",
    "section": "9. Compare our decision tree to a logistic regression",
    "text": "9. Compare our decision tree to a logistic regression\nAlthough our tree’s performance is decent, it’s a bad idea to immediately assume that it’s therefore the perfect tool for this job – there’s always the possibility of other models that will perform even better! It’s always a worthwhile idea to at least test a few other algorithms and find the one that’s best for our data.\nSometimes simplest is best, and so we will start by applying logistic regression. Logistic regression makes use of what’s called the logistic function to calculate the odds that a given data point belongs to a given class. Once we have both models, we can compare them on a few performance metrics, such as false positive and false negative rate (or how many points are inaccurately classified).\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate our LogisticRegression \nlogreg = LogisticRegression(random_state=10)\n\n# Fit our Logistic Regression model to the training data\nlogreg.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_logit = logreg.predict(test_pca)\n\n# Create the classification report for both models\nfrom sklearn.metrics import classification_report\n\nclass_rep_tree = classification_report(test_labels,pred_labels_tree)\nclass_rep_log = classification_report(test_labels,pred_labels_logit)\n\nprint(\"Decision Tree: \\n\", class_rep_tree)\nprint(\"Logistic Regression: \\n\", class_rep_log)\n\nDecision Tree: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.52      0.59      0.55       235\n        Rock       0.90      0.87      0.88       966\n\n    accuracy                           0.81      1201\n   macro avg       0.71      0.73      0.72      1201\nweighted avg       0.82      0.81      0.82      1201\n\nLogistic Regression: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.75      0.55      0.63       235\n        Rock       0.90      0.96      0.93       966\n\n    accuracy                           0.88      1201\n   macro avg       0.82      0.75      0.78      1201\nweighted avg       0.87      0.88      0.87      1201\n\n\n\n\n\n\n\n\n\nscikit-learn documentation\n\n\n\nLogisticRegression()\n\n\n\n\n\n\n\n\nscikit-learn documentation\n\n\n\nclassification_report()"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#balance-our-data-for-greater-performance",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#balance-our-data-for-greater-performance",
    "title": "Name that Genre",
    "section": "10. Balance our data for greater performance",
    "text": "10. Balance our data for greater performance\nBoth our models do well, with the Decision Tree scoring average precision of 82% and Logistic Regression scoring average precision of 87%. However, looking at our classification report, we can see that rock songs are fairly well classified, but hip-hop songs are disproportionately misclassified as rock songs.\nWhy might this be the case? Well, just by looking at the number of data points we have for each class (966 for rock, 235 for hip-hop) we can see that our data is imbalanced, potentially skewing our model’s ability to distinguish between classes. This also tells us that most of our model’s accuracy is driven by its ability to classify just rock songs, which is less than ideal.\nTo account for this, we can weight the value of a correct classification in each class inversely to the occurrence of data points for each class. Since a correct classification for “Rock” is not more important than a correct classification for “Hip-Hop” (and vice versa), we only need to account for differences in sample size of our data points when weighting our classes here, and not relative importance of each class.\n\n# Subset only the hip-hop tracks, and then only the rock tracks\nhop_only = echo_tracks.loc[echo_tracks['genre_top'] == 'Hip-Hop']\nrock_only = echo_tracks.loc[echo_tracks['genre_top'] == 'Rock']\n\n# sample the rocks songs to be the same number as there are hip-hop songs\nrock_only = rock_only.sample(hop_only.shape[0],random_state=10)\n\n# concatenate the dataframes rock_only and hop_only\nrock_hop_bal = pd.concat([rock_only,hop_only])\n\n# The features, labels, and pca projection are created for the balanced dataframe\nfeatures = rock_hop_bal.drop(['genre_top', 'track_id'], axis=1) \nlabels = rock_hop_bal['genre_top']\n\n# Redefine the train and test set with the pca_projection from the balanced data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features,labels, random_state=10)\n\ntrain_pca = pca.fit_transform(scaler.fit_transform(train_features))\ntest_pca = pca.transform(scaler.transform(test_features))\n\n\n\n\n\n\n\npandas documentation\n\n\n\nDataFrame.loc[]\n\n\n\n\n\n\n\n\npandas documentation\n\n\n\nconcat()\n\n\n\n\n\n\n\n\npandas documentation\n\n\n\nDataFrame.sample()"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#does-balancing-our-dataset-improve-model-bias",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#does-balancing-our-dataset-improve-model-bias",
    "title": "Name that Genre",
    "section": "11. Does balancing our dataset improve model bias?",
    "text": "11. Does balancing our dataset improve model bias?\nWe’ve now balanced our dataset, but in doing so, we’ve removed a lot of data points that might have been crucial to training our models. Let’s test to see if balancing our data improves model bias towards the “Rock” classification while retaining overall classification performance. Note that we have already reduced the size of our dataset and will go forward without applying any dimensionality reduction. In practice, we would consider dimensionality reduction more rigorously when dealing with vastly large datasets and when computation times become prohibitively large.\n\n# Instantiate our DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=10)\n\n# Fit our DecisionTree Classifier model to the balanced training set\ntree.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_tree = tree.predict(test_pca)\n\n# Instantiate our LogisticRegression \nlogreg = LogisticRegression(random_state=10)\n\n# Fit our Logistic Regression model to the balanced training set\nlogreg.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_logit = logreg.predict(test_pca)\n\n# Compare the models\nprint(\"Decision Tree: \\n\", classification_report(test_labels,pred_labels_tree))\nprint(\"Logistic Regression: \\n\", classification_report(test_labels,pred_labels_logit))\n\nDecision Tree: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.82      0.77      0.79       230\n        Rock       0.78      0.82      0.80       225\n\n    accuracy                           0.80       455\n   macro avg       0.80      0.80      0.80       455\nweighted avg       0.80      0.80      0.80       455\n\nLogistic Regression: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.84      0.80      0.82       230\n        Rock       0.81      0.85      0.83       225\n\n    accuracy                           0.82       455\n   macro avg       0.82      0.82      0.82       455\nweighted avg       0.83      0.82      0.82       455\n\n\n\nSuccess! Balancing our data has removed bias towards the more prevalent class - the precision scores for Hip-Hop and rock are now similar. To get a good sense of how well our models are actually performing, we can apply what’s called cross-validation (CV). This step allows us to compare models in a more rigorous fashion."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#using-cross-validation-to-evaluate-our-models",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#using-cross-validation-to-evaluate-our-models",
    "title": "Name that Genre",
    "section": "12. Using cross-validation to evaluate our models",
    "text": "12. Using cross-validation to evaluate our models\nBefore we can perform cross-validation we will need to create pipelines to scale our data, perform PCA, and instantiate our model of choice - DecisionTreeClassifier or LogisticRegression.\nSince the way our data is split into train and test sets can impact model performance, CV attempts to split the data multiple ways and test the model on each of the splits. Although there are many different CV methods, all with their own advantages and disadvantages, we will use what’s known as K-fold CV here. K-fold first splits the data into K different, equally sized subsets. Then, it iteratively uses each subset as a test set while using the remainder of the data as train sets. Finally, we can then aggregate the results from each fold for a final model performance score.\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\n\ntree_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=5)), \n                      (\"tree\", DecisionTreeClassifier(random_state=10))])\n\nlogreg_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=5)), \n                        (\"logreg\", LogisticRegression(random_state=10))])\n\n# Set up our K-fold cross-validation with 10 folds\nkf = KFold(n_splits=10)\n\n# Train our models using KFold cv\ntree_score = cross_val_score(tree_pipe, features,labels,cv=kf)\nlogit_score = cross_val_score(logreg_pipe, features,labels,cv=kf)\n\n# Print the mean of each array of scores\nprint(\"Decision Tree:\", np.mean(tree_score), \"Logistic Regression:\", np.mean(logit_score))\n\nDecision Tree: 0.7417582417582418 Logistic Regression: 0.7835164835164835\n\n\nSo our final performance scores are:\n- Decision Tree 74.2%\n- Logistic Regression 78.4%\n\n\n\n\n\n\nsklearn documentation\n\n\n\nKFold()\n\n\n\n\n\n\n\n\nsklearn documentation\n\n\n\ncross_val_score()"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#key-takeaways",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#key-takeaways",
    "title": "Name that Genre",
    "section": "Key takeaways",
    "text": "Key takeaways\nIn this project we learned how to:\n\nload in two different file types, csv and json, using pandas pd.read\nmerge two datasets using pandas .merge\nexplore and visulaize the relationship between continuous variables using pandas .corr()\nsplit our data into a training and test set using scikit-learn train_test_split\nnormalize our data using StandardScaler from scikit-learn\nimprove model performance by reducing the dimensionality of our data via Principle Component Analysis\nvisualize feature importance using scree and cumulative explained variance plots\ntrain a Decison Tree and Logistic Regression model\nenhance model performance by balancing our data\nevaluate model performance using cross-validation"
  },
  {
    "objectID": "posts/PySpark/Spark.html",
    "href": "posts/PySpark/Spark.html",
    "title": "What is Spark, anyway?",
    "section": "",
    "text": "I’ve noticed that Apache Spark is cited as a requirement on many data science job specs. My natural curiosity led me to the Introduction to PySpark course available through DataCamp which is a superb coding portal, offering slides, interactive lessons, projects, courses and career tracks. DataCamp was one of my first ports of call when I decided to upskill and make the transition from chartered accountancy to data science.\nToday we will learn how to use Spark from Python! Spark is a tool for doing parallel computation with large datasets and it integrates well with Python. PySpark is the Python package that makes the magic happen. We’ll use this package to work with data about flights from Portland and Seattle. We’ll learn to wrangle this data and build a whole machine learning pipeline to predict whether or not flights will be delayed. Get ready to put some Spark in your Python code and dive into the world of high-performance machine learning!\n\n\nIn this section, we’ll learn how Spark manages data and how to read and write tables from Python.\n\n\nSpark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\nHowever, with greater computing power comes greater complexity.\nDeciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n- Is my data too big to work with on a single machine?\n- Can my calculations be easily parallelized?\n\n\n\nThe first step in using Spark is connecting to a cluster.\nIn practice, the cluster will be hosted on a remote machine that’s connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called workers. The master sends the workers data and calculations to run, and they send their results back to the master.\nWhen you’re just getting started with Spark it’s simpler to just run a cluster locally. Creating the connection is as simple as creating an instance of the SparkContext class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you’re connecting to.\nAn object holding all these attributes can be created with the SparkConf() constructor. Take a look at the documentation for all the details!\n\n\n\n\n\n\nHow do you connect to a Spark cluster from PySpark?\n\n\n\n\n\nCreate an instance of the SparkContext class.\n\n\n\n\n\n\nYou’ll probably notice that code takes longer to run than you might expect. This is because Spark is some serious software. It takes more time to start up than you might be used to. You may also find that running simpler computations might take longer than expected. That’s because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions!\nPyspark can be installed from the command line using:\n\nmamba install pyspark\n\nFor detailed installation guidance consult the Pyspark documentation.\n\n# import SparkContext class\nfrom pyspark import SparkContext as sc\nfrom pyspark.sql import SparkSession\n\n\n# Create SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nprint('PySpark Version :'+spark.version)\nprint('PySpark Version :'+spark.sparkContext.version)\n\n\n\n\nSpark’s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you’ll be using the Spark DataFrame abstraction built on top of RDDs.\nThe Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\nWhen you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it’s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\nTo start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.\n\n\n\n\n\n\nWhat is the advantage of Spark DataFrames over RDDs?\n\n\n\n\n\nOperations using DataFrames are automatically optimized.\n\n\n\n\n\n\nWe’ve already created a SparkSession called spark, but what if you’re not sure there already is one?\n\n\n\n\n\n\nCreating multiple SparkSessions and SparkContexts can cause issues.\n\n\n\nIt’s best practice to use the SparkSession.builder.getOrCreate() method. This returns an existing SparkSession if there’s already one in the environment, or creates a new one if necessary!\n\n\n\n# Import SparkSession from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create my_spark\nmy_spark = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(my_spark)\n\n\n\n\nmy_spark.PNG\n\n\n\n\n\nOnce we’ve created a SparkSession, we can start poking around to see what data is in our cluster! Our SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\nOne of the most useful is the .listTables() method, which returns the names of all the tables in our cluster as a list.\n\n# Print the tables in the catalog\nprint(spark.catalog.listTables())\n\n[Table(name=‘flights’, database=None, description=None, tableType=‘TEMPORARY’, isTemporary=True)]\n\n\n\nOne of the advantages of the DataFrame interface is that you can run SQL queries on the tables in your Spark cluster.\nAs we saw above, one of the tables in our cluster is the flights table. This table contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015. Running a query on this table is as easy as using the .sql() method on your SparkSession. This method takes a string containing the query and returns a DataFrame with the results!\nIf you look closely, you’ll notice that the table flights is only mentioned in the query, not as an argument to any of the methods. This is because there isn’t a local object in our environment that holds that data, so it wouldn’t make sense to pass the table as an argument.\n\n# get the first 10 rows of the flights table using SQL query\nquery = \"FROM flights SELECT * LIMIT 10\"\n\n# assign SQL query \nflights10 = spark.sql(query)\n\n# Show the results\nflights10.show()\n\n\n\n\nQuery-ious.PNG\n\n\n\n\n\nSuppose we’ve run a query on our huge dataset and aggregated it down to something a little more manageable.\nSometimes it makes sense to then take that table and work with it locally using a tool like pandas. Spark DataFrames make that easy with the .toPandas() method. Calling this method on a Spark DataFrame returns the corresponding pandas DataFrame. It’s as simple as that!\nThis time the query counts the number of flights to each airport from SEA and PDX.\n\n# Count the number of flights to each airport from SEA and PDX using a SQL query\nquery = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n\n# assign the query \nflight_counts = spark.sql(query)\n\n# Convert the results to a pandas DataFrame\npd_counts = flight_counts.toPandas()\n\n# Print the head of pd_counts\nprint(pd_counts.head())\n\n\n\n\npandafy.PNG\n\n\n\n\n\nIn the last section, we saw how to move data from Spark to pandas. However, maybe we want to go in the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well.\nThe .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\nThe output of this method is stored locally, not in the SparkSession catalog. This means that we can use all the Spark DataFrame methods on it, but we can’t access the data in other contexts.\nFor example, a SQL query (using the .sql() method) that references our DataFrame will throw an error. To access the data in this way, we have to save it as a temporary table. We can do this using the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you’d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\nThere is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. We’ll use this method to avoid running into problems with duplicate tables.\nCheck out the diagram below to see all the different ways our Spark data structures interact with each other:\n\n\n# Create a pandas Dataframe of random numbers\npd_temp = pd.DataFrame(np.random.random(10))\n\n# Create a Spark DataFrame from the pandas DataFrame\nspark_temp = spark.createDataFrame(pd_temp)\n\n# Examine the list of tables in our Spark cluster and verify that the new DataFrame is not present\nprint(spark.catalog.listTables())\n\n# Register the newly created Spark DataFrame as a temporary table - name it \"temp\"\nspark_temp.createOrReplaceTempView(\"temp\")\n\n# Examine the list of tables in the catalog again\nprint(spark.catalog.listTables())\n\n\n\n\nput_some_spark.PNG\n\n\n\n\n\nNow we know how to put data into Spark via pandas, but you’re probably wondering why deal with pandas at all? Wouldn’t it be easier to just read a text file straight into Spark? Of course it would! Luckily, our SparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these we can create a DataFrame from a .csv file just like with regular pandas DataFrames!\nThe variable file_path is a string with the path to the file airports.csv. This file contains information about different airports all over the world.\n\n# Set the path\nfile_path = \"/usr/local/share/datasets/airports.csv\"\n\n# Read in the airports data creating a Spark DataFrame\nairports = spark.read.csv(file_path, header=True)\n\n# Show the data\nairports.show()\n\n\n\n\ndropping middle man.PNG\n\n\n\n\n\n\nIn this section, we’ll learn about the pyspark.sql module, which provides optimized data queries to your Spark session.\n\n\nIn this section, we’ll learn how to use the methods defined by Spark’s DataFrame class to perform common data operations.\nLet’s look at performing column-wise operations. In Spark we can do this using the .withColumn() method, which takes two arguments. First, a string with the name of our new column, and second the new column itself. The new column must be an object of class Column. Creating one of these is as easy as extracting a column from our DataFrame using df.colName.\n\n\n\n\n\n\nUpdating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can’t be changed, and so columns can’t be updated in place. Thus, all these methods return a new DataFrame.\n\n\n\n\n\n\n\n\n\n\n\n\nTo overwrite the original DataFrame we must reassign the returned DataFrame using the method like so:\n\n\n\ndf = df.withColumn(\"newCol\", df.oldCol + 1)\n\n\nThe above code creates a DataFrame with the same columns as df plus a new column, newCol, where every entry is equal to the corresponding entry from oldCol, plus one. To overwrite an existing column, just pass the name of the column as the first argument!\n\n# Use the spark.table() method to create a DataFrame containing the values of the flights table\nflights = spark.table(\"flights\")\n\n# Add a new column \"duration_hrs\" - the column air_time includes minutes\nflights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n\n# Show the head\nflights.show()\n\n\n\n\nflights.PNG\n\n\n\n\n\nA SQL query returns a table derived from one or more tables contained in a database. Every SQL query is made up of commands that tell the database what you want to do with the data. The two commands that every query has to contain are SELECT and FROM.\nThe SELECT command is followed by the columns we want in the resulting table.\nThe FROM command is followed by the name of the table that contains those columns. The minimal SQL query is:\nSELECT * FROM my_table;\nThe * selects all columns, so this returns the entire table named my_table.\nSimilar to .withColumn(), we can do column-wise computations within a SELECT statement. For example,\nSELECT origin, dest, air_time / 60 FROM flights;\nreturns a table with the origin, destination, and duration in hours for each flight.\n\n\n\nAnother commonly used command is *WHERE. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where our condition is true. For example, if we had a table of students and grades we could do:\nSELECT * FROM students\nWHERE grade = 'A';\nto select all the columns and the rows containing information about students who got As.\n\n\n\nAnother common database task is aggregation. That is, reducing our data by breaking it into chunks and summarizing each chunk. This is done in SQL using the GROUP BY command. This command breaks our data into groups and applies a function from our SELECT statement to each group.\nFor example, if we wanted to COUNT the number of flights from each of two origin destinations, we could use the query\nSELECT COUNT(*) FROM flights\nGROUP BY origin;\nGROUP BY origin tells SQL that we want the output to have a row for each unique value of the origin column. The SELECT statement selects the values we want to populate each of the columns. Here, we want to COUNT() every row in each of the groups.\nIt’s possible to GROUP BY more than one column. When we do this, the resulting table has a row for every combination of the unique values in each column. The following query counts the number of flights from SEA and PDX to every destination airport:\nSELECT origin, dest, COUNT(*) FROM flights\nGROUP BY origin, dest;\nThe output will have a row for every combination of the values in origin and dest (i.e. a row listing each origin and destination that a flight flew to). There will also be a column with the COUNT() of all the rows in each group.\n\n\n\nLet’s now talk about the analogous operations using Spark DataFrames.\nLet’s take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL’s WHERE clause. The .filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values.\nFor example, the following two expressions will produce the same output:\nflights.filter(\"air_time > 120\").show()\nflights.filter(flights.air_time > 120).show()\nNotice that in the first case, we pass a string to .filter(). In SQL, we would write this filtering task as SELECT * FROM flights WHERE air_time > 120. Spark’s .filter() can accept any expression that could go in the WHERE clause of a SQL query (in this case, “air_time > 120”), as long as it is passed as a string. Notice that in this case, we do not reference the name of the table in the string – as we wouldn’t in the SQL request.\nIn the second case, we actually pass a column of boolean values to .filter(). Remember that flights.air_time > 120 returns a column of boolean values that has True in place of those records in flights.air_time that are over 120, and False otherwise.\n\n# Filter all flights that flew over 1000 miles by passing a string\nlong_flights1 = flights.filter(\"distance > 1000\").show()\n\n# Filter flights that flew over 1000 miles by passing a column of boolean values\nlong_flights2 = flights.filter(flights.distance > 1000).show()\n\nBoth methods return the same output:\n\n\n\nFiltering using Spark.PNG\n\n\n\n\n\nThe Spark variant of SQL’s SELECT is the .select() method. This method takes multiple arguments - one for each column we want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). When we pass a column object, we can perform operations like addition or subtraction on the column to change the data contained in it, much like inside .withColumn().\nThe difference between .select() and .withColumn() methods is that .select() returns only the columns you specify, while .withColumn() returns ALL the columns of the DataFrame in addition to the one we defined.\n\n\n\n\n\n\nIt’s often a good idea to drop columns you don’t need at the beginning of an operation so that you’re not dragging around extra data as you’re wrangling\n\n\n\nIn this case, we would use .select() and not .withColumn().\n\n\n\n# Select the columns \"tailnum\", \"origin\", and \"dest\" from flights by passing the column names as strings\nselected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n\n#Select the columns \"origin\", \"dest\", and \"carrier\" using the df.colName syntax and then filter the result  \ntemp = flights.select(flights.origin, flights.dest, flights.carrier)\n\n# Define first filter\nfilterA = flights.origin == \"SEA\"\n\n# Define second filter\nfilterB = flights.dest == \"PDX\"\n\n# Filter the data, first by filterA then by filterB and show result\nselected2 = temp.filter(filterA).filter(filterB).show()\n\n\n\n\nselecting using spark.PNG\n\n\nSimilar to SQL, we can also use the .select() method to perform column-wise operations. When we’re selecting a column using the df.colName notation, we can perform any column operation and the .select() method will return the transformed column. For example,\nflights.select(flights.air_time/60)\nreturns a column of flight durations in hours instead of minutes. We can also use the .alias() method to rename a column you’re selecting. So if you wanted to .select() the column duration_hrs (which isn’t in oour DataFrame) we could do\nflights.select((flights.air_time/60).alias(\"duration_hrs\"))\nThe equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string:\nflights.selectExpr(\"air_time/60 as duration_hrs\")\nwith the SQL as keyword being equivalent to the .alias() method. To select multiple columns, we can pass multiple strings.\n\n# Calculate average speed and use the .alias() method to name the column\navg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n\n# Select the required columns\nspeed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed).show()\n\n# Create the same table using an SQL expression\nspeed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\").show()\n\nBoth methods return the same output:\n\n\n\nselecting using spark 2.PNG\n\n\n\n\n\nAll of the common aggregation methods, like .min(), .max(), and .count() are GroupedData methods. These are created by calling the .groupBy() DataFrame method. All we have to do to use these functions is call that method on your DataFrame. For example, to find the minimum value of a column, col, in a DataFrame, df, we could do\ndf.groupBy().min(\"col\").show()\nThis creates a GroupedData object (so we can use the .min() method), then finds the minimum value in col, and returns it as a DataFrame.\n\n# Find the length of the shortest flight that left from PDX in terms of distance\nflights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n\n\n\n\nmin_distance.PNG\n\n\n\n# Find the longest flight from SEA in terms of air time\nflights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()\n\n\n\n\nmax_air_time.PNG\n\n\nTo get us familiar with more of the built in aggregation methods, here’s a few more exercises involving the flights table!\n\n# Get the average air time of Delta Airlines flights (where the carrier column has the value \"DL\") that left SEA - using the .avg() method\nflights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n\n# get the total number of hours all planes in this dataset spent in the air - using the .sum() method \nflights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n\n\n\n\naggregate_spark_2.PNG\n\n\n\n\n\nPart of what makes aggregating so powerful is the addition of groups. PySpark has a whole class devoted to grouped data frames: pyspark.sql.GroupedData. We’ve learned how to create a grouped DataFrame by calling the .groupBy() method on a DataFrame with no arguments.\nNow we’ll see that when we pass the name of one or more columns in our DataFrame to the .groupBy() method, the aggregation methods behave like when we use a GROUP BY statement in a SQL query!\n\n# Create a DataFrame that is grouped by the column tailnum\nby_plane = flights.groupBy(\"tailnum\")\n\n# se the .count() method with no arguments to count the number of flights each plane made.\nby_plane.count().show()\n\n\n\n\ntail_number.PNG\n\n\n\n# Create a DataFrame that is grouped by the column origin\nby_origin = flights.groupBy(\"origin\")\n\n# Find the .avg() of the air_time column to find average duration of flights from PDX and SEA.\nby_origin.avg(\"air_time\").show()\n\n\n\n\nby_origin.PNG\n\n\nIn addition to the GroupedData methods we’ve already seen, there is also the .agg() method. This method lets us pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.\nThis submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table.\n\n# Import the submodule pyspark.sql.functions\nimport pyspark.sql.functions as F\n\n# Create a GroupedData table, grouped by month and dests. Refer to the two columns by passing both strings as separate arguments.\nby_month_dest = flights.groupBy(\"month\", \"dest\")\n\n# Get the average dep_delay in each month for each destination, by using the .avg() method \nby_month_dest.avg(\"dep_delay\").show()\n\n\n\n\nave_dep_delay.PNG\n\n\n\n# Find the standard deviation of dep_delay by using the .agg() method with the function F.stddev()\nby_month_dest.agg(F.stddev(\"dep_delay\")).show()\n\n\n\n\nstd_dev_dep_delay.PNG\n\n\n\n\n\nAnother very common data operation is the join. Joins are a whole topic unto themselves, so in this blog we’ll just look at simple joins. If you’d like to learn more about joins, you can take a look here. A join will combine two different tables along a column that they share. This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table.\nFor example, suppose that we want to know more than just the tail number of the plane that flew a flight. This information isn’t in the flights table because the same plane flies many different flights over the course of two years, so including this information in every row would result in a lot of duplication. To avoid this, we’d have a second table that has only one row for each plane and whose columns list all the information about the plane, including its tail number. You could call this table planes.\nWhen we join the flights table to this table of airplane information, we’re adding all the columns from the planes table to the flights table. To fill these columns with information, we’ll look at the tail number from the flights table and find the matching one in the planes table, and then use that row to fill out all the new columns.\nNow we’ll have a much bigger table than before, but now every row has all information about the plane that flew that flight!\nIn PySpark, joins are performed using the DataFrame method .join(). This method takes three arguments\n\nthe second DataFrame that you want to join with the first one.\non - the name of the key column(s) as a string. The names of the key column(s) must be the same in each table.\nhow - specifies the kind of join to perform.\n\n\n# Examine the airports DataFrame\nairports.show()\n\n\n\n\nairports.PNG\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe faa column refers to the alphanumeric code identifying United States airports. The dest column of the flights DataFrame has this same information and will therefore be our key but it is in a different format (the typical airport ‘LAX’ format).\nPrior to joining we must therefore rename the faa column.\n\n\n\n# Rename the faa column to enable joining\nairports = airports.withColumnRenamed(\"faa\", \"dest\")\n\n# Join the flights to the airports DataFrame \n# First argument should be the other DatafRame\n# Second argument (on) should be the key\n# Thirs argument (how) should be 'leftover'\nflights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n\n# Examine the new DataFrame\nflights_with_airports.show()\n\n\n\n\nflights_with_airports.PNG\n\n\n\n\n\n\nPySpark has built-in, cutting-edge machine learning routines, along with utilities to create full machine learning pipelines. We’ll learn about them in this section.\nAt the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\nTransformer classes have a .transform() method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class Bucketizer to create discrete bins from a continuous feature or the class PCA to reduce the dimensionality of your dataset using principal component analysis.\nEstimator classes all implement a .fit() method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a StringIndexerModel for including categorical data* saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression.\n\n\nIn the next two sections we’ll be working towards building a model that predicts whether or not a flight will be delayed based on the flights data we’ve been working with. This model will also include information about the plane that flew the route, so the first step is to join the two tables: flights and planes!\n\n# Rename year column of planes to avoid duplicate column names\nplanes = planes.withColumnRenamed(\"year\",\"plane_year\")\n\n# Join the planes table to the flights table on tailnum\nmodel_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")\n\n\n\n\n\n\n\n\n\n\nWhat kind of data does Spark need for modeling?\n\n\n\n\n\nDoubles\n\n\n\nBefore we start modeling, it’s important to know that Spark only handles numeric data. That means all of the columns in our DataFrame must be either integers or decimals (called ‘doubles’ in Spark). When we imported our data, we let Spark guess what kind of information each column held. Unfortunately, Spark doesn’t always guess right and you can see that some of the columns in our DataFrame are strings containing numbers as opposed to actual numeric values.\n\n\n\n\n\n\nTo find out the datatype of each of the columns of a dataset use .dtypes\n\n\n\n\n\n\n\nmodel_data.dtypes\n\n\n\n\ndata_types_before.PNG\n\n\nAs we can see all columns are of type ‘string’.\nTo remedy this, we can use the .cast() method in combination with the .withColumn() method. It’s important to note that .cast() works on columns, while .withColumn() works on DataFrames.\nThe only argument we need to pass to .cast() is the kind of value we want to create, in string form. For example, to create integers, we’ll pass the argument “integer” and for decimal numbers you’ll use “double”.\nWe can put this call to .cast() inside a call to .withColumn() to overwrite the already existing column, just like we did in the previous section.\n\n\n\nNow we’ll use the .cast() method to convert all the appropriate columns from our DataFrame model_data to integers! To convert the type of a column using the .cast() method, we can write code like this:\ndataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\")\n\n# Cast the columns to integers\nmodel_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n\nmodel_data.dtypes\n\n\n\n\ndata_types_after.PNG\n\n\nWe can see that the our arr_delay, air_time, month, and plane_year columns are now of the required type - ‘int’.\n\n\n\nNow we have converted the column plane_year to an integer. This column holds the year each plane was manufactured. However, our model will use the planes’ age, which is slightly different from the year it was made!\n\n# Create the column plane_age by subtracting the year of manufacture from the year of the flight\nmodel_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n\n\n\n\nConsider that we’re modeling a yes or no question: is the flight late? However, our data contains the arrival delay in minutes for each flight. Thus, we’ll need to create a boolean column which indicates whether the flight was late or not!\n\n# Create an is_late column \nmodel_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n\n# Convert our is_late column to an integer\nmodel_data = model_data.withColumn(\"label\",model_data.is_late.cast(\"integer\"))   \n\n# Remove missing values\nmodel_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")\n\n\n\n\n\n\n\nWarning\n\n\n\n“label” is the default name for the response variable in Spark’s machine learning routines\n\n\nGreat. Now we’ve defined the label column - the column that we’re going to use as the outcome in our model.\n\n\n\nAs we know, Spark requires numeric data for modeling. So far this hasn’t been an issue; even boolean columns can easily be converted to integers without any trouble. But we’ll also be using the airline and the plane’s destination as features in our model. These are coded as strings and there isn’t any obvious way to convert them to a numeric data type.\nFortunately, PySpark has functions for handling this built into the pyspark.ml.features submodule. WE can create what are called ‘one-hot vectors’ to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one (1).\nEach element in the vector corresponds to a level of the feature, so it’s possible to tell what the right level is by seeing which element of the vector is equal to one (1).\n\nThe first step to encoding our categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\nThe second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer.\n\nThe end result is a column that encodes our categorical feature as a vector that’s suitable for machine learning routines! This may seem complicated, but don’t worry! All we have to remember is that we need to create a StringIndexer and a OneHotEncoder, and the Pipeline will take care of the rest.\n\n\n\n\n\n\nWhy do you have to encode a categorical feature as a one-hot vector?\n\n\n\n\n\nSpark can only model numeric features.\n\n\n\n\n\n\nLet’s now create a StringIndexer and a OneHotEncoder to code the carrier column. To do this, we’ll call the class constructors with the arguments inputCol and outputCol.\n\nthe inputCol is the name of the column you want to index or encode;\nthe outputCol is the name of the new column that the Transformer should create\n\n\n# Create a StringIndexer for the carrier column\ncarr_indexer = StringIndexer(inputCol=\"carrier\",outputCol=\"carrier_index\")\n\n# Create a OneHotEncoder \ncarr_encoder = OneHotEncoder(inputCol=\"carrier_index\",outputCol=\"carrier_fact\")\n\nNow the carrier information is in the correct format for inclusion in our model. Let’s do the same thing for the dest column:\n\n# Create a StringIndexer for the dest column\ndest_indexer = StringIndexer(inputCol=\"dest\",outputCol=\"dest_index\")\n\n# Create a OneHotEncoder\ndest_encoder = OneHotEncoder(inputCol=\"dest_index\",outputCol=\"dest_fact\")\n\n\n\n\nThe last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. We can do this by storing each of the values from a column as an entry in a vector. Then, from the model’s point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\nBecause of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column.\n\n# Create a VectorAssembler with or inputCols as a list, and outputCol name \"features\"\nvec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")\n\n\n\n\nPipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that you’ve already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object.\n\n# Import Pipeline\nfrom pyspark.ml import Pipeline\n\n# Call the pipeline with keyword argument \"stages\"\nflights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n\n\n\n\n\n\n\nImportant\n\n\n\nstages should be a list holding all the stages you want your data to go through in the pipeline.\n\n\nExcellent, we now have a fully reproducible machine learning pipeline!\n\n\n\nAfter we’ve cleaned our data and knocked it into shape for modeling, one of the most important steps is to split the data into a test set and a train set. After that, we don’t touch our test data until we think we have a good model! As we’re building models and forming hypotheses, we can test them on our training data to get an idea of their performance.\nOnce we’ve got our favorite model, we can see how well it predicts the new data in our test set. This never-before-seen data will give us a much more realistic idea of our model’s performance in the real world when we’re trying to predict or classify new data.\nIn Spark it’s important to make sure we split the data after all the transformations. This is because operations like StringIndexer don’t always produce the same index even when given the same list of strings.\n\n\n\n\n\n\nWhy is it important to use a test set in model evaluation?\n\n\n\n\n\nBy evaluating your model with a test set you can get a good idea of performance on new data.\n\n\n\n\n\n\nWe’re finally ready to pass our data through the Pipeline we created!\n\n# Fit and transform the model_data\npiped_data = flights_pipe.fit(model_data).transform(model_data)\n\n\n\n\nNow that we’ve done all your manipulations, the last step before modeling is to split the data!\n\n# Split the data into a training set (60%) and test set (40%)\ntraining, test = piped_data.randomSplit([.6, .4])\n\n\n\n\n\n\n\nImportant\n\n\n\nThe train:test split is implemented by passing a list containing the desired split in decimal format\n\n\nNow we are ready to start fitting the model.\n\n\n\n\n\nIn this last section, we’ll apply what we’ve learned to create a model that predicts which flights will be delayed.\n\n\nThe model we’ll be fitting in this section is called a logistic regression. This model is very similar to a *linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event.\nTo use this as a classification algorithm, all we have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, we classify that observation as a ‘yes’ (in this case, the flight being late), if it’s below, we classify it as a ‘no’!\nWe’ll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that’s not estimated from the data, but rather is supplied by the user to maximize performance.\n\n\n\n\n# Import LogisticRegression\nfrom pyspark.ml.classification import LogisticRegression\n\n# Instantiate a LogisticRegression Estimator\nlr = LogisticRegression()\n\n\n\n\nIn the next few exercises we’ll be tuning our logistic regression model using a procedure called k-fold cross validation. This is a method of estimating the model’s performance on unseen data (like our test DataFrame). It works by splitting the training data into a few different partitions. The exact number is up to you, but we’ll be using PySpark’s default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data.\nWe’ll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare all the different models so we can choose the best one!\n\n\n\n\n\n\nWhat does cross validation allow you to estimate?\n\n\n\n\n\nThe model’s error on held out data.\n\n\n\n\n\n\nThe first thing we need when doing cross validation for model selection is a way to compare different models. Luckily, the pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Our model is a binary classification model, so we’ll be using the BinaryClassificationEvaluator from the pyspark.ml.evaluation module.\nThis evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. We’ll learn more about this later!\n\n# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n\n\n\n\nNext, we need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that (maybe you’re starting to notice a pattern here; PySpark has a submodule for just about everything!).\nWE’ll need to use the .addGrid() and .build() methods to create a grid that we can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that we created earlier) and a list of values that we want to try. The .build() method takes no arguments, it just returns the grid that we’ll use later.\n\n# Import the tuning submodule\nimport pyspark.ml.tuning as tune\n\n# Create the parameter grid\ngrid = tune.ParamGridBuilder()\n\n# Add the learning rate hyperparameters \ngrid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n\n\n\n\n\n\n\nThe lr. regParam parameters are set by using np.arange which creates a list of numbers (from, increment, to) which give us the learning rate values to try.\n\n\n\n\n\n\n\n# Add the elasticNetParam hyperparameter\ngrid = grid.addGrid(lr.elasticNetParam, [0, 1])\n\n\n\n\n\n\n\nElastic net regularization uses a weighted combination of LASSO (L1) and Ridge (L2). The specified value relates to the weighting applied to L1 regularization, so a value of 0 would be pure L2, and a value of 1 would be pure Ridge. Any values in between result in a blend of the two.\n\n\n\n\n\n\n\n# Build the grid\ngrid = grid.build()\n\n\n\n\nThe submodule pyspark.ml.tuning also has a class called CrossValidator for performing cross validation. This Estimator takes the modeler we want to fit, the grid of hyperparameters we created, and the evaluator we want to use to compare our models.\nWe’ll create the CrossValidator by passing it the logistic regression Estimator lr, the parameter grid, and the evaluator we created earlier.\n\n# Create the CrossValidator\ncv = tune.CrossValidator(estimator=lr,\n               estimatorParamMaps=grid,\n               evaluator=evaluator\n               )\n\n\n\n\nUnfortunately, cross validation is a very computationally intensive procedure. To do this locally you would use the code:\n# Fit cross validation models\nmodels = cv.fit(training)\n\n# Extract the best model\nbest_lr = models.bestModel\nRemember, the training data is called training and we’re using lr to fit a logistic regression model. Cross validation selected the parameter values regParam=0 and elasticNetParam=0 as being the best. These are the default values, so we don’t need to do anything else with lr before fitting the model.\n\n# Extract the best model \nbest_lr = lr.fit(training)\n\n# Print best_lr to verify it's an object of the LogisticRegressionModel class\nprint(best_lr)\n\nLogisticRegressionModel: uid=LogisticRegression_2bd11bb498b4, numClasses=2, numFeatures=83\n\n\n\nWe’ll be using a common metric for binary classification algorithms call the AUC, or area under the curve. In this case, the curve is the ROC, or receiver operating curve. For our purposes, the closer the AUC is to one (1), the better the model is!\n\n\n\n\n\n\nIf you’ve created a perfect binary classification model, what would the AUC be?\n\n\n\n\n\n1\n\n\n\n\n\n\nIt’s finally time to test our model on the test data we set aside earlier. We can use the same evaluator we made to fit the model.\n\n# Use the model to generate predictions on the test set\ntest_results = best_lr.transform(test)\n\n# Evaluate the predictions to compute the AUC\nprint(evaluator.evaluate(test_results))\n\n0.7123313100891033\n\n\n\n\nl learned how to use Spark from Python to wrangle data about flights from Portland and Seattle and using this data, build a machine learning pipeline to predict whether or not flights will be delayed."
  },
  {
    "objectID": "posts/Random_Forests/Random_Forests.html",
    "href": "posts/Random_Forests/Random_Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "This is my follow up to the first part of Lesson 6: Practical Deep Learning for Coders 2022 in which Jeremy introduces Decision Trees and Random Forests.\nFor tabular data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it’s more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines. In this notebook, we’re going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition!\nWe’ll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:\n\n# import required package\nfrom fastai.imports import *\n\n# optimize display settings\nnp.set_printoptions(linewidth=130)\n\nNow let’s create DataFrames from the CSV files and carry out some preprocessing:\n\n# grab our data from Kaggle\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)   \n\ntitanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\n\n# read in our training and test datasets\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\n\n# let's see what is the most common value for each colums\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\nOne difference with Random Forests however is that we don’t generally have to create dummy variables like we do for non-numeric columns in linear models and neural networks. Instead, we can just convert those fields to categorical variables, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.\n\ndf.dtypes\n\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\n\n# create a function to carry out some preprocessing\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0) # replace Fare Na with 0\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare']) # take log of fares and add 1 - normalization\n    df['Embarked'] = pd.Categorical(df.Embarked) # convert embaked column to categorical\n    df['Sex'] = pd.Categorical(df.Sex) # convert sex column to categorical\n\n\n# apply our pre-processign function to our training set\nproc_data(df)\n\n# apply our pre-processign function to our test set\nproc_data(tst_df)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      LogFare\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      B96 B98\n      S\n      2.110213\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n      4.280593\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      B96 B98\n      S\n      2.188856\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n      3.990834\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      B96 B98\n      S\n      2.202765\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      B96 B98\n      S\n      2.639057\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n      3.433987\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      24.0\n      1\n      2\n      W./C. 6607\n      23.4500\n      B96 B98\n      S\n      3.196630\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n      3.433987\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      B96 B98\n      Q\n      2.169054\n    \n  \n\n891 rows × 13 columns\n\n\n\nWe’ll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider pclass a categorical variable. That’s because it’s ordered (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we’ll see, only care about order, not about absolute value.\n\n# set our categorical variables\ncats=[\"Sex\",\"Embarked\"]\n\n# set our continuous variables\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n\n# set our dependent(target/y) variable\ndep=\"Survived\"\n\nEven although we’ve made the cats columns categorical, they are still shown by Pandas as their original values:\n\n# take a look at first 5 rows of sex column\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nHowever behind the scenes they’re now stored as integers, with indices that are looked up in the Categories list shown in the output above. We can view the stored values by looking in the pandas **cat.codes** attribute:\n\n# take a look at indexes applied to values in first 5 rows of sex column\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8\n\n\n\n\nBefore we create a Random Forest or Gradient Boosting Machine, we’ll first need to learn how to create a decision tree, from which both of these models are built. And to create a decision tree, we’ll first need to create a binary split, since that’s what a decision tree is built from.\nA binary split is where all rows are placed into one of two groups, based on whether they’re above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold 0.5 and the column Sex (since the values in the column are 0 for female and 1 for male). We can use a plot to see how that would split up our data – we’ll use the Seaborn library, which is a layer on top of matplotlib that makes some useful charts easier to create, and more aesthetically pleasing by default:\n\n\n\n# import required package for plotting\nimport seaborn as sns\n\n\n# create side by side histograms \nfig,axs = plt.subplots(1,2, figsize=(11,5))\n\n# survival rate histogram by sex - axs[0] so this is first plot\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\n\n# countplot by sex - axs[1] so this is second plot\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\ndf['Sex'].value_counts()\n\nmale      577\nfemale    314\nName: Sex, dtype: int64\n\n\n\ndf['Survived'].sum()\n\n342\n\n\nHere we see that (on the left) if we split the data into males and females, we’d have groups that have very different survival rates: >70% for females, and <20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of 891) in each group.\nWe could create a very simple “model” which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:\n\n# import required packages\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\n\n# set seee for reproducibility\nrandom.seed(42)\n\n# create training & validation sets\ntrn_df,val_df = train_test_split(df, test_size=0.25)\n\n# replace categorical variables with their integer codes\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n\ntrn_df\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      LogFare\n    \n  \n  \n    \n      298\n      299\n      1\n      1\n      Saalfeld, Mr. Adolphe\n      1\n      24.00\n      0\n      0\n      19988\n      30.5000\n      C106\n      2\n      3.449988\n    \n    \n      884\n      885\n      0\n      3\n      Sutehall, Mr. Henry Jr\n      1\n      25.00\n      0\n      0\n      SOTON/OQ 392076\n      7.0500\n      B96 B98\n      2\n      2.085672\n    \n    \n      247\n      248\n      1\n      2\n      Hamalainen, Mrs. William (Anna)\n      0\n      24.00\n      0\n      2\n      250649\n      14.5000\n      B96 B98\n      2\n      2.740840\n    \n    \n      478\n      479\n      0\n      3\n      Karlsson, Mr. Nils August\n      1\n      22.00\n      0\n      0\n      350060\n      7.5208\n      B96 B98\n      2\n      2.142510\n    \n    \n      305\n      306\n      1\n      1\n      Allison, Master. Hudson Trevor\n      1\n      0.92\n      1\n      2\n      113781\n      151.5500\n      C22 C26\n      2\n      5.027492\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      106\n      107\n      1\n      3\n      Salkjelsvik, Miss. Anna Kristine\n      0\n      21.00\n      0\n      0\n      343120\n      7.6500\n      B96 B98\n      2\n      2.157559\n    \n    \n      270\n      271\n      0\n      1\n      Cairns, Mr. Alexander\n      1\n      24.00\n      0\n      0\n      113798\n      31.0000\n      B96 B98\n      2\n      3.465736\n    \n    \n      860\n      861\n      0\n      3\n      Hansen, Mr. Claus Peter\n      1\n      41.00\n      2\n      0\n      350026\n      14.1083\n      B96 B98\n      2\n      2.715244\n    \n    \n      435\n      436\n      1\n      1\n      Carter, Miss. Lucile Polk\n      0\n      14.00\n      1\n      2\n      113760\n      120.0000\n      B96 B98\n      2\n      4.795791\n    \n    \n      102\n      103\n      0\n      1\n      White, Mr. Richard Frasar\n      1\n      21.00\n      0\n      1\n      35281\n      77.2875\n      D26\n      2\n      4.360388\n    \n  \n\n668 rows × 13 columns\n\n\n\nNow we can create our independent variables (the x variables) and dependent (the y variable):\n\n# create a function \ndef xs_y(df):\n    xs = df[cats+conts].copy() # independent variables are catoegorical and continuous\n    return xs,df[dep] if dep in df else None # return independent variables, dependent variable\n\n\n# apply function to training & validation sets\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\n\n# check last 5 rows of our training set\ntrn_xs.tail()\n\n\n\n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      106\n      0\n      2\n      21.0\n      0\n      0\n      2.157559\n      3\n    \n    \n      270\n      1\n      2\n      24.0\n      0\n      0\n      3.465736\n      1\n    \n    \n      860\n      1\n      2\n      41.0\n      2\n      0\n      2.715244\n      3\n    \n    \n      435\n      0\n      2\n      14.0\n      1\n      2\n      4.795791\n      1\n    \n    \n      102\n      1\n      2\n      21.0\n      0\n      1\n      4.360388\n      1\n    \n  \n\n\n\n\n\n# check last 5 rows of our validation set\nval_xs.tail()\n\n\n\n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      880\n      0\n      2\n      25.0\n      0\n      1\n      3.295837\n      2\n    \n    \n      425\n      1\n      2\n      24.0\n      0\n      0\n      2.110213\n      3\n    \n    \n      101\n      1\n      2\n      24.0\n      0\n      0\n      2.185579\n      3\n    \n    \n      199\n      0\n      2\n      24.0\n      0\n      0\n      2.639057\n      2\n    \n    \n      424\n      1\n      2\n      18.0\n      1\n      1\n      3.054591\n      3\n    \n  \n\n\n\n\nHere’s the predictions for our extremely simple model, where female is coded as 0:\n\n# set predictions for survival for validation set as Sex = female\npreds = val_xs.Sex==0\n\nWe’ll use mean absolute error to measure how good this model is:\n\n# import required package for our metric and calculate\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\n\n\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work – here’s an example of how we could look at LogFare using a boxenplot and kernel density estimate(KDE) plot:\n\n# create subset of data to include only logfare\ndf_fare = trn_df[trn_df.LogFare>0]\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\n\n# create a boxenplot of logfare v survived \nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\n\n# create a kernel density estimate plot\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\nThe boxenplot (above left) shows quantiles of LogFare for each group - didn’t survive Survived==0, and did survive, Survived==1. It shows that the average LogFare for passengers that didn’t survive is around 2.5, and for those that did it’s around 3.2. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\nLet’s create a simple model based on this observation:\n\n# set prediction for survival for validation set as > 2.7 logfare\npreds = val_xs.LogFare>2.7\n\n…and test it out:\n\nmean_absolute_error(val_y, preds) # binary split based on sex 0.21524663677130046\n\n0.336322869955157\n\n\nThis is quite a bit less accurate than our model that used Sex as the single binary split.\nIdeally, we’d like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We’ll create a score function to do this. Instead of returning the mean absolute error, we’ll calculate a measure of impurity – that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it’s higher, then it means the rows are more different to each other. We’ll then multiply this by the number of rows, since a bigger group has more impact than a smaller group:\n\n# create a function to calculate IMPURITY score\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot<=1: return 0\n    return y[side].std()*tot\n\nNow we’ve got that written, we can calculate the score for a split by adding up the scores for the “left hand side” (lhs) and “right hand side” (rhs):\n\n# create a fucntion to calculate score for a split\ndef score(col, y, split):\n    lhs = col<=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nFor instance, here’s the impurity score for the split on Sex:\n\n# apply our score function to a split based on Sex\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\nand for LogFare:\n\n# apply our score function to a split based on LogFare\nscore(trn_xs[\"LogFare\"], trn_y, 2.7) # score based on split by sex 0.40787530982063946\n\n0.47180873952099694\n\n\nA higher score means the values within the split are more different to eacch other i.e. impure, so as we’d expect from our earlier tests, Sex appears to be a better split as it has a lower impurity score. To make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle if you click “Copy and Edit” in the top right to open the notebook editor):\n\n# create interactve tool to play around with splits\nfrom ipywidgets import interact\n\n# create function that shows score for chosen splits\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\n\n# set variables (nm) to play around with as our continuous variables\n# set initial split point\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\nTry selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\nWe can do the same thing for the categorical variables:\n\n# set variables (nm) to play around with as our cotegorical variables\n# set initial split point\ninteract(nm=cats, split=2)(iscore);\n\n\n\n\nThat works well enough, but it’s rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for age we’d first need to make a list of all the possible split points (i.e all the unique values of that field) :\n\n# obtain all unique age values\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\n…and find which index of those values is where score() is the lowest:\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)]) # use list comprehension rather than for loop\nunq[scores.argmin()] # grab lowest score \n\n6.0\n\n\nBased on this, it looks like, for instance, that for the Age column, 6 is the optimal cutoff according to our training set. We can write a little function that implements this idea:\n\n# create function that pulls this idea together\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx] # return value that gives lowest score, and that score\n\n\n# find age value that gives lowest impurity score\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\nLet’s try all the columns:\n\n# combine categorical and continuous as previously defined\ncols = cats+conts\n\n# return col name: and then result from function i.e (split value that gives lowest score, and that score)\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\nAccording to this, Sex<=0 is the best split we can use.\nWe’ve just re-invented the OneR classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it’s so simple and surprisingly effective, it makes for a great baseline – that is, a starting point that you can use to compare your more sophisticated models to.\nWe found earlier that out OneR rule had an error of around 0.215, so we’ll keep that in mind as we try out more sophisticated approaches.\n\n\n\n\nHow can we improve our OneR classifier, which predicts survival based only on Sex?\nHow about we take each of our two groups, female and male, and create one more binary split for each of them. That is: find the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section’s steps, once for males, and once for females.\nFirst, we’ll remove Sex from the list of possible splits (since we’ve already used it, and there’s only one possible split for that binary column), and create our two groups:\n\n# remove Sex column from our previous defined cols\ncols.remove(\"Sex\")\n\n# create ouur 2 groups males and females\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nNow let’s find the single best binary split for males:\n\n# return col name: and then result from function i.e (split value that gives lowest score, and that score)\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n…and for females:\n\n# return col name: and then result from function i.e (split value that gives lowest score, and that score)\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nWe can see that the next best binary split for males is Age<=6 and for females is Pclass<=2.\nBy adding these rules, we have created a decision tree, where our model will first check whether Sex is female or male, and depending on the result will then check either the above Age or Pclass rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we’ve now created.\nRather than writing that code manually, we can use DecisionTreeClassifier, from sklearn, which does exactly that for us:\n\n# import decision tree classifier and graphical \nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\n\n\n# fit a decision tree to our training data\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\n\n# create a function that draws decision tree \ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\n# draw decision teee based on \ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\nThe first split looks at Sex\n\n\nless than or equal to 0.5 True effectively means 0 i.e female (229) which sets us off down the LEFT hand side of the tree\nless than or equal to 0.5 False effectively means 1 i.e. male (439) which sets us off down the RIGHT hand side of the tree\n\n\nThe second split\n\n\nfor females is based on below (120) and above (109) Pclass 2; and\nfor males is based on below (21) and above (418) age 6\n\nWe can see that our training set of 668 rows (415 survivors, 253 not survived) has been split exactly as we did!\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (“samples”) match that set of rules, and shows how many perish or survive (“values”). There’s also something called gini. That’s another measure of impurity, and it’s very similar to the score() function we created earlier.\nGini is defined as follows:\n\n# derive the gini calculation\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2 # probability that if you pick two rows from a group that you get same survived result each time\n\nWhat this calculates is the probability that, if you pick two rows from a group, you’ll get the same Survived result each time. If the group is all the same, the probability is 0.0, and 1.0 if they’re all different.\n\n# apply our function to split by sex\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nLet’s see how this model compares to our OneR version:\n\nmean_absolute_error(val_y, m.predict(val_xs)) # oneR score 0.21524663677130046\n\n0.2242152466367713\n\n\nIt’s actually marginally worse. Since this is such a small dataset (we’ve only got around 200 rows in our validation set) this small difference isn’t really meaningful. Perhaps we’ll see better results if we create a bigger tree:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=12)\n\n\n\n\nLet’s check how many leaf nodes and data points we have:\n\nm.get_n_leaves(), len(trn_xs)\n\n(11, 668)\n\n\n\n\n\nSo we have 11 leaf nodes, and 668 data points. This seems reasonable, no suggestion of overfitting.\n\nHere’s some intuition for an overfitting decision tree with more leaf nodes than data items. Consider the game Twenty Questions. In that game, the chooser secretly imagines an object (like, “our television set”), and the guesser gets to pose 20 yes or no questions to try to guess what the object is (like “Is it bigger than a breadbox?”). The guesser is not trying to predict a numerical value, but just to identify a particular object out of the set of all imaginable objects. When your decision tree has more leaves than there are possible objects in your domain, then it is essentially a well-trained guesser. It has learned the sequence of questions needed to identify a particular data item in the training set, and it is “predicting” only by describing that item’s value. This is a way of memorizing the training set—i.e., of overfitting.\n\n\nmean_absolute_error(val_y, m.predict(val_xs)) # oneR score 0.21524663677130046\n\n0.18385650224215247\n\n\nIt looks like this is an improvement, although again it’s a bit hard to tell with small datasets like this. Let’s try submitting it to Kaggle:\n\n# create a Kaggle submission csv file\ntst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(m.predict(tst_xs), 'tree')\n\nWhen I submitted this I got a score of 0.76555, which isn’t as good as our linear models or most of our neural nets, but it’s pretty close to those results.\nHopefully you can now see why we didn’t really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here’s how the first few items of Embarked are labeled:\n\ndf.Embarked.head()\n\n0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']\n\n\n…resulting in these integer codes:\n\ndf.Embarked.cat.codes.head()\n\n0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8\n\n\nSo let’s say we wanted to split into “C” in one group, vs “Q” or “S” in the other group. Then we just have to split on codes <=0 (since C is mapped to category 0). Note that if we wanted to split into “Q” in one group, we’d need to use two binary splits, first to separate “C” from “Q” and “S”, and then a second split to separate “Q” from “S”. For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\nAs a rough guide, consider using dummy variables for <4 levels, and numeric codes for >=4 levels.\nBuilding a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees).\nSo how do we get the best of both worlds?\n\n\n\nIn 1994 Berkeley professor Leo Breiman, one year after his retirement, published a small technical report called “Bagging Predictors”, which turned out to be one of the most influential ideas in modern machine learning. The report began:\n\nBagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions… The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.\n\nHere is the procedure that Breiman is proposing:\n\nRandomly choose a subset of the rows of your data (i.e., “bootstrap replicates of your learning set”).\nTrain a model using this subset.\nSave that model, and then return to step 1 a few times.\nThis will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model’s predictions.\n\nThis procedure is known as “bagging.” It is based on a deep and important insight: although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors, therefore, is: zero! So if we take the average of all of the models’ predictions, then we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. This is an extraordinary result—it means that we can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of the data, and averaging its predictions.\nIn 2001 Leo Breiman went on to demonstrate that this approach to building models, when applied to decision tree building algorithms, was particularly powerful. He went even further than just randomly choosing rows for each model’s training, but also randomly selected from a subset of columns when choosing each split in each decision tree. He called this method the random forest. Today it is, perhaps, the most widely used and practically important machine learning method.\nIn essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. Bagging is a particular approach to “ensembling,” or combining the results of multiple models together. To see how it works in practice, let’s get started on creating our own random forest!\nOne of the most important properties of random forests is that they aren’t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train—the more trees you have, the more accurate the model will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4 both tend to work well, although sklearn’s defaults work well too.\nThe sklearn docs show an example of the effects of different max_features choices, with increasing numbers of trees. In the plot, the blue plot line uses the fewest features and the green line uses the most (it uses all the features). As you can see below, the models with the lowest error result from using a subset of features but with a larger number of trees.\n\n\n\nsklearn_features.png\n\n\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here’s how we can create a tree on a random subset of the data:\n\n# create a function that generates a bunch of uncorrelated decision trees\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier\n    (min_samples_leaf=5)\n    .fit(trn_xs.iloc[idxs], \n         trn_y.iloc[idxs])\n\nNow we can create as many trees as we want:\n\ntrees = [get_tree() for t in range(100)]\n\nOur prediction will then be the average of these trees’ predictions:\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nThis is nearly identical to what sklearn’s RandomForestClassifier does. The main extra piece in a “real” random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here’s how we repeat the above process with a random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nWe can submit that to Kaggle too:\n\n# create a random forest Kaggle submission csv file\nsubm(rf.predict(tst_xs), 'rf')\n\nThis actually scored slightly worse 0.76315 than the original decision tree classifier.\nOne particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using feature_importances_:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\nWe can see that Sex is by far the most important predictor, with LogFare a distant second, and Age and Pclass behind that. In datasets with many columns, it is recommended to create a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn’t really need to take the log() of Fare, since random forests only care about order, and log() doesn’t change the order – we only did it to make our graphs earlier easier to read).\nThe way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1.\n\n\n\nSo what can we take away from all this?\nI think the first thing I’d note from this is that, clearly, more complex models aren’t always better. Our OneR model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn’t an improvement on the single decision tree at all.\nSo we should always be careful to benchmark simple models, as see if they’re good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there’s no need to guess – it’s so easy to try a few different models, there’s no reason not to give the simpler ones a go too!\nAnother thing I think we can take away is that random forests aren’t actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren’t sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html",
    "href": "posts/Excelosaurus/Excelosaurus.html",
    "title": "Excelosaurus",
    "section": "",
    "text": "This is my follow up to Lesson 3: Practical Deep Learning for Coders 2022 in which Jeremy built a linear regression model, and neural net from scratch (from the Titanic dataset) using the ‘dinosaur’ named ‘Excelosaurus’!"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#data-source",
    "href": "posts/Excelosaurus/Excelosaurus.html#data-source",
    "title": "Excelosaurus",
    "section": "Data source",
    "text": "Data source\nI managed to ‘reptilicate’ Jeremy’s example by creating my own dataset in Excel from my son’s favourite dino book, which is written in Polish. He is bilingual so thankfully he was able to help me with the translation :)"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#independent-variables-features",
    "href": "posts/Excelosaurus/Excelosaurus.html#independent-variables-features",
    "title": "Excelosaurus",
    "section": "Independent variables (features)",
    "text": "Independent variables (features)\nThe book provided some very basic information about each dinosaur:\n- Name \n- Period (Cretaceous, Jurassic, Triassic)\n- Discovered (South America, North America, Europe, Asia etc. - 8 unique locations)\n- Min_Len (metres)\n- Max_Len (metres)\n- Min_Height (metres)\n- Max_Height (metres)\n- Min_Weight (000 kg)\n- Max_Weight (000 kg)\nThese are our independent variables (or features).\n\nCategorical features\nOur model needs data in numerical format however as we can see from above, we have values for Period and Discovered which are clearly not numerical. How do we represent these in numerical format?\nWell, the trick is to create a dummy variable which creates n-1 columns where n is the number of categories:\nfor our Period variable ( n(3) - 1 ) = 2 columns created \n\nfor our Discovered variable ( n(8) - 1) = 7 columns created\nWe then populate these columns with a numerical value of 1 to indicate that our dino falls into that category, or 0 to signal that it does not.\n\n\nFeature engineering\nDue to the nature and limited number of features provided by the book I performed some very basic feature engineering to try to help our model with its predictions.\n\nCreating new features\nI created some new features from the existing info:\n- Ave_Len\n- Ave_Height\n- Ave_Weight\n- Ave_Weight_Length\n\n\nNormalization\nAnother thing I noticed was that the values for the dino dimensions are very large in comparison to our newly created dummy variables, and risk dominating our model, relegating the categorical features, which may actually hold some predictive insight! To counter this I used a simple normalization technique to squeeze the values for the dino dimensions between 0 and 1, by dividing each of the original values by the maximum value for that column. This results in a maximum value of 1 for the largest original value (anything divided by itself = 1) with the other values spread out proporionately betweeen 0 and 1."
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#dependent-target-variables-labels",
    "href": "posts/Excelosaurus/Excelosaurus.html#dependent-target-variables-labels",
    "title": "Excelosaurus",
    "section": "Dependent | Target variables (labels)",
    "text": "Dependent | Target variables (labels)\nThe book also classified the dinos as either:\n- roslinozernosc (veggie) ; or \n- miesozernosc (meat-eater)\nThese are our dependent or target variables (or labels).\n\n\n\nFeatures.PNG"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#linear-model",
    "href": "posts/Excelosaurus/Excelosaurus.html#linear-model",
    "title": "Excelosaurus",
    "section": "Linear model",
    "text": "Linear model\nOur task here can be represented on a basic level by a linear model\ny = wx + b\nwhere we try to map the independent variables x_1, x_2, …, x_n (our dino features) to our target variable y (1 = meat eater, 0 = veggie).\nWe already have our values for x and y, and because our dummy variables already cover the entire dataset e.g. there’s a column for “Cretaceous”, “Jurassic”, and “Triassic”, and every dino in the dataset was around during exactly one of these; therefore, we don’t need a separate intercep(or bias) term b to cover rows that aren’t otherwise part of a column.\nThat just leaves us to compute the values for w - the weights applied to each of our features!\n\nWeights | Parameters\nThe first step is to initilaise random weights | parameter for each of our chosen input features. To do this in excel:\n= RAND()-0.5\nSubtracting -0.5 is an arbitrary technique, but it is useful as it tightens up the random values around either side of zero.\n\n\n\nParameters.PNG\n\n\nNow that we have our weightings, we can populate our Prediction column, which takes our feature values and multiplies them by our randomly generated weightings.\nWe can use the SUMPRODUCT function within excel to performs this quickly and efficiently across all rows of our dataset in one fell swoop!\n=SUMPRODUCT(Table2[@[Ave_Weight_Length]:[Ones]],$AD\\(10:\\)AQ$10)\n\n\nCalculating our loss\nIt is important from the outset to specify which loss function is to be used. There are a number of different loss functions and it is often unclear which should be used for the particular task at hand. This article provides a useful guide.\n\nMean squared error\n\n\n\nMSE.jpg\n\n\nI chose mean squared error (as used in Jeremy’s Titanic dataset walkthough) as my loss function. Our task, predicting whether a dino is a meat eater [1] or veggie [0] is a binary classification problem, similar to the Kaggle Titanic competition, where the task was to predict survived or not-survived.\nFor each dino we simply square the difference between our prediction value (BB10) and the target value (Table2(@Target). The squared error for one dino prediction is calculated in excel as follows:\n=(BB10-Table2[@Target])^2\nOur total loss or MSE is then simply the average across all n (60) dinos:\n=AVERAGE(BD10:BD69)\n\n\nCross-entropy loss\nHaving done some digging on loss functions, there is the suggestion that Cross-entropy is the default loss function to use for binary classification problems. The mathematical formula is:\n\n\n\ncross_entropy.png\n\n\nIt turns out Jeremy posted a very useful video a few years back on how to calculate this using Excel and there is a useful summary provided here by Aman Arora."
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#a-neural-network",
    "href": "posts/Excelosaurus/Excelosaurus.html#a-neural-network",
    "title": "Excelosaurus",
    "section": "A neural network",
    "text": "A neural network\nWe can add another linear layer to our exisitng linear model, although this in itself would not constitute a neural network. What we can do however, is add rectified linear activation functions or ReLU for short.\nIn simple terms this function clips the values produced by our linear models so that they all lie between 0 and 1. This can easily be achieved in Excel using an if function:\nif AS<0,0,AS\nOur predictions are then updated to include the outputs from both linear models and the two ReLUs, and our loss also updated accordingly with reference to the original unchanged target values - meat-eater [1] veggie [0]."
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#using-matrix-multiplication",
    "href": "posts/Excelosaurus/Excelosaurus.html#using-matrix-multiplication",
    "title": "Excelosaurus",
    "section": "Using Matrix Multiplication",
    "text": "Using Matrix Multiplication\nMatrix Multiplication is one of the most fundamental computational tasks and, one of the core mathematical operations in today’s neural networks. Not the sort of thing you might expect to be found in Excel perhaps? Well it turns out that there is function called MMULT built into excel.\nAll we need to do is transpose our randomized weights and our features, so that the input features are now the rows, and the weights values are incuded in column format.\n\n\n\nMatrix Mult.PNG\n\n\nAn example from one of the cells from our linear model prediction is included below:\n=MMULT(Table2456[[Ave_Weight_Length]:[Const]],AE20:AF33)\nThe first part of the formula:\nTable2456[[Ave_Weight_Length]:[Const]]\nis the range over which we are computing matrix multiplicaiton - i.e our features data\nThe second part of the formula:\nAE20:AF33\nis our parameters/weightings\nFinally, it’s time to now update our initial randomized parameters using Gradient Descent, which we can do in excel!"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#gradient-descent",
    "href": "posts/Excelosaurus/Excelosaurus.html#gradient-descent",
    "title": "Excelosaurus",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nGradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\nIt might be that you are familiar with Solver function within Excel, as part of an optimization strategy? Well this is basically Gradient Descent to give it another name.\nWe can use this Solver function to optimise our loss function.\nThe steps required within excel are as follows:\n\nclick on the Data tab then select the Solver function (as highlighted below) Note, you may have to add this function to your dashboard. See this guide for details on how to do this.\nSet Objective: enter the cell reference for your total loss\nTo: select Min (we are seeking to minimise our total loss)\nBy Changing Variable cells: select the range of our parameters (we want to update our original randomized values)\nMake Unconstrianed Variables Non-Negative: ensure this is UNticked\nSelect a Solving Method: select GRG Nonlinear from the dropdown\n\nFinally, hit Solve and wait a few moments. You should find that your parameters have been updated to optimal values, and your total loss has reduced.\n\n\n\nSolver_1.PNG\n\n\n\n\n\nSolver_2.PNG"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#final-thoughts",
    "href": "posts/Excelosaurus/Excelosaurus.html#final-thoughts",
    "title": "Excelosaurus",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe main takeaway from this project, apart from hanging out with my son, and finding out about cool dinos, is that I have successfully managed to:\n\ncreate my own dataset from scratch\nclean the data and perform some feature engineering\ncarry out Matrix Multiplication within Excel using the MMULT function\n\n\n\ncarry out Gradient Descent within excel using the Solver function\n\nThis exercise really consolidated my understanding of the architecture of linear and neural models, and hopefully, some of the techniques included in this NoteBook prove to be helpful to other data science newcomers like me. Looking forward to Lesson 4!"
  },
  {
    "objectID": "posts/Introducing/Introducing.html",
    "href": "posts/Introducing/Introducing.html",
    "title": "Introducing….",
    "section": "",
    "text": "Christian Wittmann ! We’ve never actually met but I think I might owe you a drink! If you are ever in Krakow look me up :)\nAs a data science noob (I’m working my way through the fast.ai course under the guidance of Jeremy Howard, starting a blog seemed like a great way to integrate into the community and also to document the learning journey ahead.\nI did look at fastpages, but a new contender Quarto caught my eye.\nI follow the fast.ai course using Jupyter Notebooks installed on my Linux box, and the go to Quarto tutorial is perfectly fine for getting nicely rendered Jupyter Notebooks, but how to get these into a blog?\nAgain, the documentation provided by Quarto is fine, but you won’t see Jupyter in the Quick Start setup.\nAfter multiple, and ultimately abortive attempts to deploy via GitHub (I created and deleted SO many repositories) I did manage to get something up and running using Netlify, but that just adds another step in the chain.\nIf only there was a way to work on Notebooks within Jupyter and blog my findings, all under the same roof! Sounds too good to be true.\nWell the good news is that it IS possible thanks to this post which has made this blog post (my first ever) possible!\nThe key takeaway for me from reading Christian’s blog was these two steps, which although referred to in the Quarto docs are not very clear.\nTwo final activities are needed to finalize the setup of your repo:\n\nCreate a new branch called gh-pages. To do this, go to your branches (for me that is https://github.com/Stephen137/Stephen137.github.io/branches), and create the new branch by clicking the “New branch”- button in the top right.\nSet the new branch as the branch for GitHub Pages. In your repo, navigate to Settings -> Pages. (In my repo that takes me to https://github.com/Stephen137/Stephen137.github.io/settings/pages.) Change main to gh-pages.\n\nI have run into soooo many obstacles over the first few weeks of my transition into data science, but I have also learned so much in a short space of time. For any fellow newcomers, my advice is to be tenacious. Do your research, but when (not if) you hit a wall, don’t be afraid to reach out. The fast.ai forums is a great place to hang out.The community is really so collaborative.\nNow that I have my own blogging platform, I’ll try to retrace my steps and post some useful insights.\nHave a great weekend all :)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html",
    "title": "Excelosaurus meets Python",
    "section": "",
    "text": "This is my follow up to Lesson 5: Practical Deep Learning for Coders 2022 in which Jeremy builds a linear regresson model and neural net from scratch using Python."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#aim-of-the-project",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#aim-of-the-project",
    "title": "Excelosaurus meets Python",
    "section": "Aim of the project",
    "text": "Aim of the project\nThe aim of this project is :\n\nto build a linear and neural network model from scratch within Python that predicts whether a dino is a meat-eater or a vegetarian"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#data-source",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#data-source",
    "title": "Excelosaurus meets Python",
    "section": "Data source",
    "text": "Data source\nI created my own dataset in Excel from my son’s favourite dino book."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#set-path",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#set-path",
    "title": "Excelosaurus meets Python",
    "section": "Set path",
    "text": "Set path\n\nfrom pathlib import Path"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#install-required-packages",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#install-required-packages",
    "title": "Excelosaurus meets Python",
    "section": "Install required packages",
    "text": "Install required packages\nWe’ll be using NumPy and Pytorch for array calculations in this notebook, and pandas for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.\n\nimport os\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#loading-the-data",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#loading-the-data",
    "title": "Excelosaurus meets Python",
    "section": "Loading the data",
    "text": "Loading the data\nThe starting point for this project is the csv file that I created as part of my Lesson 3 project to build a linear model and neural network from scratch within Excel. See my earlier post titled Excelosaurus.\n\n\n\nFeatures.PNG\n\n\nThe data is in the form of a table, as a Comma Separated Values (CSV) file. We can open it using the pandas library, which will create a DataFrame.\n\n# Set the path and load in the spreadsheet\nfrom pathlib import Path\ndinos = pd.read_csv('Data/Dinos.csv')"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#exploratory-data-analysis",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#exploratory-data-analysis",
    "title": "Excelosaurus meets Python",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nWe can automate some of the exploratory data analysis by writing a function:\n\ndef initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))\n\n\ninitial_eda(dinos)\n\nDimensions : 60 rows, 10 columns\nTotal NA Values : 0 \n                           Column Name  Data Type      #Distinct  NA Values\n                                  Name     object           60          0\n                                Period     object            3          0\n                            Discovered     object            8          0\n                               Min_Len    float64           21          0\n                               Max_Len    float64           22          0\n                            Min_Height    float64           15          0\n                            Max_Height    float64           15          0\n                            Min_Weight    float64           29          0\n                            Max_weight    float64           27          0\n                                  Meat      int64            2          0\n\n\nAs we can see there are no missing values, amd the dataset includes:\n\n60 rows, each representing a unique dinosaur, and\n10 columns, representing 9 features and the target variable.\n\nNote that the Period and Discovered columns are of data type object. We will deal with these later in the section on Categorical Features. The other columnns are numeric (float64 or int64).\nStraight off the bat, we can safely drop the *Name” column as the dino name is clearly not an indicator of whether it is a meat eater or a plant eater:\n\ndinos = dinos.drop(['Name'], axis=1);\n\n\ndinos['Meat'].value_counts()\n\n1    36\n0    24\nName: Meat, dtype: int64\n\n\n\nThere are two possible prediction outputs so this is a binary classification problem. It’s a small dataset but reasonably balanced; 36 meat-eating dinos [indexed 1] and 24 veggies [indexed 0]."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#numeric-features",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#numeric-features",
    "title": "Excelosaurus meets Python",
    "section": "Numeric features",
    "text": "Numeric features\nHere’s how we get a quick summary of all the numeric columns in the dataset:\n\nimport numpy as np\n\ndinos.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      Min_Len\n      Max_Len\n      Min_Height\n      Max_Height\n      Min_Weight\n      Max_weight\n      Meat\n    \n  \n  \n    \n      count\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n    \n    \n      mean\n      8.480000\n      9.015000\n      2.788333\n      3.075000\n      6.666750\n      7.333992\n      0.600000\n    \n    \n      std\n      7.152356\n      7.757661\n      2.548518\n      2.754326\n      13.310511\n      14.709908\n      0.494032\n    \n    \n      min\n      0.300000\n      0.400000\n      0.000000\n      0.000000\n      0.000500\n      0.000500\n      0.000000\n    \n    \n      25%\n      3.750000\n      4.500000\n      1.000000\n      1.000000\n      0.175000\n      0.175000\n      0.000000\n    \n    \n      50%\n      6.500000\n      7.000000\n      2.000000\n      2.750000\n      2.500000\n      3.000000\n      1.000000\n    \n    \n      75%\n      10.250000\n      11.000000\n      4.000000\n      4.000000\n      7.000000\n      8.000000\n      1.000000\n    \n    \n      max\n      35.000000\n      40.000000\n      14.000000\n      14.000000\n      70.000000\n      80.000000\n      1.000000\n    \n  \n\n\n\n\n\nWe need to watch out for dominant features as these can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will strongly influence the result.\nMin_Weight and Max_Weight contain mainly values of around 0.0005 to 8, but there are a few much bigger ones.\n\nLet’s illustrate this visually using a histogram :"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#histogram-of-max_weight",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#histogram-of-max_weight",
    "title": "Excelosaurus meets Python",
    "section": "Histogram of Max_weight",
    "text": "Histogram of Max_weight\n\ndinos ['Max_weight'].hist();"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#feature-engineering",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#feature-engineering",
    "title": "Excelosaurus meets Python",
    "section": "Feature engineering",
    "text": "Feature engineering\nAs you can see the distribution is long-tailed which linear models don’t like. To fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable:\n\n# Note that the log of zero will return NaN (not a number) so the workaround is to include +1\ndinos['Max_weight'] = np.log(dinos['Max_weight']+1)\n\nThe histogram now shows a more even distribution of values without the long tail:\n\ndinos ['Max_weight'].hist()\n\n<AxesSubplot: >\n\n\n\n\n\nLet’s do the same for Min_Weight :\n\ndinos['Min_Weight'] = np.log(dinos['Min_Weight']+1)\ndinos ['Min_Weight'].hist()\n\n<AxesSubplot: >"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#categorical-features",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#categorical-features",
    "title": "Excelosaurus meets Python",
    "section": "Categorical features",
    "text": "Categorical features\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\ndinos.describe(include=[object])\n\n\n\n\n\n  \n    \n      \n      Period\n      Discovered\n    \n  \n  \n    \n      count\n      60\n      60\n    \n    \n      unique\n      3\n      8\n    \n    \n      top\n      Cretaceous\n      North America\n    \n    \n      freq\n      41\n      36\n    \n  \n\n\n\n\n\ndinos.tail(5)\n\n\n\n\n\n  \n    \n      \n      Period\n      Discovered\n      Min_Len\n      Max_Len\n      Min_Height\n      Max_Height\n      Min_Weight\n      Max_weight\n      Meat\n    \n  \n  \n    \n      55\n      Cretaceous\n      Africa\n      12.0\n      12.0\n      1.5\n      1.5\n      2.197225\n      2.197225\n      1\n    \n    \n      56\n      Cretaceous\n      North America\n      11.0\n      11.0\n      1.5\n      1.5\n      2.302585\n      2.302585\n      1\n    \n    \n      57\n      Jurassic\n      Europe\n      0.3\n      0.4\n      0.2\n      0.2\n      0.001000\n      0.001000\n      1\n    \n    \n      58\n      Triassic\n      Europe\n      5.0\n      6.0\n      2.0\n      3.0\n      0.470004\n      0.470004\n      1\n    \n    \n      59\n      Triassic\n      Europe\n      4.0\n      5.0\n      3.0\n      3.0\n      2.302585\n      2.302585\n      0\n    \n  \n\n\n\n\nAs you can see from the above our data includes the following categorical data -\n\nthe period when the dinos lived (Cretaceous, Jurassic, Triassic)\nthe place of discovery (Africa, North America, Europe etc.)\n\nClearly we can’t multiply strings like Jurassic or Africa by coefficients, so we need to replace those with numbers.\nWe do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Period = ‘Cretaceous’, which would be a new column containing 1 for rows where Period is ‘Cretaceous’, and 0 for rows where it isn’t, and apply the same logic for the ‘Discovered’ values.\nPandas can create these automatically using get_dummies, which also remove the original columns. We’ll create dummy variables for Period and Discovered\n\ndinos = pd.get_dummies(dinos, columns=[\"Period\",\"Discovered\"])\ndinos.columns\n\nIndex(['Min_Len', 'Max_Len', 'Min_Height', 'Max_Height', 'Min_Weight', 'Max_weight', 'Meat', 'Period_Cretaceous', 'Period_Jurassic',\n       'Period_Triassic', 'Discovered_Africa', 'Discovered_Antartica', 'Discovered_Asia', 'Discovered_Australia', 'Discovered_Europe',\n       'Discovered_North America', 'Discovered_South America', 'Discovered_UK'],\n      dtype='object')\n\n\nWe can see that columns have been added to the end – one for each of the possible values for Period and Discovered, and that the original Period and Discovered columns have been removed.\nHere’s what the first few rows of those newly added columns look like:\n\nadded_cols = ['Period_Cretaceous', 'Period_Jurassic',\n       'Period_Triassic', 'Discovered_Africa', 'Discovered_Antartica', 'Discovered_Asia', 'Discovered_Australia', 'Discovered_Europe',\n       'Discovered_North America', 'Discovered_South America', 'Discovered_UK']\ndinos[added_cols].head()\n\n\n\n\n\n  \n    \n      \n      Period_Cretaceous\n      Period_Jurassic\n      Period_Triassic\n      Discovered_Africa\n      Discovered_Antartica\n      Discovered_Asia\n      Discovered_Australia\n      Discovered_Europe\n      Discovered_North America\n      Discovered_South America\n      Discovered_UK\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n  \n\n\n\n\nLet’s review our dataset now following the changes prior to building a model:\n\ninitial_eda(dinos)\n\nDimensions : 60 rows, 18 columns\nTotal NA Values : 0 \n                           Column Name  Data Type      #Distinct  NA Values\n                               Min_Len    float64           21          0\n                               Max_Len    float64           22          0\n                            Min_Height    float64           15          0\n                            Max_Height    float64           15          0\n                            Min_Weight    float64           29          0\n                            Max_weight    float64           27          0\n                                  Meat      int64            2          0\n                     Period_Cretaceous      uint8            2          0\n                       Period_Jurassic      uint8            2          0\n                       Period_Triassic      uint8            2          0\n                     Discovered_Africa      uint8            2          0\n                  Discovered_Antartica      uint8            2          0\n                       Discovered_Asia      uint8            2          0\n                  Discovered_Australia      uint8            2          0\n                     Discovered_Europe      uint8            2          0\n              Discovered_North America      uint8            2          0\n              Discovered_South America      uint8            2          0\n                         Discovered_UK      uint8            2          0"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#create-our-independentpredictors-and-dependenttarget-variables",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#create-our-independentpredictors-and-dependenttarget-variables",
    "title": "Excelosaurus meets Python",
    "section": "Create our independent(predictors) and dependent(target) variables",
    "text": "Create our independent(predictors) and dependent(target) variables\n\nThey both need to be PyTorch tensors. Our dependent variable is Meat :\n\n\nfrom torch import tensor\n\nt_dep = tensor(dinos.Meat)\n\n\nOur independent variables are all the continuous variables of interest plus all the dummy variables we created earlier:\n\n\nindep_cols = [\"Min_Len\",\"Max_Len\",\"Min_Height\",\"Max_Height\",\"Min_Weight\",\"Max_weight\"] + added_cols\n\nt_indep = tensor(dinos[indep_cols].values, dtype=torch.float)\nt_indep\n\ntensor([[   35.0000,    40.0000,    11.0000,    11.0000,     4.2627,     4.3944,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     0.0000,     1.0000,     0.0000],\n        [   25.0000,    30.0000,    10.0000,    12.0000,     3.9318,     3.9318,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   30.0000,    30.0000,     5.0000,     6.0000,     2.4849,     2.5649,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   20.0000,    22.0000,     5.0000,     6.0000,     3.0445,     3.1781,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   25.0000,    25.0000,    14.0000,    14.0000,     3.9318,     3.9318,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [    7.0000,     7.0000,     2.5000,     2.5000,     1.3863,     1.3863,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   12.0000,    12.0000,     4.0000,     4.0000,     1.6094,     1.6094,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        ...,\n        [    5.0000,     5.0000,     2.0000,     2.0000,     0.4055,     0.4055,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   20.0000,    20.0000,     4.0000,     4.0000,     3.7136,     3.9318,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   12.0000,    12.0000,     1.5000,     1.5000,     2.1972,     2.1972,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     0.0000,     0.0000,     0.0000],\n        [   11.0000,    11.0000,     1.5000,     1.5000,     2.3026,     2.3026,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [    0.3000,     0.4000,     0.2000,     0.2000,     0.0010,     0.0010,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             1.0000,     0.0000,     0.0000,     0.0000],\n        [    5.0000,     6.0000,     2.0000,     3.0000,     0.4700,     0.4700,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             1.0000,     0.0000,     0.0000,     0.0000],\n        [    4.0000,     5.0000,     3.0000,     3.0000,     2.3026,     2.3026,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             1.0000,     0.0000,     0.0000,     0.0000]])\n\n\n\nHere’s the number of rows and columns we have for our independent variables:\n\n\nt_indep.shape\n\ntorch.Size([60, 17])\n\n\n\nSo 60 rows or examples, and 17 columns, or features."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#setting-up-a-linear-model",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#setting-up-a-linear-model",
    "title": "Excelosaurus meets Python",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nNow that we’ve got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we’re going to manually do a single step of calculating predictions and loss for every row of our data.\nOur first model will be a simple linear model. We’ll need a coefficient for each column in t_indep. We’ll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625,  0.1722,  0.2324,\n        -0.3575, -0.0010, -0.1833])\n\n\nOur predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don’t need a separate constant term (also known as a “bias” or “intercept” term), or a column of all 1s to give the same effect has having a constant term. That’s because our dummy variables already cover the entire dataset – e.g. there’s a column for “Cretaceous”, “Jurassic”, and “Triassic”, and every dino in the dataset was around during exactly one of these; therefore, we don’t need a separate intercept term to cover rows that aren’t otherwise part of a column.\nHere’s what the multiplication looks like:\n\nt_indep*coeffs\n\ntensor([[   -16.2015,      5.5432,      2.6499,     -2.4877,     -1.1221,     -1.3830,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0010,     -0.0000],\n        [   -11.5725,      4.1574,      2.4090,     -2.7138,     -1.0350,     -1.2374,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [   -13.8870,      4.1574,      1.2045,     -1.3569,     -0.6541,     -0.8072,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -9.2580,      3.0487,      1.2045,     -1.3569,     -0.8015,     -1.0002,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [   -11.5725,      3.4645,      3.3726,     -3.1662,     -1.0350,     -1.2374,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -3.2403,      0.9701,      0.6023,     -0.5654,     -0.3649,     -0.4363,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -5.5548,      1.6629,      0.9636,     -0.9046,     -0.4237,     -0.5065,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        ...,\n        [    -2.3145,      0.6929,      0.4818,     -0.4523,     -0.1067,     -0.1276,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -9.2580,      2.7716,      0.9636,     -0.9046,     -0.9776,     -1.2374,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -5.5548,      1.6629,      0.3614,     -0.3392,     -0.5784,     -0.6915,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000],\n        [    -5.0919,      1.5244,      0.3614,     -0.3392,     -0.6062,     -0.7247,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.1389,      0.0554,      0.0482,     -0.0452,     -0.0003,     -0.0003,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -2.3145,      0.8315,      0.4818,     -0.6785,     -0.1237,     -0.1479,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -1.8516,      0.6929,      0.7227,     -0.6785,     -0.6062,     -0.7247,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000]])\n\n\nWe can see we’ve got a problem here. The sums of each row will be dominated by the first three columns, which represent average length, height, and weight, since these values are bigger on average than all the others.\nLet’s make all the columns contain numbers from -1 to 1, by dividing each column by its max():\n\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\nAs we see, that removes the problem of one column dominating all the others:\n\nt_indep*coeffs\n\ntensor([[    -0.4629,      0.1386,      0.1893,     -0.1777,     -0.2632,     -0.3147,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0010,     -0.0000],\n        [    -0.3306,      0.1039,      0.1721,     -0.1938,     -0.2428,     -0.2816,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.3968,      0.1039,      0.0860,     -0.0969,     -0.1535,     -0.1837,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.2645,      0.0762,      0.0860,     -0.0969,     -0.1880,     -0.2276,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.3306,      0.0866,      0.2409,     -0.2262,     -0.2428,     -0.2816,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.0926,      0.0243,      0.0430,     -0.0404,     -0.0856,     -0.0993,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.1587,      0.0416,      0.0688,     -0.0646,     -0.0994,     -0.1153,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        ...,\n        [    -0.0661,      0.0173,      0.0344,     -0.0323,     -0.0250,     -0.0290,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.2645,      0.0693,      0.0688,     -0.0646,     -0.2293,     -0.2816,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.1587,      0.0416,      0.0258,     -0.0242,     -0.1357,     -0.1574,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000],\n        [    -0.1455,      0.0381,      0.0258,     -0.0242,     -0.1422,     -0.1649,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.0040,      0.0014,      0.0034,     -0.0032,     -0.0001,     -0.0001,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -0.0661,      0.0208,      0.0344,     -0.0485,     -0.0290,     -0.0337,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -0.0529,      0.0173,      0.0516,     -0.0485,     -0.1422,     -0.1649,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000]])\n\n\nNote this line of code in particular:\nt_indep = t_indep / vals\nThat is dividing a [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics) by a [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics). The trick here is that we’re taking advantage of a technique in Numpy and PyTorch (and many other languages, going all the way back to APL) called broadcasting. In short, this acts as if there’s a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn’t actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we’re using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it’s well worth studying and practicing.\nWe can now create predictions from our linear model, by adding up the rows of the product:\n\npreds = (t_indep*coeffs).sum(axis=1)\n\nLet’s take a look at the first few:\n\npreds[:10]\n\ntensor([-0.4040, -0.8168, -0.5108, -0.6587, -0.6236, -0.1205, -0.1975, -0.1085, -0.1350, -0.1344])\n\n\nOf course, these predictions aren’t going to be any use, since our coefficients are random – they’re just a starting point for our gradient descent process.\nTo do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\ntensor(0.6421)\n\n\n\nNow that we’ve tested out a way of calculating predictions, and loss, let’s pop them into functions to make life easier:\n\n\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#doing-a-gradient-descent-step",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#doing-a-gradient-descent-step",
    "title": "Excelosaurus meets Python",
    "section": "Doing a gradient descent step",
    "text": "Doing a gradient descent step\nIn this section, we’re going to do a single “epoch” of gradient descent manually. The only thing we’re going to automate is calculating gradients, because let’s face it that’s pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we’ll need to call requires_grad_() on our coeffs (if you’re not sure why, review the previous notebook, How does a neural net really work?, before continuing):\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625,  0.1722,  0.2324,\n        -0.3575, -0.0010, -0.1833], requires_grad=True)\n\n\nNow when we calculate our loss, PyTorch will keep track of all the steps, so we’ll be able to get the gradients afterwards:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.6421, grad_fn=<MeanBackward0>)\n\n\nUse backward() to ask PyTorch to calculate gradients now:\n\nloss.backward()\n\nLet’s see what they look like:\n\ncoeffs.grad\n\ntensor([-0.2290, -0.2129, -0.1825, -0.2030, -0.2670, -0.2669, -0.6167, -0.2000, -0.0500, -0.0667, -0.0167, -0.0833, -0.0167, -0.0500,\n        -0.5667, -0.0500, -0.0167])\n\n\nNote that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. Let’s try running the above steps again:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\ntensor([-0.4579, -0.4258, -0.3650, -0.4060, -0.5340, -0.5337, -1.2333, -0.4000, -0.1000, -0.1333, -0.0333, -0.1667, -0.0333, -0.1000,\n        -1.1333, -0.1000, -0.0333])\n\n\nAs you see, our .grad values have doubled. That’s because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\nWe can now do one gradient descent step, and check that our loss decreases:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)\n    coeffs.grad.zero_()\n    print(calc_loss(coeffs, t_indep, t_dep))\n\ntensor(0.4833)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#training-the-linear-model",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#training-the-linear-model",
    "title": "Excelosaurus meets Python",
    "section": "Training the linear model",
    "text": "Training the linear model\nBefore we begin training our model, we’ll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see “Getting started with NLP for absolute beginners”.\nlet’s use RandomSplitter to get indices that will split our data into training and validation sets:\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=137)(dinos)\n\nNow we can apply those indicies to our independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(48, 12)\n\n\nSo our 60 examples have been split 80:20 between training set and validation set\nWe’ll create functions for the three things we did manually above:\n\nupdating coeffs\ndoing one full gradient descent step, and\ninitilising coeffs to random numbers:\n\n\n# updating coeffs\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\n# doing one full gradient descent step\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n\n# initilising coeffs to random numbers\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nWe can now use these functions to train our model:\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(137)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nLet’s try it. Our loss will print at the end of every step, so we hope we’ll see it going down:\n\ncoeffs = train_model(7, lr=0.9)\n\n0.935; 0.722; 0.595; 0.535; 0.497; 0.463; 0.450; \n\n\nIt does. Let’s take a look at the coefficients for each column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()\n\n{'Min_Len': tensor(0.2610),\n 'Max_Len': tensor(0.1809),\n 'Min_Height': tensor(0.2024),\n 'Max_Height': tensor(0.3190),\n 'Min_Weight': tensor(-0.4248),\n 'Max_weight': tensor(-0.2794),\n 'Period_Cretaceous': tensor(0.5788),\n 'Period_Jurassic': tensor(0.4300),\n 'Period_Triassic': tensor(0.6414),\n 'Discovered_Africa': tensor(0.4464),\n 'Discovered_Antartica': tensor(0.3232),\n 'Discovered_Asia': tensor(0.2413),\n 'Discovered_Australia': tensor(-0.0671),\n 'Discovered_Europe': tensor(0.1147),\n 'Discovered_North America': tensor(0.2534),\n 'Discovered_South America': tensor(-0.2927),\n 'Discovered_UK': tensor(0.5903)}"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#measuring-accuracy",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#measuring-accuracy",
    "title": "Excelosaurus meets Python",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nAn alternative metric to absolute error (which is our loss function) is accuracy – the proportion of rows where we correctly predict meat-eater. Let’s see how accurate we were on the validation set. First, calculate the predictions:\n\npreds = calc_preds(coeffs, val_indep)\n\nWe’ll assume that any dinosaur with a score of over 0.5 is predicted to be a meat-eater. So that means we’re correct for each row where preds>0.5 is the same as the dependent variable:\n\nresults = val_dep.bool()==(preds>0.5)\nresults[:10]\n\ntensor([ True, False, False, False, False,  True, False,  True,  True, False])\n\n\nLet’s see what our average accuracy is:\n\nresults.float().mean()\n\ntensor(0.4167)\n\n\nThat’s not a great start, worse than a 50:50 guess. We’ll create a function so we can calcuate the accuracy easy for other models we train.\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nacc(coeffs)\n\ntensor(0.4167)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#using-sigmoid",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#using-sigmoid",
    "title": "Excelosaurus meets Python",
    "section": "Using sigmoid",
    "text": "Using sigmoid\n\n! pip install sympy\n\nLooking at our predictions, all of our predictions of the probability of meat-eater are between 0 and 1 so there is no benefit from using a sigmoid function.\n\npreds[:]\n\ntensor([0.7410, 0.8814, 0.8523, 0.8898, 0.8832, 0.8720, 0.7918, 0.9176, 0.5626, 0.6464, 0.6439, 0.7686])\n\n\nThe sigmoid function, has a minimum at zero and maximum at one, and is defined as follows:\nHowever, let’s proceed in any event for illustrative purposes:\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\nPyTorch already defines that function for us, so we can modify calc_preds to use it:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\nLet’s train a new model now, using this updated function to calculate predictions:\n\ncoeffs = train_model(lr=100)\n\n0.515; 0.365; 0.357; 0.329; 0.340; 0.343; 0.277; 0.256; 0.299; 0.268; 0.319; 0.261; 0.221; 0.204; 0.195; 0.205; 0.353; 0.316; 0.234; 0.315; 0.300; 0.257; 0.196; 0.299; 0.284; 0.257; 0.248; 0.188; 0.230; 0.271; \n\n\nThe loss has improved by a lot. Let’s check the accuracy:\n\nacc(coeffs)\n\ntensor(0.4167)\n\n\nAs expected, that hasn’t improved. Here’s the coefficients of our trained model:\n\nshow_coeffs()\n\n{'Min_Len': tensor(1.1502),\n 'Max_Len': tensor(0.5529),\n 'Min_Height': tensor(-3.6500),\n 'Max_Height': tensor(-1.4523),\n 'Min_Weight': tensor(-8.8942),\n 'Max_weight': tensor(-7.9555),\n 'Period_Cretaceous': tensor(10.2919),\n 'Period_Jurassic': tensor(8.6934),\n 'Period_Triassic': tensor(7.9045),\n 'Discovered_Africa': tensor(9.1483),\n 'Discovered_Antartica': tensor(1.6134),\n 'Discovered_Asia': tensor(7.2239),\n 'Discovered_Australia': tensor(5.5307),\n 'Discovered_Europe': tensor(-2.3057),\n 'Discovered_North America': tensor(-1.3442),\n 'Discovered_South America': tensor(3.7572),\n 'Discovered_UK': tensor(3.2253)}\n\n\nThese coefficients seem reasonable – in general, heavier dinos were less agile and therefore more likely to be veggie."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#using-matrix-multiplication",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#using-matrix-multiplication",
    "title": "Excelosaurus meets Python",
    "section": "Using Matrix Multiplication",
    "text": "Using Matrix Multiplication\nWe can make things quite a bit neater…\nTake a look at the inner-most calculation we’re doing to get the predictions:\n\n(val_indep*coeffs).sum(axis=1)\n\ntensor([ 6.7534,  3.3826,  2.4559,  8.3519, 14.9058, 17.1442,  2.3888,  8.3883,  6.4539, -0.3698, -0.0583,  4.3533])\n\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:\n\nimport time\nfrom datetime import timedelta\nstart_time = time.monotonic()\nend_time = time.monotonic()\nprint(timedelta(seconds=end_time - start_time))\n\n(val_indep*coeffs).sum(axis=1)\nval_indep@coeffs\n\n0:00:00.000017\n\n\ntensor([ 6.7534,  3.3826,  2.4559,  8.3519, 14.9058, 17.1442,  2.3888,  8.3883,  6.4539, -0.3698, -0.0583,  4.3533])\n\n\nIt also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\nLet’s use this to replace how calc_preds works:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\nIn order to do matrix-matrix products (which we’ll need in the next section), we need to turn coeffs into a column vector (i.e. a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\nWe’ll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\nWe can now train our model as before and confirm we get identical outputs…:\n\ncoeffs = train_model(lr=100)\n\n0.494; 0.365; 0.360; 0.358; 0.357; 0.357; 0.357; 0.356; 0.356; 0.356; 0.356; 0.356; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; \n\n\n…and identical accuracy:\n\nacc(coeffs)\n\ntensor(0.4167)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#a-neural-network",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#a-neural-network",
    "title": "Excelosaurus meets Python",
    "section": "A neural network",
    "text": "A neural network\nWe’ve now got what we need to implement our neural network.\nFirst, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.\nThen our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.\n\nimport torch.nn.functional as F\n\n\ndef calc_preds(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nThat’s it – we’re now ready to train our model!\n\ncoeffs = train_model(lr=1.4)\n\n0.456; 0.446; 0.437; 0.428; 0.421; 0.414; 0.408; 0.403; 0.398; 0.394; 0.391; 0.388; 0.385; 0.383; 0.381; 0.379; 0.377; 0.375; 0.374; 0.373; 0.372; 0.371; 0.370; 0.369; 0.368; 0.367; 0.366; 0.366; 0.365; 0.364; \n\n\n\ncoeffs = train_model(lr=20)\n\n0.456; 0.373; 0.363; 0.360; 0.358; 0.357; 0.357; 0.356; 0.356; 0.356; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; \n\n\nIt’s looking good – our loss is lower than before. Let’s see if that translates to a better result on the validation set:\n\nacc(coeffs)\n\ntensor(0.4167)\n\n\nIn this case our neural net isn’t showing better results than the linear model. That’s not surprising; this dataset is very small and very simple, and isn’t the kind of thing we’d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#deep-learning",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#deep-learning",
    "title": "Excelosaurus meets Python",
    "section": "Deep learning",
    "text": "Deep learning\nThe neural net in the previous section only uses one hidden layer, so it doesn’t count as “deep” learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\nFirst, we’ll need to create additional coefficients for each layer:\n\ndef init_coeffs():\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nYou’ll notice here that there’s a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you’ll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days – it’s very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we’ll learn about in other notebooks.\nOur deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\nWe also need a minor update to update_coeffs since we’ve got layers and consts separated now:\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nLet’s train our model…\n\ncoeffs = train_model(lr=4)\n\n0.376; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; \n\n\n…and check its accuracy:\n\nacc(coeffs)\n\ntensor(0.4167)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#final-thoughts",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#final-thoughts",
    "title": "Excelosaurus meets Python",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe main takeaway from this project, apart from hanging out with my son, and finding out about cool dinos, is that I have successfully managed to:\n\nclean the data using Python and carry out some Exploratory Data Analysis (EDA)\nuse Broadcasting to carry out matrix multiplicaiton\n\n\n\ncreate a real deep learning model from scratch using Python and train it\n\nThe “real” deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you’ll recognise the basic steps are the same.\nThe biggest differences in practical models to what we have above are:\n\nHow initialisation and normalisation is done to ensure the model trains correctly every time\nRegularization (to avoid over-fitting)\nModifying the neural net itself to take advantage of knowledge of the problem domain\nDoing gradient descent steps on smaller batches, rather than the whole dataset.\n\nHopefully, some of the techniques included in this NoteBook prove to be helpful to other data science newcomers like me. Looking forward to Lesson 6!"
  },
  {
    "objectID": "posts/Convolutional_Neural_Network/CNN.html",
    "href": "posts/Convolutional_Neural_Network/CNN.html",
    "title": "Convolutional Neural Networks (CNNs)",
    "section": "",
    "text": "This is my follow up to the second half of Lesson 8: Practical Deep Learning for Coders 2022 in which Jeremy demonstrates the inner workings of a Convolutional Neural Network (CNN) using Excel. In the video, which draws heavily on Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD : Chapter 13, Jeremy walks through the CNN architecture for a handwritten digit 7 in CSV format taken from the MNIST dataset. To consolidate my understanding I replicated the same process for a sample handwritten digit 3.\n\n\nI downloaded the MNIST dataset in csv format from here. Each row of the CSV file has 785 columns, the first column specifies which of the ten handwritten digits is represented (between 0 and 9). The remaining 784 columns contain values which lie on the scale 0 (white) to 255 (black). Why 0 to 255 you may ask? Well, it has to do with computer memory. The number 255 is written as 11111111 in binary form which is 8 bits. In our case we have a grayscale (or 1 channel image) so each pixel takes up 8 bits. Note that a colour image is most commonly represented by 3 channels (Reg, Green, Blue) in which case each pixel requires 24 bits.\nTo see the digit visually we first have to re-arrange the 784 values into a 28 x 28 square, and then use conditional formatting to match the scale - darker for large values, lighter for low values:\n\n\n\n3.JPG\n\n\nOne of the most powerful tools that machine learning practitioners have at their disposal is feature engineering. A feature is a transformation of the data which is designed to make it easier to model. In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the number 3 is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels?\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a convolution. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model.\n\n\n\nThe underlying concept of CNNs has not changed although there have been architecture modifications. First, let’s look at a traditional CNN - how they generally used to be constructed. A convolution applies a kernel across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of the image below:\n\n\n\nkernel.JPG\n\n\nWe can think of a convolution as a sliding window of little mini dot products of these 3 x 3 matrices or kernels. Note that they don’t have to be of size 3 x 3. We randomly initalize these kernels and then use Stochastic Gradient Descent(SGD) to optimize these parameters. We can repeat the same idea as we add layers.\nAfter the application of the first filter, we now have two channels:\n\nChannel 1: which picks out horizontal edges\n\n\n\n\nhorizontal_detector.JPG\n\n\nNote that we clip the resulting value to zero by taking the maximum of 0 and the mini dot product, and our grid sizes have reduced from 28 x 28 to 26 x 26.\n\nChannnel 2: which picks out vertical edges\n\n\n\n\nvertical_detector.JPG\n\n\nIn the second layer we have 2 kernels applied to each channel:\n\n\n\nconv2.JPG\n\n\nNote that our grid sizes have reduced further to 24 x 24 from our original 28 x 28.\nAt this stage, instead of applying further kernels, we would instead use MaxPool which just takes the maximum value over say 2 x 2 grid areas, with no overlap:\n\n\n\nmaxpool.JPG\n\n\nIn the spreadsheet screenshot above, note that the maximum value of the 2 x 2 grid on the left is 3.54, which is the value returned in the MaxPool layer on the right. Note also that our grid size is now just 12 x 12 compared with our original size of 28 x 28.\nThe final step would be to apply a dense layer which is just randomized weightings applied as SUMPRODUCT in excel over the Maxpool layer outputs, to give a final activation value for conversion to a probability using Softmax:\n\n\n\ndense.JPG\n\n\n\n\n\nAs intimated earlier, the architecture of modern CNNs is generally a slight variant of that illustrated above. In the above examples our kernels applied mini dot products across our initial image grid, with no overlap. As a result, it takes a lot of steps (and therfore layers) to reduce our grid into the number of activations that we are interested in - for our handwritten digits problem we would perhaps be interested in 10 activations, the probability for each of the 10 digits, or maybe just 1 - the probability of it being a particular digit.\n\n\n\n\n\n\nstride.png\n\n\nIt’s all about getting there faster!\nModern approaches tend to apply something called stride 2 convolutions. This works by skipping over a column and row when sliding the kernel over our input grid. This effectively reduces the grid feature size by a factor of 4 (2 rows x 2 columns) each convolution, resulting in fewer steps to get down to the required number of activations.\n\n\n\n\n\n\ndrop_out.png\n\n\nThis isn’t our model giving up on the image classification problem! Drop out refers to the removal of different random bits of our image from each batch. Why would we want to do this? It sounds somewhat counter-intuitive - surely we want to give our model the best possible chance of classifying our image? Well, yes and no.\nEssentially there is, as with all models, an inherent compromise between a model that generalizes well to new images and getting good training results, with the risk of overfitting. You can think of dropout as a kind of data augmentation, except we are applying the corruption or augmentation to our activations, rather than our original input image. These dropout layers are really helpful for avoiding overfitting. Setting a higher drop out rate will mean that our model generalizes well to new images, but perform less well on the training set.\nHere is an example of how it works:\n\n\n\ndropout.JPG\n\n\nWe generate a grid of random numbers to match the size of our 24 x 24 input layer, and then set our drop our rate (a value between o and 1) to create a mask to be applied to our image. In this case we have chosen a drop out rate of 0.4 - which basically removes 40% of the pixels - thus corrupting our image, forcing our model to interpret the underlying structure of the image, thus reducing the risk of overfitting.\n\n\n\nNowadays there is no single dense layer matrix multiply at the end as illustrated previously. In addition, once we get down to say a 7 x 7 grid after stride convolutions, instead of doing a Maxpool generally we carry out an Average Pooling.\nSay, for example we have a bear detector image classifier - the model will basically be asking “Is there a bear in this part of the image?” for each of the say 49 remaining pixels in our final 7 x 7 activation. This works well for a single image that fills the whole grid, but if it is a small image in the corner, or a multi-image image then it might not be classified correctly (maybe only 1 out of the 49 pixels has a bear in it). So we might be better choosing Maxpool in this case.\nThe key takeaway is that it is very important that we undestand the architecture of our model, especially the final layer, to take account of the specific task at hand and the nature of the data included in our model.\nFast.ai in fact goes for a blend, and concatenates both MaxPool and AvgPool.\n\n\n\nThis blog does not by any means attempt to understate the complexity of CNNs, but hopefully by simplifying the concept it might help provide a satisfactory overview, and after working through an example of your own, you will have the confidence to dig deeper.\nAn excellent and comprehensive coverage of CNNs is included in Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD : Chapter 13."
  },
  {
    "objectID": "posts/Cat_or_Dog/Cat_or_dog.html",
    "href": "posts/Cat_or_Dog/Cat_or_dog.html",
    "title": "Cat or Dog?",
    "section": "",
    "text": "This is my follow up to Lesson 1: Practical Deep Learning for Coders 2022 taught by Jeremy Howard, co-founder, along with Dr. Rachel Thomas, of fast.ai. This is my first attempt at building a classifier model."
  },
  {
    "objectID": "posts/Cat_or_Dog/Cat_or_dog.html#introducing-dúi",
    "href": "posts/Cat_or_Dog/Cat_or_dog.html#introducing-dúi",
    "title": "Cat or Dog?",
    "section": "Introducing Dúi",
    "text": "Introducing Dúi\nA puppy named Dúi went viral on Reddit a couple of years back, after some people pointed out that he looks like a mix of a dog and a cat. What do you think?\n\nLet’s put a Machine Learning model to the test and see what it predicts.\nBelieve it or not, the following few lines of code represent a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let’s train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:\n\nA dataset called the Oxford-IIIT Pet Dataset that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\nA pretrained model that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\nThe pretrained model will be fine-tuned using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats.\n\nThe first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let’s take a look at the contents of the cell, and the results:\n::: {.cell _cell_guid=‘936cf7ef-02f7-4a8c-bcfe-f028b63b6b4c’ _kg_hide-output=‘true’ _uuid=‘803d9fc5-fed7-464c-b6b6-5efdd0af1961’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\n!pip install -Uqq fastbook\nfrom fastbook import *\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      33.70% [31/92 01:30<02:57 0.7032]\n    \n    \n\n:::\nSo, how do we know if this model is any good? In the last column of the table you can see the error rate, which is the proportion of images that were incorrectly identified. The error rate serves as our metric—our measure of model quality, chosen to be intuitive and comprehensible. As you can see, the model is nearly perfect, even though the training time was only a few minutes (not including the one-time downloading of the dataset and the pretrained model). There are a lot of sources of small random variation involved in training models, however we generally see an error rate of well less than 0.02. In this example, the error rate is approx 0.008, which equates to 99.2% accuracy.\nFinally, let’s check that this model actually works by uploading a picture of Dúi for our model to classify:\n::: {.cell _cell_guid=‘392c10cc-9782-4c29-b131-6f42289856f8’ _uuid=‘eac26ec1-f23a-41ba-96e4-0eb504324959’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\nimport ipywidgets as widgets\nuploader = widgets.FileUpload()\nuploader\n:::\n::: {.cell _cell_guid=‘efa0ad57-03a5-43c1-abf1-ff8eb9d0600a’ _uuid=‘08497abe-f829-476a-ba24-bc5079922994’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\nimg = PILImage.create(uploader.data[0])\ndisplay(img)\n:::\nNow let’s see what the model predicts:\n::: {.cell _cell_guid=‘f98c4e24-adf7-4272-a6e9-42236bd686df’ _uuid=‘a1e1b32c-476e-4fbc-8feb-7a902f5e58e3’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is Dúi a cat?: {is_cat}.\")\nprint(f\"Probability Dúi is a cat: {probs[1].item():.6f}\")\n:::"
  },
  {
    "objectID": "posts/Cat_or_Dog/Cat_or_dog.html#conclusion",
    "href": "posts/Cat_or_Dog/Cat_or_dog.html#conclusion",
    "title": "Cat or Dog?",
    "section": "Conclusion",
    "text": "Conclusion\nSo, it seems that the confusion out there was justified. Although our model recorded an error rate of just 0.008119 (which means that over 99.1% of images were correctly classified), the model predicts with almost certainty (93.6% probability) that Dúi is in fact a cat (and not a dog)!"
  },
  {
    "objectID": "posts/World Cup 2022/World_Cup_2022.html",
    "href": "posts/World Cup 2022/World_Cup_2022.html",
    "title": "FIFA World Cup - Qatar 2022",
    "section": "",
    "text": "At the time of writing the 2022 World Cup is already underway, with 32 teams battling it out in Qatar for the famous golden globe. I’ve lost touch a bit in recent years with football, and thought it would be interesting to use Python to get back up to speed.\nThe main aim of this project is to try and uncover some insights about the 32 teams - who has played it safe, and gone for experience, who has been bold and decided to give youth a chance to flourish. I’m also interested in where the players play their football at club level, and hope to quantify their geographical spread.\n\n\nThe data was sourced from Sporting News website.\n\n\n\nLet’s dive in!\n\n\n\nfootbal dive.jpg\n\n\n\n## Import the required packages\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib as plt\n\npd.options.display.float_format = '{:.2f}'.format\n\n\n# Load in our data\nworld_cup = pd.read_csv('Data/World_Cup_2022.csv')\n\nWe can automate some of the exploratory data analysis by writing a function:\n\ndef initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))  \n\n\ninitial_eda(world_cup)\n\nDimensions : 830 rows, 11 columns\nTotal NA Values : 0 \n                           Column Name  Data Type      #Distinct  NA Values\n                                  Name     object          830          0\n                              Position     object            4          0\n                                   Age    float64           29          0\n                               Country     object           32          0\n                              Ctry_cap     object           32          0\n                                  Caps      int64          132          0\n                                 Group     object            8          0\n                                  Club     object          446          0\n                             Club_ctry     object           43          0\n                         Club_ctry_cap     object           43          0\n                             Home_Away     object            2          0\n\n\nLooks like there is no missing data within the dataset. However, there are 8 groups of 4, so 32 teams competing and the maximum permitted squad size is 26, which would be 832 players. We only have 830 observations. Let’s investigate this by grouping the squad numbers by country:\n\nworld_cup.groupby(\"Country\")[\"Name\"].count().sort_values(ascending=False)\n\nCountry\nArgentina       26\nAustralia       26\nUruguay         26\nUSA             26\nTunisia         26\nSwitzerland     26\nSpain           26\nSouth Korea     26\nSerbia          26\nSenegal         26\nSaudi Arabia    26\nQatar           26\nPortugal        26\nPoland          26\nNetherlands     26\nMorocco         26\nMexico          26\nJapan           26\nGhana           26\nGermany         26\nEngland         26\nEcuador         26\nDenmark         26\nCroatia         26\nCosta Rica      26\nCanada          26\nCameroon        26\nBrazil          26\nBelgium         26\nWales           26\nIran            25\nFrance          25\nName: Name, dtype: int64\n\n\nIran and France only have 25 players in their squad. On follow up this was confirmed to be correct, so we haven’t lost any players! Let’s move on and take a closer look at our data by looking at the first few rows:\n\n# View the first 5 rows\nworld_cup.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      0\n      Saad Al-Sheeb\n      GK\n      32.00\n      Qatar\n      Doha\n      80\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      1\n      Meshaal Barsham\n      GK\n      24.00\n      Qatar\n      Doha\n      15\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      2\n      Yousuf Hassan\n      GK\n      26.00\n      Qatar\n      Doha\n      9\n      A\n      Al-Gharafa\n      Qatar\n      Doha\n      Home\n    \n    \n      3\n      Pedro Miguel\n      DEF\n      32.00\n      Qatar\n      Doha\n      78\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      4\n      Musaab Khidir\n      DEF\n      29.00\n      Qatar\n      Doha\n      29\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n  \n\n\n\n\n….and the last few rows:\n\n# View the last 5 rows\nworld_cup.tail()\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      825\n      Kamaldeen Sulemana\n      MID\n      20.00\n      Ghana\n      Accra\n      11\n      H\n      Stade Rennes\n      France\n      Paris\n      Away\n    \n    \n      826\n      Antoine Semenyo\n      FWD\n      22.00\n      Ghana\n      Accra\n      1\n      H\n      Bristol City\n      England\n      London\n      Away\n    \n    \n      827\n      Andre Ayew\n      FWD\n      32.00\n      Ghana\n      Accra\n      107\n      H\n      Al Sadd\n      Qatar\n      Doha\n      Away\n    \n    \n      828\n      Jordan Ayew\n      FWD\n      31.00\n      Ghana\n      Accra\n      82\n      H\n      Crystal Palace\n      England\n      London\n      Away\n    \n    \n      829\n      Inaki Williams\n      FWD\n      28.00\n      Ghana\n      Accra\n      1\n      H\n      Athletic Club\n      Spain\n      Madrid\n      Away\n    \n  \n\n\n\n\nOK, so we can see that we have some basic information about each player:\n- Name\n- Position (GK = Goalkeeper DEF = Defender MID = Midfielder FWD = Forward)\n- Age\n- Country they represent\n- Ctry_cap - capital of country they represent\n- Caps (Number of matches played for their country)\n- Group (32 teams divided into 8 groups A-H of 4)\n- Club (the team that pays the player's wages!)\n- Club_ctry (the location of the player's domestic team)\n- Club_ctry_cap (capital of their club country)\n- Home_Away - where they play their club football\n\n\n\n\nimport numpy as np\n\nworld_cup.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      Age\n      Caps\n    \n  \n  \n    \n      count\n      830.00\n      830.00\n    \n    \n      mean\n      26.80\n      33.87\n    \n    \n      std\n      4.59\n      33.81\n    \n    \n      min\n      0.00\n      0.00\n    \n    \n      25%\n      24.00\n      8.00\n    \n    \n      50%\n      27.00\n      23.00\n    \n    \n      75%\n      30.00\n      47.00\n    \n    \n      max\n      45.00\n      191.00\n    \n  \n\n\n\n\n\n\n\n\n\n\nIt turned out of course that Alan was wrong - Manchester United went on to win the English Premier league that season. There is no master recipe for success it seems at these tournaments. Some managers like to lean on the old guard, some like to throw the gauntlet down and give the kids a chance. Let’s take a look at the age profile of the players using a histogram:\n\nworld_cup ['Age'].hist();\n\n\n\n\nSomething doesn’t look right here, some ages between 0 and 5! Let’s look into this. We can use .loc to access a group of rows and columns by name or .iloc to access by index:\n\nworld_cup.loc[world_cup['Age'] <5 ]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      34\n      Diego Palacios\n      DEF\n      2.00\n      Ecuador\n      Quito\n      11\n      A\n      LAFC\n      USA\n      Washington, DC\n      Away\n    \n    \n      520\n      Ahmed Reda Tagnaouti\n      GK\n      3.00\n      Morocco\n      Rabat\n      3\n      F\n      Wydad Casablanca\n      Morocco\n      Rabat\n      Home\n    \n    \n      642\n      Fabian Rieder\n      MID\n      0.00\n      Switzerland\n      Berne\n      0\n      G\n      Young Boys\n      Switzerland\n      Berne\n      Home\n    \n  \n\n\n\n\nOn follow up, Diego Palacios Fabian Redier is 23, Ahmed Reda Tagnaouti is 26, and Fabian Rieder is 20 years old. We can correct these errors using .iat. The values we wish to update are located at rows 34, 520 and 642 of column 2 - watch out, indexing starts at 0 in Python!\n\n# Update Diego Palacios age\nworld_cup.iat[34,2]=23\n\n# Update Ahmed Reda Tagnaouti age\nworld_cup.iat[520,2]=26\n\n# Update Fabian Rieder age\nworld_cup.iat[642,2]=20\n\nLet’s check that’s worked:\n\nworld_cup.loc[34:34]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      34\n      Diego Palacios\n      DEF\n      23.00\n      Ecuador\n      Quito\n      11\n      A\n      LAFC\n      USA\n      Washington, DC\n      Away\n    \n  \n\n\n\n\n\nworld_cup.loc[520:520]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      520\n      Ahmed Reda Tagnaouti\n      GK\n      26.00\n      Morocco\n      Rabat\n      3\n      F\n      Wydad Casablanca\n      Morocco\n      Rabat\n      Home\n    \n  \n\n\n\n\n\nworld_cup.loc[642:642]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      642\n      Fabian Rieder\n      MID\n      20.00\n      Switzerland\n      Berne\n      0\n      G\n      Young Boys\n      Switzerland\n      Berne\n      Home\n    \n  \n\n\n\n\nGreat, the ages have been successfully updated. Let’s now take a look at the age profile of each of the 32 squads:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Average age of squad\"\n\nx = world_cup.groupby('Country')['Age'].mean().sort_values()\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Average age of squad')\n\n\n\n\n\n\nave_age = world_cup.groupby(\"Country\")[\"Age\"].mean()\nave_age.sort_values(ascending=False)\n\nCountry\nIran           29.08\nMexico         28.58\nTunisia        27.92\nBrazil         27.86\nBelgium        27.77\nSouth Korea    27.77\nUruguay        27.73\nJapan          27.69\nArgentina      27.69\nSaudi Arabia   27.38\nCroatia        27.31\nAustralia      27.23\nCosta Rica     27.15\nPoland         27.00\nCanada         26.92\nDenmark        26.92\nQatar          26.92\nPortugal       26.77\nSerbia         26.77\nSwitzerland    26.73\nGermany        26.69\nNetherlands    26.58\nEngland        26.35\nWales          26.23\nMorocco        26.19\nCameroon       26.19\nFrance         26.12\nSenegal        26.04\nEcuador        25.54\nSpain          25.31\nUSA            25.00\nGhana          24.73\nName: Age, dtype: float64\n\n\nSo Ghana and Ecaudor have the youngest squads (average age 24.73) whilst Iran has the oldest, with an average age of 29.08.\n\n\n\n\n\n\nThe players below will be looking at the board and hoping for a large number. Players are looking after themselves more and more, extending their playing careers, but realistically, for the players below, this is possibly their last opportunity to appear in a World Cup. So make sure to see catch them while you can!\n\nmature = world_cup[world_cup['Age'] > 33]\nmature_sorted = mature.sort_values(by=\"Age\",ascending = False)\nmature_sorted.head(10)\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      143\n      Ramin Rezaeian\n      DEF\n      45.00\n      Iran\n      Tehran\n      2\n      B\n      Sepahan\n      Iran\n      Tehran\n      Home\n    \n    \n      261\n      Alfredo Talavera\n      GK\n      40.00\n      Mexico\n      Mexico City\n      40\n      C\n      FC Juarez\n      Mexico\n      Mexico City\n      Home\n    \n    \n      440\n      Eiji Kawashima\n      GK\n      39.00\n      Japan\n      Tokyo\n      95\n      E\n      Strasbourg\n      France\n      Paris\n      Away\n    \n    \n      781\n      Pepe\n      DEF\n      39.00\n      Portugal\n      Lisbon\n      128\n      H\n      Porto\n      Portugal\n      Lisbon\n      Home\n    \n    \n      608\n      Atiba Hutchinson\n      MID\n      39.00\n      Canada\n      Ottawa\n      98\n      F\n      Besiktas\n      Turkey\n      Istanbul\n      Away\n    \n    \n      682\n      Dani Alves\n      DEF\n      39.00\n      Brazil\n      Brasilia\n      125\n      G\n      Pumas UNAM\n      Mexico\n      Mexico City\n      Away\n    \n    \n      679\n      Thiago Silva\n      DEF\n      38.00\n      Brazil\n      Brasilia\n      108\n      G\n      Chelsea\n      England\n      London\n      Away\n    \n    \n      78\n      Remko Pasveer\n      GK\n      38.00\n      Netherlands\n      Amsterdam\n      2\n      A\n      Ajax\n      Netherlands\n      Amsterdam\n      Home\n    \n    \n      340\n      Aymen Mathlouthi\n      GK\n      38.00\n      Tunisia\n      Tunis\n      73\n      D\n      Etoile du Sahel\n      Tunisia\n      Tunis\n      Home\n    \n    \n      365\n      Steve Mandanda\n      GK\n      37.00\n      France\n      Paris\n      34\n      D\n      Rennes\n      France\n      Paris\n      Home\n    \n  \n\n\n\n\n\n\n\n\nnew = world_cup[world_cup['Age'] < 20]\nnew_sorted = new.sort_values(by=\"Age\",ascending = True)\nnew_sorted.head(10)\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      435\n      Youssoufa Moukoko\n      FWD\n      17.00\n      Germany\n      Berlin\n      1\n      E\n      Borussia Dortmund\n      Germany\n      Berlin\n      Home\n    \n    \n      820\n      Fatawu Issahaku\n      MID\n      18.00\n      Ghana\n      Accra\n      11\n      H\n      Sporting\n      Portugal\n      Lisbon\n      Away\n    \n    \n      577\n      Zeno Debast\n      DEF\n      18.00\n      Belgium\n      Brussels\n      3\n      F\n      Anderlecht\n      Belgium\n      Brussels\n      Home\n    \n    \n      531\n      Bilal El Khannouss\n      MID\n      18.00\n      Morocco\n      Rabat\n      0\n      F\n      Racing Genk\n      Belgium\n      Brussels\n      Away\n    \n    \n      505\n      Jewison Bennette\n      MID\n      18.00\n      Costa Rica\n      San Jose\n      7\n      E\n      Sunderland\n      England\n      London\n      Away\n    \n    \n      412\n      Garang Kuol\n      FWD\n      18.00\n      Australia\n      Canberra\n      1\n      D\n      Central Coast Mariners\n      Australia\n      Canberra\n      Home\n    \n    \n      478\n      Gavi\n      MID\n      18.00\n      Spain\n      Madrid\n      13\n      E\n      Barcelona\n      Spain\n      Madrid\n      Home\n    \n    \n      784\n      Antonio Silva\n      DEF\n      19.00\n      Portugal\n      Lisbon\n      0\n      H\n      Benfica\n      Portugal\n      Lisbon\n      Home\n    \n    \n      648\n      Simon Ngapandouetnbu\n      GK\n      19.00\n      Cameroon\n      Yaounde\n      0\n      G\n      Marseille\n      France\n      Paris\n      Away\n    \n    \n      504\n      Brandon Aguilera\n      MID\n      19.00\n      Costa Rica\n      San Jose\n      4\n      E\n      Guanacasteca\n      Costa Rica\n      San Jose\n      Home\n    \n  \n\n\n\n\nSo it looks like the youngest player at the tournament is Youssoufa Moukoko of Germany at just 17.\n\n\n\n\n\n\nexperience.jpg\n\n\nAlthough in some sports physical caps may not now always be given (whether at all or for each appearance) the term cap for an international or other appearance has been retained as an indicator of the number of occasions on which a sportsperson has represented a team in a particular sport. Thus, a “cap” is awarded for each game played and so a player who has played x games for the team is said to have been capped x times or have won x caps.\nLet’s first look at the distribution of the number of caps received going into this tournament, using a histogram:\n\nworld_cup ['Caps'].hist();\n\n\n\n\nAs we can see the majority of players are relatively inexperienced, with less than around 40 appearances, although there are some very experienced players, with over 150 caps. Let’s have a look at who they are:\n\nworld_cup.loc[world_cup['Caps'] > 150]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      20\n      Hassan Al-Haydos\n      MID\n      31.00\n      Qatar\n      Doha\n      160\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      232\n      Lionel Messi\n      FWD\n      35.00\n      Argentina\n      Buenos Aires\n      165\n      C\n      PSG\n      France\n      Paris\n      Away\n    \n    \n      272\n      Andres Guardado\n      MID\n      36.00\n      Mexico\n      Mexico City\n      180\n      C\n      Real Betis\n      Spain\n      Madrid\n      Away\n    \n    \n      506\n      Celso Borges\n      MID\n      34.00\n      Costa Rica\n      San Jose\n      154\n      E\n      Alajuelense\n      Costa Rica\n      San Jose\n      Home\n    \n    \n      558\n      Luka Modric\n      MID\n      37.00\n      Croatia\n      Zagreb\n      155\n      F\n      Real Madrid\n      Spain\n      Madrid\n      Away\n    \n    \n      733\n      Diego Godin\n      DEF\n      36.00\n      Uruguay\n      Montevideo\n      159\n      G\n      Velez Sarsfield\n      Argentina\n      Buenos Aires\n      Away\n    \n    \n      798\n      Cristiano Ronaldo\n      FWD\n      37.00\n      Portugal\n      Lisbon\n      191\n      H\n      Manchester United\n      England\n      London\n      Away\n    \n  \n\n\n\n\nCristiano Ronaldo is the most capped player at the tournament with 191. The 200 mark is in sight, although at 37 maybe it’s time to make way for some new blood?\n\n\n\n\n\n\nThe minimum number of caps shown is 0 which means there are players at this tournament who have yet to play for their country - the stage has been set! Let’s find out who they are:\n\nworld_cup.loc[world_cup['Caps'] == 0]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      12\n      Jassim Jabir\n      MID\n      20.00\n      Qatar\n      Doha\n      0\n      A\n      Al-Arabi\n      Qatar\n      Doha\n      Home\n    \n    \n      33\n      William Pacho\n      DEF\n      21.00\n      Ecuador\n      Quito\n      0\n      A\n      Royal Antwerp\n      Belgium\n      Brussels\n      Away\n    \n    \n      50\n      Kevin Rodriguez\n      FWD\n      22.00\n      Ecuador\n      Quito\n      0\n      A\n      Imbabura SC\n      Ecuador\n      Quito\n      Home\n    \n    \n      61\n      Moussa Ndiaye\n      DEF\n      20.00\n      Senegal\n      Dakar\n      0\n      A\n      Anderlecht\n      Belgium\n      Brussels\n      Away\n    \n    \n      70\n      Pathe Ciss\n      MID\n      28.00\n      Senegal\n      Dakar\n      0\n      A\n      Rayo Vallecano\n      Spain\n      Madrid\n      Away\n    \n    \n      79\n      Andries Noppert\n      GK\n      28.00\n      Netherlands\n      Amsterdam\n      0\n      A\n      Heerenveen\n      Netherlands\n      Amsterdam\n      Home\n    \n    \n      83\n      Jeremie Frimpong\n      DEF\n      21.00\n      Netherlands\n      Amsterdam\n      0\n      A\n      Bayer Leverkusen\n      Germany\n      Berlin\n      Away\n    \n    \n      92\n      Xavi Simons\n      MID\n      19.00\n      Netherlands\n      Amsterdam\n      0\n      A\n      PSV Eindhoven\n      Netherlands\n      Amsterdam\n      Home\n    \n    \n      234\n      Nawaf Al-Aqidi\n      GK\n      22.00\n      Saudi Arabia\n      Riyadh\n      0\n      C\n      Al-Nassr FC\n      Saudi Arabia\n      Riyadh\n      Home\n    \n    \n      235\n      Mohamed Al-Yami\n      GK\n      25.00\n      Saudi Arabia\n      Riyadh\n      0\n      C\n      Al-Ahli Saudi FC\n      Saudi Arabia\n      Riyadh\n      Home\n    \n    \n      366\n      Axel Disasi\n      DEF\n      24.00\n      France\n      Paris\n      0\n      D\n      Monaco\n      France\n      Paris\n      Home\n    \n    \n      474\n      Alejandro Balde\n      DEF\n      19.00\n      Spain\n      Madrid\n      0\n      E\n      Barcelona\n      Spain\n      Madrid\n      Home\n    \n    \n      531\n      Bilal El Khannouss\n      MID\n      18.00\n      Morocco\n      Rabat\n      0\n      F\n      Racing Genk\n      Belgium\n      Brussels\n      Away\n    \n    \n      597\n      James Pantemis\n      GK\n      25.00\n      Canada\n      Ottawa\n      0\n      F\n      CF Montreal\n      Canada\n      Ottawa\n      Home\n    \n    \n      624\n      Philipp Kohn\n      GK\n      24.00\n      Switzerland\n      Berne\n      0\n      G\n      RB Salzburg\n      Austria\n      Vienna\n      Away\n    \n    \n      642\n      Fabian Rieder\n      MID\n      20.00\n      Switzerland\n      Berne\n      0\n      G\n      Young Boys\n      Switzerland\n      Berne\n      Home\n    \n    \n      648\n      Simon Ngapandouetnbu\n      GK\n      19.00\n      Cameroon\n      Yaounde\n      0\n      G\n      Marseille\n      France\n      Paris\n      Away\n    \n    \n      779\n      Jose Sa\n      GK\n      29.00\n      Portugal\n      Lisbon\n      0\n      H\n      Wolves\n      England\n      London\n      Away\n    \n    \n      784\n      Antonio Silva\n      DEF\n      19.00\n      Portugal\n      Lisbon\n      0\n      H\n      Benfica\n      Portugal\n      Lisbon\n      Home\n    \n    \n      802\n      Goncalo Ramos\n      FWD\n      21.00\n      Portugal\n      Lisbon\n      0\n      H\n      Benfica\n      Portugal\n      Lisbon\n      Home\n    \n    \n      806\n      Ibrahim Danlad\n      GK\n      19.00\n      Ghana\n      Accra\n      0\n      H\n      Asante Kotoko\n      Ghana\n      Accra\n      Home\n    \n    \n      823\n      Salis Abdul Samed\n      MID\n      22.00\n      Ghana\n      Accra\n      0\n      H\n      Lens\n      France\n      Paris\n      Away\n    \n    \n      824\n      Kamal Sowah\n      MID\n      22.00\n      Ghana\n      Accra\n      0\n      H\n      Club Brugge\n      Belgium\n      Brussels\n      Away\n    \n  \n\n\n\n\nAs expected, these players are generally quite young (although Portugal’s Jose Sa is 29 - better late than never) or goalkeepers, where the first choice tends to be difficult to oust! Keep an eye out for these names - they might be the stars of the future.\nLet’s take a look at the average number of caps for each squad:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Average number of caps per squad\"\n\nx = world_cup.groupby('Country')['Caps'].mean().sort_values()\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Average number of caps per squad')\n\n\n\n\n\n\ncaps = world_cup.groupby(\"Country\")[\"Caps\"].mean()\ncaps.sort_values(ascending=False)\n\nCountry\nQatar          53.46\nBelgium        52.19\nMexico         51.12\nUruguay        45.85\nCosta Rica     43.42\nPortugal       40.42\nSwitzerland    38.58\nWales          37.96\nTunisia        37.88\nCroatia        37.65\nIran           36.72\nDenmark        36.65\nBrazil         36.31\nSouth Korea    35.46\nGermany        35.19\nJapan          35.15\nArgentina      34.12\nPoland         33.88\nFrance         31.80\nEngland        31.54\nCanada         31.46\nSerbia         29.77\nSaudi Arabia   28.50\nSpain          28.42\nNetherlands    26.23\nCameroon       24.73\nUSA            24.69\nEcuador        23.73\nAustralia      22.42\nSenegal        21.62\nMorocco        20.04\nGhana          17.00\nName: Caps, dtype: float64\n\n\nThe host nation Quatar have the most experienced squad with an average of 53.46 international apperances per player. The least experienced squad is Ghana, with an average of 17.\nTo caveat this, it is worth noting that qualification for the World Cup is segregated by region, and there can be a wide disparity between the number of qualifying matches played. This can result in some nations playing a large number of matches, without necessarily playing in a major tournament, which is perhaps a better indicator of experience.\n\n\n\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\nworld_cup.describe(include=[object])\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Country\n      Ctry_cap\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      count\n      830\n      830\n      830\n      830\n      830\n      830\n      830\n      830\n      830\n    \n    \n      unique\n      830\n      4\n      32\n      32\n      8\n      446\n      43\n      43\n      2\n    \n    \n      top\n      Saad Al-Sheeb\n      DEF\n      Qatar\n      Doha\n      G\n      Al-Sadd\n      England\n      London\n      Away\n    \n    \n      freq\n      1\n      274\n      26\n      26\n      130\n      13\n      159\n      159\n      551\n    \n  \n\n\n\n\n\ncols = ['Position', 'Group', 'Home_Away']\nworld_cup[cols] = world_cup[cols].astype('category')\n\n\nworld_cup[['Name','Country','Ctry_cap','Club','Club_ctry','Club_ctry_cap']] = world_cup[['Name','Country','Ctry_cap','Club','Club_ctry','Club_ctry_cap']].astype(str)\n\n\n\n\nFirst of all let’s look at where these 830 players play their club football:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Number of players playing in....\"\n\nx = world_cup.groupby('Club_ctry')['Club'].count().sort_values(ascending=True).tail(10)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=16)\n\nText(0.5, 1.0, 'Number of players playing in....')\n\n\n\n\n\n\nworld_cup.groupby(\"Club_ctry\")[\"Club\"].count().sort_values(ascending=False).head(10)\n\nClub_ctry\nEngland         159\nSpain            86\nGermany          80\nItaly            67\nFrance           58\nSaudi Arabia     34\nQatar            33\nUSA              27\nBelgium          25\nMexico           23\nName: Club, dtype: int64\n\n\nSo out of 830 players represented at the World Cup, 159 play their football in England. Let’s take a look at the top 10 clubs with the most players playing at this tournament:\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Number of players playing for..\"\n\nx = world_cup.groupby(\"Club\")[\"Club_ctry\"].count().sort_values(ascending=True).tail(10)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=16)\n\nText(0.5, 1.0, 'Number of players playing for..')\n\n\n\n\n\nAl-Sadd, wo play in the Qatar Stars league have 13 players (all playing for Qatar) at the tournament, closely followed by Barcelona with 12, Munich and Manchester City, with 11 and Manchester United having 10. Saudi Arabia’s squad all play within Saudi Arabia, 7 of them for Al_Hilal.\n\n\n\n\n\n\nSadly, my country didn’t make it, but there are some players who play domestically in Scotland who will be in Qatar representing their country. Let’s have a look and see who they are:\n\nscotland = world_cup[world_cup['Club_ctry'] == 'Scotland']\nscotland\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      159\n      Cameron Carter-Vickers\n      DEF\n      24.00\n      USA\n      Washington, DC\n      10\n      B\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      200\n      Dylan Levitt\n      MID\n      21.00\n      Wales\n      Cardiff\n      13\n      B\n      Dundee United\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      392\n      Aziz Behich\n      DEF\n      31.00\n      Australia\n      Canberra\n      53\n      D\n      Dundee United\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      394\n      Nathaniel Atkinson\n      DEF\n      23.00\n      Australia\n      Canberra\n      5\n      D\n      Hearts\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      397\n      Kye Rowles\n      DEF\n      24.00\n      Australia\n      Canberra\n      3\n      D\n      Hearts\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      400\n      Aaron Mooy\n      MID\n      32.00\n      Australia\n      Canberra\n      53\n      D\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      403\n      Keanu Baccus\n      MID\n      29.00\n      Australia\n      Canberra\n      53\n      D\n      St Mirren\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      404\n      Cameron Devlin\n      MID\n      24.00\n      Australia\n      Canberra\n      1\n      D\n      Hearts\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      464\n      Daizen Maeda\n      FWD\n      25.00\n      Japan\n      Tokyo\n      8\n      E\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      548\n      Borna Barisic\n      DEF\n      29.00\n      Croatia\n      Zagreb\n      28\n      F\n      Rangers\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      554\n      Josip Juranovic\n      DEF\n      27.00\n      Croatia\n      Zagreb\n      21\n      F\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      614\n      David Wotherspoon\n      MID\n      32.00\n      Canada\n      Ottawa\n      10\n      F\n      St. Johnstone\n      Scotland\n      Edinburgh\n      Away\n    \n  \n\n\n\n\nInterestingly, out of the 12 players who play their club football in Scotland, 6 are Australian. The shared language is probably a contributory factor, certainly not the search for warmer weather.\n\n\n\n\n\n\nHaving recently relocated here, at least I now have a team to follow! Let’s have a look at the players who play their club football in Poland:\n\npoland_club = world_cup[world_cup['Club_ctry'] == 'Poland']\npoland_club\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      295\n      Artur Jedrzejczyk\n      DEF\n      34.00\n      Poland\n      Warsaw\n      40\n      C\n      Legia Warsaw\n      Poland\n      Warsaw\n      Home\n    \n    \n      297\n      Michal Skoras\n      MID\n      22.00\n      Poland\n      Warsaw\n      1\n      C\n      Lech Poznan\n      Poland\n      Warsaw\n      Home\n    \n    \n      301\n      Kamil Grosicki\n      MID\n      34.00\n      Poland\n      Warsaw\n      87\n      C\n      Pogon Szczecin\n      Poland\n      Warsaw\n      Home\n    \n    \n      706\n      Filip Mladenovic\n      DEF\n      31.00\n      Serbia\n      Belgrade\n      20\n      G\n      Legia Warsaw\n      Poland\n      Warsaw\n      Away\n    \n  \n\n\n\n\nOnly 4 players, 3 of which are Polish. The lone soldier is Filip Mladenovic of Serbia who plays his club football with Legia Warsaw. Let’s have a look at the Poland squad in general:\n\npoland = world_cup[world_cup['Country'] == 'Poland']\npoland\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      285\n      Wojciech Szczesny\n      GK\n      32.00\n      Poland\n      Warsaw\n      66\n      C\n      Juventus\n      Italy\n      Rome\n      Away\n    \n    \n      286\n      Lukasz Skorupski\n      GK\n      31.00\n      Poland\n      Warsaw\n      8\n      C\n      Bologna\n      Italy\n      Rome\n      Away\n    \n    \n      287\n      Kamil Grabara\n      GK\n      23.00\n      Poland\n      Warsaw\n      1\n      C\n      Copenhagen\n      Denmark\n      Copenhagen\n      Away\n    \n    \n      288\n      Jan Bednarek\n      DEF\n      26.00\n      Poland\n      Warsaw\n      45\n      C\n      Aston Villa\n      England\n      London\n      Away\n    \n    \n      289\n      Kamil Glik\n      DEF\n      34.00\n      Poland\n      Warsaw\n      99\n      C\n      Benevento\n      Italy\n      Rome\n      Away\n    \n    \n      290\n      Matty Cash\n      DEF\n      25.00\n      Poland\n      Warsaw\n      7\n      C\n      Aston Villa\n      England\n      London\n      Away\n    \n    \n      291\n      Jakub Kiwior\n      DEF\n      22.00\n      Poland\n      Warsaw\n      3\n      C\n      Spezia\n      Italy\n      Rome\n      Away\n    \n    \n      292\n      Robert Gumny\n      DEF\n      24.00\n      Poland\n      Warsaw\n      5\n      C\n      FC Augsburg\n      Germany\n      Berlin\n      Away\n    \n    \n      293\n      Bartosz Bereszynski\n      DEF\n      30.00\n      Poland\n      Warsaw\n      46\n      C\n      Sampdoria\n      Italy\n      Rome\n      Away\n    \n    \n      294\n      Mateusz Wieteska\n      DEF\n      25.00\n      Poland\n      Warsaw\n      2\n      C\n      Clermont\n      France\n      Paris\n      Away\n    \n    \n      295\n      Artur Jedrzejczyk\n      DEF\n      34.00\n      Poland\n      Warsaw\n      40\n      C\n      Legia Warsaw\n      Poland\n      Warsaw\n      Home\n    \n    \n      296\n      Nicola Zalewski\n      MID\n      20.00\n      Poland\n      Warsaw\n      7\n      C\n      Roma\n      Italy\n      Rome\n      Away\n    \n    \n      297\n      Michal Skoras\n      MID\n      22.00\n      Poland\n      Warsaw\n      1\n      C\n      Lech Poznan\n      Poland\n      Warsaw\n      Home\n    \n    \n      298\n      Grzegorz Krychowiak\n      MID\n      32.00\n      Poland\n      Warsaw\n      94\n      C\n      Al Shabab\n      Saudi Arabia\n      Riyadh\n      Away\n    \n    \n      299\n      Piotr Zielinski\n      MID\n      28.00\n      Poland\n      Warsaw\n      74\n      C\n      Napoli\n      Italy\n      Rome\n      Away\n    \n    \n      300\n      Krystian Bielik\n      MID\n      24.00\n      Poland\n      Warsaw\n      5\n      C\n      Birmingham\n      England\n      London\n      Away\n    \n    \n      301\n      Kamil Grosicki\n      MID\n      34.00\n      Poland\n      Warsaw\n      87\n      C\n      Pogon Szczecin\n      Poland\n      Warsaw\n      Home\n    \n    \n      302\n      Przemyslaw Frankowski\n      MID\n      27.00\n      Poland\n      Warsaw\n      26\n      C\n      Lens\n      France\n      Paris\n      Away\n    \n    \n      303\n      Sebastian Szymanski\n      MID\n      23.00\n      Poland\n      Warsaw\n      18\n      C\n      Feyenoord\n      Netherlands\n      Amsterdam\n      Away\n    \n    \n      304\n      Damian Szymanski\n      MID\n      27.00\n      Poland\n      Warsaw\n      9\n      C\n      AEK Athens\n      Greece\n      Athens\n      Away\n    \n    \n      305\n      Szymon Zurkowski\n      MID\n      25.00\n      Poland\n      Warsaw\n      7\n      C\n      Fiorentina\n      Italy\n      Rome\n      Away\n    \n    \n      306\n      Jakub Kaminski\n      MID\n      20.00\n      Poland\n      Warsaw\n      4\n      C\n      Wolfsburg\n      Germany\n      Berlin\n      Away\n    \n    \n      307\n      Krzysztof Piatek\n      FWD\n      27.00\n      Poland\n      Warsaw\n      11\n      C\n      Salernitana\n      Italy\n      Rome\n      Away\n    \n    \n      308\n      Karol Swiderski\n      FWD\n      25.00\n      Poland\n      Warsaw\n      18\n      C\n      Charlotte\n      USA\n      Washington, DC\n      Away\n    \n    \n      309\n      Arkadiusz Milik\n      FWD\n      28.00\n      Poland\n      Warsaw\n      64\n      C\n      Juventus\n      Italy\n      Rome\n      Away\n    \n    \n      310\n      Robert Lewandowski\n      FWD\n      34.00\n      Poland\n      Warsaw\n      134\n      C\n      Barcelona\n      Spain\n      Madrid\n      Away\n    \n  \n\n\n\n\n\n\n\n\n\n\nhome and away.jpg\n\n\nThese days players travel far and wide to ply their trade. I wondered what the impact of that might be on the tightness of a squad, and thought it would be interesting to take a closer look at where players play domestically.\nLet’s take a look at the distribution of where the players play their club football:\n\nworld_cup['Home_Away'].value_counts()\n\nAway    551\nHome    279\nName: Home_Away, dtype: int64\n\n\nMost players play their club football outside of their home nation. Let’s illustrate that graphically with a bar plot:\n\nhome_away_plot = world_cup.groupby(['Home_Away', 'Country']).size().sort_values(ascending=False).reset_index().pivot(columns='Home_Away', index='Country', values=0)\nhome_away_plot.plot(kind='barh', stacked=True)\n\n<AxesSubplot: ylabel='Country'>\n\n\n\n\n\nThat’s quite insightful and re-emphasises that most players do play their club football outside of their home nation, exemplified at the extreme by Senegal, where the entire squad are based outside of Senegal. At the other extreme, all of the Qatar and Saudi Arabia squads are based at home. England are the next ‘tightest’ squad. Will this contribute to a successful tournament? All will be revealed over the next few weeks!\n\n\n\n\n\n\nhuddle.jpg\n\n\nWe have already established that the total distance for Saudi Arabia and Qatar is zero (their squads all play club football locally), but let’s try to establish just how far flung the other squads are, by calculating the distance for each player from their nation’s capital to the capital of the country where they play their club football.\nApologies for this next section which is not very Pythonic! In hindsight this was probably a tad ambitious for me as someone just starting out, and I ran into all sorts of obstacles, but I got there in the end, and thankfully before the tournament ended!\n\n\nWe can obtain the co-ordinates of the capital cities of the countries where the players play their club football using the Geopy library in Python. There is a very useful guide available here:\n\n## Install required package\n!pip install geopy\n\n# import required module\nfrom geopy.geocoders import Nominatim\n\nRequirement already satisfied: geopy in /home/stephen137/mambaforge/lib/python3.10/site-packages (2.2.0)\nRequirement already satisfied: geographiclib<2,>=1.49 in /home/stephen137/mambaforge/lib/python3.10/site-packages (from geopy) (1.52)\n\n\n\ncountries = ('England', 'Spain', 'Germany', 'Italy', 'France', 'Saudi Arabia', 'Qatar', 'United States', 'Belgium', 'Mexico', 'Turkey', 'Netherlands', 'Portugal', 'Costa Rica', 'South Korea', 'Greece', 'Scotland', 'Japan',             \n'Canada', 'Switzerland', 'Iran',  'Denmark',  'Tunisia',  'Australia',  'Croatia' , 'Brazil',  'Argentina',  'Wales', 'Poland',  'Morocco',  'Ecuador', 'Austria', 'Serbia', 'Uruguay',  'Kuwait', 'Russia',  'Ghana', 'Egypt',                                    \n'Cyprus', 'China', 'Cameroon', 'Colombia',  'United Arab Emirates', 'Senegal') \n\nFirst, we can create a list of capital cities for the above countries, and then create a function to loop through this list, and extract the longitude and latitude for each of the cities:\n\n# List of capitals\ncapitals = ['London', 'Madrid', 'Berlin', 'Rome', 'Paris', 'Riyadh', 'Doha', 'Washington, DC', 'Brussels', 'Mexico City', 'Ankara', 'Amsterdam', 'Lisbon', 'San Jose', 'Seoul', 'Athens', 'Edinburgh', 'Tokyo', \n'Ottawa', 'Berne', 'Tehran', 'Copenhagen', 'Tunis', 'Canberra', 'Zagreb', 'Brasilia', 'Buenos Aires', 'Cardiff', 'Warsaw', 'Rabat', 'Quito', 'Vienna', 'Belgrade', 'Montevideo', 'Kuwait City', 'Moscow', 'Accra', 'Cairo',\n'Nicosia', 'Beijing', 'Yaounde', 'Bogota','Abu Dhabi','Dakar']\n\ngeolocator = Nominatim(user_agent=\"GetLoc\")\n\n# loop through list of capitals and return their co-ordinates\nfor capital in capitals:\n    location = geolocator.geocode(capital)\n    lat = location.latitude\n    long = location.longitude \n    print(lat,long)\n\n51.5073219 -0.1276474\n40.4167047 -3.7035825\n52.5170365 13.3888599\n41.8933203 12.4829321\n48.8588897 2.3200410217200766\n24.638916 46.7160104\n25.2856329 51.5264162\n38.8950368 -77.0365427\n50.8465573 4.351697\n19.4326296 -99.1331785\n39.9207886 32.8540482\n52.3727598 4.8936041\n38.7077507 -9.1365919\n37.3361663 -121.890591\n37.5666791 126.9782914\n37.9839412 23.7283052\n55.9533456 -3.1883749\n35.6828387 139.7594549\n45.4208777 -75.6901106\n46.9482713 7.4514512\n35.6892523 51.3896004\n55.6867243 12.5700724\n33.8439408 9.400138\n-35.2975906 149.1012676\n45.84264135 15.962231476593626\n-10.3333333 -53.2\n-34.6075682 -58.4370894\n51.4816546 -3.1791934\n52.2337172 21.071432235636493\n34.022405 -6.834543\n-0.2201641 -78.5123274\n48.2083537 16.3725042\n44.8178131 20.4568974\n-34.9058916 -56.1913095\n29.3796532 47.9734174\n55.7504461 37.6174943\n5.5571096 -0.2012376\n30.0443879 31.2357257\n35.1748976 33.3638568\n39.906217 116.3912757\n3.8689867 11.5213344\n4.6534649 -74.0836453\n24.4538352 54.3774014\n14.693425 -17.447938\n\n\nNow create a list which combines the capitals and their co-ordinates:\n\nco_ordinates = [[\"London\", 51.5073219, -0.1276474],\n['Madrid', 40.4167047, -3.7035825],\n['Berlin', 52.5170365, 13.3888599],\n['Rome',  41.8933203, 12.4829321],\n['Paris', 48.8588897, 2.3200410217200766],\n['Riyadh', 24.638916, 46.7160104],\n['Doha', 25.2856329, 51.5264162],\n['Washington, DC', 38.8950368, -77.0365427],\n['Brussels', 50.8465573, 4.351697],\n['Mexico City', 19.4326296, -99.1331785],\n['Ankara', 39.9207886, 32.8540482],\n['Amsterdam', 52.3727598, 4.8936041],\n['Lisbon', 38.7077507, -9.1365919],\n['San Jose', 37.3361663, -121.890591],\n['Seoul', 37.5666791, 126.9782914],\n['Athens', 37.9839412, 23.7283052],\n['Edinburgh', 55.9533456, -3.1883749],\n['Tokyo', 35.6828387, 139.7594549],\n['Ottawa', 45.4208777, -75.6901106],\n['Berne', 46.9482713, 7.4514512],\n['Tehran', 35.6892523, 51.3896004],\n['Copenhagen', 55.6867243, 12.5700724],\n['Tunis', 33.8439408, 9.400138],\n['Canberra', 35.2975906, 149.1012676],\n['Zagreb', 45.84264135, 15.962231476593626],\n['Brasilia', -10.3333333, -53.2],\n['Buenos Aires', -34.6075682, -58.4370894],\n['Cardiff', 51.4816546, -3.1791934],\n['Warsaw', 52.2337172, 21.071432235636493],\n['Rabat', 34.022405, -6.834543],\n['Quito', -0.2201641, -78.5123274],\n['Vienna', 48.2083537, 16.3725042],\n['Belgrade', 44.8178131, 20.4568974],\n['Montevideo', -34.9058916, -56.1913095],\n['Kuwait City', 29.3796532, 47.9734174],\n['Moscow', 55.7504461, 37.6174943],\n['Accra', 5.5571096, -0.2012376],\n['Cairo', 30.0443879, 31.2357257],\n['Nicosia', 35.1748976, 33.3638568],\n['Beijing', 39.906217, 116.3912757],\n['Yaounde', 3.8689867, 11.5213344],\n['Bogota', 4.6534649, -74.0836453],\n['Abu Dhabi', 24.4538352, 54.3774014],\n['Dakar', 14.693425, -17.447938]]    \n\nAnd now create separate DataFrames for our ‘from’ and ‘to’ destinations:\n\nfrom_co_ord = pd.DataFrame(co_ordinates,columns=['from', 'from_long', 'from_lat'])\nto_co_ord = pd.DataFrame(co_ordinates,columns=['to', 'to_long', 'to_lat'])\n\n\nfrom_co_ord.head()\n\n\n\n\n\n  \n    \n      \n      from\n      from_long\n      from_lat\n    \n  \n  \n    \n      0\n      London\n      51.51\n      -0.13\n    \n    \n      1\n      Madrid\n      40.42\n      -3.70\n    \n    \n      2\n      Berlin\n      52.52\n      13.39\n    \n    \n      3\n      Rome\n      41.89\n      12.48\n    \n    \n      4\n      Paris\n      48.86\n      2.32\n    \n  \n\n\n\n\n\nto_co_ord.head()\n\n\n\n\n\n  \n    \n      \n      to\n      to_long\n      to_lat\n    \n  \n  \n    \n      0\n      London\n      51.51\n      -0.13\n    \n    \n      1\n      Madrid\n      40.42\n      -3.70\n    \n    \n      2\n      Berlin\n      52.52\n      13.39\n    \n    \n      3\n      Rome\n      41.89\n      12.48\n    \n    \n      4\n      Paris\n      48.86\n      2.32\n    \n  \n\n\n\n\n\n\n\nWe need to find all the possible ‘from’ : ‘to’ combinations in order to calculate the distances between the cities. We have 44 cities which gives according to this handy calculator , 946 pairings without repetitions.\nAfter some digging around I found this post on stackoverflow which gave a general overview of how this might be achieved. In order to obtain the pairings and index them, we can use the MultiIndex.from_product pandas class:\n\nidx = pd.MultiIndex.from_product([from_co_ord.index, to_co_ord.index], names=['from', 'to'])\n\n\n# create a combined DataFrame that joins our from and to DataFrames\n# Includes all possible pairings (including duplicates)\n\nfrom_to = pd.DataFrame(index=idx) \\\n        .join(from_co_ord[['from','from_lat', 'from_long']], on='from') \\\n        .join(to_co_ord[['to','to_lat', 'to_long']], on='to')\n\n\nfrom_to\n\n\n\n\n\n  \n    \n      \n      \n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n    \n    \n      from\n      to\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      London\n      -0.13\n      51.51\n      London\n      -0.13\n      51.51\n    \n    \n      1\n      London\n      -0.13\n      51.51\n      Madrid\n      -3.70\n      40.42\n    \n    \n      2\n      London\n      -0.13\n      51.51\n      Berlin\n      13.39\n      52.52\n    \n    \n      3\n      London\n      -0.13\n      51.51\n      Rome\n      12.48\n      41.89\n    \n    \n      4\n      London\n      -0.13\n      51.51\n      Paris\n      2.32\n      48.86\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      39\n      Dakar\n      -17.45\n      14.69\n      Beijing\n      116.39\n      39.91\n    \n    \n      40\n      Dakar\n      -17.45\n      14.69\n      Yaounde\n      11.52\n      3.87\n    \n    \n      41\n      Dakar\n      -17.45\n      14.69\n      Bogota\n      -74.08\n      4.65\n    \n    \n      42\n      Dakar\n      -17.45\n      14.69\n      Abu Dhabi\n      54.38\n      24.45\n    \n    \n      43\n      Dakar\n      -17.45\n      14.69\n      Dakar\n      -17.45\n      14.69\n    \n  \n\n1936 rows × 6 columns\n\n\n\nThat’s not quite as concise as we would like - we have 1936 pairings. That’s because MultiIndex.from_prodcut has returned values for London to London, Dakar to Dakar, etc. We also have London to Madrid, and Madrid to London which is also duplication. Let’s move forward.\n\n\n\n\n\n\nproclaimers.jpg\n\n\nWe can calculate the distance between two locations using Haversine. Let’s create a function that allows us to return values for all our pairings:\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    miles = km * 0.621371\n    return miles\n\n# Add a new column to our from_to DataFrame to include the distance calculations\nfrom_to['Distance_miles'] = haversine_np(*from_to[['from_lat', 'from_long', 'to_lat', 'to_long']].values.T)\n\nLet’s take a look at our completed distances DataFrame:\n\nfrom_to\n\n\n\n\n\n  \n    \n      \n      \n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n      Distance_miles\n    \n    \n      from\n      to\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      London\n      -0.13\n      51.51\n      London\n      -0.13\n      51.51\n      0.00\n    \n    \n      1\n      London\n      -0.13\n      51.51\n      Madrid\n      -3.70\n      40.42\n      784.56\n    \n    \n      2\n      London\n      -0.13\n      51.51\n      Berlin\n      13.39\n      52.52\n      577.80\n    \n    \n      3\n      London\n      -0.13\n      51.51\n      Rome\n      12.48\n      41.89\n      890.44\n    \n    \n      4\n      London\n      -0.13\n      51.51\n      Paris\n      2.32\n      48.86\n      212.47\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      39\n      Dakar\n      -17.45\n      14.69\n      Beijing\n      116.39\n      39.91\n      7634.29\n    \n    \n      40\n      Dakar\n      -17.45\n      14.69\n      Yaounde\n      11.52\n      3.87\n      2107.38\n    \n    \n      41\n      Dakar\n      -17.45\n      14.69\n      Bogota\n      -74.08\n      4.65\n      3906.72\n    \n    \n      42\n      Dakar\n      -17.45\n      14.69\n      Abu Dhabi\n      54.38\n      24.45\n      4673.86\n    \n    \n      43\n      Dakar\n      -17.45\n      14.69\n      Dakar\n      -17.45\n      14.69\n      0.00\n    \n  \n\n1936 rows × 7 columns\n\n\n\n\n\n\n\nWe now want to include the distance figures in our orginal world_cup DataFrame to allow us to calculate the total distance per squad, which will give us some sort of comaprison of the ‘closeness’ of the 32 teams. I am familiar with this feature in Excel but you can do the same thing in Python. Here is a useful article on how to do it.\nLet’s first get our DataFrames tee’d up by adding a common column for joining on:\n\nfrom_to[\"from_to\"] = from_to['from'] + \" to \" + from_to['to']\nfrom_to\n\n\n\n\n\n  \n    \n      \n      \n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n      Distance_miles\n      from_to\n    \n    \n      from\n      to\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      London\n      -0.13\n      51.51\n      London\n      -0.13\n      51.51\n      0.00\n      London to London\n    \n    \n      1\n      London\n      -0.13\n      51.51\n      Madrid\n      -3.70\n      40.42\n      784.56\n      London to Madrid\n    \n    \n      2\n      London\n      -0.13\n      51.51\n      Berlin\n      13.39\n      52.52\n      577.80\n      London to Berlin\n    \n    \n      3\n      London\n      -0.13\n      51.51\n      Rome\n      12.48\n      41.89\n      890.44\n      London to Rome\n    \n    \n      4\n      London\n      -0.13\n      51.51\n      Paris\n      2.32\n      48.86\n      212.47\n      London to Paris\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      39\n      Dakar\n      -17.45\n      14.69\n      Beijing\n      116.39\n      39.91\n      7634.29\n      Dakar to Beijing\n    \n    \n      40\n      Dakar\n      -17.45\n      14.69\n      Yaounde\n      11.52\n      3.87\n      2107.38\n      Dakar to Yaounde\n    \n    \n      41\n      Dakar\n      -17.45\n      14.69\n      Bogota\n      -74.08\n      4.65\n      3906.72\n      Dakar to Bogota\n    \n    \n      42\n      Dakar\n      -17.45\n      14.69\n      Abu Dhabi\n      54.38\n      24.45\n      4673.86\n      Dakar to Abu Dhabi\n    \n    \n      43\n      Dakar\n      -17.45\n      14.69\n      Dakar\n      -17.45\n      14.69\n      0.00\n      Dakar to Dakar\n    \n  \n\n1936 rows × 8 columns\n\n\n\n\nworld_cup[\"from_to\"] = world_cup['Ctry_cap'] + \" to \" + world_cup['Club_ctry_cap']\nworld_cup\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n      from_to\n    \n  \n  \n    \n      0\n      Saad Al-Sheeb\n      GK\n      32.00\n      Qatar\n      Doha\n      80\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      1\n      Meshaal Barsham\n      GK\n      24.00\n      Qatar\n      Doha\n      15\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      2\n      Yousuf Hassan\n      GK\n      26.00\n      Qatar\n      Doha\n      9\n      A\n      Al-Gharafa\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      3\n      Pedro Miguel\n      DEF\n      32.00\n      Qatar\n      Doha\n      78\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      4\n      Musaab Khidir\n      DEF\n      29.00\n      Qatar\n      Doha\n      29\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      825\n      Kamaldeen Sulemana\n      MID\n      20.00\n      Ghana\n      Accra\n      11\n      H\n      Stade Rennes\n      France\n      Paris\n      Away\n      Accra to Paris\n    \n    \n      826\n      Antoine Semenyo\n      FWD\n      22.00\n      Ghana\n      Accra\n      1\n      H\n      Bristol City\n      England\n      London\n      Away\n      Accra to London\n    \n    \n      827\n      Andre Ayew\n      FWD\n      32.00\n      Ghana\n      Accra\n      107\n      H\n      Al Sadd\n      Qatar\n      Doha\n      Away\n      Accra to Doha\n    \n    \n      828\n      Jordan Ayew\n      FWD\n      31.00\n      Ghana\n      Accra\n      82\n      H\n      Crystal Palace\n      England\n      London\n      Away\n      Accra to London\n    \n    \n      829\n      Inaki Williams\n      FWD\n      28.00\n      Ghana\n      Accra\n      1\n      H\n      Athletic Club\n      Spain\n      Madrid\n      Away\n      Accra to Madrid\n    \n  \n\n830 rows × 12 columns\n\n\n\nFinally we can join our two DataFrames together which will give us a distance column and value for each player in the tournament:\n\ninner_join = pd.merge(world_cup, from_to, on='from_to',how='inner')\n\n\ninner_join\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n      from_to\n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n      Distance_miles\n    \n  \n  \n    \n      0\n      Saad Al-Sheeb\n      GK\n      32.00\n      Qatar\n      Doha\n      80\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      1\n      Meshaal Barsham\n      GK\n      24.00\n      Qatar\n      Doha\n      15\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      2\n      Yousuf Hassan\n      GK\n      26.00\n      Qatar\n      Doha\n      9\n      A\n      Al-Gharafa\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      3\n      Pedro Miguel\n      DEF\n      32.00\n      Qatar\n      Doha\n      78\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      4\n      Musaab Khidir\n      DEF\n      29.00\n      Qatar\n      Doha\n      29\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      805\n      Mohammed Kudus\n      MID\n      22.00\n      Ghana\n      Accra\n      16\n      H\n      Ajax\n      Netherlands\n      Amsterdam\n      Away\n      Accra to Amsterdam\n      Accra\n      -0.20\n      5.56\n      Amsterdam\n      4.89\n      52.37\n      3245.62\n    \n    \n      806\n      Daniel Kofi-Kyereh\n      MID\n      26.00\n      Ghana\n      Accra\n      12\n      H\n      Freiburg\n      Germany\n      Berlin\n      Away\n      Accra to Berlin\n      Accra\n      -0.20\n      5.56\n      Berlin\n      13.39\n      52.52\n      3333.41\n    \n    \n      807\n      Fatawu Issahaku\n      MID\n      18.00\n      Ghana\n      Accra\n      11\n      H\n      Sporting\n      Portugal\n      Lisbon\n      Away\n      Accra to Lisbon\n      Accra\n      -0.20\n      5.56\n      Lisbon\n      -9.14\n      38.71\n      2356.37\n    \n    \n      808\n      Osman Bukari\n      MID\n      23.00\n      Ghana\n      Accra\n      5\n      H\n      Red Star Belgrade\n      Serbia\n      Belgrade\n      Away\n      Accra to Belgrade\n      Accra\n      -0.20\n      5.56\n      Belgrade\n      20.46\n      44.82\n      2983.47\n    \n    \n      809\n      Andre Ayew\n      FWD\n      32.00\n      Ghana\n      Accra\n      107\n      H\n      Al Sadd\n      Qatar\n      Doha\n      Away\n      Accra to Doha\n      Accra\n      -0.20\n      5.56\n      Doha\n      51.53\n      25.29\n      3674.63\n    \n  \n\n810 rows × 19 columns\n\n\n\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Total miles travelled by squad - from their club country capital to their nation capital\"\n\nx = inner_join.groupby('Country')['Distance_miles'].sum().sort_values(ascending=False)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Total miles travelled by squad - from their club country capital to their nation capital')\n\n\n\n\n\n\ninner_join.groupby(\"Country\")[\"Distance_miles\"].sum().sort_values(ascending=True)\n\nCountry\nSaudi Arabia        0.00\nQatar               0.00\nEngland           577.80\nGermany          4631.29\nSpain            6389.33\nNetherlands      6658.89\nWales            7674.01\nBelgium          7824.11\nFrance           9663.44\nSwitzerland     12648.83\nCroatia         13322.72\nDenmark         15194.21\nPortugal        18366.13\nSerbia          19948.33\nPoland          23557.92\nTunisia         25922.17\nIran            26752.28\nMorocco         27042.30\nCosta Rica      37651.80\nCanada          47922.17\nSouth Korea     53858.03\nMexico          55687.73\nUSA             61448.54\nSenegal         66198.44\nGhana           73052.50\nCameroon        78656.09\nEcuador         80700.85\nUruguay         93702.58\nAustralia       98441.33\nJapan          112062.53\nBrazil         118045.02\nArgentina      162925.56\nName: Distance_miles, dtype: float64\n\n\n\n\n\n\n\n\nHome advantage often counts. If you recall South Korea reached the semi final when they hosted the tournament back in 2002, and if you look back even further to 1966 then… Let’s end that there.\nWill this togetherness give Qatar an advantage? Time will tell. They also have the ‘tightest’ squad (along with Saudi Arabia) in terms of the fact that all of their players play their club football at home. Outside of those two, England are the ‘closest’ squad, with only Jude Bellingham (Borussia Dortmund) playing his club football outside of England.\nThe Argentina squad are the most scattered, followed by Brazil, Argentina, and Japan. This makes sense, as most of their squads play in Europe, which is a long way from home!\n\n\n\nThis has been a rewarding project overall for me. I achieved what I set out to do, which began with a vague idea of the ‘tightness’ of the World Cup squads, and how I might quantify this, perhaps by looking at where the players play their club football, and how far they would have to travel to begin preparations back in their home nation.\nThere were many obstacles along the way, the main one was working out how to calculate the distance between two points, and this gave me a first introduction to working with geospatial data, including the Geopy library and Haversine. I also managed to create one or two functions to automate the extraction of co-ordinate data and distance calculations and found about one of the more advanced panda classes, MultiIndex.from_product.\nI was familiar with the VLOOKUP fuction in Excel, and the various join clauses in SQL, but I now know how to achieve the same end result using pandas merge.\nI’m looking forward to seeing how this Tournament unfolds - may the best team win!"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html",
    "href": "posts/Huggy Bear/huggy-bear.html",
    "title": "Huggy Bear",
    "section": "",
    "text": "This is my follow up to Lesson 2: Practical Deep Learning for Coders 2022 in which Jeremy created a dog | cat classifier model and deployed to Hugging Face. During this project I will try to replicate on an image classification model, which discriminates between three types of bear: grizzly, black, and teddy bears. Once we’ve done this we will proceed to deploy the model as a working app on Hugging Face!"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#install-the-required-packages",
    "href": "posts/Huggy Bear/huggy-bear.html#install-the-required-packages",
    "title": "Huggy Bear",
    "section": "Install the required packages",
    "text": "Install the required packages\n::: {.cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ outputId=‘ba21b811-767c-459a-ccdf-044758720a55’ papermill=‘{“duration”:23.212506,“end_time”:“2022-10-10T06:59:22.512822”,“exception”:false,“start_time”:“2022-10-10T06:58:59.300316”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\n! pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n:::"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#grab-our-images",
    "href": "posts/Huggy Bear/huggy-bear.html#grab-our-images",
    "title": "Huggy Bear",
    "section": "Grab our images",
    "text": "Grab our images\nNow, it’s time to get hold of our bear images. There are many images on the internet of each type of bear that we can use. We just need a way to find them and download them. One method is to use fastai’s search_images_ddg which grabs images from DuckDuckGo. Note that the number of images is restricted by default to a maximum of 200:\n\nsearch_images_ddg\nims = search_images_ddg('grizzly bear')\nlen(ims)\n\n200\n\n\nNB: there’s no way to be sure exactly what images a search like this will find. The results can change over time. We’ve heard of at least one case of a community member who found some unpleasant pictures of dead bears in their search results. You’ll receive whatever images are found by the web search engine. If you’re running this at work, or with kids, etc, then be cautious before you display the downloaded images.\nLet’s take a look at an image:\n\ndest = 'images/grizzly.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      102.83% [262144/254920 00:00<00:00]\n    \n    \n\n\nPath('images/grizzly.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(128,128)\n\n\n\n\n\nbear_types = 'grizzly','black','teddy'\npath = Path('bears')\n\nThis seems to have worked nicely. Let’s use fastai’s download_images to download all the URLs for each of our search terms. We’ll put each in a separate folder:\n\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o} bear')\n        download_images(dest, urls=results)\n\nOur folder has image files, as we’d expect:\n\nfns = get_image_files(path)\nfns\n\n(#295) [Path('bears/grizzly/bd6ab4a5-5126-492d-b69e-bd15e8f8db86.jpg'),Path('bears/grizzly/54c2968d-f1eb-473f-8979-abd2c6a1e9b9.jpg'),Path('bears/grizzly/06c6eeac-94a8-4634-a979-5de67ced9b3e.jpg'),Path('bears/grizzly/6d6c394c-596b-4953-b98c-f90df597d7c1.jpg'),Path('bears/grizzly/d0fb653a-1328-4f3d-95ce-1b5bc588ef04.jpg'),Path('bears/grizzly/b5179606-5957-4a29-8623-fa05ff910ccc.jpg'),Path('bears/grizzly/3e5bb95f-76bf-4847-9c86-a000077fc371.jpg'),Path('bears/grizzly/484483b9-7453-4e08-8487-9066da336bbb.jpg'),Path('bears/grizzly/d39dd49d-669a-4f15-96e7-9476875d859f.jpg'),Path('bears/grizzly/d3f5a27e-a2bc-474a-8acb-afd0708dee20.jpg')...]"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#cleaning-our-data",
    "href": "posts/Huggy Bear/huggy-bear.html#cleaning-our-data",
    "title": "Huggy Bear",
    "section": "Cleaning our data",
    "text": "Cleaning our data\nOften when we download files from the internet, there are a few that are corrupt. Let’s check:\n\nfailed = verify_images(fns)\nfailed\n\n(#5) [Path('bears/grizzly/8ce9dba9-9dc8-4bf3-b392-097b28d86baf.jpg'),Path('bears/grizzly/13127ddd-1639-4b89-b81a-439c43a56fc6.jpg'),Path('bears/grizzly/e537fa0c-c6bf-43f3-acda-024af91b1745.jpg'),Path('bears/black/946c1956-6068-4468-bd3a-10af1fc2ae6a.jpg'),Path('bears/black/001594ba-562a-4ff0-8e3a-d84c654146ea.jpg')]\n\n\nTo remove all the failed images, you can use unlink on each of them. Note that, like most fastai functions that return a collection, verify_images returns an object of type L, which includes the map method. This calls the passed function on each element of the collection:\n\nfailed.map(Path.unlink);"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#from-data-to-dataloaders",
    "href": "posts/Huggy Bear/huggy-bear.html#from-data-to-dataloaders",
    "title": "Huggy Bear",
    "section": "From Data To DataLoaders",
    "text": "From Data To DataLoaders\nNow that we have downloaded some data, we need to assemble it in a format suitable for model training. In fastai, that means creating an object called DataLoaders.\nTo turn our downloaded data into a DataLoaders object we need to tell fastai at least four things:\n\nWhat kinds of data we are working with\nHow to get the list of items\nHow to label these items\nHow to create the validation set\n\nFastai has an extremely flexible system called the data block API. With this API you can fully customize every stage of the creation of your DataLoaders. Here is what we need to create a DataLoaders for the dataset that we just downloaded:\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nThis command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data — in this case, the path where the images can be found:\n\ndls = bears.dataloaders(path)\n\nA DataLoaders includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader:\n\ndls.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#data-augmentation",
    "href": "posts/Huggy Bear/huggy-bear.html#data-augmentation",
    "title": "Huggy Bear",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. By default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them:\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\nAll of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use.\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn’t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\nHere’s another example where we replace Resize with RandomResizedCrop, which is the transform that provides the behavior we just described. The most important parameter to pass in is min_scale, which determines how much of the image to select at minimum each time:\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\nWe used unique=True to have the same image repeated with different versions of this RandomResizedCrop transform.\nFor natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the aug_transforms function. Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms parameter (note that we’re not using RandomResizedCrop in this example, so you can see the differences more clearly; we’re also using double the amount of augmentation compared to the default, for the same reason):\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nNow that we have assembled our data in a format fit for model training, let’s actually train an image classifier using it."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#training-our-model-and-using-it-to-clean-our-data",
    "href": "posts/Huggy Bear/huggy-bear.html#training-our-model-and-using-it-to-clean-our-data",
    "title": "Huggy Bear",
    "section": "Training our model and using it to clean our data",
    "text": "Training our model and using it to clean our data\nWe don’t have a lot of data for our problem (200 pictures of each sort of bear at most), so to train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms:\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\nWe can now create our Learner and fine-tune it in the usual way:\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/stephen137/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.192118\n      0.813263\n      0.327586\n      00:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.407396\n      0.375259\n      0.155172\n      00:13\n    \n    \n      1\n      0.308869\n      0.153977\n      0.051724\n      00:14\n    \n    \n      2\n      0.247075\n      0.101119\n      0.051724\n      00:13\n    \n    \n      3\n      0.217112\n      0.085641\n      0.051724\n      00:13"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#confusion-matrix",
    "href": "posts/Huggy Bear/huggy-bear.html#confusion-matrix",
    "title": "Huggy Bear",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nNow let’s see whether the mistakes the model is making are mainly thinking that grizzlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or something else. To visualize this, we can create a confusion matrix:\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rows represent all the black, grizzly, and teddy bears in our dataset, respectively. The columns represent the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the off-diagonal cells represent those which were classified incorrectly. This is one of the many ways that fastai allows you to view the results of your model. It is (of course!) calculated using the validation set. With the color-coding, the goal is to have white everywhere except the diagonal, where we want dark blue. Our bear classifier isn’t making many mistakes!"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#losses",
    "href": "posts/Huggy Bear/huggy-bear.html#losses",
    "title": "Huggy Bear",
    "section": "Losses",
    "text": "Losses\nIt’s helpful to see where exactly our errors are occurring, to see whether they’re due to a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn’t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss.\nThe loss is a number that is higher if the model is incorrect (especially if it’s also confident of its incorrect answer), or if it’s correct, but not confident of its correct answer. In a couple of chapters we’ll learn in depth how loss is calculated and used in the training process. For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction:\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\nThis output shows that the images with the highest losses are ones where the prediction matches the label, however with a low degree of confidence.\nThe intuitive approach to doing data cleaning is to do it before you train a model. But as you’ve seen in this case, a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#image-classifier-cleaner",
    "href": "posts/Huggy Bear/huggy-bear.html#image-classifier-cleaner",
    "title": "Huggy Bear",
    "section": "Image Classifier Cleaner",
    "text": "Image Classifier Cleaner\nfastai includes a handy GUI for data cleaning called ImageClassifierCleaner that allows you to choose a category and the training versus validation set and view the highest-loss images (in order), along with menus to allow images to be selected for removal or relabeling:\n\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that amongst our “black bears” is an image that contains two bears: one grizzly, one black. So, we should choose ‘Delete’ in the menu under this image\nImageClassifierCleaner doesn’t actually do the deleting or changing of labels for you; it just returns the indices of items to change. So, for instance, to delete i.e. (unlink) all images selected for deletion, we would run:\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\nTo recatogrize i.e.(move) images for which we’ve selected a different category, we would run:\n\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#saving-our-model",
    "href": "posts/Huggy Bear/huggy-bear.html#saving-our-model",
    "title": "Huggy Bear",
    "section": "Saving our model",
    "text": "Saving our model\nOnce you’ve got a model you’re happy with, you need to save it, so that you can then copy it over to a server where you’ll use it in production. Remember that a model consists of two parts: the architecture and the trained parameters. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the export method.\nThis method even saves the definition of how to create your DataLoaders. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set DataLoader for inference by default, so your data augmentation will not be applied, which is generally what you want.\nWhen you call export, fastai will save a file called “export.pkl”:\n\nlearn.export('export.pkl')\n\nAfter a few seconds, your model will be downloaded to your computer, where you can then create your app that uses the model."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#hugging-face",
    "href": "posts/Huggy Bear/huggy-bear.html#hugging-face",
    "title": "Huggy Bear",
    "section": "Hugging Face",
    "text": "Hugging Face\nNow that we have a saved model that we are happy with, we can go ahead and deploy it as a working app on Hugging Face.\nYou can view the app here:\nI did run into some issues which required quite a bit of troubleshooting. My problem was that my Hugging Face environment was missing a requirements.txt file - which Hugging Face needs to recognise the fastai library. The text file should include the following text: fastai>=2.0.0"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#key-takeaways",
    "href": "posts/Huggy Bear/huggy-bear.html#key-takeaways",
    "title": "Huggy Bear",
    "section": "Key takeaways",
    "text": "Key takeaways\nThis project involved the following:\n\ndownloading an image set for training our model\ncleaning the data and employing various data augmentation techniques\ntraining our model\nevaluating our model using a Confusion Matrix\nsaving our model and ‘pickling’ it\n\n\n\ndeployment of our image classification app via Hugging Face"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html",
    "href": "posts/Pandas/Effective_Pandas.html",
    "title": "Efficient Pandas",
    "section": "",
    "text": "Having code that is clean, readable and has a logical flow is invaluable. I discovered Structured Query Language (SQL) before Python, and as the name suggests, this already pushes you down this structured, logical road. I have only recently started to explore Python, but my experience so far is that the code can quickly become scattered and difficult to follow, particulary during the exploratory data analysis (EDA) phase.\nI have just finished actively watching Efficient Pandas by Matt Harrison and decided to share the content via this blog. The video feels like a bit of a breakthrough for me, someone who is just starting out in the world of data science, and hopefully others will also benefit from reading this. Adopting the chaining method covered in this blog, whenever possible, should ensure that your code is cleaner, and reads like a recipe of ordered steps, reducing any potential ambiguities."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#initial-set-up",
    "href": "posts/Pandas/Effective_Pandas.html#initial-set-up",
    "title": "Efficient Pandas",
    "section": "Initial set up",
    "text": "Initial set up\n\n# bring in the pandas!\nimport pandas as pd\n\n\n# check which version of pandas we're on\npd.__version__\n\n'1.5.0'\n\n\n\n# control the pandas display features\npd.options.display.min_rows = 20"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#dataset",
    "href": "posts/Pandas/Effective_Pandas.html#dataset",
    "title": "Efficient Pandas",
    "section": "Dataset",
    "text": "Dataset\nThe dataset we will be exploring is from https://www.fueleconomy.gov/feg/download.shtml which is the official U.S. government source for fuel economy information. The zipped csv file can be downloaded from here but we can just read in the file using pandas:\n\n# read in our dataset\nautos = pd.read_csv('https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip')\n\n/tmp/ipykernel_2753/3884461791.py:2: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n  autos = pd.read_csv('https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip')\n\n\n\n# Let's take a look\nautos\n\n\n\n\n\n  \n    \n      \n      barrels08\n      barrelsA08\n      charge120\n      charge240\n      city08\n      city08U\n      cityA08\n      cityA08U\n      cityCD\n      cityE\n      ...\n      mfrCode\n      c240Dscr\n      charge240b\n      c240bDscr\n      createdOn\n      modifiedOn\n      startStop\n      phevCity\n      phevHwy\n      phevComb\n    \n  \n  \n    \n      0\n      15.695714\n      0.0\n      0.0\n      0.0\n      19\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      1\n      29.964545\n      0.0\n      0.0\n      0.0\n      9\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      2\n      12.207778\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      3\n      29.964545\n      0.0\n      0.0\n      0.0\n      10\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      4\n      17.347895\n      0.0\n      0.0\n      0.0\n      17\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      5\n      14.982273\n      0.0\n      0.0\n      0.0\n      21\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      6\n      13.184400\n      0.0\n      0.0\n      0.0\n      22\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      7\n      13.733750\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      8\n      12.677308\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      9\n      13.184400\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41134\n      16.480500\n      0.0\n      0.0\n      0.0\n      18\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41135\n      12.677308\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41136\n      13.733750\n      0.0\n      0.0\n      0.0\n      21\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41137\n      11.771786\n      0.0\n      0.0\n      0.0\n      24\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41138\n      13.184400\n      0.0\n      0.0\n      0.0\n      21\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41139\n      14.982273\n      0.0\n      0.0\n      0.0\n      19\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41140\n      14.330870\n      0.0\n      0.0\n      0.0\n      20\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41141\n      15.695714\n      0.0\n      0.0\n      0.0\n      18\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41142\n      15.695714\n      0.0\n      0.0\n      0.0\n      18\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41143\n      18.311667\n      0.0\n      0.0\n      0.0\n      16\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n  \n\n41144 rows × 83 columns\n\n\n\nSo our dataset includes 41,144 rows and 83 columns - that’s a lot of data! Let’s have a look at the columns:\n\nautos.columns\n\nIndex(['barrels08', 'barrelsA08', 'charge120', 'charge240', 'city08',\n       'city08U', 'cityA08', 'cityA08U', 'cityCD', 'cityE', 'cityUF', 'co2',\n       'co2A', 'co2TailpipeAGpm', 'co2TailpipeGpm', 'comb08', 'comb08U',\n       'combA08', 'combA08U', 'combE', 'combinedCD', 'combinedUF', 'cylinders',\n       'displ', 'drive', 'engId', 'eng_dscr', 'feScore', 'fuelCost08',\n       'fuelCostA08', 'fuelType', 'fuelType1', 'ghgScore', 'ghgScoreA',\n       'highway08', 'highway08U', 'highwayA08', 'highwayA08U', 'highwayCD',\n       'highwayE', 'highwayUF', 'hlv', 'hpv', 'id', 'lv2', 'lv4', 'make',\n       'model', 'mpgData', 'phevBlended', 'pv2', 'pv4', 'range', 'rangeCity',\n       'rangeCityA', 'rangeHwy', 'rangeHwyA', 'trany', 'UCity', 'UCityA',\n       'UHighway', 'UHighwayA', 'VClass', 'year', 'youSaveSpend', 'guzzler',\n       'trans_dscr', 'tCharger', 'sCharger', 'atvType', 'fuelType2', 'rangeA',\n       'evMotor', 'mfrCode', 'c240Dscr', 'charge240b', 'c240bDscr',\n       'createdOn', 'modifiedOn', 'startStop', 'phevCity', 'phevHwy',\n       'phevComb'],\n      dtype='object')"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#data-types",
    "href": "posts/Pandas/Effective_Pandas.html#data-types",
    "title": "Efficient Pandas",
    "section": "Data Types",
    "text": "Data Types\nLet’s concentrate our focus on a subset of the data. Let’s look at 14 of the 83 columns and also find out about the types of data included. Getting the right types will enable analysis and correctness.\n\n# Let's drill down and focus on just 14 of the 83 columns\ncols = ['city08','comb08', 'highway08','cylinders', 'displ', 'drive', 'eng_dscr', 'fuelCost08',\n      'make', 'model', 'trany', 'range', 'createdOn', 'year']       \n\n\n# Let's see the data types for each column\nautos[cols].dtypes\n\ncity08          int64\ncomb08          int64\nhighway08       int64\ncylinders     float64\ndispl         float64\ndrive          object\neng_dscr       object\nfuelCost08      int64\nmake           object\nmodel          object\ntrany          object\nrange           int64\ncreatedOn      object\nyear            int64\ndtype: object\n\n\n\n# Let's see how much memory is being used by column\nautos[cols].memory_usage(deep=True)\n\nIndex             128\ncity08         329152\ncomb08         329152\nhighway08      329152\ncylinders      329152\ndispl          329152\ndrive         3028369\neng_dscr      2135693\nfuelCost08     329152\nmake          2606267\nmodel         2813134\ntrany         2933276\nrange          329152\ncreatedOn     3497240\nyear           329152\ndtype: int64\n\n\n\n# Let's see how much memory is being used in total\nautos[cols].memory_usage(deep=True).sum()\n\n19647323"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#integers-int",
    "href": "posts/Pandas/Effective_Pandas.html#integers-int",
    "title": "Efficient Pandas",
    "section": "Integers (int)",
    "text": "Integers (int)\nIntegers(int) are numbers without a decimal point. Let’s grab some summary statistics for our integer columns:\n\n# summary stats for integer columns\n(autos\n [cols]\n .select_dtypes(int)\n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      7400.000000\n      370.000000\n      2020.000000"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#chaining",
    "href": "posts/Pandas/Effective_Pandas.html#chaining",
    "title": "Efficient Pandas",
    "section": "Chaining",
    "text": "Chaining\nThe above code is fine but it can quickly become cluttered and unreadable. A better way is to lean on SQL coding best practice which means that our code reads more like a recipe of ordered steps:\n\n# use chaining to grab summary stats for integer columns\n(autos\n [cols]\n .select_dtypes(int)\n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      7400.000000\n      370.000000\n      2020.000000\n    \n  \n\n\n\n\nSame result, much more readable code! Chaining is also known as ‘flow programming’. Rather than creating intermediate variables, leverage the fact that most operations return a new object which can be worked on.\nNote, if you can’t find a way to chain we can use pandas .pipe. We’ll see how this works later."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#saving-space",
    "href": "posts/Pandas/Effective_Pandas.html#saving-space",
    "title": "Efficient Pandas",
    "section": "Saving Space",
    "text": "Saving Space\n\nimport numpy as np\n\nThe int columns are currently in int64 format. Let’s try to free up some space by representing our data more memory efficiently. We can use Numpy to help with this:\n\n# Can comb08 column be int8?\nnp.iinfo(np.int8)\n\niinfo(min=-128, max=127, dtype=int8)\n\n\nThe range -128 to 127 can be represented as 8 bits. (There are 256 values which in binary form can be represented by 11111111 i.e. 8 bits). We have a maximum value of 136 for the comb08 column so we can’t convert to int8 without losing info. We can however convert the highway08 column to int8. Let’s try int16:\n\n# Can comb08 column be int16?\nnp.iinfo(np.int16)\n\niinfo(min=-32768, max=32767, dtype=int16)\n\n\nAll of our data is within this range, so we can go ahead and convert all int64 columns to int16 (and int8 for highway08):\n\n# convert from int64 to int16 and int8 in order to free up some memory\n# also obtain summary statistics for integer columns\n(autos\n [cols]\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .select_dtypes(['integer'])\n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      7400.000000\n      370.000000\n      2020.000000\n    \n  \n\n\n\n\nLet’s see if we have saved any space by converting:\n\n# check memory usage\n(autos\n [cols]\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n18124995\n\n\nSo a saving, but not substantial - just under 8%. Let’s see if we can improve on this:"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#floats",
    "href": "posts/Pandas/Effective_Pandas.html#floats",
    "title": "Efficient Pandas",
    "section": "Floats",
    "text": "Floats\nA floating point (known as a float) number has decimal points even if that decimal point value is 0. For example: 1.13, 2.0, 1234.345. If we have a column that contains both integers and floating point numbers, Pandas will assign the entire column to the float data type so the decimal points are not lost.\n\n# Let's take a look at the columns with a float data type\n(autos\n [cols]\n .select_dtypes('float')\n)\n\n\n\n\n\n  \n    \n      \n      cylinders\n      displ\n    \n  \n  \n    \n      0\n      4.0\n      2.0\n    \n    \n      1\n      12.0\n      4.9\n    \n    \n      2\n      4.0\n      2.2\n    \n    \n      3\n      8.0\n      5.2\n    \n    \n      4\n      4.0\n      2.2\n    \n    \n      5\n      4.0\n      1.8\n    \n    \n      6\n      4.0\n      1.8\n    \n    \n      7\n      4.0\n      1.6\n    \n    \n      8\n      4.0\n      1.6\n    \n    \n      9\n      4.0\n      1.8\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      41134\n      4.0\n      2.1\n    \n    \n      41135\n      4.0\n      1.9\n    \n    \n      41136\n      4.0\n      1.9\n    \n    \n      41137\n      4.0\n      1.9\n    \n    \n      41138\n      4.0\n      1.9\n    \n    \n      41139\n      4.0\n      2.2\n    \n    \n      41140\n      4.0\n      2.2\n    \n    \n      41141\n      4.0\n      2.2\n    \n    \n      41142\n      4.0\n      2.2\n    \n    \n      41143\n      4.0\n      2.2\n    \n  \n\n41144 rows × 2 columns\n\n\n\nCylinders look int like - we would expect the number of cylinders to be an integer, and not a float (decimal).\n\n# summary stats for cylinders\n(autos\n .cylinders\n .describe()\n)\n\ncount    40938.000000\nmean         5.717084\nstd          1.755517\nmin          2.000000\n25%          4.000000\n50%          6.000000\n75%          6.000000\nmax         16.000000\nName: cylinders, dtype: float64\n\n\nOops, we have missing values - count = 40,938 but we have 41,144 rows.\n\n# Let's count the various values for cylinders\n(autos\n .cylinders\n .value_counts(dropna=False)\n)\n\n4.0     15938\n6.0     14284\n8.0      8801\n5.0       771\n12.0      626\n3.0       279\nNaN       206\n10.0      170\n2.0        59\n16.0       10\nName: cylinders, dtype: int64\n\n\nAs anticipated, we have missing values (206) represented by NaN\n\n## where are they missing? We can use .query\n(autos\n [cols]\n .query('cylinders.isna()')\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      eng_dscr\n      fuelCost08\n      make\n      model\n      trany\n      range\n      createdOn\n      year\n    \n  \n  \n    \n      7138\n      81\n      85\n      91\n      NaN\n      NaN\n      NaN\n      NaN\n      800\n      Nissan\n      Altra EV\n      NaN\n      90\n      Tue Jan 01 00:00:00 EST 2013\n      2000\n    \n    \n      7139\n      81\n      72\n      64\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      900\n      Toyota\n      RAV4 EV\n      NaN\n      88\n      Tue Jan 01 00:00:00 EST 2013\n      2000\n    \n    \n      8143\n      81\n      72\n      64\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      900\n      Toyota\n      RAV4 EV\n      NaN\n      88\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8144\n      74\n      65\n      58\n      NaN\n      NaN\n      NaN\n      NaN\n      1000\n      Ford\n      Th!nk\n      NaN\n      29\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8146\n      45\n      39\n      33\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      1700\n      Ford\n      Explorer USPS Electric\n      NaN\n      38\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8147\n      84\n      75\n      66\n      NaN\n      NaN\n      NaN\n      NaN\n      900\n      Nissan\n      Hyper-Mini\n      NaN\n      33\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      9212\n      87\n      78\n      69\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      850\n      Toyota\n      RAV4 EV\n      NaN\n      95\n      Tue Jan 01 00:00:00 EST 2013\n      2002\n    \n    \n      9213\n      45\n      39\n      33\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      1700\n      Ford\n      Explorer USPS Electric\n      NaN\n      38\n      Tue Jan 01 00:00:00 EST 2013\n      2002\n    \n    \n      10329\n      87\n      78\n      69\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      850\n      Toyota\n      RAV4 EV\n      NaN\n      95\n      Tue Jan 01 00:00:00 EST 2013\n      2003\n    \n    \n      21413\n      22\n      24\n      28\n      NaN\n      NaN\n      4-Wheel Drive\n      NaN\n      1750\n      Subaru\n      RX Turbo\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34407\n      73\n      72\n      71\n      NaN\n      NaN\n      Front-Wheel Drive\n      NaN\n      900\n      BYD\n      e6\n      Automatic (A1)\n      187\n      Wed Mar 13 00:00:00 EDT 2019\n      2019\n    \n    \n      34408\n      118\n      108\n      97\n      NaN\n      NaN\n      Front-Wheel Drive\n      NaN\n      600\n      Nissan\n      Leaf (62 kW-hr battery pack)\n      Automatic (A1)\n      226\n      Wed Mar 13 00:00:00 EDT 2019\n      2019\n    \n    \n      34409\n      114\n      104\n      94\n      NaN\n      NaN\n      Front-Wheel Drive\n      NaN\n      650\n      Nissan\n      Leaf SV/SL (62 kW-hr battery pack)\n      Automatic (A1)\n      215\n      Wed Mar 13 00:00:00 EDT 2019\n      2019\n    \n    \n      34538\n      74\n      74\n      73\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      900\n      Audi\n      e-tron\n      Automatic (A1)\n      204\n      Tue Apr 16 00:00:00 EDT 2019\n      2019\n    \n    \n      34561\n      80\n      76\n      72\n      NaN\n      NaN\n      4-Wheel Drive\n      NaN\n      850\n      Jaguar\n      I-Pace\n      Automatic (A1)\n      234\n      Thu May 02 00:00:00 EDT 2019\n      2020\n    \n    \n      34563\n      138\n      131\n      124\n      NaN\n      NaN\n      Rear-Wheel Drive\n      NaN\n      500\n      Tesla\n      Model 3 Standard Range\n      Automatic (A1)\n      220\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34564\n      140\n      133\n      124\n      NaN\n      NaN\n      Rear-Wheel Drive\n      NaN\n      500\n      Tesla\n      Model 3 Standard Range Plus\n      Automatic (A1)\n      240\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34565\n      115\n      111\n      107\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      600\n      Tesla\n      Model S Long Range\n      Automatic (A1)\n      370\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34566\n      104\n      104\n      104\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      650\n      Tesla\n      Model S Performance (19\" Wheels)\n      Automatic (A1)\n      345\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34567\n      98\n      97\n      96\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      700\n      Tesla\n      Model S Performance (21\" Wheels)\n      Automatic (A1)\n      325\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n  \n\n206 rows × 14 columns\n\n\n\n\n## chaining - add cylinders and displ columns replacing NaN with 0\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      5.688460\n      3.277904\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      1.797009\n      1.373415\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      0.000000\n      0.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      4.000000\n      2.200000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      6.000000\n      3.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      6.000000\n      4.300000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      16.000000\n      8.400000\n      7400.000000\n      370.000000\n      2020.000000\n    \n  \n\n\n\n\n\n# use this to inspect float sizes\nnp.finfo(np.float16)\n\nfinfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16)\n\n\n\n## chaining - add cylinders and displ columns replacing NaN with 0\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n )\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      eng_dscr\n      fuelCost08\n      make\n      model\n      trany\n      range\n      createdOn\n      year\n    \n  \n  \n    \n      0\n      19\n      21\n      25\n      4\n      2.000000\n      Rear-Wheel Drive\n      (FFS)\n      2000\n      Alfa Romeo\n      Spider Veloce 2000\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      1\n      9\n      11\n      14\n      12\n      4.898438\n      Rear-Wheel Drive\n      (GUZZLER)\n      3850\n      Ferrari\n      Testarossa\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      2\n      23\n      27\n      33\n      4\n      2.199219\n      Front-Wheel Drive\n      (FFS)\n      1550\n      Dodge\n      Charger\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      3\n      10\n      11\n      12\n      8\n      5.199219\n      Rear-Wheel Drive\n      NaN\n      3850\n      Dodge\n      B150/B250 Wagon 2WD\n      Automatic 3-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      4\n      17\n      19\n      23\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      2700\n      Subaru\n      Legacy AWD Turbo\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      5\n      21\n      22\n      24\n      4\n      1.799805\n      Front-Wheel Drive\n      (FFS)\n      1900\n      Subaru\n      Loyale\n      Automatic 3-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      6\n      22\n      25\n      29\n      4\n      1.799805\n      Front-Wheel Drive\n      (FFS)\n      1700\n      Subaru\n      Loyale\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      7\n      23\n      24\n      26\n      4\n      1.599609\n      Front-Wheel Drive\n      (FFS)\n      1750\n      Toyota\n      Corolla\n      Automatic 3-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      8\n      23\n      26\n      31\n      4\n      1.599609\n      Front-Wheel Drive\n      (FFS)\n      1600\n      Toyota\n      Corolla\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      9\n      23\n      25\n      30\n      4\n      1.799805\n      Front-Wheel Drive\n      (FFS)\n      1700\n      Toyota\n      Corolla\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41134\n      18\n      20\n      24\n      4\n      2.099609\n      Front-Wheel Drive\n      (FFS)\n      2100\n      Saab\n      900\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41135\n      23\n      26\n      33\n      4\n      1.900391\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      1600\n      Saturn\n      SL\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41136\n      21\n      24\n      30\n      4\n      1.900391\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      1750\n      Saturn\n      SL\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41137\n      24\n      28\n      33\n      4\n      1.900391\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      1500\n      Saturn\n      SL\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41138\n      21\n      25\n      32\n      4\n      1.900391\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      1700\n      Saturn\n      SL\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41139\n      19\n      22\n      26\n      4\n      2.199219\n      Front-Wheel Drive\n      (FFS)\n      1900\n      Subaru\n      Legacy\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41140\n      20\n      23\n      28\n      4\n      2.199219\n      Front-Wheel Drive\n      (FFS)\n      1850\n      Subaru\n      Legacy\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41141\n      18\n      21\n      24\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      2000\n      Subaru\n      Legacy AWD\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41142\n      18\n      21\n      24\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      2000\n      Subaru\n      Legacy AWD\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41143\n      16\n      18\n      21\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      2900\n      Subaru\n      Legacy AWD Turbo\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n  \n\n41144 rows × 14 columns\n\n\n\n\n# new memory usage\n(autos\n #[cols]\n .loc[:,cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n17590123\n\n\nA further reduction."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#objects",
    "href": "posts/Pandas/Effective_Pandas.html#objects",
    "title": "Efficient Pandas",
    "section": "Objects",
    "text": "Objects\nAt the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices.\n\n# let's take a look at our object columns\n(autos\n [cols]\n .select_dtypes(object)\n)\n\n\n\n\n\n  \n    \n      \n      drive\n      eng_dscr\n      make\n      model\n      trany\n      createdOn\n    \n  \n  \n    \n      0\n      Rear-Wheel Drive\n      (FFS)\n      Alfa Romeo\n      Spider Veloce 2000\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      1\n      Rear-Wheel Drive\n      (GUZZLER)\n      Ferrari\n      Testarossa\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      2\n      Front-Wheel Drive\n      (FFS)\n      Dodge\n      Charger\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      3\n      Rear-Wheel Drive\n      NaN\n      Dodge\n      B150/B250 Wagon 2WD\n      Automatic 3-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      4\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      Subaru\n      Legacy AWD Turbo\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      5\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Loyale\n      Automatic 3-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      6\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Loyale\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      7\n      Front-Wheel Drive\n      (FFS)\n      Toyota\n      Corolla\n      Automatic 3-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      8\n      Front-Wheel Drive\n      (FFS)\n      Toyota\n      Corolla\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      9\n      Front-Wheel Drive\n      (FFS)\n      Toyota\n      Corolla\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41134\n      Front-Wheel Drive\n      (FFS)\n      Saab\n      900\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41135\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      Saturn\n      SL\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41136\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      Saturn\n      SL\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41137\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      Saturn\n      SL\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41138\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      Saturn\n      SL\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41139\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41140\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41141\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy AWD\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41142\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy AWD\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41143\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      Subaru\n      Legacy AWD Turbo\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n  \n\n41144 rows × 6 columns\n\n\n\n\n## drive looks categorical\n(autos\n .drive\n .value_counts(dropna=False)\n)\n\nFront-Wheel Drive             14236\nRear-Wheel Drive              13831\n4-Wheel or All-Wheel Drive     6648\nAll-Wheel Drive                3015\n4-Wheel Drive                  1460\nNaN                            1189\n2-Wheel Drive                   507\nPart-time 4-Wheel Drive         258\nName: drive, dtype: int64\n\n\n\n# Where are the missing values NaN ?\n(autos\n [cols]\n .query('drive.isna()')\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      eng_dscr\n      fuelCost08\n      make\n      model\n      trany\n      range\n      createdOn\n      year\n    \n  \n  \n    \n      7138\n      81\n      85\n      91\n      NaN\n      NaN\n      NaN\n      NaN\n      800\n      Nissan\n      Altra EV\n      NaN\n      90\n      Tue Jan 01 00:00:00 EST 2013\n      2000\n    \n    \n      8144\n      74\n      65\n      58\n      NaN\n      NaN\n      NaN\n      NaN\n      1000\n      Ford\n      Th!nk\n      NaN\n      29\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8147\n      84\n      75\n      66\n      NaN\n      NaN\n      NaN\n      NaN\n      900\n      Nissan\n      Hyper-Mini\n      NaN\n      33\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      18217\n      18\n      21\n      25\n      4.0\n      2.0\n      NaN\n      (FFS)\n      2000\n      Alfa Romeo\n      Spider Veloce 2000\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18218\n      20\n      22\n      26\n      4.0\n      1.5\n      NaN\n      (FFS)\n      1900\n      Bertone\n      X1/9\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18219\n      13\n      15\n      20\n      8.0\n      5.7\n      NaN\n      (350 V8) (FFS)\n      2800\n      Chevrolet\n      Corvette\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18220\n      13\n      15\n      20\n      8.0\n      5.7\n      NaN\n      (350 V8) (FFS)\n      2800\n      Chevrolet\n      Corvette\n      Manual 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18221\n      15\n      17\n      20\n      6.0\n      3.0\n      NaN\n      (FFS,TRBO)\n      2500\n      Nissan\n      300ZX\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18222\n      16\n      18\n      20\n      6.0\n      3.0\n      NaN\n      (FFS)\n      2350\n      Nissan\n      300ZX\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18223\n      16\n      18\n      22\n      6.0\n      3.0\n      NaN\n      (FFS,TRBO)\n      2350\n      Nissan\n      300ZX\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      20063\n      13\n      15\n      19\n      8.0\n      5.0\n      NaN\n      (FFS) CA model\n      2800\n      Mercury\n      Grand Marquis Wagon\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      20064\n      13\n      15\n      20\n      8.0\n      5.0\n      NaN\n      (GM-OLDS) CA model\n      2800\n      Oldsmobile\n      Custom Cruiser Wagon\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      20065\n      14\n      16\n      19\n      8.0\n      5.0\n      NaN\n      (GM-CHEV) CA model\n      2650\n      Pontiac\n      Parisienne Wagon\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      20387\n      14\n      14\n      15\n      4.0\n      2.4\n      NaN\n      (FFS) CA model\n      3000\n      Nissan\n      Pickup Cab Chassis\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      21129\n      14\n      16\n      21\n      8.0\n      3.5\n      NaN\n      GUZZLER  FFS,TURBO\n      3250\n      Lotus\n      Esprit V8\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      2002\n    \n    \n      23029\n      79\n      85\n      94\n      NaN\n      NaN\n      NaN\n      Lead Acid\n      800\n      GMC\n      EV1\n      Automatic (A1)\n      55\n      Tue Jan 01 00:00:00 EST 2013\n      1999\n    \n    \n      23030\n      35\n      37\n      39\n      NaN\n      NaN\n      NaN\n      NiMH\n      1750\n      GMC\n      EV1\n      Automatic (A1)\n      105\n      Tue Jan 01 00:00:00 EST 2013\n      1999\n    \n    \n      23032\n      49\n      48\n      46\n      NaN\n      NaN\n      NaN\n      NaN\n      1400\n      Honda\n      EV Plus\n      Automatic (A1)\n      81\n      Tue Jan 01 00:00:00 EST 2013\n      1999\n    \n    \n      23037\n      49\n      48\n      46\n      NaN\n      NaN\n      NaN\n      NaN\n      1400\n      Honda\n      EV Plus\n      Automatic (A1)\n      81\n      Tue Jan 01 00:00:00 EST 2013\n      1998\n    \n    \n      23040\n      102\n      98\n      94\n      NaN\n      NaN\n      NaN\n      NaN\n      650\n      MINI\n      MiniE\n      Automatic (A1)\n      100\n      Tue Jan 01 00:00:00 EST 2013\n      2008\n    \n  \n\n1189 rows × 14 columns\n\n\n\n\n# let's look at the drive column, grouped by year\n(autos\n [cols]\n .groupby('year')\n .drive\n .nunique()\n) \n\nyear\n1984    3\n1985    4\n1986    4\n1987    3\n1988    3\n1989    3\n1990    3\n1991    3\n1992    3\n1993    3\n1994    3\n1995    4\n1996    3\n1997    4\n1998    4\n1999    4\n2000    4\n2001    4\n2002    4\n2003    4\n2004    4\n2005    4\n2006    4\n2007    4\n2008    3\n2009    4\n2010    6\n2011    5\n2012    5\n2013    5\n2014    5\n2015    5\n2016    5\n2017    5\n2018    5\n2019    5\n2020    5\nName: drive, dtype: int64\n\n\n\n# let's convert drive to category, replacing NaN with 'Other using .assign .astype\n# and convert make to category, updating .astype dictionary \n# and check our memory usage\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n12093275\n\n\nAs we can see, converting to category has freed up a lot of space, a reduction from 17590123 - just over 30%\n\n# Let's inspect trany \n# looks like it has two pices of info embedded in the column\n\n(autos\n .trany\n .value_counts(dropna=False)\n)\n\nAutomatic 4-spd                     11047\nManual 5-spd                         8361\nAutomatic 3-spd                      3151\nAutomatic (S6)                       3106\nManual 6-spd                         2757\nAutomatic 5-spd                      2203\nAutomatic (S8)                       1665\nAutomatic 6-spd                      1619\nManual 4-spd                         1483\nAutomatic (S5)                        833\nAutomatic (variable gear ratios)      826\nAutomatic 7-spd                       724\nAutomatic 8-spd                       433\nAutomatic (AM-S7)                     424\nAutomatic (S7)                        327\nAutomatic 9-spd                       293\nAutomatic (AM7)                       245\nAutomatic (S4)                        233\nAutomatic (AV-S6)                     208\nAutomatic (A1)                        201\nAutomatic (AM6)                       151\nAutomatic (AV-S7)                     139\nAutomatic (S10)                       124\nAutomatic (AM-S6)                     116\nManual 7-spd                          114\nAutomatic (S9)                         86\nManual 3-spd                           77\nAutomatic (AM-S8)                      60\nAutomatic (AV-S8)                      47\nAutomatic 10-spd                       25\nManual 4-spd Doubled                   17\nAutomatic (AM5)                        14\nNaN                                    11\nAutomatic (AV-S10)                     11\nAutomatic (AM8)                         6\nAutomatic (AM-S9)                       3\nAutomatic (L3)                          2\nAutomatic (L4)                          2\nName: trany, dtype: int64\n\n\n11 NaN values\nThe information from the trany column seems to have two components:\n\nAutomatic v Manual\nSpeed\n\n\n# add new columns for automatic using .str.contains\n# add new column for speeds using .str.extract\n# drop exisitng trany column\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8')\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany'])\n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n10631047\n\n\nGreat, another reduction - we have almost halved our original memory usage."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#dates",
    "href": "posts/Pandas/Effective_Pandas.html#dates",
    "title": "Efficient Pandas",
    "section": "Dates",
    "text": "Dates\npandas contains extensive capabilities and features for working with time series data for all domains. Check out the documentation for more info.\nWe can convert the CreatedOn column from an object to datetime using.\n\n# add createdOn using pd.to_datetime .dt.tz_localize\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn).dt.tz_localize('America/New_York')\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany'])\n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n/home/stephen137/mambaforge/lib/python3.10/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n\n\n7462959\n\n\nExcellent, we have successfully reduced our memory usage by 62%!\n\n# Pythom doesn't like EST/EDT\n(autos\n [cols]\n .createdOn\n)\n\n0        Tue Jan 01 00:00:00 EST 2013\n1        Tue Jan 01 00:00:00 EST 2013\n2        Tue Jan 01 00:00:00 EST 2013\n3        Tue Jan 01 00:00:00 EST 2013\n4        Tue Jan 01 00:00:00 EST 2013\n5        Tue Jan 01 00:00:00 EST 2013\n6        Tue Jan 01 00:00:00 EST 2013\n7        Tue Jan 01 00:00:00 EST 2013\n8        Tue Jan 01 00:00:00 EST 2013\n9        Tue Jan 01 00:00:00 EST 2013\n                     ...             \n41134    Tue Jan 01 00:00:00 EST 2013\n41135    Tue Jan 01 00:00:00 EST 2013\n41136    Tue Jan 01 00:00:00 EST 2013\n41137    Tue Jan 01 00:00:00 EST 2013\n41138    Tue Jan 01 00:00:00 EST 2013\n41139    Tue Jan 01 00:00:00 EST 2013\n41140    Tue Jan 01 00:00:00 EST 2013\n41141    Tue Jan 01 00:00:00 EST 2013\n41142    Tue Jan 01 00:00:00 EST 2013\n41143    Tue Jan 01 00:00:00 EST 2013\nName: createdOn, Length: 41144, dtype: object\n\n\n\n# Fix date warnings - move on to eng_dscr\n# https://www.fueleconomy.gov/feg/findacarhelp.shtml\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True))\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany'])\n .eng_dscr\n .value_counts(dropna=False)\n)\n\nNaN                                 16153\n(FFS)                                8827\nSIDI                                 5526\n(FFS) CA model                        926\n(FFS)      (MPFI)                     734\nFFV                                   701\n(FFS,TRBO)                            666\n(350 V8) (FFS)                        411\n(GUZZLER)  (FFS)                      366\nSOHC                                  354\n                                    ...  \nB234L/R4 (FFS,TRBO)                     1\nGUZZLER V8 FFS,TURBO                    1\n4.6M FFS MPFI                           1\nCNG FFS                                 1\nPOLICE FFS MPFI                         1\nB308E5 FFS,TURBO                        1\n5.4E-R FFS MPFI                         1\nV-6 FFS                                 1\n(GUZZLER)  (FFS)      (S-CHARGE)        1\nR-ENG (FFS,TRBO)                        1\nName: eng_dscr, Length: 558, dtype: int64\n\n\nAs we can see the majority of values within the eng_dscr column are NaN and the other values are very messy. How should we deal with this?\n\n# drop eng_dscr column, and bring in an FFS column (feedback fuel system)\n# check update to memory usage\n\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True)),\n         ffs=autos.eng_dscr.str.contains('FFS')\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany','eng_dscr'])\n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n8676214"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#functions---.apply",
    "href": "posts/Pandas/Effective_Pandas.html#functions---.apply",
    "title": "Efficient Pandas",
    "section": "Functions - .apply",
    "text": "Functions - .apply\nLet’s now create a function which brings together all the exploratory data analysis we have performed in one place:\n\ndef autos_tweak(autos):\n    cols = ['city08','comb08', 'highway08','cylinders', 'displ', 'drive', 'eng_dscr', 'fuelCost08',\n      'make', 'model', 'trany', 'range', 'createdOn', 'year']\n    return (autos\n    [cols]\n    .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True)),\n         ffs=autos.eng_dscr.str.contains('FFS')\n        ) \n    .astype({'city08': 'int16', 'comb08': 'int16',  'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n    .drop(columns=['trany','eng_dscr'])\n    )\n\nLook how neat and tidy the above code is compared to the following alternative approach:\n\na1 = autos[cols]\ncyls = autos.cylinders.fillna(0)\ncyls2 = cyls.astype('int8')\na1['cylinders'] = cyls2\ndispl = a1.displ\ndispl2 = displ.fillna(0)\ndispl3=displ2.astype('float16')\na1.displ=displ3\na1.drive=autos.drive.fillna('Other').astype('category')\na1['automatic'] = autos.trany.str.contains('Auto')\nspeed=autos.trany.str.extract(r'(\\d)+')\nspeedfill = speed.fillna('20')\nspeedint = speedfill.astype('int8')\na1['speeds'] = speedint\na1.createdOn=pd.to_datetime(autos.createdOn).dt.tz_localize('America/New_York')\na1.ffs=autos.eng_dscr.str.contains('FFS')\na1['highway08'] = autos.highway08.astype('int8')\na1['city08'] = autos.city08.astype('int8')\na1['comb08'] = autos.comb08.astype('int16')\na1['fuelCost08'] =autos.fuelCost08.astype('int16')\na1['range'] = autos.range.astype('int16')\na1['make'] = autos.make.astype('category')\na3 = a1.drop(columns=['trany','eng_dscr'])"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#dont-mutate",
    "href": "posts/Pandas/Effective_Pandas.html#dont-mutate",
    "title": "Efficient Pandas",
    "section": "Don’t Mutate",
    "text": "Don’t Mutate\n“you are missing the point, inplace rarely actually does something inplace. you are thinking that you are saving memory but you are not.”\njreback - Pandas core dev\nhttps://github.com/pandas-dev/pandas/issues/16529#issuecomment-676518136\n\nin general, no performance benefits\nprohibits chaining\nSettingWithCopyWarning fun"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#try-to-avoid-using-.apply-where-possible",
    "href": "posts/Pandas/Effective_Pandas.html#try-to-avoid-using-.apply-where-possible",
    "title": "Efficient Pandas",
    "section": "Try to avoid using .apply (where possible)",
    "text": "Try to avoid using .apply (where possible)\n\ndef autos_tweak(autos):\n    cols = ['city08','comb08', 'highway08','cylinders', 'displ', 'drive', 'eng_dscr', 'fuelCost08',\n      'make', 'model', 'trany', 'range', 'createdOn', 'year']\n    return (autos\n    [cols]\n    .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True)),\n         ffs=autos.eng_dscr.str.contains('FFS')\n        ) \n    .astype({'city08': 'int16', 'comb08': 'int16',  'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n    .drop(columns=['trany','eng_dscr'])\n    )\n\nautos2 = autos_tweak(autos)\n\n\n# try to be more Euro-centric\ndef to_lper100km(val):\n    return 235.215 / val\n\n\n%%timeit\nautos2.city08.apply(to_lper100km)\n\n5.3 ms ± 390 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n4.95 ms (milliseconds) is equivalent to 4,950 μs (microseconds)\nThere is a lot of computational overhead using this method - the function pulls out each individual entry from the Series, convert it to a Python object, pass the individual entry into the function, and then convert back to a pandas object.\n\n%%timeit\n# note that you can't run %%timeit with a leading #comment\n# this gives the same results\n235.215 / autos2.city08\n\n84.5 µs ± 3.29 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n90.7 μs (microseconds)\nThis gives the same answer but is more than 50 x faster than using the .apply method, because it is leveraging modern CPU single instruction multiple data (SIMD) architecture - here’s a block of data - do the division on it.\n\n# create a function to return whether the make of the car is US\ndef is_american(val):\n    return val in {'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}\n\n\n%%timeit\n\n# use .apply\nautos2.make.apply(is_american)\n\n233 µs ± 6.36 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n245 μs (microseconds)\n\n%%timeit\n\n# use .isin \nautos2.make.isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'})\n\n448 µs ± 9.32 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n465 μs (microseconds)\n\n%%timeit\n\n# use .astype(str) and then .isin\nautos2.make.astype(str).isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'})\n\n4.91 ms ± 65.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n5.35 ms (milliseconds) is equivalent to 5,350 μs (microseconds)\n\n%%timeit\n\n# use .astype(str) and then .apply\nautos2.make.astype(str).apply(is_american)\n\n8.23 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n8.93 ms (milliseconds) is equivalent to 8,930 μs (microseconds)\nIn the case of the categorical column - make:\n\nthe .apply method on the function was fastest\nthe .isin method was next fastest (~ 2 x slower)\nthird fastest was (~ 22 x slower)\nfinally the .astype(str).apply method (~36 x slower)\n\n\ndef country(val):\n    if val in {'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}:\n        return 'US'\n    # else\n    return 'Other'\n\n\n%%timeit\n\n# use .apply\n# Might be OK for strings, since they are not vectorized...\n(autos2\n .assign(country=autos2.make.apply(country))\n)\n\n2.14 ms ± 66.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit \n\n# use .assign\nvalues = {'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}\n(autos2\n .assign(country='US')\n .assign(country=lambda df_:df_.country.where(df_.make.isin(values), 'Other'))\n)\n\n4.31 ms ± 83 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit \n\n# using MumPy .select - allows you to specify a list of Booleans, and wherever they hold true [i.e make is Chevrolet, Ford, Dodge...Tesla, you specify the value ['US'] to put into placeholder \n# this method is not available within pandas\n(autos2\n .assign(country=np.select([autos2.make.isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'})],\n                           ['US'], 'Other'))\n)                          \n\n3.36 ms ± 145 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit \n\n# using MumPy .where - allows you to specify a list of Booleans, and wherever they hold true [i.e make is Chevrolet, Ford, Dodge...Tesla, you specify the value ['US'] to put into placeholder \n# this method is not available within pandas\n(autos2\n .assign(country=np.where(autos2.make.isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}),\n                           ['US'], 'Other'))\n)  \n\n3.38 ms ± 35.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nKey takeaways\n\nif you find yourself using a for loop alarm bells should be ringing!\nyou could use .apply but still slow\napply where or np.select\nthe same result can be obtained much faster using list comprehension"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#aggregation",
    "href": "posts/Pandas/Effective_Pandas.html#aggregation",
    "title": "Efficient Pandas",
    "section": "Aggregation",
    "text": "Aggregation\nIt is important as a data science to work with the raw data and get to know the finer details, but ultimately, providing higher level insights are our main goal. This can be obtained by aggregating data. Let’s compare mileage by country by year…\n\n# start off with auto\n# group by year\n# then grab the mean values\n(autos2\n # Year will therefore be our index\n .groupby('year')\n .mean()\n)\n\n/tmp/ipykernel_2753/262800323.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      speeds\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      17.982688\n      19.881874\n      23.075356\n      5.385438\n      3.165017\n      2313.543788\n      0.000000\n      3.928208\n    \n    \n      1985\n      17.878307\n      19.808348\n      23.042328\n      5.375661\n      3.164080\n      2334.509112\n      0.000000\n      3.924750\n    \n    \n      1986\n      17.665289\n      19.550413\n      22.699174\n      5.425620\n      3.183762\n      2354.049587\n      0.000000\n      3.984298\n    \n    \n      1987\n      17.310345\n      19.228549\n      22.445068\n      5.412189\n      3.173949\n      2403.648757\n      0.000000\n      4.037690\n    \n    \n      1988\n      17.333628\n      19.328319\n      22.702655\n      5.461947\n      3.194899\n      2387.035398\n      0.000000\n      4.129204\n    \n    \n      1989\n      17.143972\n      19.125759\n      22.465742\n      5.488291\n      3.209926\n      2433.434519\n      0.000000\n      4.166522\n    \n    \n      1990\n      17.033395\n      19.000928\n      22.337662\n      5.496289\n      3.217369\n      2436.178108\n      0.000000\n      4.238404\n    \n    \n      1991\n      16.848940\n      18.825972\n      22.253534\n      5.598940\n      3.266809\n      2490.856890\n      0.000000\n      4.301237\n    \n    \n      1992\n      16.805531\n      18.862623\n      22.439786\n      5.623550\n      3.276159\n      2494.736842\n      0.000000\n      4.318466\n    \n    \n      1993\n      16.998170\n      19.104300\n      22.780421\n      5.602928\n      3.248540\n      2454.620311\n      0.000000\n      4.339433\n    \n    \n      1994\n      16.918534\n      19.012220\n      22.725051\n      5.704684\n      3.333190\n      2461.507128\n      0.000000\n      4.332994\n    \n    \n      1995\n      16.569804\n      18.797311\n      22.671148\n      5.892451\n      3.471776\n      2497.828335\n      0.000000\n      4.356774\n    \n    \n      1996\n      17.289780\n      19.584735\n      23.569211\n      5.627426\n      3.234789\n      2375.032342\n      0.000000\n      4.364812\n    \n    \n      1997\n      17.135171\n      19.429134\n      23.451444\n      5.666667\n      3.226933\n      2405.511811\n      0.000000\n      4.402887\n    \n    \n      1998\n      17.113300\n      19.518473\n      23.546798\n      5.633005\n      3.201979\n      2382.635468\n      0.229064\n      4.419951\n    \n    \n      1999\n      17.272300\n      19.611502\n      23.552817\n      5.667840\n      3.188794\n      2392.194836\n      0.570423\n      4.421362\n    \n    \n      2000\n      17.221429\n      19.526190\n      23.414286\n      5.713095\n      3.200517\n      2429.702381\n      0.348810\n      4.508333\n    \n    \n      2001\n      17.275521\n      19.479693\n      23.328211\n      5.720088\n      3.192452\n      2448.463227\n      0.261251\n      4.660812\n    \n    \n      2002\n      16.893333\n      19.168205\n      23.030769\n      5.827692\n      3.264525\n      2479.794872\n      0.136410\n      4.757949\n    \n    \n      2003\n      16.780651\n      19.000958\n      22.836207\n      5.942529\n      3.358259\n      2525.574713\n      0.090996\n      4.911877\n    \n    \n      2004\n      16.740642\n      19.067736\n      23.064171\n      5.957219\n      3.393626\n      2512.566845\n      0.000000\n      4.976827\n    \n    \n      2005\n      16.851630\n      19.193825\n      23.297599\n      5.944254\n      3.399485\n      2518.610635\n      0.000000\n      5.192110\n    \n    \n      2006\n      16.626812\n      18.959239\n      23.048913\n      6.100543\n      3.549294\n      2539.175725\n      0.000000\n      5.315217\n    \n    \n      2007\n      16.605684\n      18.978686\n      23.083481\n      6.166075\n      3.628539\n      2535.923623\n      0.000000\n      5.610124\n    \n    \n      2008\n      16.900590\n      19.276327\n      23.455771\n      6.192923\n      3.637796\n      2536.436394\n      0.084246\n      5.773378\n    \n    \n      2009\n      17.334459\n      19.735642\n      24.017736\n      6.122466\n      3.624839\n      2427.027027\n      0.000000\n      6.043074\n    \n    \n      2010\n      18.105500\n      20.588819\n      24.947701\n      5.965735\n      3.502548\n      2351.082056\n      0.000000\n      6.271416\n    \n    \n      2011\n      18.669027\n      21.011504\n      25.169912\n      5.980531\n      3.521903\n      2333.982301\n      0.259292\n      6.560177\n    \n    \n      2012\n      19.362847\n      21.819444\n      26.105035\n      5.910590\n      3.460015\n      2289.973958\n      0.782118\n      6.706597\n    \n    \n      2013\n      20.661318\n      23.125000\n      27.504223\n      5.762669\n      3.327529\n      2210.768581\n      1.255068\n      6.896959\n    \n    \n      2014\n      21.033469\n      23.531429\n      27.978776\n      5.745306\n      3.289703\n      2198.040816\n      1.405714\n      6.985306\n    \n    \n      2015\n      21.445830\n      24.038971\n      28.586906\n      5.635230\n      3.205085\n      2148.869836\n      2.208106\n      7.035853\n    \n    \n      2016\n      22.591918\n      25.150555\n      29.606973\n      5.463550\n      3.054415\n      2091.204437\n      4.546751\n      7.080032\n    \n    \n      2017\n      22.761021\n      25.249033\n      29.554524\n      5.453210\n      3.026032\n      2096.558391\n      4.336427\n      7.225058\n    \n    \n      2018\n      22.564732\n      25.019345\n      29.273065\n      5.438988\n      2.992239\n      2103.980655\n      3.519345\n      7.017113\n    \n    \n      2019\n      23.318147\n      25.627942\n      29.664389\n      5.368261\n      2.964679\n      2093.545938\n      5.565680\n      7.136674\n    \n    \n      2020\n      22.679426\n      25.267943\n      29.617225\n      5.071770\n      2.644994\n      2023.444976\n      2.282297\n      7.746411\n    \n  \n\n\n\n\n\n# let's focus on just the comb08 and speeds columns\n(autos2\n .groupby('year')\n [['comb08','speeds']]\n .mean()\n)\n\n\n\n\n\n  \n    \n      \n      comb08\n      speeds\n    \n    \n      year\n      \n      \n    \n  \n  \n    \n      1984\n      19.881874\n      3.928208\n    \n    \n      1985\n      19.808348\n      3.924750\n    \n    \n      1986\n      19.550413\n      3.984298\n    \n    \n      1987\n      19.228549\n      4.037690\n    \n    \n      1988\n      19.328319\n      4.129204\n    \n    \n      1989\n      19.125759\n      4.166522\n    \n    \n      1990\n      19.000928\n      4.238404\n    \n    \n      1991\n      18.825972\n      4.301237\n    \n    \n      1992\n      18.862623\n      4.318466\n    \n    \n      1993\n      19.104300\n      4.339433\n    \n    \n      1994\n      19.012220\n      4.332994\n    \n    \n      1995\n      18.797311\n      4.356774\n    \n    \n      1996\n      19.584735\n      4.364812\n    \n    \n      1997\n      19.429134\n      4.402887\n    \n    \n      1998\n      19.518473\n      4.419951\n    \n    \n      1999\n      19.611502\n      4.421362\n    \n    \n      2000\n      19.526190\n      4.508333\n    \n    \n      2001\n      19.479693\n      4.660812\n    \n    \n      2002\n      19.168205\n      4.757949\n    \n    \n      2003\n      19.000958\n      4.911877\n    \n    \n      2004\n      19.067736\n      4.976827\n    \n    \n      2005\n      19.193825\n      5.192110\n    \n    \n      2006\n      18.959239\n      5.315217\n    \n    \n      2007\n      18.978686\n      5.610124\n    \n    \n      2008\n      19.276327\n      5.773378\n    \n    \n      2009\n      19.735642\n      6.043074\n    \n    \n      2010\n      20.588819\n      6.271416\n    \n    \n      2011\n      21.011504\n      6.560177\n    \n    \n      2012\n      21.819444\n      6.706597\n    \n    \n      2013\n      23.125000\n      6.896959\n    \n    \n      2014\n      23.531429\n      6.985306\n    \n    \n      2015\n      24.038971\n      7.035853\n    \n    \n      2016\n      25.150555\n      7.080032\n    \n    \n      2017\n      25.249033\n      7.225058\n    \n    \n      2018\n      25.019345\n      7.017113\n    \n    \n      2019\n      25.627942\n      7.136674\n    \n    \n      2020\n      25.267943\n      7.746411\n    \n  \n\n\n\n\n\n%%timeit\n\n# Watch out for the ordering!!!\n# here we are grouping by year\n# but then we are taking average of all columns - computationally expensive\n# we are only interested in comb08 and speeds\n(autos2\n .groupby('year')\n .mean()\n #.median()\n #.quantile(.1)\n #.std()\n [['comb08','speeds']]\n)"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#visualizations",
    "href": "posts/Pandas/Effective_Pandas.html#visualizations",
    "title": "Efficient Pandas",
    "section": "Visualizations",
    "text": "Visualizations\nThe pandas library includes a variety of visualization tools which allow us to communicate our findings visually. Note that is very easy to show a variety of different plots quickly, simply by commenting out (#) to leave the desired plot:\n\n# in pandas default plot is a line plot\n# with index as the x axis and the selected grouped columns as the lines\n(autos2\n .groupby('year')\n  [['comb08','speeds']]\n #.mean()\n #.median()\n #.quantile(.1)\n .std()\n #.var()\n .plot()\n)\n\n<AxesSubplot: xlabel='year'>\n\n\n\n\n\n\n# add country\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n)\n\n/tmp/ipykernel_2753/361744348.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n\n\n\n\n  \n    \n      \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      speeds\n    \n    \n      year\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      Other\n      19.384615\n      21.417330\n      24.847038\n      4.908046\n      2.690516\n      2118.125553\n      0.000000\n      3.969054\n    \n    \n      US\n      16.079232\n      17.797119\n      20.669868\n      6.033613\n      3.809268\n      2578.871549\n      0.000000\n      3.872749\n    \n    \n      1985\n      Other\n      19.284768\n      21.373068\n      24.816777\n      4.871965\n      2.636070\n      2141.997792\n      0.000000\n      3.958057\n    \n    \n      US\n      16.275472\n      18.025157\n      21.020126\n      5.949686\n      3.765813\n      2553.899371\n      0.000000\n      3.886792\n    \n    \n      1986\n      Other\n      19.167183\n      21.213622\n      24.650155\n      4.804954\n      2.536234\n      2149.148607\n      0.000000\n      4.069659\n    \n    \n      US\n      15.945035\n      17.645390\n      20.464539\n      6.136525\n      3.925433\n      2588.741135\n      0.000000\n      3.886525\n    \n    \n      1987\n      Other\n      18.633381\n      20.710414\n      24.186876\n      4.825963\n      2.583168\n      2227.318117\n      0.000000\n      4.142653\n    \n    \n      US\n      15.611722\n      17.326007\n      20.208791\n      6.164835\n      3.932442\n      2630.036630\n      0.000000\n      3.902930\n    \n    \n      1988\n      Other\n      18.668224\n      20.814642\n      24.437695\n      4.819315\n      2.531434\n      2207.476636\n      0.000000\n      4.205607\n    \n    \n      US\n      15.577869\n      17.372951\n      20.420082\n      6.307377\n      4.067735\n      2623.258197\n      0.000000\n      4.028689\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2016\n      Other\n      21.903749\n      24.439716\n      28.866261\n      5.493414\n      2.992272\n      2127.608916\n      1.017224\n      7.296859\n    \n    \n      US\n      25.061818\n      27.701818\n      32.265455\n      5.356364\n      3.277454\n      1960.545455\n      17.214545\n      6.301818\n    \n    \n      2017\n      Other\n      22.423795\n      24.910521\n      29.208456\n      5.431662\n      2.919041\n      2114.110128\n      1.243854\n      7.474926\n    \n    \n      US\n      24.003623\n      26.496377\n      30.829710\n      5.532609\n      3.420272\n      2031.884058\n      15.731884\n      6.304348\n    \n    \n      2018\n      Other\n      22.310442\n      24.779868\n      29.042333\n      5.396990\n      2.886801\n      2121.448730\n      1.135466\n      7.391345\n    \n    \n      US\n      23.526690\n      25.925267\n      30.145907\n      5.597865\n      3.391101\n      2037.900356\n      12.537367\n      5.601423\n    \n    \n      2019\n      Other\n      23.084221\n      25.456922\n      29.560503\n      5.315586\n      2.839671\n      2093.659245\n      2.581801\n      7.545983\n    \n    \n      US\n      24.169014\n      26.250000\n      30.042254\n      5.559859\n      3.419375\n      2093.133803\n      16.419014\n      5.647887\n    \n    \n      2020\n      Other\n      22.579487\n      25.174359\n      29.543590\n      5.148718\n      2.692823\n      2050.256410\n      2.446154\n      7.743590\n    \n    \n      US\n      24.071429\n      26.571429\n      30.642857\n      4.000000\n      1.978795\n      1650.000000\n      0.000000\n      7.785714\n    \n  \n\n74 rows × 8 columns\n\n\n\n\n# we can go deeper and apply multiple aggregates\n# this is loosely equivalent to the sort of thing that a pivot table in Excel might provide\n\n# penultimate row\ndef second_to_last(ser):\n    return ser.iloc[-2]\n\n(autos2\n .assign(country=autos2.make.apply(country))\n .groupby(['year', 'country'])\n# we can use .agg to include a list of different aggregation types - we can even call a function\n .agg(['min', 'mean', second_to_last])\n)\n\n/tmp/ipykernel_2753/2706922386.py:12: FutureWarning: ['drive', 'make', 'model', 'createdOn'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n  .agg(['min', 'mean', second_to_last])\n\n\n\n\n\n\n  \n    \n      \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      ...\n      range\n      automatic\n      speeds\n      ffs\n    \n    \n      \n      \n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      ...\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n    \n    \n      year\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      Other\n      7\n      19.384615\n      14\n      8\n      21.417330\n      14\n      9\n      24.847038\n      15\n      2\n      ...\n      0\n      False\n      0.550840\n      False\n      3\n      3.969054\n      5\n      False\n      0.714554\n      True\n    \n    \n      US\n      8\n      16.079232\n      15\n      9\n      17.797119\n      17\n      10\n      20.669868\n      19\n      4\n      ...\n      0\n      False\n      0.521059\n      False\n      3\n      3.872749\n      4\n      False\n      0.638801\n      NaN\n    \n    \n      1985\n      Other\n      7\n      19.284768\n      19\n      8\n      21.373068\n      20\n      9\n      24.816777\n      22\n      0\n      ...\n      0\n      False\n      0.554084\n      True\n      3\n      3.958057\n      4\n      False\n      0.889160\n      True\n    \n    \n      US\n      8\n      16.275472\n      14\n      10\n      18.025157\n      15\n      10\n      21.020126\n      17\n      3\n      ...\n      0\n      False\n      0.520755\n      False\n      3\n      3.886792\n      4\n      False\n      0.851351\n      NaN\n    \n    \n      1986\n      Other\n      6\n      19.167183\n      10\n      7\n      21.213622\n      11\n      9\n      24.650155\n      12\n      0\n      ...\n      0\n      False\n      0.520124\n      False\n      3\n      4.069659\n      4\n      False\n      0.934211\n      NaN\n    \n    \n      US\n      9\n      15.945035\n      16\n      10\n      17.645390\n      17\n      11\n      20.464539\n      19\n      3\n      ...\n      0\n      False\n      0.533688\n      False\n      3\n      3.886525\n      4\n      False\n      0.795699\n      NaN\n    \n    \n      1987\n      Other\n      6\n      18.633381\n      12\n      7\n      20.710414\n      12\n      9\n      24.186876\n      12\n      2\n      ...\n      0\n      False\n      0.516405\n      True\n      3\n      4.142653\n      4\n      False\n      0.949778\n      True\n    \n    \n      US\n      8\n      15.611722\n      12\n      9\n      17.326007\n      13\n      10\n      20.208791\n      14\n      3\n      ...\n      0\n      False\n      0.549451\n      True\n      3\n      3.902930\n      4\n      False\n      0.909457\n      True\n    \n    \n      1988\n      Other\n      6\n      18.668224\n      12\n      7\n      20.814642\n      12\n      10\n      24.437695\n      12\n      2\n      ...\n      0\n      False\n      0.521807\n      True\n      3\n      4.205607\n      4\n      False\n      0.993681\n      True\n    \n    \n      US\n      8\n      15.577869\n      14\n      9\n      17.372951\n      14\n      10\n      20.420082\n      15\n      3\n      ...\n      0\n      False\n      0.569672\n      True\n      3\n      4.028689\n      4\n      False\n      0.936306\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2016\n      Other\n      10\n      21.903749\n      28\n      12\n      24.439716\n      30\n      13\n      28.866261\n      32\n      0\n      ...\n      0\n      False\n      0.837893\n      True\n      1\n      7.296859\n      7\n      False\n      0.000000\n      False\n    \n    \n      US\n      11\n      25.061818\n      91\n      12\n      27.701818\n      93\n      16\n      32.265455\n      94\n      0\n      ...\n      200\n      False\n      0.850909\n      True\n      1\n      6.301818\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2017\n      Other\n      10\n      22.423795\n      21\n      11\n      24.910521\n      24\n      11\n      29.208456\n      28\n      0\n      ...\n      0\n      False\n      0.848574\n      True\n      1\n      7.474926\n      7\n      False\n      0.000000\n      False\n    \n    \n      US\n      11\n      24.003623\n      131\n      12\n      26.496377\n      126\n      15\n      30.829710\n      120\n      0\n      ...\n      310\n      False\n      0.858696\n      True\n      0\n      6.304348\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2018\n      Other\n      9\n      22.310442\n      11\n      11\n      24.779868\n      12\n      11\n      29.042333\n      15\n      0\n      ...\n      0\n      False\n      0.863594\n      True\n      0\n      7.391345\n      0\n      False\n      0.000000\n      NaN\n    \n    \n      US\n      11\n      23.526690\n      120\n      14\n      25.925267\n      116\n      15\n      30.145907\n      112\n      0\n      ...\n      310\n      False\n      0.882562\n      True\n      0\n      5.601423\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2019\n      Other\n      9\n      23.084221\n      19\n      11\n      25.456922\n      22\n      14\n      29.560503\n      27\n      0\n      ...\n      0\n      False\n      0.879961\n      True\n      0\n      7.545983\n      8\n      False\n      0.000000\n      NaN\n    \n    \n      US\n      11\n      24.169014\n      104\n      14\n      26.250000\n      104\n      15\n      30.042254\n      104\n      0\n      ...\n      345\n      False\n      0.915493\n      True\n      0\n      5.647887\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2020\n      Other\n      13\n      22.579487\n      17\n      15\n      25.174359\n      20\n      18\n      29.543590\n      24\n      0\n      ...\n      0\n      False\n      0.871795\n      True\n      0\n      7.743590\n      0\n      False\n      0.000000\n      False\n    \n    \n      US\n      20\n      24.071429\n      21\n      22\n      26.571429\n      24\n      26\n      30.642857\n      28\n      4\n      ...\n      0\n      False\n      0.857143\n      True\n      0\n      7.785714\n      0\n      False\n      0.000000\n      False\n    \n  \n\n74 rows × 30 columns\n\n\n\n\n# back to simpler example, adding plots\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n .plot()\n)\n\n/tmp/ipykernel_2753/1855101905.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n<AxesSubplot: xlabel='year,country'>\n\n\n\n\n\nThis doesn’t really work as we can see. Let’s see if we can resolve this:\n\n# back to simpler example, adding plots\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n # rotates or unstacks the innermost index, country\n .unstack()\n)\n\n/tmp/ipykernel_2753/1937283994.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      speeds\n    \n    \n      country\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      19.384615\n      16.079232\n      21.417330\n      17.797119\n      24.847038\n      20.669868\n      4.908046\n      6.033613\n      2.690516\n      3.809268\n      2118.125553\n      2578.871549\n      0.000000\n      0.000000\n      3.969054\n      3.872749\n    \n    \n      1985\n      19.284768\n      16.275472\n      21.373068\n      18.025157\n      24.816777\n      21.020126\n      4.871965\n      5.949686\n      2.636070\n      3.765813\n      2141.997792\n      2553.899371\n      0.000000\n      0.000000\n      3.958057\n      3.886792\n    \n    \n      1986\n      19.167183\n      15.945035\n      21.213622\n      17.645390\n      24.650155\n      20.464539\n      4.804954\n      6.136525\n      2.536234\n      3.925433\n      2149.148607\n      2588.741135\n      0.000000\n      0.000000\n      4.069659\n      3.886525\n    \n    \n      1987\n      18.633381\n      15.611722\n      20.710414\n      17.326007\n      24.186876\n      20.208791\n      4.825963\n      6.164835\n      2.583168\n      3.932442\n      2227.318117\n      2630.036630\n      0.000000\n      0.000000\n      4.142653\n      3.902930\n    \n    \n      1988\n      18.668224\n      15.577869\n      20.814642\n      17.372951\n      24.437695\n      20.420082\n      4.819315\n      6.307377\n      2.531434\n      4.067735\n      2207.476636\n      2623.258197\n      0.000000\n      0.000000\n      4.205607\n      4.028689\n    \n    \n      1989\n      18.533040\n      15.139831\n      20.662261\n      16.908898\n      24.252570\n      19.887712\n      4.879589\n      6.366525\n      2.542154\n      4.173385\n      2250.000000\n      2698.093220\n      0.000000\n      0.000000\n      4.264317\n      4.025424\n    \n    \n      1990\n      18.510109\n      14.850575\n      20.640747\n      16.577011\n      24.267496\n      19.485057\n      4.839813\n      6.466667\n      2.508090\n      4.265798\n      2238.258165\n      2728.735632\n      0.000000\n      0.000000\n      4.328149\n      4.105747\n    \n    \n      1991\n      18.087943\n      14.803279\n      20.174468\n      16.599532\n      23.809929\n      19.683841\n      5.029787\n      6.538642\n      2.610219\n      4.350876\n      2348.581560\n      2725.761124\n      0.000000\n      0.000000\n      4.341844\n      4.234192\n    \n    \n      1992\n      17.915374\n      14.895631\n      20.098731\n      16.735437\n      23.820874\n      20.063107\n      5.145275\n      6.446602\n      2.709618\n      4.251104\n      2373.272214\n      2703.762136\n      0.000000\n      0.000000\n      4.356841\n      4.252427\n    \n    \n      1993\n      18.084866\n      15.007772\n      20.309760\n      16.896373\n      24.172560\n      20.230570\n      5.114569\n      6.497409\n      2.683870\n      4.282793\n      2333.097595\n      2677.202073\n      0.000000\n      0.000000\n      4.371994\n      4.279793\n    \n    \n      1994\n      18.046474\n      14.952514\n      20.264423\n      16.829609\n      24.173077\n      20.201117\n      5.185897\n      6.608939\n      2.713631\n      4.413091\n      2326.041667\n      2697.625698\n      0.000000\n      0.000000\n      4.355769\n      4.293296\n    \n    \n      1995\n      17.678914\n      14.533724\n      20.091054\n      16.422287\n      24.263578\n      19.747801\n      5.444089\n      6.715543\n      2.908022\n      4.506701\n      2355.191693\n      2759.677419\n      0.000000\n      0.000000\n      4.380192\n      4.313783\n    \n    \n      1996\n      18.480545\n      14.926641\n      20.906615\n      16.961390\n      25.093385\n      20.544402\n      5.147860\n      6.579151\n      2.708768\n      4.278708\n      2250.291829\n      2622.586873\n      0.000000\n      0.000000\n      4.416342\n      4.262548\n    \n    \n      1997\n      18.090909\n      14.978632\n      20.509470\n      16.991453\n      24.678030\n      20.683761\n      5.261364\n      6.581197\n      2.786582\n      4.220544\n      2319.128788\n      2600.427350\n      0.000000\n      0.000000\n      4.452652\n      4.290598\n    \n    \n      1998\n      17.925267\n      15.288000\n      20.457295\n      17.408000\n      24.704626\n      20.944000\n      5.275801\n      6.436000\n      2.800378\n      4.104777\n      2295.373665\n      2578.800000\n      0.144128\n      0.420000\n      4.485765\n      4.272000\n    \n    \n      1999\n      17.925125\n      15.709163\n      20.386023\n      17.756972\n      24.577371\n      21.099602\n      5.377704\n      6.362550\n      2.832181\n      4.042677\n      2312.728785\n      2582.470120\n      0.251248\n      1.334661\n      4.507488\n      4.215139\n    \n    \n      2000\n      17.881849\n      15.714844\n      20.301370\n      17.757812\n      24.416096\n      21.128906\n      5.441781\n      6.332031\n      2.859111\n      3.979351\n      2385.958904\n      2529.492188\n      0.304795\n      0.449219\n      4.619863\n      4.253906\n    \n    \n      2001\n      17.941267\n      15.643939\n      20.289026\n      17.496212\n      24.372488\n      20.768939\n      5.479134\n      6.310606\n      2.872808\n      3.975823\n      2399.536321\n      2568.371212\n      0.187017\n      0.443182\n      4.761978\n      4.412879\n    \n    \n      2002\n      17.644412\n      15.083916\n      20.076923\n      16.979021\n      24.207547\n      20.195804\n      5.576197\n      6.433566\n      2.935398\n      4.057423\n      2425.689405\n      2610.139860\n      0.137881\n      0.132867\n      4.920174\n      4.367133\n    \n    \n      2003\n      17.565101\n      14.826087\n      19.953020\n      16.628763\n      24.052349\n      19.806020\n      5.683221\n      6.588629\n      3.032161\n      4.170778\n      2480.604027\n      2637.625418\n      0.127517\n      0.000000\n      5.154362\n      4.307692\n    \n    \n      2004\n      17.426290\n      14.928571\n      19.923833\n      16.805195\n      24.160934\n      20.165584\n      5.729730\n      6.558442\n      3.088666\n      4.199593\n      2476.719902\n      2607.305195\n      0.000000\n      0.000000\n      5.229730\n      4.308442\n    \n    \n      2005\n      17.412170\n      15.196610\n      19.892078\n      17.132203\n      24.189437\n      20.664407\n      5.773823\n      6.447458\n      3.151592\n      4.131402\n      2493.455798\n      2592.881356\n      0.000000\n      0.000000\n      5.362801\n      4.688136\n    \n    \n      2006\n      17.062575\n      15.300366\n      19.509025\n      17.285714\n      23.762936\n      20.875458\n      5.977136\n      6.476190\n      3.345220\n      4.170487\n      2527.496992\n      2574.725275\n      0.000000\n      0.000000\n      5.492178\n      4.776557\n    \n    \n      2007\n      16.996403\n      15.489726\n      19.452038\n      17.626712\n      23.742206\n      21.202055\n      6.044365\n      6.513699\n      3.423963\n      4.212841\n      2544.664269\n      2510.958904\n      0.000000\n      0.000000\n      5.864508\n      4.883562\n    \n    \n      2008\n      17.239869\n      15.770073\n      19.677985\n      17.937956\n      23.983571\n      21.697080\n      6.095290\n      6.518248\n      3.462049\n      4.223408\n      2551.369113\n      2486.678832\n      0.109529\n      0.000000\n      5.969332\n      5.120438\n    \n    \n      2009\n      17.696803\n      16.148014\n      20.186329\n      18.259928\n      24.590959\n      22.140794\n      5.970232\n      6.620939\n      3.402613\n      4.352489\n      2433.076075\n      2407.220217\n      0.000000\n      0.000000\n      6.189636\n      5.563177\n    \n    \n      2010\n      18.325342\n      17.278970\n      20.851598\n      19.600858\n      25.256849\n      23.785408\n      5.897260\n      6.223176\n      3.357208\n      4.048979\n      2374.429224\n      2263.304721\n      0.000000\n      0.000000\n      6.378995\n      5.866953\n    \n    \n      2011\n      19.247387\n      16.817844\n      21.635308\n      19.014870\n      25.855981\n      22.973978\n      5.851336\n      6.394052\n      3.319702\n      4.169094\n      2326.248548\n      2358.736059\n      0.340302\n      0.000000\n      6.714286\n      6.066914\n    \n    \n      2012\n      19.838052\n      17.802974\n      22.339751\n      20.111524\n      26.695357\n      24.167286\n      5.792752\n      6.297398\n      3.268908\n      4.087332\n      2282.502831\n      2314.498141\n      0.634202\n      1.267658\n      6.834655\n      6.286245\n    \n    \n      2013\n      20.982888\n      19.453815\n      23.471658\n      21.823293\n      27.860963\n      26.164659\n      5.658824\n      6.152610\n      3.179253\n      3.884311\n      2208.288770\n      2220.080321\n      0.853476\n      2.763052\n      7.033155\n      6.385542\n    \n    \n      2014\n      21.159919\n      20.506329\n      23.655870\n      23.012658\n      28.088057\n      27.523207\n      5.719636\n      5.852321\n      3.211738\n      3.614723\n      2212.196356\n      2139.029536\n      0.859312\n      3.683544\n      7.210526\n      6.046414\n    \n    \n      2015\n      21.350000\n      21.817490\n      23.935294\n      24.441065\n      28.481373\n      28.996198\n      5.604902\n      5.752852\n      3.101696\n      3.606063\n      2164.215686\n      2089.353612\n      0.638235\n      8.296578\n      7.211765\n      6.353612\n    \n    \n      2016\n      21.903749\n      25.061818\n      24.439716\n      27.701818\n      28.866261\n      32.265455\n      5.493414\n      5.356364\n      2.992272\n      3.277454\n      2127.608916\n      1960.545455\n      1.017224\n      17.214545\n      7.296859\n      6.301818\n    \n    \n      2017\n      22.423795\n      24.003623\n      24.910521\n      26.496377\n      29.208456\n      30.829710\n      5.431662\n      5.532609\n      2.919041\n      3.420272\n      2114.110128\n      2031.884058\n      1.243854\n      15.731884\n      7.474926\n      6.304348\n    \n    \n      2018\n      22.310442\n      23.526690\n      24.779868\n      25.925267\n      29.042333\n      30.145907\n      5.396990\n      5.597865\n      2.886801\n      3.391101\n      2121.448730\n      2037.900356\n      1.135466\n      12.537367\n      7.391345\n      5.601423\n    \n    \n      2019\n      23.084221\n      24.169014\n      25.456922\n      26.250000\n      29.560503\n      30.042254\n      5.315586\n      5.559859\n      2.839671\n      3.419375\n      2093.659245\n      2093.133803\n      2.581801\n      16.419014\n      7.545983\n      5.647887\n    \n    \n      2020\n      22.579487\n      24.071429\n      25.174359\n      26.571429\n      29.543590\n      30.642857\n      5.148718\n      4.000000\n      2.692823\n      1.978795\n      2050.256410\n      1650.000000\n      2.446154\n      0.000000\n      7.743590\n      7.785714\n    \n  \n\n\n\n\n\n# back to simpler example, adding plots\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n # .std()\n # rotates or unstacks the innermost index, country\n .unstack()\n # focus on city08 column\n .city08\n .plot()\n .legend(bbox_to_anchor=(1,1))\n)\n\n/tmp/ipykernel_2753/2308684326.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n<matplotlib.legend.Legend at 0x7f2091b75e10>\n\n\n\n\n\n\n# time series smoothing - rolling average(2)\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n # .std()\n # rotates or unstacks the innermost index, country\n .unstack()\n # focus on city08 column\n .city08\n # rolling average to smooth time series\n .rolling(2)\n .mean()\n .plot()\n .legend(bbox_to_anchor=(1,1))\n)\n\n/tmp/ipykernel_2753/2271732648.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n<matplotlib.legend.Legend at 0x7f2091a62ad0>\n\n\n\n\n\n\n# can we emulate a SQL GROUP BY - HAVING query?\n# let's only show grouped values over say 750\ndef vals_gt(df_, num):\n    return df_[df_.gt(num)].dropna()\n\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .count()\n .pipe(vals_gt, 750)\n)\n\n\n\n\n\n  \n    \n      \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      fuelCost08\n      make\n      model\n      range\n      createdOn\n      automatic\n      speeds\n      ffs\n    \n    \n      year\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      Other\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1065.0\n    \n    \n      1985\n      Other\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      821.0\n    \n    \n      2017\n      Other\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      766.0\n    \n    \n      2018\n      Other\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      844.0\n    \n    \n      2019\n      Other\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      855.0"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#key-takeaways",
    "href": "posts/Pandas/Effective_Pandas.html#key-takeaways",
    "title": "Efficient Pandas",
    "section": "Key takeaways",
    "text": "Key takeaways\nHaving code that is clean, readable and has a logical flow is invaluable. Structured Query Language (SQL), as the name suggests, already pushes you down this road. Python, although often referred to as the swiss army knife of programming languages, can quickly beocme scattered and difficult to follow particulary during the exploratory data analysis (EDA) phase. However by adopting the chaining method covered in this blog, your code should ensure that others (as well as yourself!) can follow exactly what is going on.\nAlthough computer memory continues to get cheaper, I think it is good practice to keep track of how much data is being used, and there are some interesting ways covered in this blog as to how we might go about achieving this. e.g. numpy.iinfo. In the same breath, speed is also of the essence, and the handy Python cell magic %%timeit provides an easy way to quantify this. With pandas there is generally a variety of different ways to arrive at the same outcome, but as this blog covers there can be substantial time savings if you know which method to use and when, and which to avoid if possible, in particular the use of .apply.\nThis blog has been produced in response to the Effective Pandas video presented by Matt Harrison which you can view below. Thanks also to Santiago for the heads up on Twitter.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zgbUk90aQ6A?start=482\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n\n\nIf the above %%HTML cell magic is unfamiliar to you, then I recommend you watch the excellent video below posted by Corey Shafer. An excellent introduction for anyone starting out with Jupyter NoteBooks.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HW29067qVWk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
  },
  {
    "objectID": "posts/Seaborn/Seaborn.html",
    "href": "posts/Seaborn/Seaborn.html",
    "title": "Seaborn Tutorial",
    "section": "",
    "text": "There is no universally best way to visualize data. Different questions are best answered by different plots. Seaborn makes it easy to switch between different visual representations by using a consistent dataset-oriented API. Seaborn helps you explore and understand your data.\nIts plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\n\n\n\nseaborn has a number of built in datases:\n\n# Seaborn provides a playground of built in datasets\nprint(sns.get_dataset_names())\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\nFor the purposes of this blog we will be working with the car_crashes dataset.\n\n\n\ncar_crash.jpg\n\n\n\n# Load a built in dataset of US car crash\ncrash_df = sns.load_dataset('car_crashes')\ncrash_df.head()\n\n\n\n\n\n  \n    \n      \n      total\n      speeding\n      alcohol\n      not_distracted\n      no_previous\n      ins_premium\n      ins_losses\n      abbrev\n    \n  \n  \n    \n      0\n      18.8\n      7.332\n      5.640\n      18.048\n      15.040\n      784.55\n      145.08\n      AL\n    \n    \n      1\n      18.1\n      7.421\n      4.525\n      16.290\n      17.014\n      1053.48\n      133.93\n      AK\n    \n    \n      2\n      18.6\n      6.510\n      5.208\n      15.624\n      17.856\n      899.47\n      110.35\n      AZ\n    \n    \n      3\n      22.4\n      4.032\n      5.824\n      21.056\n      21.280\n      827.34\n      142.39\n      AR\n    \n    \n      4\n      12.0\n      4.200\n      3.360\n      10.920\n      10.680\n      878.41\n      165.63\n      CA\n    \n  \n\n\n\n\n\n\n\n\n\nA distribution plot provides a way to look at a univariate distribution. A univeriate distribution provides a distribution for one variable. For more detail check out the seaborn documentation.\n\n# Kernal Density Estimation with a Histogram is provided\nsns.distplot(crash_df['alcohol'])\n\n/tmp/ipykernel_206/3403426496.py:3: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(crash_df['alcohol'])\n\n\n<AxesSubplot: xlabel='alcohol', ylabel='Density'>\n\n\n\n\n\nIf we just want the KDE on its own:\n\n# Get just the KDE plot\nsns.kdeplot(crash_df['alcohol'])\n\n<AxesSubplot: xlabel='alcohol', ylabel='Density'>\n\n\n\n\n\n\n\n\n\n\n\nKernel Density Estimation (KDE)\n\n\n\nThe kernel density estimation is included by default but can be removed by setting kde=False.\n\n\n\n# kde=False removes the KDE\n# Bins define how many buckets to divide the data up into between intervals\n\nsns.distplot(crash_df['alcohol'], \n             kde=False, \n             bins=25)\n\n/tmp/ipykernel_206/1004551440.py:5: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(crash_df['alcohol'], kde=False, bins=25)\n\n\n<AxesSubplot: xlabel='alcohol'>\n\n\n\n\n\n\n\n\nLet’s say we want to investigate the alcohol and speeding variables and also the relationship between the two. A jointplot compares two distributions and plots a scatter plot by default. We can also include a ‘best fit’ regression line by passing in the argument kind=‘reg’.\n\n# joint plot of speeding vs alcohol\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'reg')\n\n<seaborn.axisgrid.JointGrid at 0x7f31f0add2a0>\n\n\n\n\n\nAs we can see there looks to be a clear positive correlation between speeding and drink-driving.\nWe can also create a 2D KDE by passing in the argument kind=‘kde’\n\n# joint plot of speeding vs alcohol\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'kde')\n\n<seaborn.axisgrid.JointGrid at 0x7f31ec15ac80>\n\n\n\n\n\n…and we can create a hexagon distribution with kind=‘hex’\n\n# joint plot of speeding vs alcohol\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'hex')\n\n<seaborn.axisgrid.JointGrid at 0x7f31ecd43c10>\n\n\n\n\n\n\n\n\nPair Plots plot relationships across the entire DataFrame’s numerical values:\n\n# # pair plot for car_crashes DataFrame\nsns.pairplot(crash_df)\n\n<seaborn.axisgrid.PairGrid at 0x7f31cecd4be0>\n\n\n\n\n\nNote that in addition to the relationship scatterplots we also have a histogram for each variable on the leading diagonal.\n\n\n\n\n\n\nUsing hue as a categorical variable\n\n\n\nTWe can pass in hue=‘some_categorical_variable’ to effectively include a THIRD variable in our pair plot!\n\n\nLet’s illustrate this using one of the other inbuilt seaborn datasets tips which includes some categorical variables:\n\n# With hue you can pass in a categorical column and the charts will be colorized\n# You can use color maps (palette) from Matplotlib to define what colors to use\ntips_df = sns.load_dataset('tips')\nsns.pairplot(tips_df, \n             hue = 'sex', \n             palette = 'Blues')\n\n<seaborn.axisgrid.PairGrid at 0x7f31cdf078b0>\n\n\n\n\n\n\n\n\nA Rug Plot plots a single column of datapoints in an array as sticks on an axis. You’ll see a more dense number of lines where the amount is most common. This is like how a histogram is taller where values are more common.\n\n# plot a rugplot of tips\nsns.rugplot(tips_df['tip'])\n\n<AxesSubplot: xlabel='tip'>\n\n\n\n\n\nWe can see that the lines are denser between just below 2 dollars and up towards 3 dollars.\n\n\n\n\nWe can control the overall look of our plots using set_style\n\n# set overall style\nsns.set_style('ticks')\n\n# set size of our plot\nplt.figure(figsize = (8,4))\n\n# labels\nsns.set_context('paper', font_scale = 1.5)\n\n# jointplot of speeding v alcohol from car_crashes DF, include regression line\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'reg') \n\n# we can turn the axis spines on or off\nsns.despine(left = False, \n            bottom = True)\n\n<Figure size 800x400 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nBar plot or bar chart can be used to aggregate categorical data based on a function. Mean is the default but we can change the reported summary statistic. For more information see the seaborn documentation.\n\n# plot a barplot showing mean total bill by sex\nsns.barplot(x = 'sex', \n            y = 'total_bill', \n            data = tips_df)\n\n<AxesSubplot: xlabel='sex', ylabel='total_bill'>\n\n\n\n\n\nWe can estimate total bill amount based on sex. With estimator we can define functions to use other than the mean like those provided by NumPy : median, std, var, cov or make your own functions\n\n# plot a barplot showing median total bill by sex\nsns.barplot(x = 'sex',\n            y = 'total_bill',\n            data = tips_df, \n            estimator = np.median)\n\n<AxesSubplot: xlabel='sex', ylabel='total_bill'>\n\n\n\n\n\n\n\n\nA count plot is like a bar plot, but the estimator is counting the number of occurences. For more information see the seaborn documentation.\n\n# plot a count of number of males and females\nsns.countplot(x = 'sex',\n              data = tips_df)\n\n<AxesSubplot: xlabel='sex', ylabel='count'>\n\n\n\n\n\n\n\n\n\n\n\nshift and tab\n\n\n\nHit shift and tab after the function to pull up docstring and the list of possible arguments\n\n\n\nshift_tab.JPG\n\n\n\n\n\n\n\nA box plot allows you to compare different variables. The box shows the quartiles of the data. The bar in the middle is the median and the box extends 1 standard deviation from the median. The whiskers extend to all the other data aside from the points that are considered to be outliers. For more information see the seaborn documentation.\n\n# create box plots of total bill per day\n# use hue to allow comparison between male and female\nsns.boxplot(x = 'day',\n            y = 'total_bill', \n            data = tips_df, \n            hue = 'sex')\n\n\n# Sometimes the labelling can get a bit cluttered. We can control this\n# loc=0 moves legend to the best position\nplt.legend(loc=0)\n\n<matplotlib.legend.Legend at 0x7f31c52d92d0>\n\n\n\n\n\nInterestingly, we can see that men spend more than women on a Friday, but less than women on a Saturday.\n\n\n\nA violin plot is a combination of the boxplot and KDE. Whereas a box plot corresponds to data points, the violin plot uses the KDE estimation of the data points. For more information see the seaborn documentation.\n\n# create a violin plot showing distribution of total bill by day and sex\nsns.violinplot(x = 'day', \n               y = 'total_bill', \n               data = tips_df, \n               hue = 'sex')\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n# Split allows you to compare how the categories compare to each other\nsns.violinplot(x = 'day',\n               y = 'total_bill',\n               data = tips_df,\n               hue = 'sex',\n               split = True)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n\n\nThe strip plot or dotplot draws a scatter plot representing all data points where one variable is categorical. It is often used to show all observations with a box plot that represents the average distribution. For more information see the seaborn documentation.\n\nplt.figure(figsize = (8,5))\n\n# Jitter spreads data points out so that they aren't stacked on top of each other\n# Hue breaks data into men and women\nsns.stripplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              jitter = True, \n              hue = 'sex',\n              dodge = False)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\nThis looks great, but the data points for male and female are difficult to distinguish. We can set the argument dode=True to separate them amd show them side by side:\n\nplt.figure(figsize = (8,5))\n\n# Jitter spreads data points out so that they aren't stacked on top of each other\n# Hue breaks data into men and women\nsns.stripplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              jitter = True, \n              hue = 'sex',\n              dodge = True)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n\n\nA swarm plot is like a strip plot, but points are adjusted so they don’t overlap. It looks like a combination of the violin and strip plots. For more information see the seaborn documentation.\n\n# create a raw swarmplot of total bill bu day\nsns.swarmplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\nWe can then stack a violin plot with a swarm:\n\n# stack a violin plot on top of our swarmplot\nsns.violinplot(x = 'day',\n               y = 'total_bill',\n               data = tips_df)\n\nsns.swarmplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              color = 'white')\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor maps\n\n\n\nYou can use matplotlib’s color maps for styling\n\n\n\nplt.figure(figsize = (8,8))\n\n# let's go for a presentation style using 'talk'\nsns.set_style('ticks')\nsns.set_context('paper')\n\n# customize our colours using palette argument\nsns.stripplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              hue = 'sex',\n              palette = 'plasma')\n\n# control location of legend \nplt.legend(loc=2)\n\n<matplotlib.legend.Legend at 0x7f31c50bcc40>\n\n\n\n\n\n\n\n\n\n\n\nSetting the legend location\n\n\n\nWe can control the position of the legend by using plt.legend(loc = )\n1 - upper left\n2 - lower left\n3 - lower right\nSee the other options\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo create a heatmap you must have data set up as a matrix where variables are on the columns and rows\n\n\nFor example let’s look at an extract from our car_crashes dataset :\n\n# grab first 5 rows \ncrash_df.head()\n\n\n\n\n\n  \n    \n      \n      total\n      speeding\n      alcohol\n      not_distracted\n      no_previous\n      ins_premium\n      ins_losses\n      abbrev\n    \n  \n  \n    \n      0\n      18.8\n      7.332\n      5.640\n      18.048\n      15.040\n      784.55\n      145.08\n      AL\n    \n    \n      1\n      18.1\n      7.421\n      4.525\n      16.290\n      17.014\n      1053.48\n      133.93\n      AK\n    \n    \n      2\n      18.6\n      6.510\n      5.208\n      15.624\n      17.856\n      899.47\n      110.35\n      AZ\n    \n    \n      3\n      22.4\n      4.032\n      5.824\n      21.056\n      21.280\n      827.34\n      142.39\n      AR\n    \n    \n      4\n      12.0\n      4.200\n      3.360\n      10.920\n      10.680\n      878.41\n      165.63\n      CA\n    \n  \n\n\n\n\nIn its current format (with the variables across the columns but not the rows) we won’t be able to produce a heat map for our car_crashes DataFrame. We can transform our DataFrame into the correct format using a correlation function:\n\nplt.figure(figsize=(8,6))\nsns.set_context('paper',\n                font_scale = 1.4)\n\n# transform our dataset into a correlation matrix\ncrash_mx = crash_df.corr()\ncrash_mx\n\n/tmp/ipykernel_206/2429762792.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  crash_mx = crash_df.corr()\n\n\n\n\n\n\n  \n    \n      \n      total\n      speeding\n      alcohol\n      not_distracted\n      no_previous\n      ins_premium\n      ins_losses\n    \n  \n  \n    \n      total\n      1.000000\n      0.611548\n      0.852613\n      0.827560\n      0.956179\n      -0.199702\n      -0.036011\n    \n    \n      speeding\n      0.611548\n      1.000000\n      0.669719\n      0.588010\n      0.571976\n      -0.077675\n      -0.065928\n    \n    \n      alcohol\n      0.852613\n      0.669719\n      1.000000\n      0.732816\n      0.783520\n      -0.170612\n      -0.112547\n    \n    \n      not_distracted\n      0.827560\n      0.588010\n      0.732816\n      1.000000\n      0.747307\n      -0.174856\n      -0.075970\n    \n    \n      no_previous\n      0.956179\n      0.571976\n      0.783520\n      0.747307\n      1.000000\n      -0.156895\n      -0.006359\n    \n    \n      ins_premium\n      -0.199702\n      -0.077675\n      -0.170612\n      -0.174856\n      -0.156895\n      1.000000\n      0.623116\n    \n    \n      ins_losses\n      -0.036011\n      -0.065928\n      -0.112547\n      -0.075970\n      -0.006359\n      0.623116\n      1.000000\n    \n  \n\n\n\n\n<Figure size 800x600 with 0 Axes>\n\n\nNow that we have our data in the correct format we can go ahead and plot out heatmap:\n\n# create a heatmap, annotate and customize from colormaps\nsns.heatmap(crash_mx, \n            annot = True, \n            cmap = 'Blues')\n\n<AxesSubplot: >\n\n\n\n\n\nAnother way to prep our data for plotting is to create a pivot table. Let’s use another of seaborn’s built in datasets to illustrate:\n\nflights = sns.load_dataset(\"flights\")\nflights\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      passengers\n    \n  \n  \n    \n      0\n      1949\n      Jan\n      112\n    \n    \n      1\n      1949\n      Feb\n      118\n    \n    \n      2\n      1949\n      Mar\n      132\n    \n    \n      3\n      1949\n      Apr\n      129\n    \n    \n      4\n      1949\n      May\n      121\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      139\n      1960\n      Aug\n      606\n    \n    \n      140\n      1960\n      Sep\n      508\n    \n    \n      141\n      1960\n      Oct\n      461\n    \n    \n      142\n      1960\n      Nov\n      390\n    \n    \n      143\n      1960\n      Dec\n      432\n    \n  \n\n144 rows × 3 columns\n\n\n\nWe can create a matrix with an index of month, columns representing years and the number of passengers for each:\n\nflights = flights.pivot_table(index = 'month',\n                              columns = 'year',\n                              values = 'passengers')\nflights\n\n\n\n\n\n  \n    \n      year\n      1949\n      1950\n      1951\n      1952\n      1953\n      1954\n      1955\n      1956\n      1957\n      1958\n      1959\n      1960\n    \n    \n      month\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Jan\n      112\n      115\n      145\n      171\n      196\n      204\n      242\n      284\n      315\n      340\n      360\n      417\n    \n    \n      Feb\n      118\n      126\n      150\n      180\n      196\n      188\n      233\n      277\n      301\n      318\n      342\n      391\n    \n    \n      Mar\n      132\n      141\n      178\n      193\n      236\n      235\n      267\n      317\n      356\n      362\n      406\n      419\n    \n    \n      Apr\n      129\n      135\n      163\n      181\n      235\n      227\n      269\n      313\n      348\n      348\n      396\n      461\n    \n    \n      May\n      121\n      125\n      172\n      183\n      229\n      234\n      270\n      318\n      355\n      363\n      420\n      472\n    \n    \n      Jun\n      135\n      149\n      178\n      218\n      243\n      264\n      315\n      374\n      422\n      435\n      472\n      535\n    \n    \n      Jul\n      148\n      170\n      199\n      230\n      264\n      302\n      364\n      413\n      465\n      491\n      548\n      622\n    \n    \n      Aug\n      148\n      170\n      199\n      242\n      272\n      293\n      347\n      405\n      467\n      505\n      559\n      606\n    \n    \n      Sep\n      136\n      158\n      184\n      209\n      237\n      259\n      312\n      355\n      404\n      404\n      463\n      508\n    \n    \n      Oct\n      119\n      133\n      162\n      191\n      211\n      229\n      274\n      306\n      347\n      359\n      407\n      461\n    \n    \n      Nov\n      104\n      114\n      146\n      172\n      180\n      203\n      237\n      271\n      305\n      310\n      362\n      390\n    \n    \n      Dec\n      118\n      140\n      166\n      194\n      201\n      229\n      278\n      306\n      336\n      337\n      405\n      432\n    \n  \n\n\n\n\nNow that we have our data in the correct format we can go ahead and plot out heatmap:\n\n# You can separate data with lines\nsns.heatmap(flights,\n            cmap = 'Blues',\n            linecolor = 'white',\n            linewidth = 1)\n\n<AxesSubplot: xlabel='year', ylabel='month'>\n\n\n\n\n\nWe see that flights have increased over time and that most people travel in July and August.\n\n\n\nA Cluster map is a hierarchically clustered heatmap. The distance between points is calculated, the closest are joined, and this continues for the next closest (It compares columns / rows of the heatmap). For more information see the seaborn documentation.\nLet’s illustrate using another of seaborn’s built-in datasets:\n\niris = sns.load_dataset('iris')\niris\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n\n\nIt would be useful to carve up our numerical data based on the different species in order to identify any trends.\n\n# remove the column species using .pop\nspecies = iris.pop('species')\n\nsns.clustermap(iris)\n\n<seaborn.matrix.ClusterGrid at 0x7f31bc001ba0>\n\n\n\n\n\nLet’s try this out on our flight data we looked at earlier:\n\n# standard_scale normalizes the data to focus on the clustering\nsns.clustermap(flights,\n               cmap = \"Blues\",\n               standard_scale = 1)\n\n<seaborn.matrix.ClusterGrid at 0x7f31c2681c00>\n\n\n\n\n\nWith our flights data we can see that years have been reoriented to place like data closer together. You can see clusters of data for July & August for the years 59 & 60.\n\n\n\nPair Grids allow us to create a grid of different plots with complete control over what is displayed. For more information see the seaborn documentation.\n\n# load in iris dataset\niris = sns.load_dataset('iris')\n\n# Create the empty grid system using the provided data\n# Colorize based on species\niris_g = sns.PairGrid(iris,\n                      hue = 'species')\n\n# Put a histogram on the diagonal \niris_g.map_diag(plt.hist)\n\n# And a scatter plot every place else \niris_g.map_offdiag(plt.scatter)\n\n<seaborn.axisgrid.PairGrid at 0x7f31bbc31de0>\n\n\n\n\n\nWe can further tailor by having different plots in the upper, lower and diagonal:\n\n# load in iris dataset\niris = sns.load_dataset('iris')\n\n# Create the empty grid system using the provided data\n# Colorize based on species\niris_g = sns.PairGrid(iris,\n                      hue = 'species')\n\n# Put a histogram on the diagonal \niris_g.map_diag(plt.hist)\n\n# Have different plots in upper, lower and diagonal\niris_g.map_upper(plt.scatter)\niris_g.map_lower(sns.kdeplot)\n\n<seaborn.axisgrid.PairGrid at 0x7f31b6d41960>\n\n\n\n\n\nWe can also define the x and y variables for our custom grids:\n\n# load in iris dataset\niris = sns.load_dataset('iris')\n\n# You can define define variables for x & y for a custom grid\niris_g = sns.PairGrid(iris,\n                      hue = \"species\",\n                      x_vars = [\"sepal_length\", \"sepal_width\"],\n                      y_vars = [\"petal_length\", \"petal_width\"])\n\niris_g.map(plt.scatter)\n\n# Add a legend last\niris_g.add_legend()\n\n<seaborn.axisgrid.PairGrid at 0x7f31b8eecdc0>\n\n\n\n\n\n\n\n\nA facet grid allows us to print multiple plots in a grid where we can define columns & rows. For further information see the seaborn documentation.\nLet’s return to our tips dataset to illustrate what can be done usin facet grids.\n\n# Get histogram for smokers and non with total bill for lunch & dinner\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        row = 'smoker')\n\n# You can pass in attributes for the histogram\ntips_fg.map(plt.hist,\n            'total_bill',\n            bins = 8)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31bad559f0>\n\n\n\n\n\n\n# Get histogram for smokers and non with total bill for lunch & dinner\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        row = 'smoker')\n\n# Create a scatter plot with data on total bill & tip (You need 2 parameters)\ntips_fg.map(plt.scatter,\n            'total_bill',\n            'tip')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b61160b0>\n\n\n\n\n\n\n# We can assign variables to different colors and increase size of grid\n# Aspect is 1.3 x the size of height\n# You can change the order of the columns\n# Define the palette used\n\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        hue = 'smoker',\n                        height = 4,\n                        aspect = 1.3)\n                 \ntips_fg.map(plt.scatter,\n            \"total_bill\",\n            \"tip\")\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b80d1180>\n\n\n\n\n\nWe can change the column order:\n\n# We can assign variables to different colors and increase size of grid\n# Aspect is 1.3 x the size of height\n# Define the palette used\n\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        hue = 'smoker',\n                        height = 4,\n                        aspect = 1.3,\n\n# We can change the order of the columns and paletter\n                        col_order = ['Dinner', 'Lunch'], \n                        palette = 'Set1')\n              \n# We can change the edge colour of our dots to white      \ntips_fg.map(plt.scatter,\n            \"total_bill\",\n            \"tip\",\n            edgecolor = 'w')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b82530a0>\n\n\n\n\n\n\n# create a dictionary to define the size, linewidth and edge color of our markers\nkws = dict(s=50, \n           linewidth=.5, \n           edgecolor='w')\n\n# let's go to town on the customizations! note we can reference the dictionary we just created\ntips_fg = sns.FacetGrid(tips_df, \n                        col='sex', \n                        hue='smoker', \n                        height=4, \n                        aspect=1.3, \n                        hue_order=['Yes', 'No'], \n                        hue_kws=dict(marker=['^', 'v'])\n                       )\n\ntips_fg.map(plt.scatter,\n            'total_bill',\n            'tip',\n            **kws\n           )\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31bb0fbb20>\n\n\n\n\n\nHere hue has been used to denote smoker vs non-smoker - blue upward pointing markers represent smokers, and the orange downward facing markers represent non-smokers.\nLet’s look at a seaborn dataset that we haven’t seeen yet. This dataframe provides scores for different students based on the level of attention they could provide during testing:\n\n# load in attention dataset and take a look at first 13 rows\natt_df = sns.load_dataset('attention')\natt_df.head(13)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      subject\n      attention\n      solutions\n      score\n    \n  \n  \n    \n      0\n      0\n      1\n      divided\n      1\n      2.0\n    \n    \n      1\n      1\n      2\n      divided\n      1\n      3.0\n    \n    \n      2\n      2\n      3\n      divided\n      1\n      3.0\n    \n    \n      3\n      3\n      4\n      divided\n      1\n      5.0\n    \n    \n      4\n      4\n      5\n      divided\n      1\n      4.0\n    \n    \n      5\n      5\n      6\n      divided\n      1\n      5.0\n    \n    \n      6\n      6\n      7\n      divided\n      1\n      5.0\n    \n    \n      7\n      7\n      8\n      divided\n      1\n      5.0\n    \n    \n      8\n      8\n      9\n      divided\n      1\n      2.0\n    \n    \n      9\n      9\n      10\n      divided\n      1\n      6.0\n    \n    \n      10\n      10\n      11\n      focused\n      1\n      6.0\n    \n    \n      11\n      11\n      12\n      focused\n      1\n      8.0\n    \n    \n      12\n      12\n      13\n      focused\n      1\n      6.0\n    \n  \n\n\n\n\n\n# Put each student (subject) in their own plot with 5 per line and plot their scores\natt_fg = sns.FacetGrid(att_df, \n                       col = 'subject',\n                       col_wrap = 5,\n                       height = 1.5\n                      )\n\natt_fg.map(plt.plot,\n           'solutions',\n           'score',\n           marker ='.')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b5546ce0>\n\n\n\n\n\n\n\n\n\nLet’s revisit the tips dataset used earlier. Let’s recall what our dataset looks like:\n\n# grab first 5 rows\ntips_df.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n  \n\n\n\n\nSay we wanted to explore the relationship between tip size and total bill size comparing men and women:\n\nplt.figure(figsize=(8,6))\n\nsns.set_context('paper',\n                font_scale = 1.4)\n\nsns.lmplot(x = 'total_bill',\n           y = 'tip',\n           hue = 'sex',\n           data = tips_df,\n           markers = [ 'o', '^'],\n           scatter_kws = { 's' : 100, 'linewidth': 0.5, 'edgecolor': 'w'})                            \n                                                                                 \n\n<seaborn.axisgrid.FacetGrid at 0x7f31b4cd5720>\n\n\n<Figure size 800x600 with 0 Axes>\n\n\n\n\n\nIt might be useful to see the equivalent broken down by day:\n\n# sns.lmplot(x='total_bill', y='tip', col='sex', row='time', data=tips_df)\ntips_df.head()\n\n# Makes the fonts more readable\nsns.set_context('poster',\n                font_scale=1.4)\n\nsns.lmplot(x='total_bill', \n           y='tip', data=tips_df,\n           col='day',\n           hue='sex',\n          height=8,\n           aspect=0.6)\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b4d48fd0>\n\n\n\n\n\n\n\n\nThanks once again to Santiago for signposting this video posted by Derek Banas. This blog was written after interactively working through it.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6GUZXDef2U0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
  }
]