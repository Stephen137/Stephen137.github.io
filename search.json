[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Stephen Barrie",
    "section": "",
    "text": "Building on 20+ years in data analytics as a chartered accountant and auditor, I have spent the last 9 months learning new skills and technologies in the field of data engineering and machine learning. My first data engineering project was ranked 9 out of 298, and I am currently learning MLOps to strengthen my skillset further. In my previous role as a Client Relations Manager I collaborated and engaged with multiple internal and external stakeholders to develop long-term relationships, provide optimal solutions, and shape the strategic direction of my clients. I am ready for a new challenge."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Into the Unknown",
    "section": "",
    "text": "Numpy\n\n\nSciPy\n\n\nImage\n\n\nBiomedical\n\n\nSegmentation\n\n\nDataCamp\n\n\n\n\n\n\n\nStephen Barrie\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerraform\n\n\nPrefect\n\n\nGCS\n\n\nBigQuery\n\n\ndbt\n\n\nLooker\n\n\nDataTalksClub\n\n\n\n\n\n\n\nStephen Barrie\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpark\n\n\nDataproc\n\n\nBigQuery\n\n\nDataTalksClub\n\n\n\n\n\n\n\nStephen Barrie\n\n\nApr 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbt\n\n\nBigQuery\n\n\nLooker\n\n\nDataTalksClub\n\n\n\n\n\n\n\nStephen Barrie\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGCP\n\n\nBigQuery\n\n\nData Lakes\n\n\nETL\n\n\nPrefect\n\n\nDataTalksClub\n\n\n\n\n\n\n\nStephen Barrie\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGCP\n\n\nBigQuery\n\n\nData Lakes\n\n\nETL\n\n\nPrefect\n\n\nDataTalksClub\n\n\n\n\n\n\n\nStephen Barrie\n\n\nMar 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker\n\n\nGCP\n\n\nTerraform\n\n\nPostgreSQL\n\n\nDataTalksClub\n\n\n\n\n\n\n\nStephen Barrie\n\n\nMar 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableau\n\n\nDataCamp\n\n\n\n\n\n\n\n\nStephen Barrie\n\n\nFeb 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost\n\n\nLogistic Regression\n\n\nDataCamp\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting\n\n\nDataCamp\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nMulti-Label\n\n\nCross-Entropy\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Accumulation\n\n\nfastai\n\n\nEnsembling\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle\n\n\nfastai\n\n\nvision\n\n\ntimm\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nKaggle\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forests\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseaborn\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumPy\n\n\n\n\n\n\n\nStephen Barrie\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nImage\n\n\nData Carpentry\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nData Carpentry\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nGeospatial\n\n\nData Carpentry\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nAudio\n\n\nlibrosa\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nPython\n\n\nDataCamp\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL\n\n\nDataCamp\n\n\nApache Spark\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nSQL\n\n\n\n\n\n\n\nStephen Barrie\n\n\nDec 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nPython\n\n\n\n\n\n\n\nStephen Barrie\n\n\nNov 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nNov 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\nHugging Face\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Journey\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging\n\n\nQuarto\n\n\n\n\n\n\n\nStephen Barrie\n\n\nOct 21, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html",
    "href": "posts/Programming_with_Python/Programming_with_Python.html",
    "title": "Programming with Python",
    "section": "",
    "text": "This blog has been produced after working through the Programming with Python lessons provided by Data Carpentry.\nThe best way to learn how to program is to do something useful, so this introduction to Python is built around a common scientific task: data analysis."
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#scenario-a-miracle-arthritis-inflammation-cure",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#scenario-a-miracle-arthritis-inflammation-cure",
    "title": "Programming with Python",
    "section": "Scenario: A Miracle Arthritis Inflammation Cure",
    "text": "Scenario: A Miracle Arthritis Inflammation Cure\nOur imaginary colleague “Dr. Maverick” has invented a new miracle drug that promises to cure arthritis inflammation flare-ups after only 3 weeks since initially taking the medication! Naturally, we wish to see the clinical trial data, and after months of asking for the data they have finally provided us with a CSV spreadsheet containing the clinical trial data.\nThe CSV file contains the number of inflammation flare-ups per day for the 60 patients in the initial clinical trial, with the trial lasting 40 days. Each row corresponds to a patient, and each column corresponds to a day in the trial. Once a patient has their first inflammation flare-up they take the medication and wait a few weeks for it to take effect and reduce flare-ups.\nTo see how effective the treatment is we would like to:\n\nCalculate the average inflammation per day across all patients.\nPlot the result to discuss and share with colleagues.\n\n\nData Format\nThe data sets are stored in comma-separated values (CSV) format:\n\neach row holds information for a single patient,\ncolumns represent successive days.\n\nThe first three rows of our first file look like this:\n\n\n\nthree_rows.JPG\n\n\nEach number represents the number of inflammation bouts that a particular patient experienced on a given day. For example, value “6” at row 3 column 7 of the data set above means that the third patient was experiencing inflammation six times on the seventh day of the clinical study.\nIn order to analyze this data and report to our colleagues, we’ll have to learn a little bit about programming."
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#python-fundamentals",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#python-fundamentals",
    "title": "Programming with Python",
    "section": "1. Python Fundamentals",
    "text": "1. Python Fundamentals\n\nVariables\nAny Python interpreter can be used as a calculator:\n\n3 + 5 * 4\n\n23\n\n\nThis is great but not very interesting. To do anything useful with data, we need to assign its value to a variable. In Python, we can assign a value to a variable, using the equals sign =. For example, we can track the weight of a patient who weighs 60 kilograms by assigning the value 60 to a variable weight_kg:\n\nweight_kg = 60\n\nFrom now on, whenever we use weight_kg, Python will substitute the value we assigned to it. In layperson’s terms, a variable is a name for a value.\nIn Python, variable names:\n\ncan include letters, digits, and underscores\ncannot start with a digit\nare case sensitive.\n\nThis means that, for example:\n\nweight0 is a valid variable name, whereas 0weight is not\nweight and Weight are different variables\n\n\n\nTypes of data\nPython knows various types of data. Three common ones are:\n\ninteger numbers\nfloating point numbers, and\nstrings.\n\nIn the example above, variable weight_kg has an integer value of 60. If we want to more precisely track the weight of our patient, we can use a floating point value by executing:\n\nweight_kg = 60.3\n\nTo create a string, we add single or double quotes around some text. To identify and track a patient throughout our study, we can assign each person a unique identifier by storing it in a string:\n\npatient_id = '001'\n\n\n\nUsing Variables in Python\nOnce we have data stored with variable names, we can make use of it in calculations. We may want to store our patient’s weight in pounds as well as kilograms:\n\nweight_lb = 2.2 * weight_kg\n\nWe might decide to add a prefix to our patient identifier:\n\npatient_id = 'inflam_' + patient_id\n\n\n\nBuilt-in Python functions\nTo carry out common tasks with data and variables in Python, the language provides us with several built-in functions. To display information to the screen, we use the print function:\n\nprint(weight_lb)\nprint(patient_id)\n\n132.66\ninflam_inflam_001\n\n\nWhen we want to make use of a function, referred to as calling the function, we follow its name by parentheses. The parentheses are important: if you leave them off, the function doesn’t actually run! Sometimes you will include values or variables inside the parentheses for the function to use. In the case of print, we use the parentheses to tell the function what value we want to display. We will learn more about how functions work and how to create our own in later sections.\nWe can display multiple things at once using only one print call:\n\nprint(patient_id, 'weight in kilograms:', weight_kg)\n\ninflam_inflam_inflam_001 weight in kilograms: 60.3\n\n\nWe can also call a function inside of another function call. For example, Python has a built-in function called type that tells you a value’s data type:\n\nprint(type(60.3))\nprint(type(patient_id))\n\n<class 'float'>\n<class 'str'>\n\n\nMoreover, we can do arithmetic with variables right inside the print function:\n\nprint('weight in pounds:', 2.2 * weight_kg)\n\nweight in pounds: 132.66\n\n\nThe above command, however, did not change the value of weight_kg:\n\nprint(weight_kg)\n\n60.3\n\n\nTo change the value of the weight_kg variable, we have to assign weight_kg a new value using the equals = sign:\n\nweight_kg = 65.0\nprint('weight in kilograms is now:', weight_kg)\n\nweight in kilograms is now: 65.0\n\n\n\n\nVariables as Sticky Notes\nA variable in Python is analogous to a sticky note with a name written on it: assigning a value to a variable is like putting that sticky note on a particular value. Using this analogy, we can investigate how assigning a value to one variable does not change values of other, seemingly related, variables. For example, let’s store the subject’s weight in pounds in its own variable:\n\n# There are 2.2 pounds per kilogram\nweight_lb = 2.2 * weight_kg\nprint('weight in kilograms:', weight_kg, 'and in pounds:', weight_lb)\n\nweight in kilograms: 65.0 and in pounds: 143.0\n\n\nSimilar to above, the expression 2.2 * weight_kg is evaluated to 143.0, and then this value is assigned to the variable weight_lb (i.e. the sticky note weight_lb is placed on 143.0). At this point, each variable is “stuck” to completely distinct and unrelated values.\nLet’s now change weight_kg:\n\nweight_kg = 100.0\nprint('weight in kilograms is now:', weight_kg, 'and weight in pounds is still:', weight_lb)\n\nweight in kilograms is now: 100.0 and weight in pounds is still: 143.0\n\n\n\nSince weight_lb doesn’t “remember” where its value comes from, it is not updated when we change weight_kg.\n\n\n\nSorting out references\nPython allows you to assign multiple values to multiple variables in one line by separating the variables and values with commas.\n\nfirst, second = 'Grace', 'Hopper'\nthird, fourth = second, first\nprint(third, fourth)\n\nHopper Grace\n\n\n\n\nSeeing Data Types\n\nplanet = 'Earth'\napples = 5\ndistance = 10.5\n\n\nprint(type(planet))\nprint(type(apples))\nprint(type(distance))\n\n<class 'str'>\n<class 'int'>\n<class 'float'>\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nbasic data types in Python include integers, strings, and floating-point numbers\nuse variable = value to assign a value to a variable in order to record it in memory\nvariables are created on demand whenever a value is assigned to them\nuse print(something) to display the value of something\nbuilt-in functions are always available to use"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-patient-data",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-patient-data",
    "title": "Programming with Python",
    "section": "2. Analyzing Patient Data",
    "text": "2. Analyzing Patient Data\n\nLoading data into Python\n\nimport numpy\n\n\nnumpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\n\narray([[0., 0., 1., ..., 3., 0., 0.],\n       [0., 1., 2., ..., 1., 0., 1.],\n       [0., 1., 1., ..., 2., 1., 1.],\n       ...,\n       [0., 1., 1., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 0., 2., 0.],\n       [0., 0., 1., ..., 1., 1., 0.]])\n\n\nThe expression numpy.loadtxt(…) is a function call that asks Python to run the function loadtxt which belongs to the NumPy library. The dot notation in Python is used most of all as an object attribute/property specifier or for invoking its method. object.property will give you the object.property value, object_name.method() will invoke on object_name method.\nAs an example, John Smith is the John that belongs to the Smith family. We could use the dot notation to write his name smith.john, just as loadtxt is a function that belongs to the numpy library.\nnumpy.loadtxt has two parameters: - the name of the file we want to read and - the delimiter that separates values on a line.\nThese both need to be character strings (or strings for short), so we put them in quotes.\nSince we haven’t told it to do anything else with the function’s output, the notebook displays it. In this case, that output is the data we just loaded. By default, only a few rows and columns are shown (with … to omit elements when displaying big arrays). Note that, to save space when displaying NumPy arrays, Python does not show us trailing zeros, so 1.0 becomes 1..\nOur call to numpy.loadtxt read our file but didn’t save the data in memory. To do that, we need to assign the array to a variable. In a similar manner to how we assign a single value to a variable, we can also assign an array of values to a variable using the same syntax. Let’s re-run numpy.loadtxt and save the returned data:\n\ndata = numpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\n\nThis statement doesn’t produce any output because we’ve assigned the output to the variable data. If we want to check that the data have been loaded, we can print the variable’s value:\n\nprint(data)\n\n[[0. 0. 1. ... 3. 0. 0.]\n [0. 1. 2. ... 1. 0. 1.]\n [0. 1. 1. ... 2. 1. 1.]\n ...\n [0. 1. 1. ... 1. 1. 1.]\n [0. 0. 0. ... 0. 2. 0.]\n [0. 0. 1. ... 1. 1. 0.]]\n\n\nNow that the data are in memory, we can manipulate them. First, let’s ask what type of thing data refers to:\n\nprint(type(data))\n\n<class 'numpy.ndarray'>\n\n\nThe output tells us that data currently refers to an N-dimensional array, the functionality for which is provided by the NumPy library. These data correspond to arthritis patients’ inflammation. The rows are the individual patients, and the columns are their daily inflammation measurements.\n\n\nData Type\nA Numpy array contains one or more elements of the same type. The type function will only tell you that a variable is a NumPy array but won’t tell you the type of thing inside the array. We can find out the type of the data contained in the NumPy array:\n\nprint(data.dtype)\n\nfloat64\n\n\nThis tells us that the NumPy array’s elements are floating-point numbers. With the following command, we can see the array’s shape:\n\nprint(data.shape)\n\n(60, 40)\n\n\nThe output tells us that the data array variable contains 60 rows and 40 columns. When we created the variable data to store our arthritis data, we did not only create the array; we also created information about the array, called members or attributes. This extra information describes data in the same way an adjective describes a noun. data.shape is an attribute of data which describes the dimensions of data. We use the same dotted notation for the attributes of variables that we use for the functions in libraries because they have the same part-and-whole relationship.\nIf we want to get a single number from the array, we must provide an index in square brackets after the variable name, just as we do in math when referring to an element of a matrix. Our inflammation data has two dimensions, so we will need to use two indices to refer to one specific value:\n\nprint('first value in data:', data[0, 0])\n\nfirst value in data: 0.0\n\n\n\nprint('middle value in data:', data[30, 20])\n\nmiddle value in data: 13.0\n\n\nThe expression data[30, 20] accesses the element at row 30, column 20. While this expression may not surprise you, data[0, 0] might. Programming languages like Fortran, MATLAB and R start counting at 1 because that’s what human beings have done for thousands of years. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because it represents an offset from the first value in the array (the second value is offset by one index from the first value). This is closer to the way that computers represent arrays (if you are interested in the historical reasons behind counting indices from zero, you can read Mike Hoye’s blog post). As a result, if we have an M×N array in Python, its indices go from 0 to M-1 on the first axis and 0 to N-1 on the second. It takes a bit of getting used to, but one way to remember the rule is that the index is how many steps we have to take from the start to get the item we want. \nWhat may also surprise you is that when Python displays an array, it shows the element with index [0, 0] in the upper left corner rather than the lower left. This is consistent with the way mathematicians draw matrices but different from the Cartesian coordinates. The indices are (row, column) instead of (column, row) for the same reason, which can be confusing when plotting data.\n\n\nSlicing data\n\nAn index like [30, 20] selects a single element of an array, but we can select whole sections as well. For example, we can select the first ten days (columns) of values for the first four patients (rows) like this:\n\nprint(data[0:4, 0:10])\n\n[[0. 0. 1. 3. 1. 2. 4. 7. 8. 3.]\n [0. 1. 2. 1. 2. 1. 3. 2. 2. 6.]\n [0. 1. 1. 3. 3. 2. 6. 2. 5. 9.]\n [0. 0. 2. 0. 4. 2. 2. 1. 6. 7.]]\n\n\nThe slice 0:4 means, “Start at index 0 and go up to, but not including, index 4”. Again, the up-to-but-not-including takes a bit of getting used to, but the rule is that the difference between the upper and lower bounds is the number of values in the slice.\nWe don’t have to start slices at 0:\n\nprint(data[5:10, 0:10])\n\n[[0. 0. 1. 2. 2. 4. 2. 1. 6. 4.]\n [0. 0. 2. 2. 4. 2. 2. 5. 5. 8.]\n [0. 0. 1. 2. 3. 1. 2. 3. 5. 3.]\n [0. 0. 0. 3. 1. 5. 6. 5. 5. 8.]\n [0. 1. 1. 2. 1. 3. 5. 3. 5. 8.]]\n\n\nWe also don’t have to include the upper and lower bound on the slice. If we don’t include the lower bound, Python uses 0 by default; if we don’t include the upper, the slice runs to the end of the axis, and if we don’t include either (i.e., if we use ‘:’ on its own), the slice includes everything:\n\nsmall = data[:3, 36:]\nprint('small is:')\nprint(small)\n\nsmall is:\n[[2. 3. 0. 0.]\n [1. 1. 0. 1.]\n [2. 2. 1. 1.]]\n\n\nThe above example selects rows 0 through 2 and columns 36 through to the end of the array.\n\n\nAnalyzing data\nNumPy has several useful functions that take an array as input to perform operations on its values. If we want to find the average inflammation for all patients on all days, for example, we can ask NumPy to compute data’s mean value:\n\nprint(numpy.mean(data))\n\n6.14875\n\n\nmean is a function that takes an array as an argument.\n\n\nNot all functions have inputs\nGenerally, a function uses inputs to produce outputs. However, some functions produce outputs without needing any input. For example, checking the current time doesn’t require any input.\n\nimport time\nprint(time.ctime())\n\nThu Dec 15 22:41:53 2022\n\n\nFor functions that don’t take in any arguments, we still need parentheses (()) to tell Python to go and do something for us. Let’s use three other NumPy functions to get some descriptive values about the dataset. We’ll also use multiple assignment, a convenient Python feature that will enable us to do this all in one line.\n\n# assigned the return value from numpy.max(data) to the variable maxval, the value from numpy.min(data) to minval, and so on\nmaxval, minval, stdval = numpy.max(data), numpy.min(data), numpy.std(data)\n\nprint('maximum inflammation:', maxval)\nprint('minimum inflammation:', minval)\nprint('standard deviation:', stdval)\n\nmaximum inflammation: 20.0\nminimum inflammation: 0.0\nstandard deviation: 4.613833197118566\n\n\n\n\nMystery Functions in IPython\nHow did we know what functions NumPy has and how to use them? If you are working in IPython or in a Jupyter Notebook, there is an easy way to find out. If you type the name of something followed by a dot, then you can use tab completion (e.g. type numpy. and then press Tab) to see a list of all functions and attributes that you can use. After selecting one, you can also add a question mark (e.g. numpy.cumprod?), and IPython will return an explanation of the method! This is the same as doing help(numpy.cumprod).\nSimilarly, if you are using the “plain vanilla” Python interpreter, you can type numpy. and press the Tab key twice for a listing of what is available. You can then use the help() function to see an explanation of the function you’re interested in, for example: help(numpy.cumprod).\nWhen analyzing data, though, we often want to look at variations in statistical values, such as the maximum inflammation per patient or the average inflammation per day. One way to do this is to create a new temporary array of the data we want, then ask it to do the calculation:\n\n# 0 on the first axis (rows), everything on the second (columns)\npatient_0 = data[0, :] \nprint('maximum inflammation for patient 0:', numpy.max(patient_0))\n\nmaximum inflammation for patient 0: 18.0\n\n\nEverything in a line of code following the ‘#’ symbol is a comment that is ignored by Python. Comments allow programmers to leave explanatory notes for other programmers or their future selves. We don’t actually need to store the row in a variable of its own. Instead, we can combine the selection and the function call:\n\nprint('maximum inflammation for patient 2:', numpy.max(data[2, :]))\n\nmaximum inflammation for patient 2: 19.0\n\n\nWhat if we need the maximum inflammation for each patient over all days (as in the next diagram on the left) or the average for each day (as in the diagram on the right)? As the diagram below shows, we want to perform the operation across an axis:\n\n\n\npython-operations-across-axes.png\n\n\nTo support this functionality, most array functions allow us to specify the axis we want to work on. If we ask for the average across axis 0 (rows in our 2D example), we get:\n\nprint(numpy.mean(data, axis=0))\n\n[ 0.          0.45        1.11666667  1.75        2.43333333  3.15\n  3.8         3.88333333  5.23333333  5.51666667  5.95        5.9\n  8.35        7.73333333  8.36666667  9.5         9.58333333 10.63333333\n 11.56666667 12.35       13.25       11.96666667 11.03333333 10.16666667\n 10.          8.66666667  9.15        7.25        7.33333333  6.58333333\n  6.06666667  5.95        5.11666667  3.6         3.3         3.56666667\n  2.48333333  1.5         1.13333333  0.56666667]\n\n\nAs a quick check, we can ask this array what its shape is:\n\nprint(numpy.mean(data, axis=0).shape)\n\n(40,)\n\n\nThe expression (40,) tells us we have an N×1 vector, so this is the average inflammation per day for all patients. If we average across axis 1 (columns in our 2D example), we get:\n\nprint(numpy.mean(data, axis=1))\n\n[5.45  5.425 6.1   5.9   5.55  6.225 5.975 6.65  6.625 6.525 6.775 5.8\n 6.225 5.75  5.225 6.3   6.55  5.7   5.85  6.55  5.775 5.825 6.175 6.1\n 5.8   6.425 6.05  6.025 6.175 6.55  6.175 6.35  6.725 6.125 7.075 5.725\n 5.925 6.15  6.075 5.75  5.975 5.725 6.3   5.9   6.75  5.925 7.225 6.15\n 5.95  6.275 5.7   6.1   6.825 5.975 6.725 5.7   6.25  6.4   7.05  5.9  ]\n\n\nwhich is the average inflammation per patient across all days.\n\n\nSlicing Strings\nA section of an array is called a slice. We can take slices of character strings as well:\n\nelement = 'oxygen'\n\nprint('first three characters:', element[0:3])\nprint('last three characters:', element[3:6])\nprint('first four characters:', element[:4])\nprint('last two characters:', element[4:])\nprint('all characters:', element[:])\nprint('last character:', element[-1])\nprint('second last character:', element[-2])\n\nfirst three characters: oxy\nlast three characters: gen\nfirst four characters: oxyg\nlast two characters: en\nall characters: oxygen\nlast character: n\ne\n\n\n\n\nThin slices\n\nprint(element[3:3])\n\n\n\n\n\ndata[3:3, 4:4]\n\narray([], shape=(0, 0), dtype=float64)\n\n\n\ndata[3:3, :]\n\narray([], shape=(0, 40), dtype=float64)\n\n\n\n\nStacking arrays\nArrays can be concatenated and stacked on top of one another, using NumPy’s vstack and hstack functions for vertical and horizontal stacking, respectively.\n\nA = numpy.array([[1,2,3], [4,5,6], [7, 8, 9]])\nprint('A = ')\nprint(A)\n\nB = numpy.hstack([A, A])\nprint('B = ')\nprint(B)\n\nC = numpy.vstack([A, A])\nprint('C = ')\nprint(C)\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nB = \n[[1 2 3 1 2 3]\n [4 5 6 4 5 6]\n [7 8 9 7 8 9]]\nC = \n[[1 2 3]\n [4 5 6]\n [7 8 9]\n [1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\nA ‘gotcha’ with array indexing is that singleton dimensions are dropped by default. That means A[:, 0] is a one dimensional array, which won’t stack as desired. To preserve singleton dimensions, the index itself can be a slice or array. For example, A[:, :1] returns a two dimensional array with one singleton dimension (i.e. a column vector).\n\nD = numpy.hstack((A[:, :1], A[:, -1:]))\nprint('D = ')\nprint(D)\n\nD = \n[[1 3]\n [4 6]\n [7 9]]\n\n\nAn alternative way to achieve the same result is to use Numpy’s delete function to remove the second column of A:\n\nD = numpy.delete(A, 1, 1)\nprint('D = ')\nprint(D)\n\nD = \n[[1 3]\n [4 6]\n [7 9]]\n\n\n\n\nChange In Inflammation\nThe patient data is longitudinal in the sense that each row represents a series of observations relating to one individual. This means that the change in inflammation over time is a meaningful concept. Let’s find out how to calculate changes in the data contained in an array with NumPy.\nThe numpy.diff() function takes an array and returns the differences between two successive values. Let’s use it to examine the changes each day across the first week of patient 3 from our inflammation dataset.\n\npatient3_week1 = data[3, :7]\nprint(patient3_week1)\n\n[0. 0. 2. 0. 4. 2. 2.]\n\n\nCalling numpy.diff(patient3_week1) would do the following calculations\n\n[ 0 - 0, 2 - 0, 0 - 2, 4 - 0, 2 - 4, 2 - 2 ]\n\n[0, 2, -2, 4, -2, 0]\n\n\nand return the 6 difference values in a new array.\n\nnumpy.diff(patient3_week1)\n\narray([ 0.,  2., -2.,  4., -2.,  0.])\n\n\nNote that the array of differences is shorter by one element (length 6).\nWhen calling numpy.diff with a multi-dimensional array, an axis argument may be passed to the function to specify which axis to process. When applying numpy.diff to our 2D inflammation array data, which axis would we specify?\nSince the row axis (0) is patients, it does not make sense to get the difference between two arbitrary patients. The column axis (1) is in days, so the difference is the change in inflammation – a meaningful concept.\n\nnumpy.diff(data, axis=1)\n\narray([[ 0.,  1.,  2., ...,  1., -3.,  0.],\n       [ 1.,  1., -1., ...,  0., -1.,  1.],\n       [ 1.,  0.,  2., ...,  0., -1.,  0.],\n       ...,\n       [ 1.,  0.,  0., ..., -1.,  0.,  0.],\n       [ 0.,  0.,  1., ..., -2.,  2., -2.],\n       [ 0.,  1., -1., ..., -2.,  0., -1.]])\n\n\n\n\n\n\n\n\nIf the shape of an individual data file is (60, 40) (60 rows and 40 columns), what would the shape of the array be after you run the diff() function and why?\n\n\n\n\n\nThe shape will be (60, 39) because there is one fewer difference between columns than there are columns in the data.\n\n\n\n\n\n\n\n\n\nHow would you find the largest change in inflammation for each patient? Does it matter if the change in inflammation is an increase or a decrease?\n\n\n\n\n\nBy using the numpy.max() function after you apply the numpy.diff() function, you will get the largest difference between days.\n\n\n\n\nnumpy.max(numpy.diff(data, axis=1), axis=1)\n\narray([ 7., 12., 11., 10., 11., 13., 10.,  8., 10., 10.,  7.,  7., 13.,\n        7., 10., 10.,  8., 10.,  9., 10., 13.,  7., 12.,  9., 12., 11.,\n       10., 10.,  7., 10., 11., 10.,  8., 11., 12., 10.,  9., 10., 13.,\n       10.,  7.,  7., 10., 13., 12.,  8.,  8., 10., 10.,  9.,  8., 13.,\n       10.,  7., 10.,  8., 12., 10.,  7., 12.])\n\n\nIf inflammation values decrease along an axis, then the difference from one element to the next will be negative. If you are interested in the magnitude of the change and not the direction, the numpy.absolute() function will provide that.\nNotice the difference if you get the largest absolute difference between readings.\n\nnumpy.max(numpy.absolute(numpy.diff(data, axis=1)), axis=1)\n\narray([12., 14., 11., 13., 11., 13., 10., 12., 10., 10., 10., 12., 13.,\n       10., 11., 10., 12., 13.,  9., 10., 13.,  9., 12.,  9., 12., 11.,\n       10., 13.,  9., 13., 11., 11.,  8., 11., 12., 13.,  9., 10., 13.,\n       11., 11., 13., 11., 13., 13., 10.,  9., 10., 10.,  9.,  9., 13.,\n       10.,  9., 10., 11., 13., 10., 10., 12.])\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nimport a library into a program using import libraryname\nuse the numpy library to work with arrays in Python\nthe expression array.shape gives the shape of an array\nuse array[x, y] to select a single element from a 2D array\narray indices start at 0, not 1\nuse low:high to specify a slice that includes the indices from low to high-1\nuse # some kind of explanation to add comments to programs\nuse numpy.mean(array), numpy.max(array), and numpy.min(array) to calculate simple statistics\nuse numpy.mean(array, axis=0) or numpy.mean(array, axis=1) to calculate statistics across the specified axis"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#visualizing-tabular-data",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#visualizing-tabular-data",
    "title": "Programming with Python",
    "section": "3. Visualizing Tabular Data",
    "text": "3. Visualizing Tabular Data\n\nVisualizing data\nThe mathematician Richard Hamming once said, “The purpose of computing is insight, not numbers,” and the best way to develop insight is often to visualize data. Visualization deserves an entire lecture of its own, but we can explore a few features of Python’s matplotlib library here. While there is no official plotting library, matplotlib is the de facto standard. First, we will import the pyplot module from matplotlib and use two of its functions to create and display a heat map of our data:\n\nimport matplotlib.pyplot\nimage = matplotlib.pyplot.imshow(data)\nmatplotlib.pyplot.show()\n\n\n\n\nEach row in the heat map corresponds to a patient in the clinical trial dataset, and each column corresponds to a day in the dataset. Blue pixels in this heat map represent low values, while yellow pixels represent high values. As we can see, the general number of inflammation flare-ups for the patients rises and falls over a 40-day period.\nSo far so good as this is in line with our knowledge of the clinical trial and Dr. Maverick’s claims:\n\nthe patients take their medication once their inflammation flare-ups begin\nit takes around 3 weeks for the medication to take effect and begin reducing flare-ups\nand flare-ups appear to drop to zero by the end of the clinical trial\n\nNow let’s take a look at the average inflammation over time:\n\nave_inflammation = numpy.mean(data, axis=0)\nave_plot = matplotlib.pyplot.plot(ave_inflammation)\nmatplotlib.pyplot.show()\n\n\n\n\nHere, we have put the average inflammation per day across all patients in the variable ave_inflammation, then asked matplotlib.pyplot to create and display a line graph of those values. The result is a reasonably linear rise and fall, in line with Dr. Maverick’s claim that the medication takes 3 weeks to take effect.\nBut a good data scientist doesn’t just consider the average of a dataset, so let’s have a look at two other statistics:\n\n# maximum inflammation per day \nmax_plot = matplotlib.pyplot.plot(numpy.max(data, axis=0))\nmatplotlib.pyplot.show()\n\n\n\n\n\n# minimum inflammation per day \nmax_plot = matplotlib.pyplot.plot(numpy.min(data, axis=0))\nmatplotlib.pyplot.show()\n\n\n\n\nThe maximum value rises and falls linearly. The minimum seems to be a step function. Neither trend seems particularly likely, so either there’s a mistake in our calculations or something is wrong with our data. This insight would have been difficult to reach by examining the numbers themselves without visualization tools.\n\n\nGrouping plots\nYou can group similar plots in a single figure using subplots. This script below uses a number of new commands. The function matplotlib.pyplot.figure() creates a space into which we will place all of our plots. The parameter figsize tells Python how big to make this space. Each subplot is placed into the figure using its add_subplot method. The add_subplot method takes 3 parameters. The first denotes how many total rows of subplots there are, the second parameter refers to the total number of subplot columns, and the final parameter denotes which subplot your variable is referencing (left-to-right, top-to-bottom). Each subplot is stored in a different variable (axes1, axes2, axes3). Once a subplot is created, the axes can be titled using the set_xlabel() command (or set_ylabel()). Here are our three plots side by side:\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\naxes1 = fig.add_subplot(1, 3, 1)\naxes2 = fig.add_subplot(1, 3, 2)\naxes3 = fig.add_subplot(1, 3, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(data, axis=0))\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(data, axis=0))\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0))\n\nfig.tight_layout()\n\nmatplotlib.pyplot.savefig('inflammation.png')\nmatplotlib.pyplot.show()\n\n\n\n\nThe call to loadtxt reads our data, and the rest of the program tells the plotting library how large we want the figure to be, that we’re creating three subplots, what to draw for each one, and that we want a tight layout.\nIf we leave out that call to fig.tight_layout(), the graphs will actually be squeezed together more closely.\nThe call to savefig stores the plot as a graphics file. This can be a convenient way to store your plots for use in other documents, web pages etc. The graphics format is automatically determined by Matplotlib from the file name ending we specify; here PNG from ‘inflammation.png’. Matplotlib supports many different graphics formats, including SVG, PDF, and JPEG.\n\n\nImporting libraries with shortcuts\nIn this blog we use the import matplotlib.pyplot syntax to import the pyplot module of matplotlib. However, shortcuts such as import matplotlib.pyplot as plt are frequently used. Importing pyplot this way means that after the initial import, rather than writing matplotlib.pyplot.plot(…), you can now write plt.plot(…). Another common convention is to use the shortcut import numpy as np when importing the NumPy library. We then can write np.loadtxt(…) instead of numpy.loadtxt(…), for example.\nSome people prefer these shortcuts as it is quicker to type and results in shorter lines of code - especially for libraries with long names! You will frequently see Python code online using a pyplot function with plt, or a NumPy function with np, and it’s because they’ve used this shortcut. It makes no difference which approach you choose to take, but you must be consistent as if you use import matplotlib.pyplot as plt then matplotlib.pyplot.plot(…) will not work, and you must use plt.plot(…) instead. Because of this, when working with other people it is important you agree on how libraries are imported.\n\n\n\n\n\n\nWhy do all of our plots stop just short of the upper end of our graph?\n\n\n\n\n\nBecause matplotlib normally sets x and y axes limits to the min and max of our data (depending on data range). If we want to change this, we can use the set_ylim(min, max) method of each ‘axes’, for example:\n\n\n\n\n# One method\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0))\naxes3.set_ylim(0,6)\n\n(0.0, 6.0)\n\n\n\n# A more automated approach\nmin_data = numpy.min(data, axis=0)\naxes3.set_ylabel('min')\naxes3.plot(min_data)\naxes3.set_ylim(numpy.min(min_data), numpy.max(min_data) * 1.1)\n\n(0.0, 5.5)\n\n\n\n\nDrawing Straight Lines\nIn the center and right subplots above, we expect all lines to look like step functions because non-integer value are not realistic for the minimum and maximum values. However, you can see that the lines are not always vertical or horizontal, and in particular the step function in the subplot on the right looks slanted. Why is this?\nBecause matplotlib interpolates (draws a straight line) between the points. One way to avoid this is to use the Matplotlib drawstyle option:\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\naxes1 = fig.add_subplot(1, 3, 1)\naxes2 = fig.add_subplot(1, 3, 2)\naxes3 = fig.add_subplot(1, 3, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(data, axis=0), drawstyle='steps-mid')\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(data, axis=0), drawstyle='steps-mid')\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0), drawstyle='steps-mid')\n\nfig.tight_layout()\n\nmatplotlib.pyplot.show()\n\n\n\n\nLet’s now create a plot showing the standard deviation (numpy.std) of the inflammation data for each day across all patients:\n\nstd_plot = matplotlib.pyplot.plot(numpy.std(data, axis=0))\nmatplotlib.pyplot.show()\n\n\n\n\nWe can modify the program to display the three plots on top of one another instead of side by side:\n\n# change figsize (swap width and height)\nfig = matplotlib.pyplot.figure(figsize=(3.0, 10.0))\n\n# change add_subplot (swap first two parameters)\naxes1 = fig.add_subplot(3, 1, 1)\naxes2 = fig.add_subplot(3, 1, 2)\naxes3 = fig.add_subplot(3, 1, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(data, axis=0))\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(data, axis=0))\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(data, axis=0))\n\nfig.tight_layout()\n\nmatplotlib.pyplot.show()\n\n\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse the pyplot module from the matplotlib library for creating simple visualizations."
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#storing-multiple-values-in-lists",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#storing-multiple-values-in-lists",
    "title": "Programming with Python",
    "section": "4. Storing Multiple Values in Lists",
    "text": "4. Storing Multiple Values in Lists\nIn the previous section, we analyzed a single file of clinical trial inflammation data. However, after finding some peculiar and potentially suspicious trends in the trial data we ask Dr. Maverick if they have performed any other clinical trials. Surprisingly, they say that they have and provide us with 11 more CSV files for a further 11 clinical trials they have undertaken since the initial trial.\nOur goal now is to process all the inflammation data we have, which means that we still have eleven more files to go!\nThe natural first step is to collect the names of all the files that we have to process. In Python, a list is a way to store multiple values together. In this episode, we will learn how to store multiple values in a list as well as how to work with lists.\n\nPython lists\nUnlike NumPy arrays, lists are built into the language so we do not have to load a library to use them. We create a list by putting values inside [square brackets] and separating the values with commas:\n\nodds = [1, 3, 5, 7]\nprint('odds are:', odds)\n\nodds are: [1, 3, 5, 7]\n\n\nWe can access elements of a list using indices – numbered positions of elements in the list. These positions are numbered starting at 0, so the first element has an index of 0\n\nprint('first element:', odds[0])\nprint('last element:', odds[3])\nprint('\"-1\" element:', odds[-1])\n\nfirst element: 1\nlast element: 7\n\"-1\" element: 7\n\n\nYes, we can use negative numbers as indices in Python. When we do so, the index -1 gives us the last element in the list, -2 the second to last, and so on. Because of this, odds[3] and odds[-1] point to the same element here.\n\n\n\n\n\n\nThere is one important difference between lists and strings\n\n\n\nWe can change the values in a list, but we cannot change individual characters in a string.\n\n\nFor example:\n\nnames = ['Curie', 'Darwing', 'Turing']  # typo in Darwin's name\nprint('names is originally:', names)\nnames[1] = 'Darwin'  # correct the name\nprint('final value of names:', names)\n\nnames is originally: ['Curie', 'Darwing', 'Turing']\nfinal value of names: ['Curie', 'Darwin', 'Turing']\n\n\nworks, but:\n\nname = 'Darwin'\nname[0] = 'd'\n\nTypeError: 'str' object does not support item assignment\n\n\ndoes not.\n\n\nCh-Ch-Ch-Ch-Changes\n\n\n\nchanges.jpg\n\n\nData which can be modified in place is called mutable, while data which cannot be modified is called immutable. Strings and numbers are immutable. This does not mean that variables with string or number values are constants, but when we want to change the value of a string or number variable, we can only replace the old value with a completely new value.\nLists and arrays, on the other hand, are mutable: we can modify them after they have been created. We can change individual elements, append new elements, or reorder the whole list. For some operations, like sorting, we can choose whether to use a function that modifies the data in-place or a function that returns a modified copy and leaves the original unchanged.\n\n\n\n\n\n\nBe careful when modifying data in-place.\n\n\n\nIf two variables refer to the same list, and you modify the list value, it will change for both variables!\n\n\n\nsalsa = ['peppers', 'onions', 'cilantro', 'tomatoes']\nmy_salsa = salsa        # <-- my_salsa and salsa point to the *same* list data in memory\nsalsa[0] = 'hot peppers'\nprint('Ingredients in my salsa:', my_salsa)\n\nIngredients in my salsa: ['hot peppers', 'onions', 'cilantro', 'tomatoes']\n\n\nIf you want variables with mutable values to be independent, you must make a copy of the value when you assign it.\n\nsalsa = ['peppers', 'onions', 'cilantro', 'tomatoes']\nmy_salsa = list(salsa)        # <-- makes a *copy* of the list\nsalsa[0] = 'hot peppers'\nprint('Ingredients in my salsa:', my_salsa)\n\nIngredients in my salsa: ['peppers', 'onions', 'cilantro', 'tomatoes']\n\n\nBecause of pitfalls like this, code which modifies data in place can be more difficult to understand. However, it is often far more efficient to modify a large data structure in place than to create a modified copy for every small change. You should consider both of these aspects when writing your code.\n\n\nNested Lists\n\n\n\nnest.jpg\n\n\nSince a list can contain any Python variables, it can even contain other lists.For example, we could represent the products in the shelves of a small grocery shop:\n\nx = [['pepper', 'zucchini', 'onion'],\n     ['cabbage', 'lettuce', 'garlic'],\n     ['apple', 'pear', 'banana']]\n\nHere is a visual example of how indexing a list of lists x works:\n\n\n\nindexing_lists_python.png\n\n\nThanks to Hadley Wickham for the image above.\nUsing the previously declared list x, these would be the results of the index operations shown in the image:\n\nprint([x[0]])\n\n[['pepper', 'zucchini', 'onion']]\n\n\n\nprint(x[0])\n\n['pepper', 'zucchini', 'onion']\n\n\n\nprint(x[0][0])\n\npepper\n\n\n\n\nHeterogeneous Lists\nLists in Python can contain elements of different types. Example:\n\nsample_ages = [10, 12.5, 'Unknown']\n\nThere are many ways to change the contents of lists besides assigning new values to individual elements:\n\nodds = [1,3,5,7]\n\nodds.append(11)\nprint('odds after adding a value:', odds)\n\nodds after adding a value: [1, 3, 5, 7, 11]\n\n\n\nremoved_element = odds.pop(0)\nprint('odds after removing the first element:', odds)\nprint('removed_element:', removed_element)\n\nodds after removing the first element: [3, 5, 7, 11]\nremoved_element: 1\n\n\n\nodds.reverse()\nprint('odds after reversing:', odds)\n\nodds after reversing: [11, 7, 5, 3]\n\n\nWhile modifying in place, it is useful to remember that Python treats lists in a slightly counter-intuitive way. As we saw earlier, when we modified the salsa list item in-place, if we make a list, (attempt to) copy it and then modify this list, we can cause all sorts of trouble. This also applies to modifying the list using the above functions:\n\nodds = [3, 5, 7]\nprimes = odds\nprimes.append(2)\nprint('primes:', primes)\nprint('odds:', odds)\n\nprimes: [3, 5, 7, 2]\nodds: [3, 5, 7, 2]\n\n\nThis is because Python stores a list in memory, and then can use multiple names to refer to the same list. If all we want to do is copy a (simple) list, we can again use the list function, so we do not modify a list we did not mean to:\n\nodds = [3, 5, 7]\nprimes = list(odds)\nprimes.append(2)\nprint('primes:', primes)\nprint('odds:', odds)\n\nprimes: [3, 5, 7, 2]\nodds: [3, 5, 7]\n\n\nSubsets of lists and strings can be accessed by specifying ranges of values in brackets, similar to how we accessed ranges of positions in a NumPy array. This is commonly referred to as “slicing” the list/string.\n\nbinomial_name = 'Drosophila melanogaster'\ngroup = binomial_name[0:10]\nprint('group:', group)\n\nspecies = binomial_name[11:23]\nprint('species:', species)\n\nchromosomes = ['X', 'Y', '2', '3', '4']\nautosomes = chromosomes[2:5]\nprint('autosomes:', autosomes)\n\nlast = chromosomes[-1]\nprint('last:', last)\n\ngroup: Drosophila\nspecies: melanogaster\nautosomes: ['2', '3', '4']\nlast: 4\n\n\n\n\nSlicing from the end\nUse slicing to access only the last four characters of a string or entries of a list:\n\nstring_for_slicing = 'Observation date: 02-Feb-2013'\nlist_for_slicing = [['fluorine', 'F'],\n                    ['chlorine', 'Cl'],\n                    ['bromine', 'Br'],\n                    ['iodine', 'I'],\n                    ['astatine', 'At']]\n\n\n# string slicing\nstring_for_slicing[-4:]\n\n'2013'\n\n\n\n# list slicing\nlist_for_slicing [-4:]\n\n[['chlorine', 'Cl'], ['bromine', 'Br'], ['iodine', 'I'], ['astatine', 'At']]\n\n\n\n\nNon-Continuous Slices\nSo far we’ve seen how to use slicing to take single blocks of successive entries from a sequence. But what if we want to take a subset of entries that aren’t next to each other in the sequence? We can achieve this by providing a third argument to the range within the brackets, called the step size. The example below shows how we can take every third entry in a list:\n\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n\n# starting point:end point(not included):step size\nsubset = primes[0:12:3]\nprint('subset', subset)\n\nsubset [2, 7, 17, 29]\n\n\nNotice that the slice taken begins with the first entry in the range, followed by entries taken at equally-spaced intervals (the steps) thereafter. If you wanted to begin the subset with the third entry, you would need to specify that as the starting point of the sliced range:\n\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37]\n\n# starting point:end point(not included):step size\nsubset = primes[2:12:3]\nprint('subset', subset)\n\nsubset [5, 13, 23, 37]\n\n\nLet’s use the step size argument to create a new string that contains only every other character in the string “In an octopus’s garden in the shade”. Start with creating a variable to hold the string:\n\nbeatles = \"In an octopus's garden in the shade\"\n\nbeatles[0:35:2]\n\n'I notpssgre ntesae'\n\n\nWe can also leave out the beginning and end of the slice to take the whole string and provide only the step argument to go every second element:\n\n# leave out beginning and end - only include step argument - will include first character\nbeatles[::2]\n\n'I notpssgre ntesae'\n\n\nIf we want to take a slice from the beginning of a sequence, we can omit the first index in the range:\n\ndate = 'Monday 4 January 2016'\nday = date[0:6]\nprint('Using 0 to begin range:', day)\n\n# omit first index - by default slicing starts at beginning\nday = date[:6]\nprint('Omitting beginning index:', day)\n\nUsing 0 to begin range: Monday\nOmitting beginning index: Monday\n\n\nAnd similarly, we can omit the ending index in the range to take a slice to the very end of the sequence:\n\nmonths = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\nsond = months[8:12]\nprint('With known last position:', sond)\n\n# we can use len(months) to reference the end point of our slicing - so we don't have to manually count!\nsond = months[8:len(months)]\nprint('Using len() to get last entry:', sond)\n\n# we can omit the ending index - by default will slice to the very end\nsond = months[8:]\nprint('Omitting ending index:', sond)\n\nWith known last position: ['sep', 'oct', 'nov', 'dec']\nUsing len() to get last entry: ['sep', 'oct', 'nov', 'dec']\nOmitting ending index: ['sep', 'oct', 'nov', 'dec']\n\n\n\n\nOverloading\n\n\n\n\n\n\n+ usually means addition, but when used on strings or lists, it means “concatenate”. Given that, what do you think the multiplication operator * does on lists? In particular, what will be the output of the following code?  counts = [2, 4, 6, 8, 10]  repeats = counts * 2  print(repeats)\n\n\n\n\n\n[2, 4, 6, 8, 10, 2, 4, 6, 8, 10]\n\n\n\nThe multiplication operator * used on a list replicates elements of the list and concatenates them together. It’s equivalent to:\n\ncounts + counts\n\n[2, 4, 6, 8, 10, 2, 4, 6, 8, 10]\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\n[value1, value2, value3, …] creates a list\nlists can contain any Python object, including lists (i.e., list of lists)\nLists are indexed and sliced with square brackets (e.g., list[0] and list[2:9]), in the same way as strings and arrays\nLists are mutable (i.e., their values can be changed in place)\nStrings are immutable (i.e., the characters in them cannot be changed)"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#repeating-actions-with-loops",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#repeating-actions-with-loops",
    "title": "Programming with Python",
    "section": "5. Repeating Actions with Loops",
    "text": "5. Repeating Actions with Loops\n\n\n\nloops.png\n\n\nIn the section about visualizing data, we wrote Python code that plots values of interest from our first inflammation dataset (inflammation-01.csv), which revealed some suspicious features in it. We have a dozen data sets right now and potentially more on the way if Dr. Maverick can keep up their surprisingly fast clinical trial rate. We want to create plots for all of our data sets with a single statement. To do that, we’ll have to teach the computer how to repeat things.\nAn example task that we might want to repeat is accessing numbers in a list, which we will do by printing each number on a line of its own:\n\nodds = [1, 3, 5, 7]\n\nprint(odds[0])\nprint(odds[1])\nprint(odds[2])\nprint(odds[3])\n\n1\n3\n5\n7\n\n\nThis is a bad approach for three reasons:\n\nNot scalable. Imagine you need to print a list that has hundreds of elements. It might be easier to type them in manually.\nDifficult to maintain. If we want to decorate each printed element with an asterisk or any other character, we would have to change four lines of code. While this might not be a problem for small lists, it would definitely be a problem for longer ones.\nFragile. If we use it with a list that has more elements than what we initially envisioned, it will only display part of the list’s elements. A shorter list, on the other hand, will cause an error because it will be trying to display elements of the list that do not exist.\n\n\nodds = [1, 3, 5]\nprint(odds[0])\nprint(odds[1])\nprint(odds[2])\nprint(odds[3])\n\n1\n3\n5\n\n\nIndexError: list index out of range\n\n\nHere’s a better approach: a for loop\n\nodds = [1, 3, 5, 7]\nfor num in odds:\n    print(num)\n\n1\n3\n5\n7\n\n\nThis is shorter — certainly shorter than something that prints every number in a hundred-number list — and more robust as well:\n\nodds = [1, 3, 5, 7, 9, 11]\nfor num in odds:\n    print(num)\n\n1\n3\n5\n7\n9\n11\n\n\nThe improved version uses a for loop to repeat an operation — in this case, printing — once for each thing in a sequence. The general form of a loop is:\nfor variable in collection: # do things using variable, such as print\nUsing the odds example above, the loop might look like this:\n\n\n\nloops_image_num.png\n\n\nwhere each number (num) in the variable odds is looped through and printed one number after another. The other numbers in the diagram denote which loop cycle the number was printed in (1 being the first loop cycle, and 6 being the final loop cycle).\n\n\n\n\n\n\nTip\n\n\n\nWe can call the loop variable anything we like, but there must be a colon at the end of the line starting the loop, and we must indent anything we want to run inside the loop. Unlike many other languages, there is no command to signify the end of the loop body (e.g. end for); what is indented after the for statement belongs to the loop.\n\n\n\nWhat’s in a name?\nIn the example above, the loop variable was given the name num as a mnemonic; it is short for ‘number’. We can choose any name we want for variables. We might just as easily have chosen the name banana for the loop variable, as long as we use the same name when we invoke the variable inside the loop:\n\nodds = [1, 3, 5, 7, 9, 11]\nfor banana in odds:\n    print(banana)\n\n1\n3\n5\n7\n9\n11\n\n\nIt is a good idea to choose variable names that are meaningful, otherwise it would be more difficult to understand what the loop is doing. Here’s another loop that repeatedly updates a variable:\n\nlength = 0\n\nnames = ['Curie', 'Darwin', 'Turing']\n\nfor value in names:\n    length = length + 1\nprint('There are', length, 'names in the list.')\n\nThere are 3 names in the list.\n\n\nIt’s worth tracing the execution of this little program step by step. Since there are three names in names, the statement on line 4 will be executed three times. The first time around, length is zero (the value assigned to it on line 1) and value is Curie. The statement adds 1 to the old value of length, producing 1, and updates length to refer to that new value. The next time around, value is Darwin and length is 1, so length is updated to be 2. After one more update, length is 3; since there is nothing left in names for Python to process, the loop finishes and the print function on line 5 tells us our final answer.\nNote that a loop variable is a variable that is being used to record progress in a loop. It still exists after the loop is over, and we can re-use variables previously defined as loop variables as well:\n\nname = 'Rosalind'\n\nfor name in ['Curie', 'Darwin', 'Turing']:\n    print(name)\nprint('after the loop, name is', name)\n\nCurie\nDarwin\nTuring\nafter the loop, name is Turing\n\n\nNote also that finding the length of an object is such a common operation that Python actually has a built-in function to do it called len:\n\nprint(len([0, 1, 2, 3]))\n\n4\n\n\n\n\n\n\n\n\nTip\n\n\n\nlen is much faster than any function we could write ourselves, and much easier to read than a two-line loop; it will also give us the length of many other things that we haven’t met yet, so we should always use it when we can.\n\n\n\n\nFrom 1 to N\nPython has a built-in function called range that generates a sequence of numbers. range can accept 1, 2, or 3 parameters.\n\nif one parameter is given, range generates a sequence of that length, starting at zero and incrementing by 1. For example, range(3) produces the numbers 0, 1, 2.\nif two parameters are given, range starts at the first and ends just before the second, incrementing by one. For example, range(2, 5) produces 2, 3, 4.\nif range is given 3 parameters, it starts at the first one, ends just before the second one, and increments by the third one. For example, range(3, 10, 2) produces 3, 5, 7, 9.\n\nUsing range, write a loop that uses range to print the first 3 natural numbers:\n\nfor number in range(1, 4):\n    print(number)\n\n1\n2\n3\n\n\n\n\nUnderstanding the loops\nGiven the following loop:\n\nword = 'oxygen'\nfor char in word:\n    print(char)\n\n\n\n\n\n\n\nHow many times is the body of the loop executed?\n\n\n\n\n\n6 times\n\n\n\n\n\nComputing Powers With Loops\nExponentiation is built into Python:\n\nprint(5 ** 3)\n\n125\n\n\nHowever, we can write a loop that calculates the same result as 5 ** 3 using multiplication (and without exponentiation):\n\nresult = 1\nfor number in range(0, 3):\n    result = result * 5\nprint(result)\n\n125\n\n\n\n\nSumming a list\n\nnumbers = [124, 402, 36]\nsummed = 0\nfor num in numbers:\n    summed = summed + num\nprint(summed)\n\n562\n\n\n\n\nEnumerate\nThe built-in function enumerate takes a sequence (e.g. a list) and generates a new sequence of the same length. Each element of the new sequence is a pair composed of the index (0, 1, 2,…) and the value from the original sequence:\nfor idx, val in enumerate(a_list): # Do something using idx and val\nThe code above loops through a_list, assigning the index to idx and the value to val. Suppose we have encoded a polynomial as a list of coefficients in the following way: the first element is the constant term, the second element is the coefficient of the linear term, the third is the coefficient of the quadratic term, etc.\n\nx = 5\ncoefs = [2, 4, 3]\ny = coefs[0] * x**0 + coefs[1] * x**1 + coefs[2] * x**2\nprint(y)\n\n97\n\n\nWe can achieve the same outcome more concisely with a loop using enumerate(coefs):\n\nx = 5\ncoefs = [2, 4, 3]\n\ny=0\n\nfor idx, coef in enumerate(coefs):\n    y = y + coef * x**idx\n    \nprint(y)\n\n97\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse for variable in sequence to process the elements of a sequence one at a time\nthe body of a for loop must be indented\nuse len(thing) to determine the length of something that contains other values"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-data-from-multiple-files",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#analyzing-data-from-multiple-files",
    "title": "Programming with Python",
    "section": "6. Analyzing Data from Multiple Files",
    "text": "6. Analyzing Data from Multiple Files\nAs a final step in processing our inflammation data, we need a way to get a list of all the files in our data directory whose names start with inflammation- and end with .csv. The following library will help us to achieve this:\n\nimport glob\n\n\n\n\n\n\n\nThe glob library contains a function, also called glob, that finds files and directories whose names match a pattern. We provide those patterns as strings: the character * matches zero or more characters, while ? matches any one character. We can use this to get the names of all the CSV files in the current directory.\n\n\n\n\n\n\n\nprint(glob.glob('Data/inflammation*.csv'))\n\n['Data/inflammation-04.csv', 'Data/inflammation-11.csv', 'Data/inflammation-12.csv', 'Data/inflammation-05.csv', 'Data/inflammation-09.csv', 'Data/inflammation-06.csv', 'Data/inflammation-02.csv', 'Data/inflammation-03.csv', 'Data/inflammation-01.csv', 'Data/inflammation-07.csv', 'Data/inflammation-10.csv', 'Data/inflammation-08.csv']\n\n\nAs these examples show, glob.glob’s result is a list of file and directory paths in arbitrary order. This means we can loop over it to do something with each filename in turn. In our case, the “something” we want to do is generate a set of plots for each file in our inflammation dataset.\nIf we want to start by analyzing just the first three files in alphabetical order, we can use the sorted built-in function to generate a new sorted list from the glob.glob output:\n\nimport glob\nimport numpy\nimport matplotlib.pyplot\n\n# sort filenames in alphabetical order\nfilenames = sorted(glob.glob('Data/inflammation*.csv'))\n# select only the first three files\nfilenames = filenames[0:3]\n\nfor filename in filenames:\n    print(filename)\n\n    data = numpy.loadtxt(fname=filename, delimiter=',')\n\n    fig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\n    axes1 = fig.add_subplot(1, 3, 1)\n    axes2 = fig.add_subplot(1, 3, 2)\n    axes3 = fig.add_subplot(1, 3, 3)\n\n    axes1.set_ylabel('average')\n    axes1.plot(numpy.mean(data, axis=0))\n\n    axes2.set_ylabel('max')\n    axes2.plot(numpy.max(data, axis=0))\n\n    axes3.set_ylabel('min')\n    axes3.plot(numpy.min(data, axis=0))\n\n    fig.tight_layout()\n    matplotlib.pyplot.show()\n\nData/inflammation-01.csv\n\n\n\n\n\nData/inflammation-02.csv\n\n\n\n\n\nData/inflammation-03.csv\n\n\n\n\n\nThe plots generated for the second clinical trial file look very similar to the plots for the first file: their average plots show similar “noisy” rises and falls; their maxima plots show exactly the same linear rise and fall; and their minima plots show similar staircase structures.\nThe third dataset shows much noisier average and maxima plots that are far less suspicious than the first two datasets, however the minima plot shows that the third dataset minima is consistently zero across every day of the trial. If we produce a heat map for the third data file we see the following:\n\ndata = numpy.loadtxt(fname='Data/inflammation-03.csv', delimiter=',')\n\n# plot a heat map for third data file\nimage = matplotlib.pyplot.imshow(data)\nmatplotlib.pyplot.show()\n\n\n\n\nWe can see that there are zero values sporadically distributed across all patients and days of the clinical trial, suggesting that there were potential issues with data collection throughout the trial. In addition, we can see that the last patient in the study didn’t have any inflammation flare-ups at all throughout the trial, suggesting that they may not even suffer from arthritis!\n\nPlotting differences\nLet’s plot the difference between the average inflammations reported in the first and second datasets (stored in inflammation-01.csv and inflammation-02.csv, correspondingly), i.e., the difference between the leftmost plots of the first two figures.\n\nimport glob\nimport numpy\nimport matplotlib.pyplot\n\n# sort filenames in alphabetical order\nfilenames = sorted(glob.glob('Data/inflammation*.csv'))\n\ndata0 = numpy.loadtxt(fname=filenames[0], delimiter=',')\ndata1 = numpy.loadtxt(fname=filenames[1], delimiter=',')\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\nmatplotlib.pyplot.ylabel('Difference in average')\nmatplotlib.pyplot.plot(numpy.mean(data0, axis=0) - numpy.mean(data1, axis=0))\n\nfig.tight_layout()\nmatplotlib.pyplot.show()\n\n\n\n\n\n\nGenerate composite statistics\nWe can use each of the files once to generate a dataset containing values averaged over all patients. Then use pyplot to generate average, max, and min for all patients:\n\nfilenames = glob.glob('Data/inflammation*.csv')\n\ncomposite_data = numpy.zeros((60,40))\nfor filename in filenames:\n    # sum each new file's data into composite_data as it's read\n    # and then divide the composite_data by number of samples\n    data = numpy.loadtxt(fname = filename, delimiter=',')\n    composite_data = composite_data + data\n    \ncomposite_data = composite_data / len(filenames)\n\nfig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\naxes1 = fig.add_subplot(1, 3, 1)\naxes2 = fig.add_subplot(1, 3, 2)\naxes3 = fig.add_subplot(1, 3, 3)\n\naxes1.set_ylabel('average')\naxes1.plot(numpy.mean(composite_data, axis=0))\n\naxes2.set_ylabel('max')\naxes2.plot(numpy.max(composite_data, axis=0))\n\naxes3.set_ylabel('min')\naxes3.plot(numpy.min(composite_data, axis=0))\n\nfig.tight_layout()\n\nmatplotlib.pyplot.show()\n\n\n\n\nAfter spending some time investigating the heat map and statistical plots, as well as doing the above exercises to plot differences between datasets and to generate composite patient statistics, we gain some insight into the twelve clinical trial datasets. The datasets appear to fall into two categories:\n\nseemingly “ideal” datasets that agree excellently with Dr. Maverick’s claims, but display suspicious maxima and minima (such as inflammation-01.csv and inflammation-02.csv)\n“noisy” datasets that somewhat agree with Dr. Maverick’s claims, but show concerning data collection issues such as sporadic missing values and even an unsuitable candidate making it into the clinical trial.\n\nIn fact, it appears that all three of the “noisy” datasets (inflammation-03.csv, inflammation-08.csv, and inflammation-11.csv) are identical down to the last value. Armed with this information, we confront Dr. Maverick about the suspicious data and duplicated files.\nDr. Maverick confesses that they fabricated the clinical data after they found out that the initial trial suffered from a number of issues, including unreliable data-recording and poor participant selection. They created fake data to prove their drug worked, and when we asked for more data they tried to generate more fake datasets, as well as throwing in the original poor-quality dataset a few times to try and make all the trials seem a bit more “realistic”.\nCongratulations! We’ve investigated the inflammation data and proven that the datasets have been synthetically generated.\nBut it would be a shame to throw away the synthetic datasets that have taught us so much already, so we’ll forgive the imaginary Dr. Maverick and continue to use the data to learn how to program.\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse glob.glob(pattern) to create a list of files whose names match a pattern\nuse * in a pattern to match zero or more characters, and ? to match any single character"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#making-choices",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#making-choices",
    "title": "Programming with Python",
    "section": "7. Making Choices",
    "text": "7. Making Choices\nIn the last section, we discovered something suspicious was going on in our inflammation data by drawing some plots. How can we use Python to automatically recognize the different features we saw, and take a different action for each? In this lesson, we’ll learn how to write code that runs only when certain conditions are true:\n\nConditionals\nWe can ask Python to take different actions, depending on a condition, with an if statement:\n\nnum = 37\nif num > 100:\n    print('greater')\nelse:\n    print('not greater')\nprint('done')\n\nnot greater\ndone\n\n\nThe second line of this code uses the keyword if to tell Python that we want to make a choice. If the test that follows the if statement is true, the body of the if (i.e., the set of lines indented underneath it) is executed, and “greater” is printed. If the test is false, the body of the else is executed instead, and “not greater” is printed. Only one or the other is ever executed before continuing on with program execution to print “done”:\n\n\n\npython-flowchart-conditional.png\n\n\nConditional statements don’t have to include an else. If there isn’t one, Python simply does nothing if the test is false:\n\nnum = 53\nprint('before conditional...')\nif num > 100:\n    print(num, 'is greater than 100')\nprint('...after conditional')\n\nbefore conditional...\n...after conditional\n\n\nWe can also chain several tests together using elif, which is short for “else if”. The following Python code uses elif to print the sign of a number:\n\nnum = -3\n\nif num > 0:\n    print(num, 'is positive')\nelif num == 0:\n    print(num, 'is zero')\nelse:\n    print(num, 'is negative')\n\n-3 is negative\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that to test for equality we use a double equals sign == rather than a single equals sign = which is used to assign values.\n\n\n\n\nComparing in Python\nAlong with the > and == operators we have already used for comparing values in our conditionals, there are a few more options to know about:\n > greater than\n < less than\n == equal to\n != does not equal\n >= greater than or equal to\n <= less than or equal to\nWe can also combine tests using and and or.\nand is only true if both parts are true:\n\nif (1 > 0) and (-1 >= 0):\n    print('both parts are true')\nelse:\n    print('at least one part is false')\n\nat least one part is false\n\n\nwhile or is true if at least one part is true:\n\nif (1 < 0) or (1 >= 0):\n    print('at least one test is true')\n\nat least one test is true\n\n\n\n\n\n\n\n\nNote\n\n\n\nTrue and False are special words in Python called booleans, which represent truth values. A statement such as 1 < 0 returns the value False, while -1 < 0 returns the value True.\n\n\n\n\nChecking our Data\nNow that we’ve seen how conditionals work, we can use them to check for the suspicious features we saw in our inflammation data. We are about to use functions provided by the numpy module again. Therefore, if you’re working in a new Python session, make sure to load the module with:\n\nimport numpy\n\nFrom the first couple of plots, we saw that maximum daily inflammation exhibits a strange behavior and raises one unit a day. Wouldn’t it be a good idea to detect such behavior and report it as suspicious? Let’s do that! However, instead of checking every single day of the study, let’s merely check if maximum inflammation in the beginning (day 0) and in the middle (day 20) of the study are equal to the corresponding day numbers:\nmax_inflammation_0 = numpy.max(data, axis=0)[0]\nmax_inflammation_20 = numpy.max(data, axis=0)[20]\n\nif max_inflammation_0 == 0 and max_inflammation_20 == 20:\n    print('Suspicious looking maxima!')   \nWe also saw a different problem in the third dataset; the minima per day were all zero (looks like a healthy person snuck into our study). We can also check for this with an elif condition:\nelif numpy.sum(numpy.min(data, axis=0)) == 0:\n    print('Minima add up to zero!')   \nAnd if neither of these conditions are true, we can use else to give the all-clear:\nelse:\n    print('Seems OK!')\nLet’s put that all together and test that out:\n\n# Study number 1\ndata = numpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\n\nmax_inflammation_0 = numpy.max(data, axis=0)[0]\nmax_inflammation_20 = numpy.max(data, axis=0)[20]\n\nif max_inflammation_0 == 0 and max_inflammation_20 == 20:\n    print('Suspicious looking maxima!')\nelif numpy.sum(numpy.min(data, axis=0)) == 0:\n    print('Minima add up to zero!')\nelse:\n    print('Seems OK!')\n\nSuspicious looking maxima!\n\n\n\n# Study number 3\ndata = numpy.loadtxt(fname='Data/inflammation-03.csv', delimiter=',')\n\nmax_inflammation_0 = numpy.max(data, axis=0)[0]\nmax_inflammation_20 = numpy.max(data, axis=0)[20]\n\nif max_inflammation_0 == 0 and max_inflammation_20 == 20:\n    print('Suspicious looking maxima!')\nelif numpy.sum(numpy.min(data, axis=0)) == 0:\n    print('Minima add up to zero!')\nelse:\n    print('Seems OK!')\n\nMinima add up to zero!\n\n\nIn this way, we have asked Python to do something different depending on the condition of our data. Here we printed messages in all cases, but we could also imagine not using the else catch-all so that messages are only printed when something is wrong, freeing us from having to manually examine every plot for features we’ve seen before.\n\n\nHow Many Paths?\nConsider this code:\n\nif 4 > 5:\n    print('A')\nelif 4 == 5:\n    print('B')\nelif 4 < 5:\n    print('C')\n\nC\n\n\nC gets printed because the first two conditions, 4 > 5 and 4 == 5, are not true, but 4 < 5 is true.\n\n\nWhat Is Truth?\nTrue and False booleans are not the only values in Python that are true and false. In fact, any value can be used in an if or elif. Consider the code below:\n\nif '':\n    print('empty string is true')\nif 'word':\n    print('word is true')\nif []:\n    print('empty list is true')\nif [1, 2, 3]:\n    print('non-empty list is true')\nif 0:\n    print('zero is true')\nif 1:\n    print('one is true')\n\nword is true\nnon-empty list is true\none is true\n\n\n\n\nThat’s Not Not What I Meant\nSometimes it is useful to check whether some condition is not true. The Boolean operator not can do this explicitly. After reading and running the code below, write some if statements that use not to test the rule that you formulated in the previous challenge.\n\nif not '':\n    print('empty string is not true')\nif not 'word':\n    print('word is not true')\nif not not True:\n    print('not not True is true')\n\nempty string is not true\nnot not True is true\n\n\n\n\nClose enough\nWrite some conditions that print True if the variable a is within 10% of the variable b and False otherwise:\n\na = 5\nb = 5.1\n\nif abs(a - b) <= 0.1 * abs(b):\n    print('True')\nelse:\n    print('False')\n\nTrue\n\n\n\nprint(abs(a - b) <= 0.1 * abs(b))\n\nTrue\n\n\nThis works because the Booleans True and False have string representations which can be printed.\n\n\nIn-Place Operators\nPython (and most other languages in the C family) provides in-place operators that work like this:\n\n# original value\nx = 1  \n\n# add one to x, assigning result back to x\nx += 1 \n\n# multiply x by 3\nx *= 3 \n\nprint(x)\n\n6\n\n\nWrite some code that sums the positive and negative numbers in a list separately, using in-place operators. Do you think the result is more or less readable than writing the same without in-place operators?\n\npositive_sum = 0\nnegative_sum = 0\ntest_list = [3, 4, 6, 1, -1, -5, 0, 7, -8]\n\nfor num in test_list:\n    if num > 0:\n        positive_sum += num\n    elif num == 0:\n        pass\n    else:\n        negative_sum += num\n        \nprint(positive_sum, negative_sum)\n\n\n\n\n\n\n\nImportant\n\n\n\npass means “don’t do anything”.\n\n\nIn this particular case, it’s not actually needed, since if num == 0 neither sum needs to change, but it illustrates the use of elif and pass.\n\n\nSorting a list into buckets\nIn our data folder, large data sets are stored in files whose names start with “inflammation-“ and small data sets – in files whose names start with “small-“. We also have some other files that we do not care about at this point. We’d like to break all these files into three lists called large_files, small_files, and other_files, respectively.\nAdd code to the template below to do this:\nfilenames = ['inflammation-01.csv',\n     'myscript.py',\n     'inflammation-02.csv',\n     'small-01.csv',\n     'small-02.csv']\nlarge_files = []\nsmall_files = []\nother_files = []\nNote that the string method startswith returns True if and only if the string it is called on starts with the string passed as an argument, that is:\n\n'String'.startswith('Str')\n\nTrue\n\n\nBut:\n\n'String'.startswith('str')\n\nFalse\n\n\n\nfilenames = ['inflammation-01.csv',\n         'myscript.py',\n         'inflammation-02.csv',\n         'small-01.csv',\n         'small-02.csv']\n\nlarge_files = []\nsmall_files = []\nother_files = []\n\nfor filename in filenames:\n    if filename.startswith('inflammation-'):\n        large_files.append(filename)\n    elif filename.startswith('small-'):\n        small_files.append(filename)\n    else:\n        other_files.append(filename)\n\nprint('large_files:', large_files)\nprint('small_files:', small_files)\nprint('other_files:', other_files)\n\nlarge_files: ['inflammation-01.csv', 'inflammation-02.csv']\nsmall_files: ['small-01.csv', 'small-02.csv']\nother_files: ['myscript.py']\n\n\n\n\nCounting vowels\nLet’s write a loop that counts the number of vowels in a character string, and test it on a few individual words and full sentences:\n\nvowels = 'aeiouAEIOU'\nsentence1 = 'Mary had a little lamb.'\ncount = 0\nfor char in sentence1:\n    if char in vowels:\n        count += 1\n\nprint('The number of vowels in this string is ' + str(count))\n\nThe number of vowels in this string is 6\n\n\n\nvowels = 'aeiouAEIOU'\nsentence2 = 'Is this a question?'\ncount = 0\nfor char in sentence2:\n    if char in vowels:\n        count += 1\n\nprint('The number of vowels in this string is ' + str(count))\n\nThe number of vowels in this string is 7\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nuse if condition to start a conditional statement, elif condition to provide additional tests, and else to provide a default\nthe bodies of the branches of conditional statements must be indented\nuse == to test for equality\nX and Y is only true if both X and Y are true\nX or Y is true if either X or Y, or both, are true\nzero, the empty string, and the empty list are considered false; all other numbers, strings, and lists are considered true\nTrue and False represent truth values"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#creating-functions",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#creating-functions",
    "title": "Programming with Python",
    "section": "8. Creating Functions",
    "text": "8. Creating Functions\nAt this point, we’ve written code to draw some interesting features in our inflammation data, loop over all our data files to quickly draw these plots for each of them, and have Python make decisions based on what it sees in our data. But, our code is getting pretty long and complicated.\nWhat if we had thousands of datasets, and didn’t want to generate a figure for every single one?\nCommenting out the figure-drawing code is a nuisance. Also, what if we want to use that code again, on a different dataset or at a different point in our program? Cutting and pasting it is going to make our code get very long and very repetitive, very quickly. We’d like a way to package our code so that it is easier to reuse, and Python provides for this by letting us define things called ‘functions’ — a shorthand way of re-executing longer pieces of code. Let’s start by defining a function fahr_to_celsius that converts temperatures from Fahrenheit to Celsius:\n\ndef fahr_to_celsius(temp):\n    return ((temp - 32) * (5/9))\n\n\nThe function definition opens with the keyword def followed by the name of the function (fahr_to_celsius) and a parenthesized list of parameter names (temp). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value.\nWhen we call the function, the values we pass to it are assigned to those variables so that we can use them inside the function. Inside the function, we use a return statement to send a result back to whoever asked for it. Let’s try running our function. This command should call our function, using “32” as the input and return the function value:\n\nfahr_to_celsius(32)\n\n0.0\n\n\nIn fact, calling our own function is no different from calling any other function:\n\nprint('freezing point of water:', fahr_to_celsius(32), 'C')\nprint('boiling point of water:', fahr_to_celsius(212), 'C')\n\nfreezing point of water: 0.0 C\nboiling point of water: 100.0 C\n\n\nWe’ve successfully called the function that we defined, and we have access to the value that we returned.\n\nComposing Functions\nNow that we’ve seen how to turn Fahrenheit into Celsius, we can also write the function to turn Celsius into Kelvin:\n\ndef celsius_to_kelvin(temp_c):\n    return temp_c + 273.15\n\nprint('freezing point of water in Kelvin:', celsius_to_kelvin(0.))\n\nfreezing point of water in Kelvin: 273.15\n\n\nWhat about converting Fahrenheit to Kelvin? We could write out the formula, but we don’t need to. Instead, we can compose the two functions we have already created:\n\ndef fahr_to_kelvin(temp_f):\n    temp_c = fahr_to_celsius(temp_f)\n    temp_k = celsius_to_kelvin(temp_c)\n    return temp_k\n\nprint('boiling point of water in Kelvin:', fahr_to_kelvin(212.0))\n\nboiling point of water in Kelvin: 373.15\n\n\nThis is our first taste of how larger programs are built:\n\nwe define basic operations, then\ncombine them in ever-larger chunks to get the effect we want.\n\nReal-life functions will usually be larger than the ones shown here — typically half a dozen to a few dozen lines — but they shouldn’t ever be much longer than that, or the next person who reads it won’t be able to understand what’s going on.\n\n\nVariable Scope\nIn composing our temperature conversion functions, we created variables inside of those functions, temp, temp_c, temp_f, and temp_k. We refer to these variables as local variables because they no longer exist once the function is done executing. If we try to access their values outside of the function, we will encounter an error:\n\nprint('Again, temperature in Kelvin was:', temp_k)\n\nNameError: name 'temp_k' is not defined\n\n\nIf we want to reuse the temperature in Kelvin after we have calculated it with fahr_to_kelvin, we can store the result of the function call in a variable:\n\ntemp_kelvin = fahr_to_kelvin(212.0)\nprint('temperature in Kelvin was:', temp_kelvin)\n\ntemperature in Kelvin was: 373.15\n\n\nThe variable temp_kelvin, being defined outside any function, is said to be global. Inside a function, we can read the value of such global variables:\n\ndef print_temperatures():\n  print('temperature in Fahrenheit was:', temp_fahr)\n  print('temperature in Kelvin was:', temp_kelvin)\n\ntemp_fahr = 212.0\ntemp_kelvin = fahr_to_kelvin(temp_fahr)\n\nprint_temperatures()\n\ntemperature in Fahrenheit was: 212.0\ntemperature in Kelvin was: 373.15\n\n\n\n\nTidying up\nNow that we know how to wrap bits of code up in functions, we can make our inflammation analysis easier to read and easier to reuse. First, let’s make a visualize function that generates our plots:\n\ndef visualize(filename):\n\n    data = numpy.loadtxt(fname=filename, delimiter=',')\n\n    fig = matplotlib.pyplot.figure(figsize=(10.0, 3.0))\n\n    axes1 = fig.add_subplot(1, 3, 1)\n    axes2 = fig.add_subplot(1, 3, 2)\n    axes3 = fig.add_subplot(1, 3, 3)\n\n    axes1.set_ylabel('average')\n    axes1.plot(numpy.mean(data, axis=0))\n\n    axes2.set_ylabel('max')\n    axes2.plot(numpy.max(data, axis=0))\n\n    axes3.set_ylabel('min')\n    axes3.plot(numpy.min(data, axis=0))\n\n    fig.tight_layout()\n    matplotlib.pyplot.show()\n\nand another function called detect_problems that checks for those systematics we noticed:\n\ndef detect_problems(filename):\n\n    data = numpy.loadtxt(fname=filename, delimiter=',')\n\n    if numpy.max(data, axis=0)[0] == 0 and numpy.max(data, axis=0)[20] == 20:\n        print('Suspicious looking maxima!')\n    elif numpy.sum(numpy.min(data, axis=0)) == 0:\n        print('Minima add up to zero!')\n    else:\n        print('Seems OK!')\n\nWait! Didn’t we forget to specify what both of these functions should return?\nWell, we didn’t. In Python, functions are not required to include a return statement and can be used for the sole purpose of grouping together pieces of code that conceptually do one thing. In such cases, function names usually describe what they do, e.g. visualize, detect_problems.\nNotice that rather than jumbling this code together in one giant for loop, we can now read and reuse both ideas separately. We can reproduce the previous analysis with a much simpler for loop:\n\nfilenames = sorted(glob.glob('inflammation*.csv'))\n\nfor filename in filenames[:3]:\n    print(filename)\n    visualize(filename)\n    detect_problems(filename)\n\nBy giving our functions human-readable names, we can more easily read and understand what is happening in the for loop. Even better, if at some later date we want to use either of those pieces of code again, we can do so in a single line.\n\n\nTesting and Documenting\nOnce we start putting things in functions so that we can re-use them, we need to start testing that those functions are working correctly. To see how to do this, let’s write a function to offset a dataset so that it’s mean value shifts to a user-defined value:\n\ndef offset_mean(data, target_mean_value):\n    return (data - numpy.mean(data)) + target_mean_value\n\nWe could test this on our actual data, but since we don’t know what the values ought to be, it will be hard to tell if the result was correct. Instead, let’s use NumPy to create a matrix of 0’s and then offset its values to have a mean value of 3:\n\nz = numpy.zeros((2,2))\nprint(offset_mean(z, 3))\n\n[[3. 3.]\n [3. 3.]]\n\n\nThat looks right, so let’s try offset_mean on our real data:\n\ndata = numpy.loadtxt(fname='Data/inflammation-01.csv', delimiter=',')\nprint(offset_mean(data, 0))\n\n[[-6.14875 -6.14875 -5.14875 ... -3.14875 -6.14875 -6.14875]\n [-6.14875 -5.14875 -4.14875 ... -5.14875 -6.14875 -5.14875]\n [-6.14875 -5.14875 -5.14875 ... -4.14875 -5.14875 -5.14875]\n ...\n [-6.14875 -5.14875 -5.14875 ... -5.14875 -5.14875 -5.14875]\n [-6.14875 -6.14875 -6.14875 ... -6.14875 -4.14875 -6.14875]\n [-6.14875 -6.14875 -5.14875 ... -5.14875 -5.14875 -6.14875]]\n\n\nIt’s hard to tell from the default output whether the result is correct, but there are a few tests that we can run to reassure us:\n\nprint('original min, mean, and max are:', numpy.min(data), numpy.mean(data), numpy.max(data))\noffset_data = offset_mean(data, 0)\nprint('min, mean, and max of offset data are:',\n      numpy.min(offset_data),\n      numpy.mean(offset_data),\n      numpy.max(offset_data))\n\noriginal min, mean, and max are: 0.0 6.14875 20.0\nmin, mean, and max of offset data are: -6.14875 2.842170943040401e-16 13.85125\n\n\nThat seems almost right: the original mean was about 6.1, so the lower bound from zero is now about -6.1. The mean of the offset data isn’t quite zero — we’ll explore why not later — but it’s pretty close. We can even go further and check that the standard deviation hasn’t changed:\n\nprint('std dev before and after:', numpy.std(data), numpy.std(offset_data))\n\nstd dev before and after: 4.613833197118566 4.613833197118566\n\n\nThose values look the same, but we probably wouldn’t notice if they were different in the sixth decimal place. Let’s do this instead:\n\nprint('difference in standard deviations before and after:',\n      numpy.std(data) - numpy.std(offset_data))\n\ndifference in standard deviations before and after: 0.0\n\n\nAgain, the difference is very small. It’s still possible that our function is wrong, but it seems unlikely enough that we should probably get back to doing our analysis. We have one more task first, though: we should write some documentation for our function to remind ourselves later what it’s for and how to use it.\nThe usual way to put documentation in software is to add comments like this:\n\n# offset_mean(data, target_mean_value):\n# return a new array containing the original data with its mean offset to match the desired value.\ndef offset_mean(data, target_mean_value):\n    return (data - numpy.mean(data)) + target_mean_value\n\nThere’s a better way, though. If the first thing in a function is a string that isn’t assigned to a variable, that string is attached to the function as its documentation:\n\ndef offset_mean(data, target_mean_value):\n    \"\"\"Return a new array containing the original data\n       with its mean offset to match the desired value.\"\"\"\n    return (data - numpy.mean(data)) + target_mean_value\n\nThis is better because we can now ask Python’s built-in help system to show us the documentation for the function:\n\nhelp(offset_mean)\n\nHelp on function offset_mean in module __main__:\n\noffset_mean(data, target_mean_value)\n    Return a new array containing the original data\n    with its mean offset to match the desired value.\n\n\n\nA string like this is called a docstring. We don’t need to use triple quotes when we write one, but if we do, we can break the string across multiple lines:\n\ndef offset_mean(data, target_mean_value):\n    \"\"\"Return a new array containing the original data\n       with its mean offset to match the desired value.\n\n    Examples\n    --------\n    >>> offset_mean([1, 2, 3], 0)\n    array([-1.,  0.,  1.])\n    \"\"\"\n    return (data - numpy.mean(data)) + target_mean_value\n\nhelp(offset_mean)\n\nHelp on function offset_mean in module __main__:\n\noffset_mean(data, target_mean_value)\n    Return a new array containing the original data\n       with its mean offset to match the desired value.\n    \n    Examples\n    --------\n    >>> offset_mean([1, 2, 3], 0)\n    array([-1.,  0.,  1.])\n\n\n\nWe have passed parameters to functions in two ways:\n\ndirectly, as in type(data), and\nby name, as in numpy.loadtxt(fname=‘something.csv’, delimiter=‘,’).\n\nIn fact, we can pass the filename to loadtxt without the fname=:\n\nnumpy.loadtxt('Data/inflammation-01.csv', delimiter=',')\n\narray([[0., 0., 1., ..., 3., 0., 0.],\n       [0., 1., 2., ..., 1., 0., 1.],\n       [0., 1., 1., ..., 2., 1., 1.],\n       ...,\n       [0., 1., 1., ..., 1., 1., 1.],\n       [0., 0., 0., ..., 0., 2., 0.],\n       [0., 0., 1., ..., 1., 1., 0.]])\n\n\nbut we still need to say delimiter=:\n\nnumpy.loadtxt('Data/inflammation-01.csv', ',')\n\nSyntaxError: invalid syntax (<unknown>, line 1)\n\n\nTo understand what’s going on, and make our own functions easier to use, let’s re-define our offset_mean function like this:\n\ndef offset_mean(data, target_mean_value=0.0):\n    \"\"\"Return a new array containing the original data\n       with its mean offset to match the desired value, (0 by default).\n\n    Examples\n    --------\n    >>> offset_mean([1, 2, 3])\n    array([-1.,  0.,  1.])\n    \"\"\"\n    return (data - numpy.mean(data)) + target_mean_value\n\nThe key change is that the second parameter is now written target_mean_value=0.0 instead of just target_mean_value. If we call the function with two arguments, it works as it did before:\n\ntest_data = numpy.zeros((2, 2))\nprint(offset_mean(test_data, 3))\n\n[[3. 3.]\n [3. 3.]]\n\n\nBut we can also now call it with just one parameter, in which case target_mean_value is automatically assigned the default value of 0.0:\n\nmore_data = 5 + numpy.zeros((2, 2))\nprint('data before mean offset:')\nprint(more_data)\nprint('offset data:')\nprint(offset_mean(more_data))\n\ndata before mean offset:\n[[5. 5.]\n [5. 5.]]\noffset data:\n[[0. 0.]\n [0. 0.]]\n\n\nThis is handy.\n\n\n\n\n\n\nIf we usually want a function to work one way, but occasionally need it to do something else, we can allow people to pass a parameter when they need to but provide a default to make the normal case easier.\n\n\n\nThe example below shows how Python matches values to parameters:\n\n\n\ndef display(a=1, b=2, c=3):\n    print('a:', a, 'b:', b, 'c:', c)\n\n# display pre-defibed initial values af a, b, and c\nprint('no parameters:')\ndisplay()\n\n# replace the initial value of a, whilst retaining initial values for b and c\nprint('one parameter:')\ndisplay(55)\n\n# replace intial values of a and b, whilst retaining initial value for c\nprint('two parameters:')\ndisplay(55, 66)\n\nno parameters:\na: 1 b: 2 c: 3\none parameter:\na: 55 b: 2 c: 3\ntwo parameters:\na: 55 b: 66 c: 3\n\n\nAs this example shows, parameters are matched up from left to right, and any that haven’t been given a value explicitly get their default value. We can override this behaviour by naming the value as we pass it in:\n\nprint('only setting the value of c')\ndisplay(c=77)\n\nonly setting the value of c\na: 1 b: 2 c: 77\n\n\nWith that in hand, let’s look at the help for numpy.loadtxt:\nhelp(numpy.loadtxt)\nThere’s a lot of information here, but the most important part is the first couple of lines:\nloadtxt(fname, dtype=<class 'float'>, comments='#', delimiter=None, converters=None, skiprows=0, use\ncols=None, unpack=False, ndmin=0, encoding='bytes')\nThis tells us that loadtxt has one parameter called fname that doesn’t have a default value, and eight others that do. If we call the function like this:\nnumpy.loadtxt('inflammation-01.csv', ',')\nthen the filename is assigned to fname (which is what we want), but the delimiter string ‘,’ is assigned to dtype rather than delimiter, because dtype is the second parameter in the list. However ‘,’ isn’t a known dtype so our code produced an error message when we tried to run it.\nWhen we call loadtxt we don’t have to provide fname= for the filename because it’s the first item in the list, but if we want the ‘,’ to be assigned to the variable delimiter, we do have to provide delimiter= for the second parameter since delimiter is not the second parameter in the list.\n\n\nReadable functions\nConsider these two functions:\n\n# Function 1 - calculate sample standard deviation\ndef s(p):\n    a = 0\n    for v in p:\n        a += v\n    m = a / len(p)\n    d = 0\n    for v in p:\n        d += (v - m) * (v - m)\n    return numpy.sqrt(d / (len(p) - 1))\n\n\n# Function 2 - calculate sample standard deviation\ndef std_dev(sample):\n    sample_sum = 0\n    for value in sample:\n        sample_sum += value\n\n    sample_mean = sample_sum / len(sample)\n\n    sum_squared_devs = 0\n    for value in sample:\n        sum_squared_devs += (value - sample_mean) * (value - sample_mean)\n\n    return numpy.sqrt(sum_squared_devs / (len(sample) - 1))\n\nFunctions 1 and 2 are computationally equivalent (they both calculate the sample standard deviation), but to a human reader, they look very different. You probably found Function 2 much easier to read and understand than Function 1.\nAs this example illustrates, both documentation and a programmer’s coding style combine to determine how easy it is for others to read and understand the programmer’s code. Choosing meaningful variable names and using blank spaces to break the code into logical “chunks” are helpful techniques for producing readable code. This is useful not only for sharing code with others, but also for the original programmer.\nIf you need to revisit code that you wrote months ago and haven’t thought about since then, you will appreciate the value of readable code!\n\n\nCombining Strings\n“Adding” two strings produces their concatenation:\n'a' + 'b' is 'ab'. \nLet’s write a function called fence that takes two parameters: original and wrapper and returns a new string that has the wrapper character at the beginning and end of the original:\n\ndef fence(original, wrapper):\n    return wrapper + original + wrapper\n\n\nfence(' in ', 'all')\n\n'all in all'\n\n\n\n\nReturn versus print\nNote that return and print are not interchangeable.\n\nprint is a Python function that prints data to the screen. It enables us, users, to see the data\nreturn statement, on the other hand, makes data visible to the program\n\nLet’s have a look at the following function:\n\ndef add(a, b):\n    print(a + b)\n    \nA = add(7, 3)\nprint(A)\n\n10\nNone\n\n\nPython first executes the function add with a = 7 and b = 3, and, therefore, prints 10. However, because function add does not have a line that starts with return (no return “statement”), it will, *by default, return nothing which, in Python world, is called None. Therefore, A is therefore assigned to None and the last line (print(A)) printed is None.\n\n\nSelecting characters from strings\nIf the variable s refers to a string, then s[0] is the string’s first character and s[-1] is its last. Let’s write a function called outer that returns a string made up of just the first and last characters of its input:\n\ndef outer(input):\n    return input[0] + input[-1:]\n\n\nouter('helium')\n\n'hm'\n\n\n\n\nRescaling an Array\nLet’s now write a function rescale that takes an array as input and returns a corresponding array of values scaled (normalized) to lie in the range 0.0 to 1.0.\nHint: If L and H are the lowest and highest values in the original array, then the replacement for a value v should be (v-L) / (H-L)\n\ndef rescale(input_array):\n    L = numpy.min(input_array)\n    H = numpy.max(input_array)\n    output_array = (input_array - L) / (H - L)\n    return output_array\n\n\ninput_array = [12,15,8,23,9,6,13,7]\n\n\nrescale(input_array)\n\narray([0.35294118, 0.52941176, 0.11764706, 1.        , 0.17647059,\n       0.        , 0.41176471, 0.05882353])\n\n\nNote that the largest value in the array, 23, has been rescaled to one, the smallest value 6, has been rescaled to zero, and all other values scaled proportionately.\n\n\nTesting and documenting our function\nFirst, let’s run the commands help(numpy.arange) and help(numpy.linspace) to see how to use these functions to generate regularly-spaced values.\nThen using these values we can test our rescale function, and add a docstring that explains what it does.\n\nrescale(numpy.arange(10.0))\n\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])\n\n\n\nrescale(numpy.linspace(0,100,5))\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\n“““Takes an array as input, and returns a corresponding array scaled so that 0 corresponds to the minimum and 1 to the maximum value of the input array.\nExamples: >>> rescale(numpy.arange(10.0)) array([ 0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ]) >>> rescale(numpy.linspace(0, 100, 5)) array([ 0. , 0.25, 0.5 , 0.75, 1. ]) ““”\n\n\nDefining defaults\nLet’s now rewrite the rescale function so that it scales data to lie between 0.0 and 1.0 by default, whilst also allowing the caller to specify lower and upper bounds if they want.\n\n# lower and upper bounds specified within function argument, for amendment if required)\ndef rescale(input_array, low_val=0.0, high_val=1.0):\n    \n    \"\"\"rescales input array values to lie between low_val and high_val\"\"\"\n    L = numpy.min(input_array)\n    H = numpy.max(input_array)\n    intermed_array = (input_array - L) / (H - L)\n    output_array = intermed_array * (high_val - low_val) + low_val\n    return output_array\n\n\n\nVariables inside and outside functions\n\nf = 0\nk = 0\n\ndef f2k(f):\n    k = ((f - 32) * (5.0 / 9.0)) + 273.15\n    return k\n\nprint(f2k(8))\nprint(f2k(41))\nprint(f2k(32))\n\nprint(k)\n\n259.81666666666666\n278.15\n273.15\n0\n\n\nk is 0 because the k inside the function f2k doesn’t know about the k defined outside the function. When the f2k function is called, it creates a local variable k. The function does not return any values and does not alter k outside of its local copy. Therefore the original value of k remains unchanged. Beware that a local k is created because f2k internal statements affect a new value to it. If k was only read, it would simply retrieve the global k value.\n\n\nMixing default and non-default parameters\n\n# second and fourth arguments are given default values\n# one and three MUST be included as arguments when function called, and must be BEFORE any default values\ndef numbers(one, two=2, three, four=4):\n    n = str(one) + str(two) + str(three) + str(four)\n    return n\n\nprint(numbers(1, three=3))\n\nSyntaxError: non-default argument follows default argument (4284410807.py, line 2)\n\n\nAttempting to define the numbers function results in a SyntaxError. The defined parameters two and four are given default values. Because one and three are not given default values, they are required to be included as arguments when the function is called and must be placed before any parameters that have default values in the function definition.\n\n# default values for b and c will be used if no value passed when function called\ndef func(a, b=3, c=6):\n    print('a: ', a, 'b: ', b, 'c:', c)\n\nfunc(-1, 2)\n\na:  -1 b:  2 c: 6\n\n\nThe given call to func displays a: -1 b: 2 c: 6. -1 is assigned to the first parameter a, 2 is assigned to the next parameter b, and c is not passed a value, so it uses its default value 6.\n\n\n\n\n\n\nKey Points\n\n\n\n\ndefine a function using def function_name(parameter)\nthe body of a function must be indented\ncall a function using function_name(value)\nnumbers are stored as integers or floating-point numbers\nvariables defined within a function can only be seen and used within the body of the function\nvariables created outside of any function are called global variables\nwithin a function, we can access global variables\nvariables created within a function override global variables if their names match\nuse help(thing) to view help for something\nput docstrings in functions to provide help for that function\nspecify default values for parameters when defining a function using name=value in the parameter list\nparameters can be passed by matching based on name, by position, or by omitting them (in which case the default value is used)\nput code whose parameters change frequently in a function, then call it with different parameter values to customize its behavior"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#errors-and-exceptions",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#errors-and-exceptions",
    "title": "Programming with Python",
    "section": "9. Errors and Exceptions",
    "text": "9. Errors and Exceptions\nEvery programmer encounters errors, both those who are just beginning, and those who have been programming for years. Encountering errors and exceptions can be very frustrating at times, and can make coding feel like a hopeless endeavour. However, understanding what the different types of errors are and when you are likely to encounter them can help a lot. Once you know why you get certain types of errors, they become much easier to fix.\nErrors in Python have a very specific form, called a traceback. Let’s examine one:\n\n# This code has an intentional error. You can type it directly or\n# use it for reference to understand the error message below.\ndef favorite_ice_cream():\n    ice_creams = [\n        'chocolate',\n        'vanilla',\n        'strawberry'\n    ]\n    print(ice_creams[3])\n\nfavorite_ice_cream()\n\nIndexError: list index out of range\n\n\nThis particular traceback has two levels. You can determine the number of levels by looking for the number of arrows on the left hand side. In this case:\n\nThe first shows code from the cell above, with an arrow pointing to Line 11 (which is favorite_ice_cream()).\nThe second shows some code in the function favorite_ice_cream, with an arrow pointing to Line 9 (which is print(ice_creams[3])).\n\nThe last level is the actual place where the error occurred. The other level(s) show what function the program executed to get to the next level down. So, in this case, the program first performed a function call to the function favorite_ice_cream. Inside this function, the program encountered an error on Line 6, when it tried to run the code print(ice_creams[3]).\n\nLong Tracebacks\nSometimes, you might see a traceback that is very long – sometimes they might even be 20 levels deep! This can make it seem like something horrible happened, but the length of the error message does not reflect severity, rather, it indicates that your program called many functions before it encountered the error. Most of the time, the actual place where the error occurred is at the bottom-most level, so you can skip down the traceback to the bottom.\nSo what error did the program actually encounter? In the last line of the traceback, Python helpfully tells us the category or type of error (in this case, it is an IndexError) and a more detailed error message (in this case, it says “list index out of range”). If you encounter an error and don’t know what it means, it is still important to read the traceback closely. That way, if you fix the error, but encounter a new one, you can tell that the error changed. Additionally, sometimes knowing where the error occurred is enough to fix it, even if you don’t entirely understand the message.\nIf you do encounter an error you don’t recognize, try looking at the official documentation on errors. However, note that you may not always be able to find the error there, as it is possible to create custom errors. In that case, hopefully the custom error message is informative enough to help you figure out what went wrong.\n\n\nSyntax Errors\nWhen you forget a colon at the end of a line, accidentally add one space too many when indenting under an if statement, or forget a parenthesis, you will encounter a syntax error. This means that Python couldn’t figure out how to read your program. This is similar to forgetting punctuation in English: for example, this text is difficult to read there is no punctuation there is also no capitalization why is this hard because you have to figure out where each sentence ends you also have to figure out where each sentence begins to some extent it might be ambiguous if there should be a sentence break or not\nPeople can typically figure out what is meant by text with no punctuation, but people are much smarter than computers. If Python doesn’t know how to read the program, it will give up and inform you with an error. For example:\n\ndef some_function()\n    msg = 'hello, world!'\n    print(msg)\n     return msg\n\nSyntaxError: expected ':' (1947819108.py, line 1)\n\n\nHere, Python tells us that there is a SyntaxError on line 1, and even puts a little arrow in the place where there is an issue. In this case the problem is that the function definition is missing a colon at the end.\nActually, the function above has two issues with syntax. If we fix the problem with the colon, we see that there is also an IndentationError, which means that the lines in the function definition do not all have the same indentation:\n\ndef some_function():\n    msg = 'hello, world!'\n    print(msg)\n     return msg\n\nIndentationError: unexpected indent (329577338.py, line 4)\n\n\nBoth SyntaxError and IndentationError indicate a problem with the syntax of your program, but an IndentationError is more specific: it always means that there is a problem with how your code is indented.\n\n\nTabs and Spaces\nSome indentation errors are harder to spot than others. In particular, mixing spaces and tabs can be difficult to spot because they are both whitespace. In the example below, the first two lines in the body of the function some_function are indented with tabs, while the third line — with spaces. If you’re working in a Jupyter notebook, be sure to copy and paste this example rather than trying to type it in manually because Jupyter automatically replaces tabs with spaces.\n\ndef some_function():\n    msg = 'hello, world!'\n    print(msg)\n        return msg\n\nTabError: inconsistent use of tabs and spaces in indentation (834453434.py, line 4)\n\n\nVisually it is impossible to spot the error. Fortunately, Python does not allow you to mix tabs and spaces.\n\n\nVariable Name Errors\nAnother very common type of error is called a NameError, and occurs when you try to use a variable that does not exist. For example:\n\nprint(i)\n\nNameError: name 'i' is not defined\n\n\nVariable name errors come with some of the most informative error messages, which are usually of the form “name ‘the_variable_name’ is not defined”.\nWhy does this error message occur? That’s a harder question to answer, because it depends on what your code is supposed to do. However, there are a few very common reasons why you might have an undefined variable. The first is that you meant to use a string, but forgot to put quotes around it:\n\nprint(hello)\n\nNameError: name 'hello' is not defined\n\n\nThe second reason is that you might be trying to use a variable that does not yet exist. In the following example, counter should have been defined (e.g., with counter = 0) before the for loop:\n\nfor number in range(10):\n    counter = counter + number\nprint('The count is:', counter)\n\nNameError: name 'counter' is not defined\n\n\nFinally, the third possibility is that you made a typo when you were writing your code. Let’s say we fixed the error above by adding the line Count = 0 before the for loop. Frustratingly, this actually does not fix the error. Remember that variables are **case-sensitive, so the variable counter is different from Counter. We still get the same error, because we still have not defined counter:\n\nCounter = 0\nfor number in range(10):\n    counter = counter + number\nprint('The count is:', counter)\n\nNameError: name 'counter' is not defined\n\n\n\n\nIndex Errors\nNext up are errors having to do with containers (like lists and strings) and the items within them. If you try to access an item in a list or a string that does not exist, then you will get an error. This makes sense: if you asked someone what day they would like to get coffee, and they answered “caturday”, you might be a bit annoyed. Python gets similarly annoyed if you try to ask it for an item that doesn’t exist:\n\nletters = ['a', 'b', 'c']\nprint('Letter #1 is', letters[0])\nprint('Letter #2 is', letters[1])\nprint('Letter #3 is', letters[2])\nprint('Letter #4 is', letters[3])\n\nLetter #1 is a\nLetter #2 is b\nLetter #3 is c\n\n\nIndexError: list index out of range\n\n\nHere, Python is telling us that there is an IndexError in our code, meaning we tried to access a list index that did not exist.\n\n\nFile Errors\nThe last type of error we’ll cover today are those associated with reading and writing files: FileNotFoundError. If you try to read a file that does not exist, you will receive a FileNotFoundError telling you so. If you attempt to write to a file that was opened read-only, Python 3 returns an UnsupportedOperationError. More generally, problems with input and output manifest as IOErrors or OSErrors, depending on the version of Python you use.\n\nfile_handle = open('myfile.txt', 'r')\n\nFileNotFoundError: [Errno 2] No such file or directory: 'myfile.txt'\n\n\nOne reason for receiving this error is that you specified an incorrect path to the file. For example, if I am currently in a folder called myproject, and I have a file in myproject/writing/myfile.txt, but I try to open myfile.txt, this will fail. The correct path would be writing/myfile.txt. It is also possible that the file name or its path contains a typo.\nA related issue can occur if you use the “read” flag instead of the “write” flag. Python will not give you an error if you try to open a file for writing when the file does not exist. However, if you meant to open a file for reading, but accidentally opened it for writing, and then try to read from it, you will get an UnsupportedOperation error telling you that the file was not opened for reading:\n\nfile_handle = open('myfile.txt', 'w')\nfile_handle.read()\n\nUnsupportedOperation: not readable\n\n\nThese are the most common errors with files, though many others exist. If you get an error that you’ve never seen before, searching the Internet for that error type often reveals common reasons why you might get that error.\n\n# This code has an intentional error. Do not type it directly;\n# use it for reference to understand the error message below.\ndef print_message(day):\n    messages = {\n        'monday': 'Hello, world!',\n        'tuesday': 'Today is Tuesday!',\n        'wednesday': 'It is the middle of the week.',\n        'thursday': 'Today is Donnerstag in German!',\n        'friday': 'Last day of the week!',\n        'saturday': 'Hooray for the weekend!',\n        'sunday': 'Aw, the weekend is almost over.'\n    }\n    print(messages[day])\n\ndef print_friday_message():\n    print_message('Friday')\n\nprint_friday_message()\n\nKeyError: 'Friday'\n\n\n\nHow many levels does the traceback have?\nthree levels as indicated by the —>\nWhat is the function name where the error occurred?\nprint_message\nOn which line number in this function did the error occur?\n13\nWhat is the type of error?\nKeyError\nWhat is the error message?\nThere isn’t really a message; you’re supposed to infer that Friday is not a key in messages\n\n\n\nIdentifying Syntax Errors\n\ndef another_function\n  print('Syntax errors are annoying.')\n   print('But at least Python tells us about them!')\n  print('So they are usually not too hard to fix.')\n\nSyntaxError: invalid syntax (3532211595.py, line 1)\n\n\n\ndef another_function():\n  print('Syntax errors are annoying.')\n   print('But at least Python tells us about them!')\n  print('So they are usually not too hard to fix.')\n\nIndentationError: unexpected indent (381338138.py, line 3)\n\n\n\ndef another_function():\n    print('Syntax errors are annoying.')\n    print('But at least Python tells us about them!')\n    print('So they are usually not too hard to fix.')\n\nFixed!\n\n\nIdentifying Variable Name Errors\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (Number % 3) == 0:\n        message = message + a\n    else:\n        message = message + 'b'\nprint(message)\n\nNameError: name 'Number' is not defined\n\n\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + a\n    else:\n        message = message + 'b'\nprint(message)\n\nNameError: name 'message' is not defined\n\n\n\nmessage = ''\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + a\n    else:\n        message = message + 'b'\nprint(message)\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nmessage = ''\n\nfor number in range(10):\n    # use a if the number is a multiple of 3, otherwise use b\n    if (number % 3) == 0:\n        message = message + 'a'\n    else:\n        message = message + 'b'\nprint(message)\n\nabbabbabba\n\n\nFixed!\n\n\nIdentifying Index Errors\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[4])\n\nIndexError: list index out of range\n\n\n\nseasons = ['Spring', 'Summer', 'Fall', 'Winter']\nprint('My favorite season is ', seasons[3])\n\nMy favorite season is  Winter\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\ntracebacks can look intimidating, but they give us a lot of useful information about what went wrong in our program, including where the error occurred and what type of error it was\nan error having to do with the ‘grammar’ or syntax of the program is called a SyntaxError. If the issue has to do with how the code is indented, then it will be called an IndentationError\na NameError will occur when trying to use a variable that does not exist. Possible causes are that a variable definition is missing, a variable reference differs from its definition in spelling or capitalization, or the code contains a string that is missing quotes around it\ncontainers like lists and strings will generate errors if you try to access items in them that do not exist. This type of error is called an IndexError\ntrying to read a file that does not exist will give you an FileNotFoundError. Trying to read a file that is open for writing, or writing to a file that is open for reading, will give you an IOError"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#defensive-programming",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#defensive-programming",
    "title": "Programming with Python",
    "section": "10. Defensive Programming",
    "text": "10. Defensive Programming\nOur previous sections have introduced the basic tools of programming: variables and lists, file I/O, loops, conditionals, and functions. What they haven’t done is show us how to tell whether a program is getting the right answer, and how to tell if it’s still getting the right answer as we make changes to it.\nTo achieve that, we need to:\n\nwrite programs that check their own operation\nwrite and run tests for widely-used functions\nmake sure we know what “correct” actually means\n\nThe good news is, doing these things will speed up our programming, not slow it down. As in real carpentry — the kind done with lumber — the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes.\n\nAssertions\nThe first step toward getting the right answers from our programs is to assume that mistakes will happen and to guard against them. This is called defensive programming, and the most common way to do it is to add assertions to our code so that it checks itself as it runs. An assertion is simply a statement that something must be true at a certain point in a program. When Python sees one, it evaluates the assertion’s condition. If it’s true, Python does nothing, but if it’s false, Python halts the program immediately and prints the error message if one is provided. For example, this piece of code halts as soon as the loop encounters a value that isn’t positive:\n\nnumbers = [1.5, 2.3, 0.7, -0.001, 4.4]\ntotal = 0.0\nfor num in numbers:\n    # assertion that numbers should be greater than zero - and error output where this condition not met\n    assert num > 0.0, 'Data should only contain positive values'\n    total += num\nprint('total is:', total)\n\nAssertionError: Data should only contain positive values\n\n\nPrograms like the Firefox browser are full of assertions: 10-20% of the code they contain are there to check that the other 80–90% are working correctly. Broadly speaking, assertions fall into three categories:\n\na precondition is something that must be true at the start of a function in order for it to work correctly.\na postcondition is something that the function guarantees is true when it finishes.\nan invariant is something that is always true at a particular point inside a piece of code.\n\nFor example, suppose we are representing rectangles using a tuple of four coordinates (x0, y0, x1, y1), representing the lower left and upper right corners of the rectangle. In order to do some calculations, we need to normalize the rectangle so that the lower left corner is at the origin and the longest side is 1.0 units long. This function does that, but checks that its input is correctly formatted and that its result makes sense:\n\ndef normalize_rectangle(rect):\n    \"\"\"Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis.\n    Input should be of the format (x0, y0, x1, y1).\n    (x0, y0) and (x1, y1) define the lower left and upper right corners\n    of the rectangle, respectively.\"\"\"\n    \n    # precondition to catch invalid input\n    assert len(rect) == 4, 'Rectangles must contain 4 coordinates'\n    x0, y0, x1, y1 = rect\n    \n    # preconditions to catch invalid inputs\n    assert x0 < x1, 'Invalid X coordinates'\n     # precondition to catch invalid input\n    assert y0 < y1, 'Invalid Y coordinates'\n\n    dx = x1 - x0\n    dy = y1 - y0\n    if dx > dy:\n        scaled = float(dx) / dy\n        upper_x, upper_y = 1.0, scaled\n    else:\n        scaled = float(dx) / dy\n        upper_x, upper_y = scaled, 1.0\n\n    # postconditions to help catch incorrect calculation\n    assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid'\n    \n    # postconditions to help catch incorrect calculation\n    assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid'\n\n    return (0, 0, upper_x, upper_y)\n\n\n# missing the fourth coordinate\nprint(normalize_rectangle( (0.0, 1.0, 2.0) )) \n\nAssertionError: Rectangles must contain 4 coordinates\n\n\n\n# correctly includes four coordinates\nprint(normalize_rectangle( (0.0, 0.0, 1.0, 5.0) ))\n\n(0, 0, 0.2, 1.0)\n\n\n\n# rectangle that is wider than it is tall\nprint(normalize_rectangle( (0.0, 0.0, 5.0, 1.0) ))\n\nAssertionError: Calculated upper Y coordinate invalid\n\n\nRe-reading our function, we realize that line 14 should divide dy by dx rather than dx by dy. In a Jupyter notebook, we can display line numbers by typing Ctrl+M followed by L. If we had left out the assertion at the end of the function, we would have created and returned something that had the right shape as a valid answer, but wasn’t. Detecting and debugging that would almost certainly have taken more time in the long run than writing the assertion.\nBut assertions aren’t just about catching errors: they also help people understand programs. Each assertion gives the person reading the program a chance to check (consciously or otherwise) that their understanding matches what the code is doing.\nMost good programmers follow two rules when adding assertions to their code:\n\nThe first is, fail early, fail often. The greater the distance between when and where an error occurs and when it’s noticed, the harder the error will be to debug, so good code catches mistakes as early as possible.\nThe second rule is, turn bugs into assertions or tests. Whenever you fix a bug, write an assertion that catches the mistake should you make it again. If you made a mistake in a piece of code, the odds are good that you have made other mistakes nearby, or will make the same mistake (or a related one) the next time you change it. Writing assertions to check that you haven’t regressed (i.e., haven’t re-introduced an old problem) can save a lot of time in the long run, and helps to warn people who are reading the code (including your future self) that this bit is tricky.\n\n\n\nTest-Driven Development\nAn assertion checks that something is true at a particular point in the program. The next step is to check the overall behavior of a piece of code, i.e., to make sure that it produces the right output when it’s given a particular input. For example, suppose we need to find where two or more time series overlap. The range of each time series is represented as a pair of numbers, which are the time the interval started and ended. The output is the largest range that they all include:\n\nMost novice programmers would solve this problem like this:\n\nwrite a function range_overlap\ncall it interactively on two or three different inputs\nif it produces the wrong answer, fix the function and re-run that test\n\nThis clearly works — after all, thousands of scientists are doing it right now — but there’s a better way:\n\nwrite a short function for each test\nwrite a range_overlap function that should pass those tests\nIf range_overlap produces any wrong answers, fix it and re-run the test functions\n\nWriting the tests before writing the function they exercise is called test-driven development (TDD). Its advocates believe it produces better code faster because:\n\nif people write tests after writing the thing to be tested, they are subject to confirmation bias, i.e., they subconsciously write tests to show that their code is correct, rather than to find errors.\nwriting tests helps programmers figure out what the function is actually supposed to do.\n\nHere are three test functions for range_overlap:\n\nassert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0)\nassert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0)\nassert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0)\n\nNameError: name 'range_overlap' is not defined\n\n\nThe error is actually reassuring: we haven’t written range_overlap yet, so if the tests passed, it would be a sign that someone else had and that we were accidentally using their function. And as a bonus of writing these tests, we’ve implicitly defined what our input and output look like: we expect a list of pairs as input, and produce a single pair as output.\nSomething important is missing, though. We don’t have any tests for the case where the ranges don’t overlap at all:\nassert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == ???\nWhat should range_overlap do in this case: fail with an error message, produce a special value like (0.0, 0.0) to signal that there’s no overlap, or something else? Any actual implementation of the function will do one of these things; writing the tests first helps us figure out which is best before we’re emotionally invested in whatever we happened to write before we realized there was an issue.\nAnd what about this case?\nassert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == ???\nDo two segments that touch at their endpoints overlap or not? Mathematicians usually say “yes”, but engineers usually say “no”. The best answer is “whatever is most useful in the rest of our program”, but again, any actual implementation of range_overlap is going to do something, and whatever it is ought to be consistent with what it does when there’s no overlap at all.\nSince we’re planning to use the range this function returns as the X axis in a time series chart, we decide that:\n\nevery overlap has to have non-zero width, and\nwe will return the special value None when there’s no overlap\n\nNone is built into Python, and means “nothing here”. (Other languages often call the equivalent value null or nil). With that decision made, we can finish writing our last two tests:\n\nassert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None\nassert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None\n\nNameError: name 'range_overlap' is not defined\n\n\nAgain, we get an error because we haven’t written our function, but we’re now ready to do so:\n\ndef range_overlap(ranges):\n    \"\"\"Return common overlap among a set of [left, right] ranges.\"\"\"\n    max_left = 0.0\n    min_right = 1.0\n    for (left, right) in ranges:\n        max_left = max(max_left, left)\n        min_right = min(min_right, right)\n    return (max_left, min_right)\n\nTake a moment to think about why we calculate the left endpoint of the overlap as the maximum of the input left endpoints, and the overlap right endpoint as the minimum of the input right endpoints. We’d now like to re-run our tests, but they’re scattered across three different cells. To make running them easier, let’s put them all in a function:\n\ndef test_range_overlap():\n    assert range_overlap([ (0.0, 1.0), (5.0, 6.0) ]) == None\n    assert range_overlap([ (0.0, 1.0), (1.0, 2.0) ]) == None\n    assert range_overlap([ (0.0, 1.0) ]) == (0.0, 1.0)\n    assert range_overlap([ (2.0, 3.0), (2.0, 4.0) ]) == (2.0, 3.0)\n    assert range_overlap([ (0.0, 1.0), (0.0, 2.0), (-1.0, 1.0) ]) == (0.0, 1.0)\n    assert range_overlap([]) == None\n\nWe can now test range_overlap with a single function call:\n\ntest_range_overlap()\n\nAssertionError: \n\n\nThe first test that was supposed to produce None fails, so we know something is wrong with our function. We don’t know whether the other tests passed or failed because Python halted the program as soon as it spotted the first error. Still, some information is better than none, and if we trace the behavior of the function with that input, we realize that we’re initializing max_left and min_right to 0.0 and 1.0 respectively, regardless of the input values. This violates another important rule of programming: always initialize from data.\n\n\nPre- and Post-Conditions\nSuppose we are writing a function called average that calculates the average of the numbers in a list. Possible pre and post-conditions are included below:\n\n# a possible pre-condition:\nassert len(input_list) > 0, 'List length must be non-zero'\n# a possible post-condition:\n\nassert numpy.min(input_list) <= average <= numpy.max(input_list),\n'Average should be between min and max of input values (inclusive)'\n\n\n\nTesting Assertions\nGiven a sequence of a number of cars, the function get_total_cars returns the total number of cars:\n\ndef get_total_cars(values):\n    \n    # assertion checks that the input sequence values is not empty. An empty sequence such as [] will make it fail\n    assert len(values) > 0\n    for element in values:\n        # assertion checks that each value in the list can be turned into an integer. Input such as [1, 2,'c', 3] will make it fail\n        assert int(element)\n    values = [int(element) for element in values]\n    total = sum(values)\n    # assertion checks that the total of the list is greater than 0. Input such as [-10, 2, 3] will make it fail\n    assert total > 0\n    return total\n\n\nget_total_cars([1, 2, 3, 4])\n\n10\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nprogram defensively, i.e., assume that errors are going to arise, and write code to detect them when they do\nput assertions in programs to check their state as they run, and to help readers understand how those programs are supposed to work\nuse preconditions to check that the inputs to a function are safe to use\nuse postconditions to check that the output from a function is safe to use\nwrite tests before writing code in order to help determine exactly what that code is supposed to do"
  },
  {
    "objectID": "posts/Programming_with_Python/Programming_with_Python.html#debugging",
    "href": "posts/Programming_with_Python/Programming_with_Python.html#debugging",
    "title": "Programming with Python",
    "section": "11. Debugging",
    "text": "11. Debugging\nOnce testing has uncovered problems, the next step is to fix them. Many novices do this by making more-or-less random changes to their code until it seems to produce the right answer, but that’s very inefficient (and the result is usually only correct for the one case they’re testing). The more experienced a programmer is, the more systematically they debug, and most follow some variation on the rules explained below.\n\nKnow What It’s Supposed to Do\nThe first step in debugging something is to know what it’s supposed to do. “My program doesn’t work” isn’t good enough: in order to diagnose and fix problems, we need to be able to tell correct output from incorrect. If we can write a test case for the failing case — i.e., if we can assert that with these inputs, the function should produce that result — then we’re ready to start debugging. If we can’t, then we need to figure out how we’re going to know when we’ve fixed things.\nBut writing test cases for scientific software is frequently harder than writing test cases for commercial applications, because if we knew what the output of the scientific code was supposed to be, we wouldn’t be running the software: we’d be writing up our results and moving on to the next program. In practice, scientists tend to do the following:\n\nTest with simplified data. Before doing statistics on a real data set, we should try calculating statistics for a single record, for two identical records, for two records whose values are one step apart, or for some other case where we can calculate the right answer by hand.\nTest a simplified case. If our program is supposed to simulate magnetic eddies in rapidly-rotating blobs of supercooled helium, our first test should be a blob of helium that isn’t rotating, and isn’t being subjected to any external electromagnetic fields. Similarly, if we’re looking at the effects of climate change on speciation, our first test should hold temperature, precipitation, and other factors constant.\nCompare to an oracle. A test oracle is something whose results are trusted, such as experimental data, an older program, or a human expert. We use test oracles to determine if our new program produces the correct results. If we have a test oracle, we should store its output for particular cases so that we can compare it with our new results as often as we like without re-running that program.\nCheck conservation laws. Mass, energy, and other quantities are conserved in physical systems, so they should be in programs as well. Similarly, if we are analyzing patient data, the number of records should either stay the same or decrease as we move from one analysis to the next (since we might throw away outliers or records with missing values). If “new” patients start appearing out of nowhere as we move through our pipeline, it’s probably a sign that something is wrong.\nVisualize. Data analysts frequently use simple visualizations to check both the science they’re doing and the correctness of their code (just as we did in the opening sections). This should only be used for debugging as a last resort, though, since it’s very hard to compare two visualizations automatically.\n\n\n\nMake It Fail Every Time\nWe can only debug something when it fails, so the second step is always to find a test case that makes it fail every time. The “every time” part is important because few things are more frustrating than debugging an intermittent problem: if we have to call a function a dozen times to get a single failure, the odds are good that we’ll scroll past the failure when it actually occurs.\nAs part of this, it’s always important to check that our code is “plugged in”, i.e., that we’re actually exercising the problem that we think we are. Every programmer has spent hours chasing a bug, only to realize that they were actually calling their code on the wrong data set or with the wrong configuration parameters, or are using the wrong version of the software entirely. Mistakes like these are particularly likely to happen when we’re tired, frustrated, and up against a deadline, which is one of the reasons late-night (or overnight) coding sessions are almost never worthwhile.\n\n\nMake It Fail Fast\nIf it takes 20 minutes for the bug to surface, we can only do three experiments an hour. This means that we’ll get less data in more time and that we’re more likely to be distracted by other things as we wait for our program to fail, which means the time we are spending on the problem is less focused. It’s therefore critical to make it fail fast.\nAs well as making the program fail fast in time, we want to make it fail fast in space, i.e., we want to localize the failure to the smallest possible region of code:\n\nThe smaller the gap between cause and effect, the easier the connection is to find. Many programmers therefore use a divide and conquer strategy to find bugs, i.e., if the output of a function is wrong, they check whether things are OK in the middle, then concentrate on either the first or second half, and so on.\nN things can interact in N! different ways, so every line of code that isn’t run as part of a test means more than one thing we don’t need to worry about.\n\n\n\nChange One Thing at a Time, For a Reason\nReplacing random chunks of code is unlikely to do much good. (After all, if you got it wrong the first time, you’ll probably get it wrong the second and third as well.) Good programmers therefore change one thing at a time, for a reason. They are either trying to gather more information (“is the bug still there if we change the order of the loops?”) or test a fix (“can we make the bug go away by sorting our data before processing it?”).\nEvery time we make a change, however small, we should re-run our tests immediately, because the more things we change at once, the harder it is to know what’s responsible for what (those N! interactions again). And we should re-run all of our tests: more than half of fixes made to code introduce (or re-introduce) bugs, so re-running all of our tests tells us whether we have regressed.\n\n\nKeep Track of What You’ve Done\nGood scientists keep track of what they’ve done so that they can reproduce their work, and so that they don’t waste time repeating the same experiments or running ones whose results won’t be interesting. Similarly, debugging works best when we keep track of what we’ve done and how well it worked. If we find ourselves asking, “Did left followed by right with an odd number of lines cause the crash? Or was it right followed by left? Or was I using an even number of lines?” then it’s time to step away from the computer, take a deep breath, and start working more systematically.\nRecords are particularly useful when the time comes to ask for help. People are more likely to listen to us when we can explain clearly what we did, and we’re better able to give them the information they need to be useful.\n\n\nVersion Control Revisited\nVersion control is often used to reset software to a known state during debugging, and to explore recent changes to code that might be responsible for bugs. In particular, most version control systems (e.g. git, Mercurial) have:\n\na blame command that shows who last changed each line of a file\na bisect command that helps with finding the commit that introduced an issue\n\n\n\nBe Humble\n\nAnd speaking of help: if we can’t find a bug in 10 minutes, we should be humble and ask for help. Explaining the problem to someone else is often useful, since hearing what we’re thinking helps us spot inconsistencies and hidden assumptions. If you don’t have someone nearby to share your problem description with, get a rubber duck!\nAsking for help also helps alleviate confirmation bias. If we have just spent an hour writing a complicated program, we want it to work, so we’re likely to keep telling ourselves why it should, rather than searching for the reason it doesn’t. People who aren’t emotionally invested in the code can be more objective, which is why they’re often able to spot the simple mistakes we have overlooked.\nPart of being humble is learning from our mistakes. Programmers tend to get the same things wrong over and over: either they don’t understand the language and libraries they’re working with, or their model of how things work is wrong. In either case, taking note of why the error occurred and checking for it next time quickly turns into not making the mistake at all.\nAnd that is what makes us most productive in the long run. As the saying goes, A week of hard work can sometimes save you an hour of thought. If we train ourselves to avoid making some kinds of mistakes, to break our code into modular, testable chunks, and to turn every assumption (or mistake) into an assertion, it will actually take us less time to produce working programs, not more.\n\n\nNot Supposed to be the Same\nImagine you are assisting a researcher with Python code that computes the Body Mass Index (BMI) of patients. The researcher is concerned because all patients seemingly have unusual and identical BMIs, despite having different physiques. BMI is calculated as weight in kilograms divided by the square of height in metres.\n\npatients = [[70, 1.8], [80, 1.9], [150, 1.7]]\n\ndef calculate_bmi(weight, height):\n    return weight / (height ** 2)\n\nfor patient in patients:\n    weight, height = patients[0]\n    bmi = calculate_bmi(height, weight)\n    print(\"Patient's BMI is:\", bmi)\n\nPatient's BMI is: 0.00036734693877551024\nPatient's BMI is: 0.00036734693877551024\nPatient's BMI is: 0.00036734693877551024\n\n\n\n\n\n\n\n\nUse the debugging principles in this section and locate problems with the above code. What suggestions would you give the researcher for ensuring any later changes they make work correctly?\n\n\n\n\n\n\nThe loop is not being utilised correctly. height and weight are always set as the first patient’s data during each iteration of the loop.\nThe height/weight variables are reversed in the function call to calculate_bmi(…), the correct BMIs are 21.604938, 22.160665 and 51.903114.\n\n\n\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nknow what code is supposed to do before trying to debug it.\nmake it fail every time.\nmake it fail fast.\nchange one thing at a time, and for a reason.\nkeep track of what you’ve done.\nbe humble."
  },
  {
    "objectID": "posts/DE_Zoomcamp_Week_3/DE_Zoomcamp_Week_3.html",
    "href": "posts/DE_Zoomcamp_Week_3/DE_Zoomcamp_Week_3.html",
    "title": "Data Engineering Zoomcamp - Week 3",
    "section": "",
    "text": "A Data Warehouse is an OLAP solution used for reporting and data analysis and generally includes raw, meta and summary data.\n\n\n\ndata_warehouse.PNG\n\n\nBigQuery (BQ) is a widely used serverless Data Warehouse which has a lot of built in features such as:\n\nmachine learning\ngeospatial analysis\nbusiness intelligence\n\nOne of its key strengths is that Big Query maximises flexibility by separating the compute engine that analyses data from the storage.\n\n\n\nweek_3.JPG\n\n\n\n\nIn OnLine Transaction Processing (OLTP) information systems typically facilitate and manage transaction-oriented applications. This is contrasted with OnLine Analytical Processing (OLAP) which is an approach to answer multi-dimensional analytical (MDA) queries swiftly in computing.\n\n\n\nOLTP_OLAP_1.PNG\n\n\n\n\n\nOLTP_OLAP_2.PNG\n\n\nLet’s now take a closer look at BQ in particular our New York taxi project dataset. BQ generally caches data, but for our purposes let’s disable this to ensure consistent results:\n\n\n\nbig_query_cache.PNG\n\n\nBQ also provides a lot of open source public datasets e.g. citibike_stations and we can run a query and save the results or explore further using Sheets, Looker Studio or Colab Notebook :\n\n\n\ncitibike_stations.PNG\n\n\nIn terms of pricing there are two tiers - on demand pricing and flat rate pricing - depending on your specific requirements.\nAs explained in the BQ docs external tables are similar to standard BigQuery tables, in that these tables store their metadata and schema in BigQuery storage. However, their data resides in an external source. External tables are contained inside a dataset, and you manage them in the same way that you manage a standard BigQuery table.\nLet’s use the data we already have stored in our data bucket and create an external table. In my particular case, project name is taxi-rides-ny-137, dataset name is de_zoomcamp and we are going to create a table named external_yellow_tripdata.\nThe files I have in GCS are parquet format and we copy the path ids and include as uris :\n\nCREATE OR REPLACE EXTERNAL TABLE `taxi-rides-ny-137.de_zoomcamp.external_yellow_tripdata`\nOPTIONS (\n  format = 'PARQUET',\n  uris = ['gs://dtc_data_lake_taxi-rides-ny-137/data/yellow/yellow_tripdata_2021-01.parquet', 'gs://dtc_data_lake_taxi-rides-ny-137/data/yellow/yellow_tripdata_2021-02.parquet', 'gs://dtc_data_lake_taxi-rides-ny-137/data/yellow/yellow_tripdata_2021-03.parquet']\n);\n\nWe run the query and our table is created. BQ is able to pick up the schema (column name and data type) directly from the parquet file :\n\n\n\nexternal_yellow_trip_table_schema.PNG\n\n\n\n\n\nexternal_yellow_trip_table_details.PNG\n\n\nLet’s now run a quick query on this table :\n\n\n\nexternal_yellow_trip_table_query.PNG\n\n\n\n\n\nA partitioned table is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance and control costs by reducing the number of bytes read by a query. You partition tables by specifying a partition column which is used to segment the table.\nFurther information is provided in the BQ docs.\n\n\n\npartition.PNG\n\n\nLet’s first create a non-partioned table to demonstrate the benefits of a partitioned table :\n\n## Create a non partitioned table from external table\nCREATE OR REPLACE TABLE taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_non_partitoned AS\nSELECT * \nFROM taxi-rides-ny-137.de_zoomcamp.external_yellow_tripdata;\n\n\n\n\nnon_partition.PNG\n\n\nAs we can see the pick up times are not in chronological order. Now let’s create a partitioned table which partitions by the column `tpep_pickup_datetime’:\n\n# Create a partitioned table from external table\nCREATE OR REPLACE TABLE taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_partitoned\nPARTITION BY\n  DATE(tpep_pickup_datetime) AS\nSELECT * FROM taxi-rides-ny-137.de_zoomcamp.external_yellow_tripdata;\n\n\n\n\npartitioned.PNG\n\n\nAs we can see our trip pickup times are now sorted in chronological order. Let’s take a closer look at how the partitioning is workng in practice.\n\n# Let's look into the partitions\nSELECT table_name, partition_id, total_rows\nFROM `de_zoomcamp.INFORMATION_SCHEMA.PARTITIONS`\nWHERE table_name = 'yellow_tripdata_partitoned'\nORDER BY total_rows DESC;\n\n\n\n\npartition_info.PNG\n\n\nWe can see the partitioning (splits) by date. Let’s move on to another interesting feature concept in BQ known as Clustering.\n\n\n\nClustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs.\nIn BigQuery, a clustered column is a user-defined table property that sorts storage blocks based on the values in the clustered columns. The storage blocks are adaptively sized based on the size of the table. A clustered table maintains the sort properties in the context of each operation that modifies it. Queries that filter or aggregate by the clustered columns only scan the relevant blocks based on the clustered columns instead of the entire table or table partition. As a result, BigQuery might not be able to accurately estimate the bytes to be processed by the query or the query costs, but it attempts to reduce the total bytes at execution.\nWhen you cluster a table using multiple columns, the column order determines which columns take precedence when BigQuery sorts and groups the data into storage blocks.\n\n\n\nclustering.PNG\n\n\nLet’s develop the partitioned table we created previously and create a new table with cluster functionality - by VendorID :\n\n# Creating a partition and cluster table\nCREATE OR REPLACE TABLE taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_partitioned_clustered\nPARTITION BY DATE(tpep_pickup_datetime)\nCLUSTER BY VendorID AS\nSELECT * FROM taxi-rides-ny-137.de_zoomcamp.external_yellow_tripdata;\n\nWe obtain an error warning as the VendorID data type is Float. We need to convert to integer in order to cluster. We can modify table schemas cast a column’s data type including casting data types, by overwriting or saving to a new destination table.\n\nSELECT *,\n CAST(VendorID AS INT64) AS Vendor_ID\nFROM taxi-rides-ny-137.de_zoomcamp.external_yellow_tripdata\nWHERE VendorID IS NOT NULL\n ;\n\nWe can configure our query further by clicking on More and specifying the destination etc.\n\n\n\ncast.PNG\n\n\nOK so with a new column Vendor_ID now created with type int64 let’s now proceed to create our partioned, clustered table:\n\n# Creating a partition and cluster table\nCREATE OR REPLACE TABLE taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_partitoned_clustered\nPARTITION BY DATE(tpep_pickup_datetime)\nCLUSTER BY Vendor_ID AS\nSELECT * FROM taxi-rides-ny-137.de_zoomcamp.external_yellow_tripdata_cast;\n\n\n\n\npartition_cluster.PNG\n\n\nTo illustrate the cost savings available through using clustering, the clustered table uses 66.25 MB against 68.73MB for the partioned table, a saving of 3.6%. The costs savings become more pronounced as the size of the dataset increases.\n\n\n\npartition_memory.PNG\n\n\n\n\n\npartition_cluster_memory.PNG\n\n\n\n\n\nWhilst, partitioning and clustering can enhance query performance, it is important to be aware that they do generate some computational overhead (meta data reads and maintenance). As a general guide, we should only consider using partitioning and clustering for tables with a data size of 1GB +.\n\n\n\nBQ_partition.PNG\n\n\nThe default ingestion time is daily, however you may require more granularity in which case BQ also offers hourly partitioning. Note that the number of partitions is limited to 4000.\n\n\n\nBQ_clustering.PNG\n\n\n\n\n\nclustering_columns.PNG\n\n\nLet’s have a look at a comparison between the two and the criteria to consider when choosing one, or the other, or both.\n\n\n\npartition_vs_cluster.PNG\n\n\nClustering should be used where :\n\npartitioning results in a small amount of data (~ < 1 GB) per partition\npartitioning results in a large number of partitions (beyond the 4k limit)\npartitioning results in our mutation operations modifying the majority of partitions in the table frequently (e.g. every few minutes)\n\n\n\n\n\n\n\nautomatic_reclustering.PNG\n\n\n\n\n\nGenerally most of our efforts are focused on :\n\ncost reduction\n\n\n\n\ncost_reduction.PNG\n\n\n\nimproved query performance\n\n \nIf we place the largest table first then it will be distributed evenly, and with the smallest table placed second it will be broadcasted to all of the nodes, which is computationally efficient. We will cover the internal workings of Biq Query in the next section.\n\n\n\nWhilst it is possible to work with Big Query (as with say machine learning) without a detailed understanding of what is going on under the hood it is helpful to have a high-level overview so that we can build our query structure in a way that maximises efficiency and minimises cost.\n\n\n\nBQ_arch.PNG\n\n\n\n\n\nWe touched earlier on the idea of broadcasting which is a computationally efficient method of reading data. The following diagram helps illustrate the concept :\n\n\n\nrecord_column.PNG\n\n\nA record-oriented structure is similar to how csv’s work - they are easy to process and understand. Big Query uses the alternative column-oriented structure which helps provide better aggregation performance.\n\n\n\nDremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. If you want to dig deeper you can read this publication.\n\n\n\ndremel.PNG\n\n\nSome additional resources are included below :\nBigQuery How-to guides\nA Deep Dive Into Google BigQuery Architecture\nA Look at Dremel\n\n\n\nBigQuery makes machine learning techniques accessible to those without any in depth knowledge of Python and its traditional machine learning libraries such as PyTorch or scikit-learn. A dangerous thing if handled without care. Nevertheless it is useful to be able to leverage machine learning to unlock even richer insights from our data.\nA model can be built within BigQuery with no need to export data into a different system.\n\n\n\nML_bigquery_price1.PNG\n\n\n\n\n\nML_bigquery_price2.PNG\n\n\nA typical machine learning flow is illustrated below. Big Query can handle the train/test split, feature engineering, parameter tuning, cross validation etc and ultimately model deployment using a Docker image.\n\n\n\nML_flow.PNG\n\n\n\n\n\nOur particular use case will drive our algorithm selection. An illustration is included below which provides examples of which algorithms might be appropriate depending on what it is that you want to do :\n\n\n\nalgorithms.PNG\n\n\n\n\n\nA wide range of models can be created within BQ - see the attached documentation. We are going to build a Linear Regression model which predicts the tip amount (our target variable) based on some selected input variable columns - or featuresas they are often referred to. We will be revisiting our yellow_tripdata_partitioned table.\n\n# SELECT THE COLUMNS (FEATURES) OF INTEREST\nSELECT passenger_count, trip_distance, PULocationID, DOLocationID, payment_type, fare_amount, tolls_amount, tip_amount\nFROM taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_partitoned\nWHERE fare_amount != 0;\n\nFeature preprocessing is one of the most important steps in developing a machine learning model. It consists of the creation of features as well as the cleaning of the data. Sometimes, the creation of features is also referred as “feature engineering”. BigQuery ML supports two types of feature preprocessing:\n\nAutomatic preprocessing\nManual preprocessing\n\nData types are important when building a machine learning model - essentially all input features need to be numerical. Our table includes some categorical data which although not strictly numerical can be coerced by something called one-hot-encoding. BQ can do this for us but we first have to cast the categorical datatypes PULocationID, DOLocationID, and payment_type from INTEGER to STRING.\nLet’s create a new table which for our machine learning model :\n\n# CREATE A ML TABLE WITH APPROPRIATE DATA TYPES\nCREATE OR REPLACE TABLE `taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_ml` (\n`passenger_count` INTEGER,\n`trip_distance` FLOAT64,\n`PULocationID` STRING,\n`DOLocationID` STRING,\n`payment_type` STRING,\n`fare_amount` FLOAT64,\n`tolls_amount` FLOAT64,\n`tip_amount` FLOAT64\n) AS (\nSELECT CAST(passenger_count AS INTEGER) , trip_distance, cast(PULocationID AS STRING), CAST(DOLocationID AS STRING),\nCAST(payment_type AS STRING), fare_amount, tolls_amount, tip_amount\nFROM `taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_partitoned` WHERE fare_amount != 0\n);\n\n\n\n\nML_table.PNG\n\n\nLet’s now go ahead and create our linear regression model :\n\n# CREATE MODEL WITH DEFAULT SETTING\nCREATE OR REPLACE MODEL `taxi-rides-ny-137.de_zoomcamp.tip_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT') AS\nSELECT\n*\nFROM\n`taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n\n\n\n\ntaxi_tip_lin_reg_model.PNG\n\n\nThat didn’t take too long - just 32 seconds to build our Linear Regression model! We can find out more about our model here :\n\n\n\ntip_model.PNG\n\n\nNote that this is by no means an optimal model - our aim here is to illustrate how to build and deploy a simple model using BigQuery.\n\n\n\ntip_model_evaluation.PNG\n\n\nLet’s now get some summary statistics for our model features - that is the columns from our table which our model will use to try to map or predict the level of tip :\n\n# CHECK FEATURES\nSELECT * FROM ML.FEATURE_INFO(MODEL `taxi-rides-ny-137.de_zoomcamp.tip_model`);\n\n\n\n\nlin_reg_model_summary_stats.PNG\n\n\nNext, let’s evaluate our model :\n\n# EVALUATE THE MODEL\nSELECT\n*\nFROM\nML.EVALUATE(MODEL `taxi-rides-ny-137.de_zoomcamp.tip_model`,\n(\nSELECT\n*\nFROM\n`taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n));\n\n\n\n\nmodel_evaluate.PNG\n\n\nAnd now let’s make some predictions on the test data which was held out from the training dataset (as part of the train/test split). The idea is that the model makes predictions using input features that it has not seen before, which provides us with some assurance on how well the model will perform.\n\n# MAKE PREDICTIONS\nSELECT\n*\nFROM\nML.PREDICT(MODEL `taxi-rides-ny-137.de_zoomcamp.tip_model`,\n(\nSELECT\n*\nFROM\n`taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n));\n\n\n\n\ntaxi_tip_predict.PNG\n\n\nThere is a trade off to be made with machine learning models. As model complexity increases performance is likely to improve, however the danger is that we do not fully understand what is going on under the hood. You may have heard the term black box being used to describe such models.\nIf you want to understand I highly recommend this book Interpretable Machine Learning by Christoph Molnar which is an excellent reference guide.\nBQ does provide some assistance with model interpretation - in the form of explain_predict :\n\n# PREDICT AND EXPLAIN\nSELECT\n*\nFROM\nML.EXPLAIN_PREDICT(MODEL `taxi-rides-ny-137.de_zoomcamp.tip_model`,\n(\nSELECT\n*\nFROM\n`taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL\n), STRUCT(3 as top_k_features));\n\n\n\n\ntaxi_tip_model_explain_predict.PNG\n\n\nBQ also allows us to refine our model by way of hyper parameter tuning :\n\n# HYPER PARAM TUNNING\nCREATE OR REPLACE MODEL `taxi-rides-ny-137.de_zoomcamp.tip_hyperparam_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT',\nnum_trials=5,\nmax_parallel_trials=2,\nl1_reg=hparam_range(0, 20),\nl2_reg=hparam_candidates([0, 0.1, 1, 10])) AS\nSELECT\n*\nFROM\n`taxi-rides-ny-137.de_zoomcamp.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n\n\n\n\nhyper_param.PNG\n\n\nUnfortunately hyperparameter tuning is not supported for my region - europe-central2. See the attached documentation for details.\n\nCome to think of it I do recall Alexey flagging the importance of regions back at the start of the course! Both my bucket and dataset regions were set as Central Europe, but based on the recommendations I have seen on the course Slack - I created a new bucket and dataset with region set to US.\n\n\n# HYPER PARAM TUNNING\nCREATE OR REPLACE MODEL `taxi-rides-ny-137.de_zoomcamp_ml.tip_hyperparam_model`\nOPTIONS\n(model_type='linear_reg',\ninput_label_cols=['tip_amount'],\nDATA_SPLIT_METHOD='AUTO_SPLIT',\nnum_trials=5,\nmax_parallel_trials=2,\nl1_reg=hparam_range(0, 20),\nl2_reg=hparam_candidates([0, 0.1, 1, 10])) AS\nSELECT\n*\nFROM\n`taxi-rides-ny-137.de_zoomcamp_ml.yellow_tripdata_ml`\nWHERE\ntip_amount IS NOT NULL;\n\n\n\n\nhyper_param_model.PNG\n\n\n\n\n\nNow that we have built our model let’s now go ahead and deploy this in a Docker image.\nIn order to access Google Cloud from the command line, you will usually have to authorize the Google Cloud CLI. See the docs here for more info. We already set this up earlier in the course.\nSo let us now export our model to Google Cloud Storage from the command line using:\nbq --project_id taxi-rides-ny-137 extract -m de_zoomcamp_ml.tip_model gs://de_zoomcamp_ml/models\n\n\n\nmodel_gcs.PNG\n\n\nAnd we can see our model is now stored in Google Cloud Storage.\nThe next step is to create a new directory to save our model locally\nmkdir /tmp/model\nand then from within that directory run :\ngsutil cp -r gs://de_zoomcamp_ml/models /tmp/model\n\n\n\nmodel_local_temp.PNG\n\n\nNext create a version subdirectory. This step sets a version number (1 in this case) for the model :\nmkdir -p serving_dir/tip_model/1\nCopy all the temp model data into this directory :\ncp -r /tmp/model/models/* serving_dir/tip_model/1\n\n\n\nserving_dir.PNG\n\n\nrm - r tmp/model\n\n\n\nOnce we have our model saved, and Tensorflow Serving correctly installed with Docker, we are going to serve it as an API endpoint. It is worth mentioning that Tensorflow Serving allows two types of API endpoint,  REST and gRPC.\n\nREST is a communication “protocol” used by web applications. It defines a communication style on how clients communicate with web services. REST clients communicate with the server using the standard HTTP methods like GET, POST, DELETE, etc. The payloads of the requests are mostly encoded in JSON format.\ngRPC on the other hand is a communication protocol initially developed at Google. The standard data format used with gRPC is called the protocol buffer. gRPC provides low- latency communication and smaller payloads than REST and is preferred when working with extremely large files during inference.\n\nIn this tutorial, we’ll use a REST endpoint, since it is easier to use and inspect. It should also be noted that Tensorflow Serving will provision both endpoints when we run it, so we do not need to worry about extra configuration and setup.\nThe next thing to do is to pull our Docker tensorflow serving image :\ndocker pull tensorflow/serving\n\n\n\ndocker_pull.PNG\n\n\nAnd now let’s run the Docker container :\ndocker run -p 8501:8501 -h 0.0.0.0 –mount type=bind,source=$(pwd)/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t tensorflow/serving\n\n\n\ndocker_tensor_flow_serving.PNG\n\n\nNow that the endpoint is up and running, we can make inference calls to it via an HTTP request. I was not able to connect to the Postman API however I was able to carry out inference within this Jupyter NoteBook using the Python requests and json libraries as demonstrated below :\n\nimport requests\nimport json\n\n# define input fetaures that we want to make prediction on\ndata = {\"instances\": [{\"passenger_count\":1, \"trip_distance\":22.2, \"PULocationID\":\"193\", \"DOLocationID\":\"264\", \"payment_type\":\"1\", \"fare_amount\":20.4,\"tolls_amount\":0.0}]}\n\ndata_json = json.dumps(data)\n        \n# define the REST endpoint URL\nurl = \"http://localhost:8501/v1/models/tip_model:predict\"\n        \nheaders = {\"content-type\":\"application/json\"}\n        \njson_response = requests.post(url, data = data_json, headers = headers)\n        \nprint(json_response.json())\n\n{‘predictions’: [[2.2043918289692783]]}\nSo the predicted taxi tip is $2.20 for the input features provided.\nThe requests package is used to construct and send an HTTP call to a server, while the json package will be used to parse the data (image) before sending it. The prediction URL is made up of a few important parts. A general structure may look like the one below:\nhttp://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB}\n\nHOST: The domain name or IP address of your model server\nPORT: The server port for your URL. By default, TF Serving uses 8501 for REST endpoint.\nMODEL_NAME: The name of the model you’re serving.\nVERB: The verb has to do with your model signature. You can specify one of predict, classify or regress.\n\n\n\n\nLet’s now have a go at some exercises to get a feel for Big Query. The data we will be working with is here - specifically the 2019 year.\nI have uploaded the files in .gz format to my data bucket\n\n\n\nfhv_bucket.PNG\n\n\n\n\n\nbucket_URI.PNG\n\n\nLet’s now create an external table.\nFirst thing to do is create a dataset :\n\n\n\ncreate_dataset.PNG\n\n\nAnd the create our external table :\n\n\n\nfhv_external_table.PNG\n\n\n\n\n\nOK, let’s create a BigQuery table. This takes a bit longer than an external table. It’s funny how impatient we have become - I was concerned it had crashed, but 98 seconds to load 43 million records is not exactly slow!!!\n\n\n\nfhv_BQ.PNG\n\n\n\nWhat is the count for fhv vehicle records for year 2019?\n\n\n\n\nq1.PNG\n\n\n\nWrite a query to count the distinct number of affiliated_base_number for the entire dataset on both the tables.\n\nWhat is the estimated amount of data that will be read when this query is executed on the External Table?\n\n\n\nq2_external.PNG\n\n\nWhat is the estimated amount of data that will be read when this query is executed on the BQ Table?\n\n\n\nq2_BQ.PNG\n\n\nThe BigQuery table is estimated to consume 317.94MB however as the external data is not held within BQ no estimate is available for the BQ query.\n\nHow many records have both a blank (null) PUlocationID and DOlocationID in the entire dataset?\n\n\n\n\nq3.PNG\n\n\n\nWhat is the best strategy to optimize the table if query always filter by pickup_datetime and order by affiliated_base_number?\n\nPartitioning will give us the filter, and clustering will provide us with the ordering so the optimized strategy would be :\nPartition by pickup_datetime Cluster on affiliated_base_number\nNote that Affiliated_base_number is a string data type which we cannot partition on.\n\nImplement the optimized solution you chose for question 4.\n\n\n\n\nq5_part_clust_table.PNG\n\n\nWrite a query to retrieve the distinct Affiliated_base_number between pickup_datetime 2019/03/01 and 2019/03/31 (inclusive).\nUse the BQ table you created earlier in your FROM clause and note the estimated bytes.\n\n\n\nq5_BQ.PNG\n\n\nNow change the table in the FROM clause to the partitioned table you created for question 4 and note the estimated bytes processed.\n\n\n\nq5_partitioned.PNG\n\n\nWe can see that there is a huge cost saving by partitioning - just 23.05MB compard with 647.87MB!\n\nWhere is the data stored in the External Table you created?\n\nThe data used to create our external table is stored in our GCS bucket - we referenced the URI on creation.\n\nIs it best practice in Big Query to always cluster your data?\n\nNo. If your data is < 1GB the additional meta data overhead could outweigh any performance boost obtained from clustering."
  },
  {
    "objectID": "posts/DE_Zoomcamp_Week_5/DE_Zoomcamp_Week_5.html",
    "href": "posts/DE_Zoomcamp_Week_5/DE_Zoomcamp_Week_5.html",
    "title": "Data Engineering Zoomcamp - Week 5",
    "section": "",
    "text": "5.0 What we’ll be covering this week\nThis week we’ll cover:\n\nSpark, Spark DataFrames, and Spark SQL\nJoins in Spark\nSpark internals\nRunning Spark in the Cloud\nConnecting Spark to a Data Warehouse, BigQuery\n\n\n\n5.1 Introduction to Batch Processing\n\n\n\nbatch_processing.PNG\n\n\nThere are typically two different ways of processing data :\nBatch processing\nBatch systems process large volumes of data and requests in sequential order.\nStreaming (week 6)\nStream processing monitors real-time data and continually passes it on in the network.\n\n\n\nstreaming.PNG\n\n\nGiven their complementary capabilities, some enterprises have implemented a hybrid system that includes batch processing and stream processing in their daily operations.\n\nWhat is Batch Processing ?\nBatch processing is the method computers use to periodically complete high-volume, repetitive data jobs. Certain data processing tasks, such as backups, filtering, and sorting, can be compute intensive and inefficient to run on individual data transactions. Instead, data systems process such tasks in batches, often in off-peak times when computing resources are more commonly available, such as at the end of the day or overnight. For example, consider an ecommerce system that receives orders throughout the day. Instead of processing every order as it occurs, the system might collect all orders at the end of each day and share them in one batch with the order fulfillment team.\n\n\nWhy is batch processing important?\nOrganizations use batch processing because it requires minimal human interaction and makes repetitive tasks more efficient to run. You can set up batches of jobs composed of millions of records to be worked through together when compute power is most readily available, putting less stress on your systems. Modern batch processing also requires minimal human supervision or management. If there is an issue, the system automatically notifies the concerned team to solve it. Managers take a hands-off approach, trusting their batch processing software to do its job. More benefits of batch processing follow.\n\n\nUse cases of batch processing systems\nFinancial services\nFinancial services organizations, from agile financial technologies to legacy enterprises, have been using batch processing in areas such as high performance computing for risk management, end-of-day transaction processing, and fraud surveillance. They use batch processing to minimize human error, increase speed and accuracy, and reduce costs with automation.\nSoftware as a service\nEnterprises that deliver software as a service (SaaS) applications often run into issues when it comes to scalability. Using batch processing, you can scale customer demand while automating job scheduling. Creating containerized application environments to scale demand for high-volume processing is a project that can take months or even years to complete, but batch processing systems help you achieve the same result in a much shorter timeframe.\nMedical research\nAnalysis of large amounts of data—or big data—is a common requirement in the field of research. You can apply batch processing in data analytics applications such as computational chemistry, clinical modeling, molecular dynamics, and genomic sequencing testing and analysis. For example, scientists use batch processing to capture better data to begin drug design and gain a deeper understanding of the role of a particular biochemical process.\nDigital media\nMedia and entertainment enterprises require highly scalable batch processing systems to automatically process data—such as files, graphics, and visual effects—for high-resolution video content. You can use batch processing to accelerate content creation, dynamically scale media packaging, and automate media workload.\n\n\nOrchestration\nA batch job is a job (a unit of work) that will process data in batches.\nBatch jobs may be scheduled in many ways:\n\nweekly\ndaily\nhourly\nthree times per hour\nevery 5 minutes\n\nand are commonly orchestrated with tools such as dbt or Airflow.\n\n\n\nbatch_workflow.PNG\n\n\n\n\nPros and cons\nAdvantages\n\nmakes repetitive tasks more efficient to run, with minimal human interaction\nre-executable. Jobs can be parameterized and easily retried if they fail\nas we are not working in real time, we can set up batches of jobs composed of millions of records to be worked through together when compute power is most readily available, which puts less stress on your systems\nscalability. Scripts can be executed on higher spec machines; Spark can be run in bigger clusters, etc\n\nDrawbacks\n\nthere is an inherent delay in obtaining the processes data. The example workflow in the earlier graphic shows a 20 minute workflow to process an hour of data, and so the initial data is almost one hour and a half out of date before we can get our hands on it.\n\n\n\n\n5.1.2 Introduction to Spark\nApache Spark is a unified analytics engine for large-scale data processing.\n\n\n\napache_spark.PNG\n\n\nIt is a multi-language engine, which provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including:\n\nSpark SQL for SQL and structured data processing\npandas API on Spark for pandas workloads\nMLlib for machine learning\nGraphX for graph processing\nStructured Streaming for incremental computation and stream processing\n\n\n\n\nwhen_spark.PNG\n\n\nA typical workflow might look something like this :\n\n\n\nspark_workflow.PNG\n\n\n\n\n5.2 Installing Spark - Linux (Ubuntu 20.04)\n\nInstall Java\nHere we’ll see how to install Spark 3.3.2 for Linux. It should also work for other Linux distros.\nDownload and unpack OpenJDK 11 (it’s important that the version is 11 - spark requires 8 or 11) from the command line :\n\nwget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n    \ntar xzfv openjdk-11.0.2_linux-x64_bin.tar.gz\n\n\n\n\njava.PNG\n\n\nAlternatively we can download Oracle JDK 11.\nDefine JAVA_HOME and add it to PATH:\n\nexport JAVA_HOME=\"${HOME}/Spark/jdk-11.0.2\"\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\n\nCheck that it works:\n\njava --version\n\n\n\n\njava_version.PNG\n\n\nRemove the archive :\n\nrm openjdk-11.0.2_linux-x64_bin.tar.gz\n\n\n\nInstall Spark\nDownload and unpack Spark (version 3.3.2) from the command line:\n\nwget https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n\ntar xzfv spark-3.3.2-bin-hadoop3.tgz\n\nDefine SPARK_HOME and add it to PATH :\n\nexport SPARK_HOME=\"${HOME}/Spark/spark-3.3.2-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\n\nCheck that it works by executing spark-shell and run the following :\n\nval data = 1 to 10000\nval distData = sc.parallelize(data)\ndistData.filter(_ < 10).collect():\n\n\n\n\nspark_test.PNG\n\n\nRemove the archive :\n\nrm spark-3.3.2-bin-hadoop3.tgz\n\nAdd these lines to the bottom of the .bashrc file using nano .bashrc :\n\nexport JAVA_HOME=\"${HOME}/Spark/jdk-11.0.2\"\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\nexport SPARK_HOME=\"${HOME}/Spark/spark-3.3.2-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\n\nPress CTRL + O to save the file, type the file name, and hit ENTER. To exit nano, all you need to do is to press CTRL + X\n\n\n\n5.2.1 Running PySpark in Jupyter\nThis document assumes you already have python. To run PySpark, we first need to add it to PYTHONPATH and run this from the command line :\n\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH\"\n\nMake sure that the version under ${SPARK_HOME}/python/lib/ matches the filename of py4j or you will encounter ModuleNotFoundError: No module named 'py4j' while executing import pyspark.\nFor example, if the file under ${SPARK_HOME}/python/lib/ is py4j-0.10.9.3-src.zip, then the export PYTHONPATH statement above should be changed to :\n\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"\n\nNow we can run Jupyter or IPython to test if things work. Go to some other directory, e.g. ~/tmp. Download a CSV file that we’ll use for testing:\n\nwget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n\nNow let’s run ipython or jupyter notebook and execute:\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('taxi+_zone_lookup.csv')\n\ndf.show()\n\n23/03/29 11:36:13 WARN Utils: Your hostname, DESKTOP-1UDJOCI resolves to a loopback address: 127.0.1.1; using 172.24.9.22 instead (on interface eth0)\n23/03/29 11:36:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n23/03/29 11:36:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/03/29 11:36:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n+----------+-------------+--------------------+------------+\n|LocationID|      Borough|                Zone|service_zone|\n+----------+-------------+--------------------+------------+\n|         1|          EWR|      Newark Airport|         EWR|\n|         2|       Queens|         Jamaica Bay|   Boro Zone|\n|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n|         4|    Manhattan|       Alphabet City| Yellow Zone|\n|         5|Staten Island|       Arden Heights|   Boro Zone|\n|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n|         7|       Queens|             Astoria|   Boro Zone|\n|         8|       Queens|        Astoria Park|   Boro Zone|\n|         9|       Queens|          Auburndale|   Boro Zone|\n|        10|       Queens|        Baisley Park|   Boro Zone|\n|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n|        12|    Manhattan|        Battery Park| Yellow Zone|\n|        13|    Manhattan|   Battery Park City| Yellow Zone|\n|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n|        16|       Queens|             Bayside|   Boro Zone|\n|        17|     Brooklyn|             Bedford|   Boro Zone|\n|        18|        Bronx|        Bedford Park|   Boro Zone|\n|        19|       Queens|           Bellerose|   Boro Zone|\n|        20|        Bronx|             Belmont|   Boro Zone|\n+----------+-------------+--------------------+------------+\nonly showing top 20 rows\n\n\n\nTest that writing works as well:\n\ndf.write.parquet('zones')\n\n\n\n\npyspark_test.PNG\n\n\n\n\n5.3.1 First Look at Spark/PySpark\nLet’s grab our required dataset which is in csv.gz format from here and unzip by running the following from the command line:\n\nwget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\ngzip -dk fhvhv_tripdata_2021-01.csv.gz\n\nLet’s see how many rows our dataset has :\n\n!wc -l Data/fhvhv_tripdata_2021-01.csv\n\n11908469 Data/fhvhv_tripdata_2021-01.csv\n\n\nSo, almost 12 million records - not insignificant. Let’s now take a look at Spark.\n\n# import required packages\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n\n# Our entry point to Spark\nspark = SparkSession.builder.getOrCreate()\nspark\n\n23/03/30 09:32:56 WARN Utils: Your hostname, DESKTOP-1UDJOCI resolves to a loopback address: 127.0.1.1; using 172.24.7.86 instead (on interface eth0)\n23/03/30 09:32:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n23/03/30 09:32:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.2\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n            \n        \n\n\nWe can access the Spark Web UI locally at port :4040 from the link above :\n\n\n\nspark_shell.PNG\n\n\nLet’s read our data to Spark :\n\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('Data/fhvhv_tripdata_2021-01.csv')   \n\n\ndf.show()\n\n+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\nonly showing top 20 rows\n\n\n\nEvery time we execute something it is reflected in the Spark Web UI :\n\n\n\nspark_web_ui.PNG\n\n\n\n# Let's look at the first 5 records\ndf.head(5)\n\n[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None)]\n\n\nNote that Spark (unlike pandas) does not try to infer datatypes - instead everything is treated as a string. We can see this more explicitly by looking at the schema :\n\ndf.schema\n\nStructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])\n\n\nLet’s now create a new csv file comprising of just 100 rows from our fhvhv_tripdata_2021-01.csv file by running the following from the command line :\n\nhead -n 101 Data/fhvhv_tripdata_2021-01.csv > head.csv\n\n\nimport pandas as pd\n\n\ndf_pandas = pd.read_csv('Data/head.csv')\n\n\ndf_pandas.dtypes\n\nhvfhs_license_num        object\ndispatching_base_num     object\npickup_datetime          object\ndropoff_datetime         object\nPULocationID              int64\nDOLocationID              int64\nSR_Flag                 float64\ndtype: object\n\n\nWe can see that pandas does a better job at figuring out datatypes but pickup_datetime and dropoff_datetime still require to be converted to a timestamp. We can use Spark to create a Spark DataFrame from a pandas DataFrame :\n\nspark.createDataFrame(df_pandas).schema\n\nStructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])\n\n\nThe above StructType comes from Scala and we can see that the PULocationID and DOLocationID are LongType which is a less memory-efficient format (8 bytes) than integer (4 bytes). So let’s convert the datatypes using Python:\n\nfrom pyspark.sql import types\n\n\n# convert datatypes of our Spark DataFrame\nschema = types.StructType([\n    types.StructField('hvfhs_license_num', types.StringType(), True), # True argument means can be NULL\n    types.StructField('dispatching_base_num', types.StringType(), True),\n    types.StructField('pickup_datetime', types.TimestampType(), True),\n    types.StructField('dropoff_datetime', types.TimestampType(), True),\n    types.StructField('PULocationID', types.IntegerType(), True),\n    types.StructField('DOLocationID', types.IntegerType(), True),\n    types.StructField('SR_Flag', types.StringType(), True)\n])\n\n\n# Load our Spark DataFrame with converted datatypes from above\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .schema(schema) \\\n    .csv('Data/fhvhv_tripdata_2021-01.csv')\n\n\ndf.head(5)\n\n[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 42, 51), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 45, 50), PULocationID=142, DOLocationID=143, SR_Flag=None),\n Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 48, 14), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 8, 42), PULocationID=143, DOLocationID=78, SR_Flag=None)]\n\n\nWe can see that the our pickup_datetime and dropoff_datetime have been properly parsed.\nAt the moment we have one large csv file (718MB) :\n\n!du -h Data/fhvhv_tripdata_2021-01.csv\n\n718M    Data/fhvhv_tripdata_2021-01.csv\n\n\nThat means that only one of our Spark cluster executors will be able to access the file - the remaining clusters will be idle :\n\n\n\ncsv_internals.PNG\n\n\nA more efficient way to store our data is a bunch of smaller files, known as partitions :\n\n\n\npartitiion_structure.PNG\n\n\nWe can achieve partitioning using .repartition :\n\n# break down our csv into multiple partitions\ndf = df.repartition(24)\n\nNote that this is a lazy command - nothing has actually been executed yet. Only when we action something, for example write to parquet, will something happen :\n\ndf.write.parquet('Data/fhvhv/2021/01/')\n\n\n\n\nparquet_DAG.PNG\n\n\nLet’s check to see if we have our parquet files :\n\n!ls -lh Data/fhvhv/2021/01\n\ntotal 136M\n-rw-r--r-- 1 stephen137 stephen137    0 Mar 30 10:13 _SUCCESS\n-rw-r--r-- 1 stephen137 stephen137 8.5M Mar 30 10:13 part-00000-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 9.3M Mar 30 10:13 part-00001-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.8M Mar 30 10:13 part-00002-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.5M Mar 30 10:13 part-00003-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.4M Mar 30 10:13 part-00004-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.8M Mar 30 10:13 part-00005-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.6M Mar 30 10:13 part-00006-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.3M Mar 30 10:13 part-00007-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.8M Mar 30 10:13 part-00008-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.8M Mar 30 10:13 part-00009-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.6M Mar 30 10:13 part-00010-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.1M Mar 30 10:13 part-00011-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.7M Mar 30 10:13 part-00012-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 8.6M Mar 30 10:13 part-00013-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 7.9M Mar 30 10:13 part-00014-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n-rw-r--r-- 1 stephen137 stephen137 7.2M Mar 30 10:13 part-00015-9e5b1ddb-b845-47c8-828f-c2733b793edb-c000.snappy.parquet\n\n\nWe can see that we have 16 parquet files (not sure why there aren’t 24 as specified in .repartition) and compression has been performed, reducing the size from 718MB to 136MB. This is more efficient and makes better use of our Spark clusters.\n\n\n5.3.2 Spark DataFrames\nLet’s now create a Spark DataFrame from our parquet file :\n\ndf = spark.read.parquet('Data/fhvhv/2021/01/')\n\nParquet files include the schema and so we don’t have to specify this :\n\ndf.printSchema()\n\nroot\n |-- hvfhs_license_num: string (nullable = true)\n |-- dispatching_base_num: string (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- SR_Flag: string (nullable = true)\n\n\n\nWe can do the usual stuff that we do with a pandas DataFrame, for example .select and .filter :\n\ndf.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \n\nDataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]\n\n\n\ndf.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n.filter(df.hvfhs_license_num == 'HV0003') \\\n.show()\n\n+-------------------+-------------------+------------+------------+\n|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n+-------------------+-------------------+------------+------------+\n|2021-01-03 00:29:29|2021-01-03 00:46:37|         197|          82|\n|2021-01-03 00:50:22|2021-01-03 00:52:52|          56|          56|\n|2021-01-03 00:05:27|2021-01-03 00:15:20|         243|         244|\n|2021-01-03 00:24:41|2021-01-03 00:30:47|         243|         127|\n|2021-01-03 00:48:14|2021-01-03 01:00:10|         235|          18|\n|2021-01-03 00:19:02|2021-01-03 00:28:57|         151|         116|\n|2021-01-03 00:09:51|2021-01-03 00:19:16|         121|          28|\n|2021-01-03 00:30:20|2021-01-03 00:45:24|          28|         160|\n|2021-01-03 00:47:31|2021-01-03 00:56:37|         160|         157|\n|2021-01-03 00:26:17|2021-01-03 00:50:36|         236|         265|\n|2021-01-03 00:10:04|2021-01-03 00:14:38|           3|          32|\n|2021-01-03 00:41:48|2021-01-03 00:48:48|         169|         136|\n|2021-01-03 00:54:32|2021-01-03 01:00:54|         235|         169|\n|2021-01-03 00:14:38|2021-01-03 00:25:55|         229|         262|\n|2021-01-03 00:13:02|2021-01-03 00:39:15|          82|         163|\n|2021-01-03 00:48:54|2021-01-03 00:53:13|         164|         186|\n|2021-01-03 00:00:50|2021-01-03 00:03:27|          21|          21|\n|2021-01-03 00:53:06|2021-01-03 01:14:24|          50|         126|\n|2021-01-03 00:17:07|2021-01-03 00:25:03|          89|          71|\n|2021-01-03 00:43:17|2021-01-03 00:57:45|         188|          17|\n+-------------------+-------------------+------------+------------+\nonly showing top 20 rows\n\n\n\n\nActions vs Transformations\nIn Spark there is a distinction between :\n\nthings that are executed right away (Actions)\nthings that are NOT executed right away (Transformations)\n\n\n\n\nactions_vs_transforms.PNG\n\n\n\n\n\ntransformation_action.PNG\n\n\n\n\nUser defined functions\nSpark like pandas already has a number of in-built functions. We can access these from within Jupyter Notebook by hitting Tab after F. :\n\n\n\nF.PNG\n\n\nFor example we can convert datetime datatype to date using :\n\ndf \\\n    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n    .show()\n\n+-----------+------------+------------+------------+\n|pickup_date|dropoff_date|PULocationID|DOLocationID|\n+-----------+------------+------------+------------+\n| 2021-01-03|  2021-01-03|         197|          82|\n| 2021-01-03|  2021-01-03|          56|          56|\n| 2021-01-03|  2021-01-03|          76|          51|\n| 2021-01-03|  2021-01-03|         208|         208|\n| 2021-01-03|  2021-01-03|         183|         208|\n| 2021-01-03|  2021-01-03|         243|         244|\n| 2021-01-03|  2021-01-03|         243|         127|\n| 2021-01-03|  2021-01-03|         235|          18|\n| 2021-01-03|  2021-01-03|          68|          49|\n| 2021-01-03|  2021-01-03|         151|         116|\n| 2021-01-03|  2021-01-03|         121|          28|\n| 2021-01-03|  2021-01-03|          28|         160|\n| 2021-01-03|  2021-01-03|         160|         157|\n| 2021-01-03|  2021-01-03|         236|         265|\n| 2021-01-03|  2021-01-03|          32|         169|\n| 2021-01-03|  2021-01-03|           3|          32|\n| 2021-01-03|  2021-01-03|         169|         136|\n| 2021-01-03|  2021-01-03|         235|         169|\n| 2021-01-03|  2021-01-03|         229|         262|\n| 2021-01-03|  2021-01-03|          82|         163|\n+-----------+------------+------------+------------+\nonly showing top 20 rows\n\n\n\nHowever, Spark is more flexible than pandas as it allows us to create and store our own user-defined-functions as illustrated below :\n\nfrom pyspark.sql import functions as F\n\n\ndef crazy_stuff(base_num):\n    num = int(base_num[1:])\n    if num % 7 == 0:\n        return f's/{num:03x}'\n    elif num % 3 == 0:\n        return f'a/{num:03x}'\n    else:\n        return f'e/{num:03x}'\n\n\ncrazy_stuff('B02884')\n\n's/b44'\n\n\n\ncrazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())\n\ndf\n.withColumn(‘pickup_date’, F.to_date(df.pickup_datetime))\n.withColumn(‘dropoff_date’, F.to_date(df.dropoff_datetime))\n.withColumn(‘base_id’, crazy_stuff_udf(df.dispatching_base_num))\n.select(‘base_id’, ‘pickup_date’, ‘dropoff_date’, ‘PULocationID’, ‘DOLocationID’)\n.show()\n\n\n\n5.3.3 Preparing Yellow and Green Taxi Data\nWe can download the required datasets by using the following bash script :\n\nset -e\n\nURL_PREFIX=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download\"\n\nfor TAXI_TYPE in \"yellow\" \"green\"\ndo\n    for YEAR in 2020 2021\n    do\n        for MONTH in {1..12}\n        do\n\n        if [ $YEAR == 2020 ] || [ $MONTH -lt 8 ]\n        then\n            FMONTH=`printf \"%02d\" ${MONTH}`\n\n            URL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\n\n            LOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\n            LOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\n            LOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\n\n            echo \"downloading ${URL} to ${LOCAL_PATH}\"\n            mkdir -p ${LOCAL_PREFIX}\n            wget ${URL} -O ${LOCAL_PATH}\n        fi\n        done\n    done\ndone\n\nAnd running the script from the command lines for yellow and green for years 2020 and 2021 :\n./download_data.sh yellow 2020\n./download_data.sh yellow 2021\n./download_data.sh green 2020\n./download_data.sh green 2021\nThen we need to configure the schema :\n\ngreen_schema = types.StructType([\n    types.StructField(\"VendorID\", types.IntegerType(), True),\n    types.StructField(\"lpep_pickup_datetime\", types.TimestampType(), True),\n    types.StructField(\"lpep_dropoff_datetime\", types.TimestampType(), True),\n    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n    types.StructField(\"PULocationID\", types.IntegerType(), True),\n    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n    types.StructField(\"passenger_count\", types.IntegerType(), True),\n    types.StructField(\"trip_distance\", types.DoubleType(), True),\n    types.StructField(\"fare_amount\", types.DoubleType(), True),\n    types.StructField(\"extra\", types.DoubleType(), True),\n    types.StructField(\"mta_tax\", types.DoubleType(), True),\n    types.StructField(\"tip_amount\", types.DoubleType(), True),\n    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n    types.StructField(\"ehail_fee\", types.DoubleType(), True),\n    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n    types.StructField(\"total_amount\", types.DoubleType(), True),\n    types.StructField(\"payment_type\", types.IntegerType(), True),\n    types.StructField(\"trip_type\", types.IntegerType(), True),\n    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n])\n\nyellow_schema = types.StructType([\n    types.StructField(\"VendorID\", types.IntegerType(), True),\n    types.StructField(\"tpep_pickup_datetime\", types.TimestampType(), True),\n    types.StructField(\"tpep_dropoff_datetime\", types.TimestampType(), True),\n    types.StructField(\"passenger_count\", types.IntegerType(), True),\n    types.StructField(\"trip_distance\", types.DoubleType(), True),\n    types.StructField(\"RatecodeID\", types.IntegerType(), True),\n    types.StructField(\"store_and_fwd_flag\", types.StringType(), True),\n    types.StructField(\"PULocationID\", types.IntegerType(), True),\n    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n    types.StructField(\"payment_type\", types.IntegerType(), True),\n    types.StructField(\"fare_amount\", types.DoubleType(), True),\n    types.StructField(\"extra\", types.DoubleType(), True),\n    types.StructField(\"mta_tax\", types.DoubleType(), True),\n    types.StructField(\"tip_amount\", types.DoubleType(), True),\n    types.StructField(\"tolls_amount\", types.DoubleType(), True),\n    types.StructField(\"improvement_surcharge\", types.DoubleType(), True),\n    types.StructField(\"total_amount\", types.DoubleType(), True),\n    types.StructField(\"congestion_surcharge\", types.DoubleType(), True)\n])\n\nAnd then convert from csv to parquet :\n\nyear = 2020\n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/green/{year}/{month:02d}/'\n    output_path = f'data/pq/green/{year}/{month:02d}/'\n\n    df_green = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(green_schema) \\\n        .csv(input_path)\n\n    df_green \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n\n\nyear = 2021 \n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/green/{year}/{month:02d}/'\n    output_path = f'data/pq/green/{year}/{month:02d}/'\n\n    df_green = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(green_schema) \\\n        .csv(input_path)\n\n    df_green \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n\n\nyear = 2020\n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/yellow/{year}/{month:02d}/'\n    output_path = f'data/pq/yellow/{year}/{month:02d}/'\n\n    df_yellow = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(yellow_schema) \\\n        .csv(input_path)\n\n    df_yellow \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n\n\nyear = 2021 \n\nfor month in range(1, 13):\n    print(f'processing data for {year}/{month}')\n\n    input_path = f'data/raw/yellow/{year}/{month:02d}/'\n    output_path = f'data/pq/yellow/{year}/{month:02d}/'\n\n    df_green = spark.read \\\n        .option(\"header\", \"true\") \\\n        .schema(yellow_schema) \\\n        .csv(input_path)\n\n    df_green \\\n        .repartition(4) \\\n        .write.parquet(output_path)\n\nWe can see the structure of our downloaded parquet files using tree :\n\n!tree data/pq/\n\ndata/pq/\n├── green\n│   ├── 2020\n│   │   ├── 01\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-8b01f98f-81fe-4596-89a1-b39326a4fc89-c000.snappy.parquet\n│   │   │   ├── part-00001-8b01f98f-81fe-4596-89a1-b39326a4fc89-c000.snappy.parquet\n│   │   │   ├── part-00002-8b01f98f-81fe-4596-89a1-b39326a4fc89-c000.snappy.parquet\n│   │   │   └── part-00003-8b01f98f-81fe-4596-89a1-b39326a4fc89-c000.snappy.parquet\n│   │   ├── 02\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-3c243630-2ac8-4a06-979b-1e8a398df62a-c000.snappy.parquet\n│   │   │   ├── part-00001-3c243630-2ac8-4a06-979b-1e8a398df62a-c000.snappy.parquet\n│   │   │   ├── part-00002-3c243630-2ac8-4a06-979b-1e8a398df62a-c000.snappy.parquet\n│   │   │   └── part-00003-3c243630-2ac8-4a06-979b-1e8a398df62a-c000.snappy.parquet\n│   │   ├── 03\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-be46dfd7-b4d2-4802-a124-99b9d191daf8-c000.snappy.parquet\n│   │   │   ├── part-00001-be46dfd7-b4d2-4802-a124-99b9d191daf8-c000.snappy.parquet\n│   │   │   ├── part-00002-be46dfd7-b4d2-4802-a124-99b9d191daf8-c000.snappy.parquet\n│   │   │   └── part-00003-be46dfd7-b4d2-4802-a124-99b9d191daf8-c000.snappy.parquet\n│   │   ├── 04\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-cbb6d825-747e-42d8-8237-400f4cf188ee-c000.snappy.parquet\n│   │   │   ├── part-00001-cbb6d825-747e-42d8-8237-400f4cf188ee-c000.snappy.parquet\n│   │   │   ├── part-00002-cbb6d825-747e-42d8-8237-400f4cf188ee-c000.snappy.parquet\n│   │   │   └── part-00003-cbb6d825-747e-42d8-8237-400f4cf188ee-c000.snappy.parquet\n│   │   ├── 05\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-199c0d3a-f772-495a-b4bd-de3261c10a84-c000.snappy.parquet\n│   │   │   ├── part-00001-199c0d3a-f772-495a-b4bd-de3261c10a84-c000.snappy.parquet\n│   │   │   ├── part-00002-199c0d3a-f772-495a-b4bd-de3261c10a84-c000.snappy.parquet\n│   │   │   └── part-00003-199c0d3a-f772-495a-b4bd-de3261c10a84-c000.snappy.parquet\n│   │   ├── 06\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-6ba25688-0e78-45b5-ae85-3460a5c5e921-c000.snappy.parquet\n│   │   │   ├── part-00001-6ba25688-0e78-45b5-ae85-3460a5c5e921-c000.snappy.parquet\n│   │   │   ├── part-00002-6ba25688-0e78-45b5-ae85-3460a5c5e921-c000.snappy.parquet\n│   │   │   └── part-00003-6ba25688-0e78-45b5-ae85-3460a5c5e921-c000.snappy.parquet\n│   │   ├── 07\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-626c45ff-38e2-4bdf-8add-f61defd0c946-c000.snappy.parquet\n│   │   │   ├── part-00001-626c45ff-38e2-4bdf-8add-f61defd0c946-c000.snappy.parquet\n│   │   │   ├── part-00002-626c45ff-38e2-4bdf-8add-f61defd0c946-c000.snappy.parquet\n│   │   │   └── part-00003-626c45ff-38e2-4bdf-8add-f61defd0c946-c000.snappy.parquet\n│   │   ├── 08\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-70a42578-baa5-4d7b-8350-3c8b54451443-c000.snappy.parquet\n│   │   │   ├── part-00001-70a42578-baa5-4d7b-8350-3c8b54451443-c000.snappy.parquet\n│   │   │   ├── part-00002-70a42578-baa5-4d7b-8350-3c8b54451443-c000.snappy.parquet\n│   │   │   └── part-00003-70a42578-baa5-4d7b-8350-3c8b54451443-c000.snappy.parquet\n│   │   ├── 09\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-7bf574b0-dfbd-461a-8ce5-f65bd261d528-c000.snappy.parquet\n│   │   │   ├── part-00001-7bf574b0-dfbd-461a-8ce5-f65bd261d528-c000.snappy.parquet\n│   │   │   ├── part-00002-7bf574b0-dfbd-461a-8ce5-f65bd261d528-c000.snappy.parquet\n│   │   │   └── part-00003-7bf574b0-dfbd-461a-8ce5-f65bd261d528-c000.snappy.parquet\n│   │   ├── 10\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-030469d0-906b-477c-accd-57a0d179dd2d-c000.snappy.parquet\n│   │   │   ├── part-00001-030469d0-906b-477c-accd-57a0d179dd2d-c000.snappy.parquet\n│   │   │   ├── part-00002-030469d0-906b-477c-accd-57a0d179dd2d-c000.snappy.parquet\n│   │   │   └── part-00003-030469d0-906b-477c-accd-57a0d179dd2d-c000.snappy.parquet\n│   │   ├── 11\n│   │   │   ├── _SUCCESS\n│   │   │   ├── part-00000-4136ae51-88ac-4aa9-9d3c-d8ee0363f9cf-c000.snappy.parquet\n│   │   │   ├── part-00001-4136ae51-88ac-4aa9-9d3c-d8ee0363f9cf-c000.snappy.parquet\n│   │   │   ├── part-00002-4136ae51-88ac-4aa9-9d3c-d8ee0363f9cf-c000.snappy.parquet\n│   │   │   └── part-00003-4136ae51-88ac-4aa9-9d3c-d8ee0363f9cf-c000.snappy.parquet\n│   │   └── 12\n│   │       ├── _SUCCESS\n│   │       ├── part-00000-5e4ac23a-9ea2-4d39-a974-72fa35bed604-c000.snappy.parquet\n│   │       ├── part-00001-5e4ac23a-9ea2-4d39-a974-72fa35bed604-c000.snappy.parquet\n│   │       ├── part-00002-5e4ac23a-9ea2-4d39-a974-72fa35bed604-c000.snappy.parquet\n│   │       └── part-00003-5e4ac23a-9ea2-4d39-a974-72fa35bed604-c000.snappy.parquet\n│   └── 2021\n│       ├── 01\n│       │   ├── _SUCCESS\n│       │   ├── part-00000-a9d9b6cb-a441-4c75-ad83-e0dabbe67945-c000.snappy.parquet\n│       │   ├── part-00001-a9d9b6cb-a441-4c75-ad83-e0dabbe67945-c000.snappy.parquet\n│       │   ├── part-00002-a9d9b6cb-a441-4c75-ad83-e0dabbe67945-c000.snappy.parquet\n│       │   └── part-00003-a9d9b6cb-a441-4c75-ad83-e0dabbe67945-c000.snappy.parquet\n│       ├── 02\n│       │   ├── _SUCCESS\n│       │   ├── part-00000-125ad13e-4d67-4cb1-8ae1-2fe33966fee4-c000.snappy.parquet\n│       │   ├── part-00001-125ad13e-4d67-4cb1-8ae1-2fe33966fee4-c000.snappy.parquet\n│       │   ├── part-00002-125ad13e-4d67-4cb1-8ae1-2fe33966fee4-c000.snappy.parquet\n│       │   └── part-00003-125ad13e-4d67-4cb1-8ae1-2fe33966fee4-c000.snappy.parquet\n│       ├── 03\n│       │   ├── _SUCCESS\n│       │   ├── part-00000-1bc79775-8fc3-4709-854c-3351661cd6c4-c000.snappy.parquet\n│       │   ├── part-00001-1bc79775-8fc3-4709-854c-3351661cd6c4-c000.snappy.parquet\n│       │   ├── part-00002-1bc79775-8fc3-4709-854c-3351661cd6c4-c000.snappy.parquet\n│       │   └── part-00003-1bc79775-8fc3-4709-854c-3351661cd6c4-c000.snappy.parquet\n│       ├── 04\n│       │   ├── _SUCCESS\n│       │   ├── part-00000-ceb35fd7-2ce7-4b84-83a3-cf31e6123391-c000.snappy.parquet\n│       │   ├── part-00001-ceb35fd7-2ce7-4b84-83a3-cf31e6123391-c000.snappy.parquet\n│       │   ├── part-00002-ceb35fd7-2ce7-4b84-83a3-cf31e6123391-c000.snappy.parquet\n│       │   └── part-00003-ceb35fd7-2ce7-4b84-83a3-cf31e6123391-c000.snappy.parquet\n│       ├── 05\n│       │   ├── _SUCCESS\n│       │   ├── part-00000-0f4cffc3-22ef-4558-9ceb-40c78fd0a048-c000.snappy.parquet\n│       │   ├── part-00001-0f4cffc3-22ef-4558-9ceb-40c78fd0a048-c000.snappy.parquet\n│       │   ├── part-00002-0f4cffc3-22ef-4558-9ceb-40c78fd0a048-c000.snappy.parquet\n│       │   └── part-00003-0f4cffc3-22ef-4558-9ceb-40c78fd0a048-c000.snappy.parquet\n│       ├── 06\n│       │   ├── _SUCCESS\n│       │   ├── part-00000-5ad0ef6e-9c3b-488a-85a0-b34a4c364eb0-c000.snappy.parquet\n│       │   ├── part-00001-5ad0ef6e-9c3b-488a-85a0-b34a4c364eb0-c000.snappy.parquet\n│       │   ├── part-00002-5ad0ef6e-9c3b-488a-85a0-b34a4c364eb0-c000.snappy.parquet\n│       │   └── part-00003-5ad0ef6e-9c3b-488a-85a0-b34a4c364eb0-c000.snappy.parquet\n│       └── 07\n│           ├── _SUCCESS\n│           ├── part-00000-4283b1a5-f5da-4219-97b2-2bd986599afd-c000.snappy.parquet\n│           ├── part-00001-4283b1a5-f5da-4219-97b2-2bd986599afd-c000.snappy.parquet\n│           ├── part-00002-4283b1a5-f5da-4219-97b2-2bd986599afd-c000.snappy.parquet\n│           └── part-00003-4283b1a5-f5da-4219-97b2-2bd986599afd-c000.snappy.parquet\n└── yellow\n    ├── 2020\n    │   ├── 01\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-13e18553-247c-48af-af4e-b64c5f6024ed-c000.snappy.parquet\n    │   │   ├── part-00001-13e18553-247c-48af-af4e-b64c5f6024ed-c000.snappy.parquet\n    │   │   ├── part-00002-13e18553-247c-48af-af4e-b64c5f6024ed-c000.snappy.parquet\n    │   │   └── part-00003-13e18553-247c-48af-af4e-b64c5f6024ed-c000.snappy.parquet\n    │   ├── 02\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-2bc461f8-6484-43c8-8757-97e638e454ae-c000.snappy.parquet\n    │   │   ├── part-00001-2bc461f8-6484-43c8-8757-97e638e454ae-c000.snappy.parquet\n    │   │   ├── part-00002-2bc461f8-6484-43c8-8757-97e638e454ae-c000.snappy.parquet\n    │   │   └── part-00003-2bc461f8-6484-43c8-8757-97e638e454ae-c000.snappy.parquet\n    │   ├── 03\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-3c79e29b-b3fb-4f4c-9ce2-6eb6341de80c-c000.snappy.parquet\n    │   │   ├── part-00001-3c79e29b-b3fb-4f4c-9ce2-6eb6341de80c-c000.snappy.parquet\n    │   │   ├── part-00002-3c79e29b-b3fb-4f4c-9ce2-6eb6341de80c-c000.snappy.parquet\n    │   │   └── part-00003-3c79e29b-b3fb-4f4c-9ce2-6eb6341de80c-c000.snappy.parquet\n    │   ├── 04\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-c7ea0f14-9b83-40d7-bc3b-168d552efe6f-c000.snappy.parquet\n    │   │   ├── part-00001-c7ea0f14-9b83-40d7-bc3b-168d552efe6f-c000.snappy.parquet\n    │   │   ├── part-00002-c7ea0f14-9b83-40d7-bc3b-168d552efe6f-c000.snappy.parquet\n    │   │   └── part-00003-c7ea0f14-9b83-40d7-bc3b-168d552efe6f-c000.snappy.parquet\n    │   ├── 05\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-c966cece-74cf-4fb9-962e-9c818a4f1964-c000.snappy.parquet\n    │   │   ├── part-00001-c966cece-74cf-4fb9-962e-9c818a4f1964-c000.snappy.parquet\n    │   │   ├── part-00002-c966cece-74cf-4fb9-962e-9c818a4f1964-c000.snappy.parquet\n    │   │   └── part-00003-c966cece-74cf-4fb9-962e-9c818a4f1964-c000.snappy.parquet\n    │   ├── 06\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-9fb65628-ffb1-49b4-9682-0c71b2099651-c000.snappy.parquet\n    │   │   ├── part-00001-9fb65628-ffb1-49b4-9682-0c71b2099651-c000.snappy.parquet\n    │   │   ├── part-00002-9fb65628-ffb1-49b4-9682-0c71b2099651-c000.snappy.parquet\n    │   │   └── part-00003-9fb65628-ffb1-49b4-9682-0c71b2099651-c000.snappy.parquet\n    │   ├── 07\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-71ac3c30-7e37-438e-873d-1de2448ef2b6-c000.snappy.parquet\n    │   │   ├── part-00001-71ac3c30-7e37-438e-873d-1de2448ef2b6-c000.snappy.parquet\n    │   │   ├── part-00002-71ac3c30-7e37-438e-873d-1de2448ef2b6-c000.snappy.parquet\n    │   │   └── part-00003-71ac3c30-7e37-438e-873d-1de2448ef2b6-c000.snappy.parquet\n    │   ├── 08\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-5f4e316f-cefd-4aa3-a5cb-b1aefe474182-c000.snappy.parquet\n    │   │   ├── part-00001-5f4e316f-cefd-4aa3-a5cb-b1aefe474182-c000.snappy.parquet\n    │   │   ├── part-00002-5f4e316f-cefd-4aa3-a5cb-b1aefe474182-c000.snappy.parquet\n    │   │   └── part-00003-5f4e316f-cefd-4aa3-a5cb-b1aefe474182-c000.snappy.parquet\n    │   ├── 09\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-9cf0d56f-f5bc-42f4-9e3c-4472f1fef599-c000.snappy.parquet\n    │   │   ├── part-00001-9cf0d56f-f5bc-42f4-9e3c-4472f1fef599-c000.snappy.parquet\n    │   │   ├── part-00002-9cf0d56f-f5bc-42f4-9e3c-4472f1fef599-c000.snappy.parquet\n    │   │   └── part-00003-9cf0d56f-f5bc-42f4-9e3c-4472f1fef599-c000.snappy.parquet\n    │   ├── 10\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-79c26c54-874e-4a96-9ad6-b38b1b938766-c000.snappy.parquet\n    │   │   ├── part-00001-79c26c54-874e-4a96-9ad6-b38b1b938766-c000.snappy.parquet\n    │   │   ├── part-00002-79c26c54-874e-4a96-9ad6-b38b1b938766-c000.snappy.parquet\n    │   │   └── part-00003-79c26c54-874e-4a96-9ad6-b38b1b938766-c000.snappy.parquet\n    │   ├── 11\n    │   │   ├── _SUCCESS\n    │   │   ├── part-00000-b3646277-b1f2-495f-a225-12cae19dc1ff-c000.snappy.parquet\n    │   │   ├── part-00001-b3646277-b1f2-495f-a225-12cae19dc1ff-c000.snappy.parquet\n    │   │   ├── part-00002-b3646277-b1f2-495f-a225-12cae19dc1ff-c000.snappy.parquet\n    │   │   └── part-00003-b3646277-b1f2-495f-a225-12cae19dc1ff-c000.snappy.parquet\n    │   └── 12\n    │       ├── _SUCCESS\n    │       ├── part-00000-ab762734-8696-4d42-8f9d-d5b924d4fb29-c000.snappy.parquet\n    │       ├── part-00001-ab762734-8696-4d42-8f9d-d5b924d4fb29-c000.snappy.parquet\n    │       ├── part-00002-ab762734-8696-4d42-8f9d-d5b924d4fb29-c000.snappy.parquet\n    │       └── part-00003-ab762734-8696-4d42-8f9d-d5b924d4fb29-c000.snappy.parquet\n    └── 2021\n        ├── 01\n        │   ├── _SUCCESS\n        │   ├── part-00000-1d24a28b-80bb-4115-9650-456962d66d59-c000.snappy.parquet\n        │   ├── part-00001-1d24a28b-80bb-4115-9650-456962d66d59-c000.snappy.parquet\n        │   ├── part-00002-1d24a28b-80bb-4115-9650-456962d66d59-c000.snappy.parquet\n        │   └── part-00003-1d24a28b-80bb-4115-9650-456962d66d59-c000.snappy.parquet\n        ├── 02\n        │   ├── _SUCCESS\n        │   ├── part-00000-79399355-08a8-44fd-9a60-1cb83a19454e-c000.snappy.parquet\n        │   ├── part-00001-79399355-08a8-44fd-9a60-1cb83a19454e-c000.snappy.parquet\n        │   ├── part-00002-79399355-08a8-44fd-9a60-1cb83a19454e-c000.snappy.parquet\n        │   └── part-00003-79399355-08a8-44fd-9a60-1cb83a19454e-c000.snappy.parquet\n        ├── 03\n        │   ├── _SUCCESS\n        │   ├── part-00000-b1231c11-d032-4088-b387-24cc04eaff40-c000.snappy.parquet\n        │   ├── part-00001-b1231c11-d032-4088-b387-24cc04eaff40-c000.snappy.parquet\n        │   ├── part-00002-b1231c11-d032-4088-b387-24cc04eaff40-c000.snappy.parquet\n        │   └── part-00003-b1231c11-d032-4088-b387-24cc04eaff40-c000.snappy.parquet\n        ├── 04\n        │   ├── _SUCCESS\n        │   ├── part-00000-828ae8bb-c582-4e3e-a25b-8ffac100299e-c000.snappy.parquet\n        │   ├── part-00001-828ae8bb-c582-4e3e-a25b-8ffac100299e-c000.snappy.parquet\n        │   ├── part-00002-828ae8bb-c582-4e3e-a25b-8ffac100299e-c000.snappy.parquet\n        │   └── part-00003-828ae8bb-c582-4e3e-a25b-8ffac100299e-c000.snappy.parquet\n        ├── 05\n        │   ├── _SUCCESS\n        │   ├── part-00000-a06be6df-4c6a-4a9c-994b-2e579cd79fd5-c000.snappy.parquet\n        │   ├── part-00001-a06be6df-4c6a-4a9c-994b-2e579cd79fd5-c000.snappy.parquet\n        │   ├── part-00002-a06be6df-4c6a-4a9c-994b-2e579cd79fd5-c000.snappy.parquet\n        │   └── part-00003-a06be6df-4c6a-4a9c-994b-2e579cd79fd5-c000.snappy.parquet\n        ├── 06\n        │   ├── _SUCCESS\n        │   ├── part-00000-1d9a9dc9-eda5-44c8-b21d-2d8c3d6d0e62-c000.snappy.parquet\n        │   ├── part-00001-1d9a9dc9-eda5-44c8-b21d-2d8c3d6d0e62-c000.snappy.parquet\n        │   ├── part-00002-1d9a9dc9-eda5-44c8-b21d-2d8c3d6d0e62-c000.snappy.parquet\n        │   └── part-00003-1d9a9dc9-eda5-44c8-b21d-2d8c3d6d0e62-c000.snappy.parquet\n        └── 07\n            ├── _SUCCESS\n            ├── part-00000-98a78c13-da79-4520-b66e-f7ba8384138e-c000.snappy.parquet\n            ├── part-00001-98a78c13-da79-4520-b66e-f7ba8384138e-c000.snappy.parquet\n            ├── part-00002-98a78c13-da79-4520-b66e-f7ba8384138e-c000.snappy.parquet\n            └── part-00003-98a78c13-da79-4520-b66e-f7ba8384138e-c000.snappy.parquet\n\n44 directories, 190 files\n\n\n\n\n5.3.4 SQL with Spark\nWe are going to recreate the monthly_zone_revenue model that we created previously in Week 4 using dbt:\n\n{{ config(materialized='table') }}\n\nwith trips_data as (\n    select * from {{ ref('fact_trips') }}\n)\n    select \n    -- Reveneue grouping \n    pickup_zone as revenue_zone,\n    date_trunc('month', pickup_datetime) as revenue_month, \n    --Note: For BQ use instead: date_trunc(pickup_datetime, month) as revenue_month, \n\n    service_type, \n\n    -- Revenue calculation \n    sum(fare_amount) as revenue_monthly_fare,\n    sum(extra) as revenue_monthly_extra,\n    sum(mta_tax) as revenue_monthly_mta_tax,\n    sum(tip_amount) as revenue_monthly_tip_amount,\n    sum(tolls_amount) as revenue_monthly_tolls_amount,\n    sum(ehail_fee) as revenue_monthly_ehail_fee,\n    sum(improvement_surcharge) as revenue_monthly_improvement_surcharge,\n    sum(total_amount) as revenue_monthly_total_amount,\n    sum(congestion_surcharge) as revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    count(tripid) as total_monthly_trips,\n    avg(passenger_count) as avg_montly_passenger_count,\n    avg(trip_distance) as avg_montly_trip_distance\n\n    from trips_data\n    group by 1,2,3\n\nFirst, let’s replicate the trips_data dataset which was created in Week 4 by combining the yellow and green datasets :\n\ndf_green = spark.read.parquet('data/pq/green/*/*')\n\n\ndf_green.printSchema()\n\nroot\n |-- VendorID: integer (nullable = true)\n |-- lpep_pickup_datetime: timestamp (nullable = true)\n |-- lpep_dropoff_datetime: timestamp (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- ehail_fee: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- trip_type: integer (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n\n\n\n\ndf_yellow = spark.read.parquet('data/pq/yellow/*/*')\n\n\ndf_yellow.printSchema()\n\nroot\n |-- VendorID: integer (nullable = true)\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n\n\n\nThe pick up and drop off fields have a different naming convention, Let’s correct for that using .withColumnRenamed:\n\ndf_green = df_green \\\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\n\ndf_yellow = df_yellow \\\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n\nWe only want to combine on common columns, but there is still some inconsistency between the two datasets, for example green has ehail_fee whereas yellow does not. We can use Python’s set object to help with this :\n\ncommon_columns = []\n\nyellow_columns = set(df_yellow.columns)\n\nfor col in df_green.columns:\n    if col in yellow_columns:\n        common_columns.append(col)\n\n\ncommon_columns\n\n['VendorID',\n 'pickup_datetime',\n 'dropoff_datetime',\n 'store_and_fwd_flag',\n 'RatecodeID',\n 'PULocationID',\n 'DOLocationID',\n 'passenger_count',\n 'trip_distance',\n 'fare_amount',\n 'extra',\n 'mta_tax',\n 'tip_amount',\n 'tolls_amount',\n 'improvement_surcharge',\n 'total_amount',\n 'payment_type',\n 'congestion_surcharge']\n\n\nWe want to be able to distinguish between the yellow and green taxi services in our combined data set. To do this, prior to combining, we should add a new column to our green and yellow datasets named service_type. To add a new column to a PySpark DataFrame we can use the .lit() function.\n\nfrom pyspark.sql import functions as F\n\n\ndf_green \\\n    .select(common_columns) \\\n    .withColumn('service_type', F.lit('green'))\n\nDataFrame[VendorID: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, payment_type: int, congestion_surcharge: double, service_type: string]\n\n\n\ndf_yellow \\\n    .select(common_columns) \\\n    .withColumn('service_type', F.lit('yellow'))\n\nDataFrame[VendorID: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, payment_type: int, congestion_surcharge: double, service_type: string]\n\n\n\ndf_green_sel = df_green \\\n    .select(common_columns) \\\n    .withColumn('service_type', F.lit('green'))\n\n\ndf_yellow_sel = df_yellow \\\n    .select(common_columns) \\\n    .withColumn('service_type', F.lit('yellow'))\n\nTo merge our green and yellow datasets we can once again make use of one of Python’s setfunctions .unionALL:\n\ndf_green_sel.unionAll(df_yellow_sel)\n\nDataFrame[VendorID: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, payment_type: int, congestion_surcharge: double, service_type: string]\n\n\n\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n\nAnd we can use .groupBy to show the distribution by service type :\n\ndf_trips_data.groupBy('service_type').count().show()\n\n[Stage 341:===================================>                  (24 + 13) / 37]\n\n\n+------------+--------+\n|service_type|   count|\n+------------+--------+\n|       green| 2304517|\n|      yellow|39649199|\n+------------+--------+\n\n\n\n                                                                                \n\n\n\nQuerying the data with SQL\nFirst, let’s register a temporary table :\n\ndf_trips_data.registerTempTable('trips_data')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n\n\nWe can run SQL queries in Spark using spark.sql :\n\nspark.sql(\"\"\"\nSELECT\n    service_type,\n    count(1)\nFROM\n    trips_data\nGROUP BY \n    service_type\n\"\"\").show()\n\n[Stage 344:=====================================>                (26 + 11) / 37]\n\n\n+------------+--------+\n|service_type|count(1)|\n+------------+--------+\n|       green| 2304517|\n|      yellow|39649199|\n+------------+--------+\n\n\n\n                                                                                \n\n\nLet’s get all the column names as a list :\n\ndf_trips_data.columns\n\n['VendorID',\n 'pickup_datetime',\n 'dropoff_datetime',\n 'store_and_fwd_flag',\n 'RatecodeID',\n 'PULocationID',\n 'DOLocationID',\n 'passenger_count',\n 'trip_distance',\n 'fare_amount',\n 'extra',\n 'mta_tax',\n 'tip_amount',\n 'tolls_amount',\n 'improvement_surcharge',\n 'total_amount',\n 'payment_type',\n 'congestion_surcharge',\n 'service_type']\n\n\nLet’s finally replicate the monthly_zone_revenue model created in Week 4 using dbt:\n\ndf_result = spark.sql(\"\"\"\nSELECT \n    -- Reveneue grouping \n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month, \n    service_type, \n\n    -- Revenue calculation \n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    AVG(passenger_count) AS avg_montly_passenger_count,\n    AVG(trip_distance) AS avg_montly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n\n\ndf_result.show()\n\n[Stage 347:=====================================================> (36 + 1) / 37]\n\n\n+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n|revenue_zone|      revenue_month|service_type|revenue_monthly_fare|revenue_monthly_extra|revenue_monthly_mta_tax|revenue_monthly_tip_amount|revenue_monthly_tolls_amount|revenue_monthly_improvement_surcharge|revenue_monthly_total_amount|revenue_monthly_congestion_surcharge|avg_montly_passenger_count|avg_montly_trip_distance|\n+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\n|          47|2020-01-01 00:00:00|       green|  16630.150000000023|               1192.0|                  160.0|        42.230000000000004|           523.0000000000002|                   210.90000000000015|          18770.429999999975|                                11.0|         1.177685950413223|       4.645000000000002|\n|          53|2020-01-01 00:00:00|       green|   6281.649999999996|               330.25|                   49.0|                     39.22|          102.50000000000001|                    68.39999999999988|           6873.770000000004|                                2.75|        1.2417582417582418|       6.929710743801654|\n|         128|2020-01-01 00:00:00|       green|   3261.330000000001|                158.0|                   35.5|                     82.25|          104.03999999999999|                    34.50000000000004|          3714.0200000000004|                                27.5|        1.0169491525423728|       7.023280000000001|\n|         150|2020-01-01 00:00:00|       green|             9996.16|                627.5|                   98.0|         97.86999999999998|                      149.14|                   109.49999999999962|          11082.069999999992|                                 0.0|        1.1147540983606556|       5.393588516746414|\n|         263|2020-01-01 00:00:00|       green|   8987.339999999998|                561.0|                 124.55|                    315.27|           335.0600000000001|                    127.1999999999995|          10684.170000000004|                              239.25|        1.4293478260869565|       5.264249422632793|\n|          19|2020-01-01 00:00:00|       green|   7798.879999999999|                377.0|                   42.0|                      6.66|          174.40000000000003|                     67.4999999999999|           8466.440000000004|                                 0.0|                  1.046875|       7.850781893004114|\n|         186|2020-01-01 00:00:00|       green|             1539.23|                85.25|                    6.0|                       0.0|           97.91999999999999|                                 13.8|          1742.1999999999998|                                 0.0|        1.6666666666666667|       9.389565217391304|\n|         238|2020-01-01 00:00:00|       green|   5397.990000000001|                403.5|                   20.0|                       0.0|                      124.68|                    61.19999999999993|           6007.370000000001|                                 0.0|                       1.2|       6.460536585365853|\n|         126|2020-01-01 00:00:00|       green|   11827.83999999999|               766.75|                  110.5|        101.86999999999999|           458.2800000000002|                   145.19999999999933|          13425.340000000004|                                11.0|                     1.275|       5.187806691449811|\n|          96|2020-01-01 00:00:00|       green|   628.8000000000001|                 27.5|                    4.0|                       0.0|                       55.08|                    5.699999999999999|           721.0799999999999|                                 0.0|        1.1428571428571428|      7.0195454545454545|\n|         116|2020-01-01 00:00:00|       green|   78706.70999999996|              3319.25|                 2632.5|         5847.950000000004|          1467.9299999999985|                   1756.4999999999027|           96138.29000000306|                              2793.0|         1.212485896953742|       3.068908940397343|\n|          90|2020-01-01 00:00:00|       green|             2921.62|                165.0|                   19.0|                       0.0|          190.56000000000006|                   25.800000000000022|          3321.9800000000005|                                 0.0|                       1.2|       8.451627906976745|\n|         101|2020-01-01 00:00:00|       green|   8472.769999999997|                460.5|                   38.0|        15.219999999999999|          232.56000000000006|                    70.49999999999986|           9289.550000000001|                                2.75|        1.0512820512820513|       9.081153846153853|\n|         227|2020-01-01 00:00:00|       green|  14857.100000000011|               776.75|                   94.5|        21.569999999999997|          271.57000000000005|                   137.09999999999937|          16173.289999999986|                                 0.0|        1.0743801652892562|       7.235522088353416|\n|         180|2020-01-01 00:00:00|       green|  4024.4600000000005|               210.75|                   26.0|                     20.24|           84.88999999999999|                    43.20000000000006|           4414.240000000002|                                2.75|        1.3396226415094339|       5.324473684210526|\n|         190|2020-01-01 00:00:00|       green|  3786.0199999999995|                175.0|                   76.5|        242.16000000000008|                       24.48|                    62.69999999999993|           4419.360000000001|                               35.75|        1.4244604316546763|      3.5506912442396326|\n|         264|2008-12-01 00:00:00|       green|                 0.0|                  0.0|                    0.0|                       0.0|                         0.0|                                  0.0|                         0.0|                                 0.0|                       1.0|                     0.0|\n|         119|2020-01-01 00:00:00|       green|  21224.650000000012|               1391.0|                  186.5|                     74.62|           730.0800000000004|                    245.7000000000011|           23870.19999999996|                               13.75|        1.1612903225806452|       5.585108695652177|\n|          26|2020-01-01 00:00:00|       green|   27098.79000000005|              1743.25|                  221.5|        121.72999999999999|           551.2800000000002|                   292.50000000000233|           30034.89999999993|                                 0.0|        1.1054852320675106|       4.771315789473685|\n|         120|2020-01-01 00:00:00|       green|   692.4300000000001|                 5.25|                    7.0|        21.970000000000002|                       12.24|                    8.399999999999999|           752.7899999999998|                                 5.5|        1.1935483870967742|               4.6365625|\n+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+--------------------------+------------------------+\nonly showing top 20 rows\n\n\n\n                                                                                \n\n\nAnd save our results :\n\ndf_result.coalesce(1).write.parquet('data/report/revenue/', mode='overwrite')\n\n                                                                                \n\n\n\n\n\nrevenue_report.PNG\n\n\nThe coalesce() is used to decrease the number of partitions in an efficient way :\n\n!ls -lh data/report/revenue\n\ntotal 516K\n-rw-r--r-- 1 stephen137 stephen137    0 Mar 30 15:37 _SUCCESS\n-rw-r--r-- 1 stephen137 stephen137 513K Mar 30 15:37 part-00000-ee412a0e-dd1d-4ab7-a912-c9ebe84034ec-c000.snappy.parquet\n\n\n\n\n\ncoalesce.PNG\n\n\nNote that we haven’t written the results to a data warehouse yet. We are still effectively operating in our data lake, with a bunch of files.\n\n\n\n5.4.1 Anatomy of a Spark Cluster\nA Spark cluster is a group of computers (also known as nodes) that work together to process and analyze large sets of data. Spark is an open-source, distributed computing system that can be used for big data processing, machine learning, and other data-intensive tasks. The cluster is managed by a master node that coordinates the work of the other nodes, and distributes the data among them. This allows Spark to process and analyze large amounts of data quickly and efficiently.\nSpark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).\nSpecifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos, YARN or Kubernetes), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.\n\n\n\ncluster_architecture.PNG\n\n\n\n\n\nspark_execution.PNG\n\n\nCluster Manager Types The system currently supports several cluster managers:\n\nStandalone – a simple cluster manager included with Spark that makes it easy to set up a cluster.\nHadoop YARN – the resource manager in Hadoop 2 and 3.\nKubernetes – an open-source system for automating deployment, scaling, and management of containerized applications.\n\nFor a comprehensive overview take a look at this useful guide with worked examples.\nCluster and partitions\nSpark/PySpark partitioning is a way to split the data into multiple partitions so that you can execute transformations on multiple partitions in parallel which results in faster job completion. You can also write partitioned data into a file system (multiple sub-directories) for faster reads by downstream systems.\n\n\n\nspark_components.PNG\n\n\n\n\n\nspark_execution_plan.PNG\n\n\nThe following table summarizes terms you’ll see used to refer to cluster concepts:\n\n\n\ncluster_glossary.PNG\n\n\n\n\n5.4.2 GroupBy in Spark\nLet’s now look at how Spark implements GroupBy :\n\n# import required packages\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n# our entry point to Spark\nspark = SparkSession.builder.getOrCreate()\nspark\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.2\n              Master\n                local[*]\n              AppName\n                test\n            \n        \n        \n            \n        \n\n\n\n# load in green taxi data\ndf_green = spark.read.parquet('data/pq/green/*/*')\ndf_green.createOrReplaceTempView(\"green\")\n\n\n# GroupBy query - green taxi\ndf_green_revenue = spark.sql(\"\"\"\nSELECT\n    date_trunc('hour', lpep_pickup_datetime) AS hour,\n    PULocationID AS zone,\n\n    SUM(total_amount) AS amount,\n    COUNT(1) AS number_records\nFROM\n    green\nWHERE\n    lpep_pickup_datetime >= '2020-01-01 00:00:00'\nGROUP BY\n    1, 2\n\"\"\")\n\n\n# Write our result to parquet\ndf_green_revenue \\\n    .repartition(20) \\\n    .write.parquet('data/report/revenue/green', mode='overwrite')\n\n\ndf_green_revenue.show(10)\n\n+-------------------+----+------------------+--------------+\n|               hour|zone|            amount|number_records|\n+-------------------+----+------------------+--------------+\n|2020-01-24 09:00:00|  81|             59.49|             2|\n|2020-01-04 21:00:00|  25|            513.83|            32|\n|2020-01-10 19:00:00|  66| 545.6800000000002|            27|\n|2020-01-30 07:00:00|  75| 556.6600000000001|            40|\n|2020-01-18 01:00:00| 260|            144.56|            12|\n|2020-01-12 08:00:00| 177|31.090000000000003|             2|\n|2020-01-20 21:00:00| 166|            133.28|            12|\n|2020-01-03 04:00:00|  14|            105.34|             2|\n|2020-01-30 20:00:00|  74| 766.4400000000002|            58|\n|2020-01-02 16:00:00|  66|             257.0|            12|\n+-------------------+----+------------------+--------------+\nonly showing top 10 rows\n\n\n\n\n# load in yellow  taxi data\ndf_yellow = spark.read.parquet('data/pq/yellow/*/*')\ndf_yellow.createOrReplaceTempView(\"yellow\")\n\n\n# GroupBy query - yellow taxi\ndf_yellow_revenue = spark.sql(\"\"\"\nSELECT\n    date_trunc('hour', tpep_pickup_datetime) AS hour,\n    PULocationID AS zone,\n\n    SUM(total_amount) AS amount,\n    COUNT(1) AS number_records\nFROM\n    yellow\nWHERE\n    tpep_pickup_datetime >= '2020-01-01 00:00:00'\nGROUP BY\n    1, 2\n\"\"\")\n\n\n# Write our result to parquet\ndf_yellow_revenue \\\n    .repartition(20) \\\n    .write.parquet('data/report/revenue/yellow', mode='overwrite')\n\n\ndf_yellow_revenue.show(10)\n\n[Stage 17:================================================>       (18 + 3) / 21]\n\n\n+-------------------+----+------------------+--------------+\n|               hour|zone|            amount|number_records|\n+-------------------+----+------------------+--------------+\n|2020-01-03 19:00:00| 142| 6023.089999999995|           403|\n|2020-01-26 14:00:00| 239| 6541.649999999994|           437|\n|2020-01-09 01:00:00| 100|            653.56|            37|\n|2020-01-31 18:00:00| 237|12689.400000000009|           810|\n|2020-01-04 03:00:00| 246|2092.5400000000004|           121|\n|2020-01-14 09:00:00| 239| 4882.359999999997|           298|\n|2020-01-27 16:00:00| 162| 7989.979999999996|           452|\n|2020-01-17 20:00:00| 170| 6884.189999999997|           407|\n|2020-01-23 15:00:00| 142| 5378.829999999997|           341|\n|2020-01-27 06:00:00|  24|            536.49|            23|\n+-------------------+----+------------------+--------------+\nonly showing top 10 rows\n\n\n\n                                                                                \n\n\n\nWhat exactly is Spark doing ?\n\n\n\ngroupby_stage_1.PNG\n\n\nShuffling\nShuffling is a mechanism Spark uses to redistribute the data across different executors and even across machines. Spark shuffling triggers for transformation operations like reducebyKey(), join(), groupBy() etc.\nSpark Shuffle is an expensive operation since it involves the following :\n\ndisk I/O\ndata serialization and deserialization\nnetwork I/O\n\nWhen creating an RDD, Spark doesn’t necessarily store the data for all keys in a partition since at the time of creation there is no way we can set the key for the data set. Hence, when we run the reduceByKey() operation to aggregate the data on keys, Spark does the following :\n\nfirst runs map tasks on all partitions which groups all values for a single key\nthe results of the map tasks are kept in memory\nwhen results do not fit in memory, Spark stores the data on a disk\nshuffles the mapped data across partitions, sometimes it also stores the shuffled data into a disk for reuse when it needs to recalculate\nrun the garbage collection\nfinally runs reduce tasks on each partition based on key\n\n\n\n\ngroupby_stage_2.PNG\n\n\n\n\n\n5.4.3 Joins in Spark\njoining our yellow and green tables by hour and by zone\n\n\n\njoin.PNG\n\n\nLet’s use the results saved previously :\n\ndf_green_revenue = spark.read.parquet('data/report/revenue/green')\ndf_yellow_revenue = spark.read.parquet('data/report/revenue/yellow')\n\nRename the columns :\n\ndf_green_revenue_tmp = df_green_revenue \\\n    .withColumnRenamed('amount', 'green_amount') \\\n    .withColumnRenamed('number_records', 'green_number_records')\n\ndf_yellow_revenue_tmp = df_yellow_revenue \\\n    .withColumnRenamed('amount', 'yellow_amount') \\\n    .withColumnRenamed('number_records', 'yellow_number_records')\n\nAnd join the two tables :\n\ndf_join = df_green_revenue_tmp.join(df_yellow_revenue_tmp, on=['hour', 'zone'], how='outer')\n\nWrite the result to parquet and show the result :\n\ndf_join.write.parquet('data/report/revenue/total', mode='overwrite')\n\ndf_join.show(5)\n\n\n\n\njoin_parquet.PNG\n\n\n\nWhat exactly is Spark doing ?\n\n\n\nmerge_sort.PNG\n\n\nJoining a large table to a small table\nOnce again we can work with our previous result :\n\ndf_join = spark.read.parquet('data/report/revenue/total')\n\n\ndf_join.printSchema()\n\nroot\n |-- hour: timestamp (nullable = true)\n |-- zone: integer (nullable = true)\n |-- green_amount: double (nullable = true)\n |-- green_number_records: long (nullable = true)\n |-- yellow_amount: double (nullable = true)\n |-- yellow_number_records: long (nullable = true)\n\n\n\n\ndf_join.show(5)\n\n+-------------------+----+------------------+--------------------+------------------+---------------------+\n|               hour|zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|\n+-------------------+----+------------------+--------------------+------------------+---------------------+\n|2020-01-01 00:00:00|  13|              null|                null|1214.8000000000002|                   56|\n|2020-01-01 00:00:00|  48|              null|                null|10773.360000000004|                  455|\n|2020-01-01 00:00:00|  76|143.77999999999997|                   4|             35.51|                    2|\n|2020-01-01 00:00:00| 130|            133.35|                   7|              null|                 null|\n|2020-01-01 00:00:00| 186|              null|                null| 4011.449999999998|                  188|\n+-------------------+----+------------------+--------------------+------------------+---------------------+\nonly showing top 5 rows\n\n\n\nLet’s find out more about the zone column. The information is stored in another table which I have already downloaded :\n\ndf_zones = spark.read.parquet('zones/')\n\n\ndf_zones.printSchema()\n\nroot\n |-- LocationID: string (nullable = true)\n |-- Borough: string (nullable = true)\n |-- Zone: string (nullable = true)\n |-- service_zone: string (nullable = true)\n\n\n\n\ndf_zones.show(5)\n\n+----------+-------------+--------------------+------------+\n|LocationID|      Borough|                Zone|service_zone|\n+----------+-------------+--------------------+------------+\n|         1|          EWR|      Newark Airport|         EWR|\n|         2|       Queens|         Jamaica Bay|   Boro Zone|\n|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n|         4|    Manhattan|       Alphabet City| Yellow Zone|\n|         5|Staten Island|       Arden Heights|   Boro Zone|\n+----------+-------------+--------------------+------------+\nonly showing top 5 rows\n\n\n\nLet’s join the df_zones table to our df_join table :\n\ndf_result = df_join.join(df_zones, df_join.zone == df_zones.LocationID)\n\n\ndf_result.show(5)\n\n+-------------------+----+------------------+--------------------+------------------+---------------------+----------+---------+--------------------+------------+\n|               hour|zone|      green_amount|green_number_records|     yellow_amount|yellow_number_records|LocationID|  Borough|                Zone|service_zone|\n+-------------------+----+------------------+--------------------+------------------+---------------------+----------+---------+--------------------+------------+\n|2020-01-01 00:00:00|  13|              null|                null|1214.8000000000002|                   56|        13|Manhattan|   Battery Park City| Yellow Zone|\n|2020-01-01 00:00:00|  48|              null|                null|10773.360000000004|                  455|        48|Manhattan|        Clinton East| Yellow Zone|\n|2020-01-01 00:00:00|  76|143.77999999999997|                   4|             35.51|                    2|        76| Brooklyn|       East New York|   Boro Zone|\n|2020-01-01 00:00:00| 130|            133.35|                   7|              null|                 null|       130|   Queens|             Jamaica|   Boro Zone|\n|2020-01-01 00:00:00| 186|              null|                null| 4011.449999999998|                  188|       186|Manhattan|Penn Station/Madi...| Yellow Zone|\n+-------------------+----+------------------+--------------------+------------------+---------------------+----------+---------+--------------------+------------+\nonly showing top 5 rows\n\n\n\nLet’s drop the LocationID, and zone columns as these are redundant :\n\ndf_result.drop('LocationID', 'zone').show(5)\n\n+-------------------+------------------+--------------------+------------------+---------------------+---------+------------+\n|               hour|      green_amount|green_number_records|     yellow_amount|yellow_number_records|  Borough|service_zone|\n+-------------------+------------------+--------------------+------------------+---------------------+---------+------------+\n|2020-01-01 00:00:00|              null|                null|1214.8000000000002|                   56|Manhattan| Yellow Zone|\n|2020-01-01 00:00:00|              null|                null|10773.360000000004|                  455|Manhattan| Yellow Zone|\n|2020-01-01 00:00:00|143.77999999999997|                   4|             35.51|                    2| Brooklyn|   Boro Zone|\n|2020-01-01 00:00:00|            133.35|                   7|              null|                 null|   Queens|   Boro Zone|\n|2020-01-01 00:00:00|              null|                null| 4011.449999999998|                  188|Manhattan| Yellow Zone|\n+-------------------+------------------+--------------------+------------------+---------------------+---------+------------+\nonly showing top 5 rows\n\n\n\nand write our results to parquet :\n\ndf_result.drop('LocationID', 'zone').write.parquet('tmp/revenue-zones')\n\n\n\nWhat exactly is Spark doing ?\n\n\n\nzones.PNG\n\n\n\neach executor processes a partition of the Revenue DataFrame\nthe zones DataFrame is a small table, so each executor gets a copy of the entire DataFrame and merges it with their partition of the Revenue DataFrame within memory\nSpark can broadcast a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster\nAfter the small DataFrame is broadcasted, Spark can perform a join without shuffling any of the data in the large DataFrame\nSpark broadcast joins are perfect for joining a large DataFrame with a small DataFrame\n\nThis is extremely fast.\n\n\n\n5.6.1 Connecting to Google Cloud Storage\nWe can move the parquet files we created using Spark to Google Cloud Storage (GCS) by running the follwowing from the command line :\ngsutil -m cp -r pq/ gs://<google_cloud_storage_bucket_name>/<bucket_folder_name>\nIn my case :\n\n\n\nupload_to_GCS.PNG\n\n\nAnd we can see that my bucket now includes a pq folder and has been loaded :\n\n\n\nGCS.PNG\n\n\nThe Google Cloud Storage connector docs provide detailed guidance, but essentially in order to enable PySpark to speak to Google Cloud we need to configure a .jar file which can be downloaded to any location (e.g. the lib folder) from the command line :\ngsutil cp gs://hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar gcs-connector-hadoop3-2.2.5.jar\n\n\n\nhadoop_connector.PNG\n\n\nWe now need to re-configure our normal entry point into Spark :\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n\nWe first need to specify the location of our Google Credential json file. Adjust your own details accordingly :\n\ncredentials_location = '/home/stephen137/.google/taxi-rides-ny-137-abb685b49ccb.json'\n\nconf = SparkConf() \\\n    .setMaster('local[*]') \\\n    .setAppName('test') \\\n    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n\nThe next thing we need to do is create a Spark context :\n\nsc = SparkContext(conf=conf)\n\nhadoop_conf = sc._jsc.hadoopConfiguration()\n\nhadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nhadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nhadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\nhadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n\n23/03/31 14:55:39 WARN Utils: Your hostname, DESKTOP-1UDJOCI resolves to a loopback address: 127.0.1.1; using 172.24.13.131 instead (on interface eth0)\n23/03/31 14:55:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n23/03/31 14:55:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\nOnce we have run this, we can now build our SparkSession using the parameters we instantiated in the previous step :\n\nspark = SparkSession.builder \\\n    .config(conf=sc.getConf()) \\\n    .getOrCreate()\n\n\nspark\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.2\n              Master\n                local[*]\n              AppName\n                test\n            \n        \n        \n            \n        \n\n\nWe are now able to read our files straight from Google Cloud Storage! Let’s do something simple to test our connection is working :\n\ndf_green = spark.read.parquet('gs://dtc_data_lake_taxi-rides-ny-137/pq/green/*/*')\n\n                                                                                \n\n\n\ndf_green.show()\n\n[Stage 1:>                                                          (0 + 1) / 1]\n\n\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|       2| 2020-01-12 18:15:04|  2020-01-12 18:19:52|                 N|         1|          41|          41|              1|         0.78|        5.5|  0.0|    0.5|      1.58|         0.0|     null|                  0.3|        7.88|           1|        1|                 0.0|\n|       2| 2020-01-31 20:24:10|  2020-01-31 20:31:51|                 N|         1|         173|          70|              1|         0.98|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|           2|        1|                 0.0|\n|       2| 2020-01-07 08:16:53|  2020-01-07 08:41:39|                 N|         1|          74|         236|              1|          2.7|       16.0|  0.0|    0.5|      3.91|         0.0|     null|                  0.3|       23.46|           1|        1|                2.75|\n|       1| 2020-01-15 14:47:15|  2020-01-15 14:54:34|                 N|         1|          25|          66|              1|          0.8|        6.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|         7.3|           2|        1|                 0.0|\n|    null| 2020-01-31 10:08:00|  2020-01-31 10:20:00|              null|      null|         259|          51|           null|         2.33|      22.49| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       25.54|        null|     null|                null|\n|       2| 2020-01-18 17:46:45|  2020-01-18 18:04:08|                 N|         1|         177|         188|              1|         2.62|       12.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        13.3|           1|        1|                 0.0|\n|       2| 2020-01-17 20:08:44|  2020-01-17 20:18:47|                 N|         1|          65|          97|              1|         1.13|        8.0|  0.5|    0.5|      1.86|         0.0|     null|                  0.3|       11.16|           1|        1|                 0.0|\n|    null| 2020-01-13 10:47:00|  2020-01-13 10:54:00|              null|      null|         165|         123|           null|         1.36|      17.51| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       20.56|        null|     null|                null|\n|    null| 2020-01-07 15:36:00|  2020-01-07 16:11:00|              null|      null|         170|         220|           null|        11.15|       46.0| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       49.05|        null|     null|                null|\n|    null| 2020-01-10 11:47:00|  2020-01-10 12:03:00|              null|      null|          74|          41|           null|         1.78|       8.76|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        9.56|        null|     null|                null|\n|       1| 2020-01-08 20:17:48|  2020-01-08 20:23:24|                 Y|         1|          75|          41|              1|          1.0|        6.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         7.3|           2|        1|                 0.0|\n|       2| 2020-01-28 10:57:21|  2020-01-28 11:15:13|                 N|         1|          74|         151|              1|         2.75|       13.5|  0.0|    0.5|      2.86|         0.0|     null|                  0.3|       17.16|           1|        1|                 0.0|\n|       1| 2020-01-16 18:02:21|  2020-01-16 18:11:21|                 N|         1|          41|          74|              1|          1.1|        7.5|  1.0|    0.5|       2.3|         0.0|     null|                  0.3|        11.6|           1|        1|                 0.0|\n|       2| 2020-01-07 14:03:38|  2020-01-07 14:17:02|                 N|         1|         116|          74|              1|         3.81|       13.5|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        14.3|           2|        1|                 0.0|\n|       2| 2020-01-14 09:52:46|  2020-01-14 10:06:41|                 N|         1|         152|         244|              1|         1.85|       11.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        11.8|           1|        1|                 0.0|\n|    null| 2020-01-23 05:40:00|  2020-01-23 06:13:00|              null|      null|          71|          88|           null|         9.14|      30.99| 2.75|    0.0|       0.0|         0.0|     null|                  0.3|       34.04|        null|     null|                null|\n|       2| 2020-01-23 10:17:52|  2020-01-23 10:24:02|                 N|         1|          43|         236|              1|         1.04|        6.0|  0.0|    0.5|       0.0|         0.0|     null|                  0.3|        9.55|           2|        1|                2.75|\n|       2| 2020-01-09 21:09:46|  2020-01-09 21:14:35|                 N|         1|          65|          49|              1|         1.14|        5.5|  0.5|    0.5|       1.2|         0.0|     null|                  0.3|         8.0|           1|        1|                 0.0|\n|    null| 2020-01-08 20:53:00|  2020-01-08 21:00:00|              null|      null|         254|         254|           null|         1.15|        8.0|  0.0|    0.0|       0.0|         0.0|     null|                  0.3|         8.3|        null|     null|                null|\n|       2| 2020-01-02 09:04:51|  2020-01-02 09:11:18|                 N|         1|         116|         244|              1|         0.92|        6.0|  0.0|    0.0|       0.0|         0.0|     null|                  0.0|         6.0|           2|        2|                 0.0|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\nonly showing top 20 rows\n\n\n\n                                                                                \n\n\n\ndf_green.count()\n\n                                                                                \n\n\n2304517\n\n\nExcellent. Spark and GCS are talking to each other :)\n\n\n5.6.2 Creating a Local Clark Cluster\nIf we consult the official documentation we can see that it is very simple to start a cluster manually. From the command line check where Spark has been downloaded using :\necho $SPARK_HOME\nand then navigate to that directory and execute the following :\n./sbin/start-master.sh\nNote that when we create a local session it runs at localhost:8080.\nKilling a localhost session\nNote, on Linux or Ubuntu, to find out the PID of process bound on a particular port you can use :\nfuser <port_number>/tcp\nand then :\nfuser -k <port_number>/tcp\nto kill that session.\n\n\n\nlocal_spark.PNG\n\n\n\n\n\nspark_master.PNG\n\n\nAnd our Spark Session configuration is as follows :\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n\nspark = SparkSession.builder \\\n    .master(\"\") \\\n    .appName('test') \\\n    .getOrCreate()\n\n\nspark\n\n\n            \n                SparkSession - in-memory\n                \n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.2\n              Master\n                spark://DESKTOP-1UDJOCI.localdomain:7077\n              AppName\n                test\n            \n        \n        \n            \n        \n\n\n\ndf_green = spark.read.parquet('data/pq/green/*/*')\n\n                                                                                \n\n\nNote that although we have started a cluster we have zero workers, we only have a master.\n\n\n\nspark_master.PNG\n\n\nWe need an executor. To start a worker run the following from the command line :\n./sbin/start-worker.sh spark://DESKTOP-1UDJOCI.localdomain:7077\nNow we have a worker and our task has completed:\n\n\n\nworker.PNG\n\n\n\nTurning the NoteBook into a Python script\nTo convert a Jupyter Notebook to a Python script you simply run the following from the command line:\njupyter nbconvert --to=script <notebook_name>\nThen we can run the created 06_spark_sql_notebook.py file :\n\n#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder\n    .master(\"spark://DESKTOP-1UDJOCI.localdomain:7077\")\n    .appName('test')\n    .getOrCreate()\n\n# In[2]:\ndf_green = spark.read.parquet('data/pq/green/*/*')\n\n# In[3]:\ndf_green.count()\n\n# In[4]:\ndf_green = df_green\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime')\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\n# In[5]:\ndf_yellow = spark.read.parquet('data/pq/yellow/*/*')\n\n\n# In[6]:\ndf_yellow = df_yellow\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime')\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n\n# In[7]:\ncommon_colums = []\nyellow_columns = set(df_yellow.columns)\n\nfor col in df_green.columns:\n    if col in yellow_columns:\n        common_colums.append(col)\n\n# In[13]:\ncommon_colums = ['VendorID',\n     'pickup_datetime',\n     'dropoff_datetime',\n     'store_and_fwd_flag',\n     'RatecodeID',\n     'PULocationID',\n     'DOLocationID',\n     'passenger_count',\n     'trip_distance',\n     'fare_amount',\n     'extra',\n     'mta_tax',\n     'tip_amount',\n     'tolls_amount',\n     'improvement_surcharge',\n     'total_amount',\n     'payment_type',\n     'congestion_surcharge'\n]\n\n# In[8]:\nfrom pyspark.sql import functions as F\n\n# In[9]:\ndf_green_sel = df_green\n    .select(common_colums)\n    .withColumn('service_type', F.lit('green'))\n\n# In[10]:\ndf_yellow_sel = df_yellow\n    .select(common_colums)\n    .withColumn('service_type', F.lit('yellow'))\n\n# In[11]:\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n\n# In[12]:\ndf_trips_data.groupBy('service_type').count().show()\n\n# In[13]:\ndf_trips_data.columns\nls\n\n# In[14]:\n# df_trips_data.registerTempTable('trips_data') # Deprecated.\ndf_trips_data.createOrReplaceTempView(\"trips_data\")\n\n# In[15]:\ndf_result = spark.sql(\"\"\"\nSELECT\n    -- Reveneue grouping\n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month,\n    service_type,\n\n    -- Revenue calculation\n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    AVG(passenger_count) AS avg_montly_passenger_count,\n    AVG(trip_distance) AS avg_montly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n\n# In[19]:\ndf_result.coalesce(1).write.parquet('data/report/revenue/', mode='overwrite')\n\n\n!python 06_spark_sql_notebook.py\n\n23/04/03 11:02:47 WARN Utils: Your hostname, DESKTOP-1UDJOCI resolves to a loopback address: 127.0.1.1; using 172.21.104.92 instead (on interface eth0)\n23/04/03 11:02:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/04/03 11:02:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \n\n\nNow, if we go to the ~/Blog/posts/DE_Zoomcamp_Week_5/data/reportdirectory, we see that the revenue directory has just been created :\n\n\n\nrevenue_report_2.PNG\n\n\nLet’s paramaterize our script using argparse like we did in week 1, to allow us to configure parameters from the command line :\n06_spark_sql.py\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport argparse\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--input_green', required=True)\nparser.add_argument('--input_yellow', required=True)\nparser.add_argument('--output', required=True)\n\nargs = parser.parse_args()\n\ninput_green = args.input_green\ninput_yellow = args.input_yellow\noutput = args.output\n\n\nspark = SparkSession.builder \\\n    .appName('test') \\\n    .getOrCreate()\n\ndf_green = spark.read.parquet(input_green)\n\ndf_green = df_green \\\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\ndf_yellow = spark.read.parquet(input_yellow)\n\n\ndf_yellow = df_yellow \\\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n\n\ncommon_colums = [\n    'VendorID',\n    'pickup_datetime',\n    'dropoff_datetime',\n    'store_and_fwd_flag',\n    'RatecodeID',\n    'PULocationID',\n    'DOLocationID',\n    'passenger_count',\n    'trip_distance',\n    'fare_amount',\n    'extra',\n    'mta_tax',\n    'tip_amount',\n    'tolls_amount',\n    'improvement_surcharge',\n    'total_amount',\n    'payment_type',\n    'congestion_surcharge'\n]\n\n\n\ndf_green_sel = df_green \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('green'))\n\ndf_yellow_sel = df_yellow \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('yellow'))\n\n\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n\ndf_trips_data.registerTempTable('trips_data')\n\n\ndf_result = spark.sql(\"\"\"\nSELECT \n    -- Reveneue grouping \n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month, \n    service_type, \n\n    -- Revenue calculation \n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    AVG(passenger_count) AS avg_montly_passenger_count,\n    AVG(trip_distance) AS avg_montly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n\n\ndf_result.coalesce(1) \\\n    .write.parquet(output, mode='overwrite')\n\nNow, we can run our script with a set of parameters :\n\n! python 06_spark_sql.py \\\n    --input_green=data/pq/green/2020/*/ \\\n    --input_yellow=data/pq/yellow/2020/*/ \\\n    --output=data/report-2020\n\n23/04/03 11:13:44 WARN Utils: Your hostname, DESKTOP-1UDJOCI resolves to a loopback address: 127.0.1.1; using 172.21.104.92 instead (on interface eth0)\n23/04/03 11:13:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/04/03 11:13:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n/home/stephen137/mambaforge/lib/python3.10/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n                                                                                \n\n\nWe see that our report is created successfully :\n\n\n\nreport_2020.PNG\n\n\nImagine we have multiple clusters. It is not convenient to hardcode the .master(\"spark://DESKTOP-1UDJOCI.localdomain:7077\") so we will remove it from our script :\n06_spark_sql_notebook.py\n\nspark = SparkSession.builder \\\n    .appName('test') \\\n    .getOrCreate()\n\nWe will specifiy the master outside of our script from the command line by using\nspark-submit \nThe spark-submit script in Spark’s bin directory is used to launch applications on a cluster. See Submitting Applications for further information.\n\nURL=\"spark://DESKTOP-1UDJOCI.localdomain:7077\"\n\nspark-submit \\\n    --master=\"${URL}\" \\\n    06_spark_sql.py \\\n        --input_green=data/pq/green/2021/*/ \\\n        --input_yellow=data/pq/yellow/2021/*/ \\\n        --output=data/report-2021\n\nWe see that our report is created successfully :\n\n\n\nreport_2021.PNG\n\n\nBefore we finish, we have to stop the workers and stop the master. Navigate to the directory where Spark is installed on your and run the following from the command line :\n./sbin/stop-slave.sh\n\n./sbin/stop-worker.sh\n\n./sbin/stop-master.sh\n\n\n\nstop.PNG\n\n\n\n\n\nstop_UI.PNG\n\n\nWe have successfully closed down our session.\n\n\n\n5.6.3 Setting up a Dataproc Cluster\nGoogle Cloud Dataproc is a managed service for running Apache Hadoop and Spark jobs. It can be used for big data processing and machine learning.\nDataproc actually uses Compute Engine instances under the hood, but it takes care of the management details for you. It’s a layer on top that makes it easy to spin up and down clusters as you need them.\nThe main benefits are that:\n\nit is a managed service, so you don’t need a system administrator to set it up\nit is fast. You can spin up a cluster in about 90 seconds\nit is cheaper than building your own cluster because you only pay when jobs are running. You can spin up a Dataproc cluster when you need to run a job and shut it down afterward\nit is integrated with other Google Cloud services, including Cloud Storage, BigQuery, and Cloud Bigtable, so it’s easy to get data into and out of it\n\nSee Cloud Dataproc docs for a quickstart tour.\n\nCreate a cluster\nFirst, navigate to Dataproc on Google Cloud and enable the API if you haven’t already done so :\n\n\n\ndataproc_API.PNG\n\n\nThen create a cluster. We’ll use Compute Engine :\n\n\n\ndataproc_compute_engine.PNG\n\n\n\n\n\ndataproc_setup.PNG\n\n\n\n\n\ndataproc_components.PNG\n\n\nBy creating a cluster we also create a virtual machine, so it might take a while :\n\n\n\ncluster_running.PNG\n\n\n\n\n\ncluster_VM.PNG\n\n\nNow that our cluster is running let’s now try to submit a job there. With Dataproc, we don’t need to use the same instructions as before to establish the connection with Google Cloud Storage (GCS). Dataproc is already configured to access GCS.\nFirst, we need to to upload the following file to our data bucket :\n06_spark_sql.py file\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport argparse\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--input_green', required=True)\nparser.add_argument('--input_yellow', required=True)\nparser.add_argument('--output', required=True)\n\nargs = parser.parse_args()\n\ninput_green = args.input_green\ninput_yellow = args.input_yellow\noutput = args.output\n\n\nspark = SparkSession.builder \\\n    .appName('test') \\\n    .getOrCreate()\n\ndf_green = spark.read.parquet(input_green)\n\ndf_green = df_green \\\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\ndf_yellow = spark.read.parquet(input_yellow)\n\n\ndf_yellow = df_yellow \\\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n\n\ncommon_colums = [\n    'VendorID',\n    'pickup_datetime',\n    'dropoff_datetime',\n    'store_and_fwd_flag',\n    'RatecodeID',\n    'PULocationID',\n    'DOLocationID',\n    'passenger_count',\n    'trip_distance',\n    'fare_amount',\n    'extra',\n    'mta_tax',\n    'tip_amount',\n    'tolls_amount',\n    'improvement_surcharge',\n    'total_amount',\n    'payment_type',\n    'congestion_surcharge'\n]\n\n\n\ndf_green_sel = df_green \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('green'))\n\ndf_yellow_sel = df_yellow \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('yellow'))\n\n\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n\ndf_trips_data.registerTempTable('trips_data')\n\n\ndf_result = spark.sql(\"\"\"\nSELECT \n    -- Reveneue grouping \n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month, \n    service_type, \n\n    -- Revenue calculation \n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    AVG(passenger_count) AS avg_montly_passenger_count,\n    AVG(trip_distance) AS avg_montly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n\n\ndf_result.coalesce(1) \\\n    .write.parquet(output, mode='overwrite')\n\nWe can do this from the command line using :\ngsutil cp 06_spark_sql.py <data_bucket_path>/code/06_spark_sql.py>\n\n\n\nspark_sql_to_gcs.PNG\n\n\nNow SUBMIT JOB from within our cluster in Google Cloud, passing the following arguments:\n\n --input_green=gs://dtc_data_lake_taxi-rides-ny-137/pq/green/2021/*/ \n        --input_yellow=gs://dtc_data_lake_taxi-rides-ny-137/pq/yellow/2021/*/ \n        --output=gs://dtc_data_lake_taxi-rides-ny-137/report-2021   \n    \n\nMake sure you specify your own unique bucket path when doing this.\n\n\n\ndataproc_submit.PNG\n\n\nSo we have successfully submitted our job to a cluster we created on Google Cloud, and it did some compute and saved the results in our bucket :\n\n\n\ndataproc_bucket.PNG\n\n\n\n\nEquivalent REST\nIf we click on EQUIVALENT REST at the bottom left of the Job Configuration we can construct an equivalent API REST request or gcloud tool command to use inyour code or from the command line to create a cluster.\n\n\n\nequivalent_REST.PNG\n\n\nThe Equivalent REST response looks like this:\n\n{\n  \"reference\": {\n    \"jobId\": \"job-baedec74\",\n    \"projectId\": \"taxi-rides-ny-137\"\n  },\n  \"placement\": {\n    \"clusterName\": \"de-zoomcamp-cluster\"\n  },\n  \"status\": {\n    \"state\": \"DONE\",\n    \"stateStartTime\": \"2023-04-03T11:46:50.903406Z\"\n  },\n  \"yarnApplications\": [\n    {\n      \"name\": \"test\",\n      \"state\": \"FINISHED\",\n      \"progress\": 1,\n      \"trackingUrl\": \"http://de-zoomcamp-cluster-m:8088/proxy/application_1680518000410_0002/\"\n    }\n  ],\n  \"statusHistory\": [\n    {\n      \"state\": \"PENDING\",\n      \"stateStartTime\": \"2023-04-03T11:46:05.044918Z\"\n    },\n    {\n      \"state\": \"SETUP_DONE\",\n      \"stateStartTime\": \"2023-04-03T11:46:05.103569Z\"\n    },\n    {\n      \"state\": \"RUNNING\",\n      \"details\": \"Agent reported job success\",\n      \"stateStartTime\": \"2023-04-03T11:46:05.575809Z\"\n    }\n  ],\n  \"driverControlFilesUri\": \"gs://dataproc-staging-europe-central2-684134901955-zy3qc9tq/google-cloud-dataproc-metainfo/9b321dfc-9e87-4daa-8a26-df51150917c0/jobs/job-baedec74/\",\n  \"driverOutputResourceUri\": \"gs://dataproc-staging-europe-central2-684134901955-zy3qc9tq/google-cloud-dataproc-metainfo/9b321dfc-9e87-4daa-8a26-df51150917c0/jobs/job-baedec74/driveroutput\",\n  \"jobUuid\": \"86629eb7-f8b1-43eb-9371-7d955f572160\",\n  \"done\": true,\n  \"pysparkJob\": {\n    \"mainPythonFileUri\": \"gs://dtc_data_lake_taxi-rides-ny-137/code/06_spark_sql.py\",\n    \"args\": [\n      \"--input_green=gs://dtc_data_lake_taxi-rides-ny-137/pq/green/2021/*/\",\n      \"--input_yellow=gs://dtc_data_lake_taxi-rides-ny-137/pq/yellow/2021/*/\",\n      \"--output=gs://dtc_data_lake_taxi-rides-ny-137/report-2021\"\n    ]\n  }\n}\n\n\n\nSubmit a job with gcloud CLI\nSee Submit a job for details as to how to submit a job with Google Cloud SDK.\nFor example, to submit a job to a Dataproc cluster with gcloud CLI, run the following from the command line :\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-central2 \\\ngs://dtc_data_lake_taxi-rides-ny-137/code/06_spark_sql.py \\\n-- \\\n    --input_green=gs://dtc_data_lake_taxi-rides-ny-137/pq/green/2021/*/ \\\n    --input_yellow=gs://dtc_data_lake_taxi-rides-ny-137/pq/yellow/2021/*/ \\\n    --output=gs://dtc_data_lake_taxi-rides-ny-137/report-2021\nBefore submitting this command, you may find that you need to add the role Dataproc Administrator to the permissions created in the previous weeks. I had already set this up possibly :\n\n\n\ndataproc_gcloud.PNG\n\n\n\n\n\n5.6.4 Connecting Spark to BigQuery\nSometimes we want to write directly to our data warehouse, BigQuery. A template example of how to connect to Spark is included within the Google Docs which we will modify for our specific purposes. The template is included below :\n\n#!/usr/bin/env python\n\n\"\"\"BigQuery I/O PySpark example.\"\"\"\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n  .builder \\\n  .master('yarn') \\\n  .appName('spark-bigquery-demo') \\\n  .getOrCreate()\n\n# Use the Cloud Storage bucket for temporary BigQuery export data used\n# by the connector.\nbucket = \"[bucket]\"\nspark.conf.set('temporaryGcsBucket', bucket)\n\n# Load data from BigQuery.\nwords = spark.read.format('bigquery') \\\n  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n  .load()\nwords.createOrReplaceTempView('words')\n\n# Perform word count.\nword_count = spark.sql(\n    'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')\nword_count.show()\nword_count.printSchema()\n\n# Saving the data to BigQuery\nword_count.write.format('bigquery') \\\n  .option('table', 'wordcount_dataset.wordcount_output') \\\n  .save()\n\nNavigate to the buckets created by Dataproc :\n\n\n\ndataproc_bucket_BQ.PNG\n\n\nand modify the 06_spark_sql_big_query.py file to reference this Dataproc bucket :\n06_spark_sql_big_query.py\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport argparse\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--input_green', required=True)\nparser.add_argument('--input_yellow', required=True)\nparser.add_argument('--output', required=True)\n\nargs = parser.parse_args()\n\ninput_green = args.input_green\ninput_yellow = args.input_yellow\noutput = args.output\n\n\nspark = SparkSession.builder \\\n    .appName('test') \\\n    .getOrCreate()\n\n# First modification.\n# Use the Cloud Storage bucket for temporary BigQuery export data used\n# by the connector.\nbucket = \"dataproc-temp-europe-central2-684134901955-awcbvroo\"\nspark.conf.set('temporaryGcsBucket', bucket)\n\ndf_green = spark.read.parquet(input_green)\n\ndf_green = df_green \\\n    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n\ndf_yellow = spark.read.parquet(input_yellow)\n\n\ndf_yellow = df_yellow \\\n    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n\n\ncommon_colums = [\n    'VendorID',\n    'pickup_datetime',\n    'dropoff_datetime',\n    'store_and_fwd_flag',\n    'RatecodeID',\n    'PULocationID',\n    'DOLocationID',\n    'passenger_count',\n    'trip_distance',\n    'fare_amount',\n    'extra',\n    'mta_tax',\n    'tip_amount',\n    'tolls_amount',\n    'improvement_surcharge',\n    'total_amount',\n    'payment_type',\n    'congestion_surcharge'\n]\n\n\n\ndf_green_sel = df_green \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('green'))\n\ndf_yellow_sel = df_yellow \\\n    .select(common_colums) \\\n    .withColumn('service_type', F.lit('yellow'))\n\n\ndf_trips_data = df_green_sel.unionAll(df_yellow_sel)\n\ndf_trips_data.registerTempTable('trips_data')\n\n\ndf_result = spark.sql(\"\"\"\nSELECT\n    -- Revenue grouping\n    PULocationID AS revenue_zone,\n    date_trunc('month', pickup_datetime) AS revenue_month,\n    service_type,\n\n    -- Revenue calculation\n    SUM(fare_amount) AS revenue_monthly_fare,\n    SUM(extra) AS revenue_monthly_extra,\n    SUM(mta_tax) AS revenue_monthly_mta_tax,\n    SUM(tip_amount) AS revenue_monthly_tip_amount,\n    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n    SUM(total_amount) AS revenue_monthly_total_amount,\n    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n\n    -- Additional calculations\n    AVG(passenger_count) AS avg_montly_passenger_count,\n    AVG(trip_distance) AS avg_montly_trip_distance\nFROM\n    trips_data\nGROUP BY\n    1, 2, 3\n\"\"\")\n\n# Second modification.\n# Saving the data to BigQuery\ndf_result.write.format('bigquery') \\\n    .option('table', output) \\\n    .save()\n\nUpload this script to our data bucket from the command line using :\ngsutil cp 06_spark_sql_big_query.py gs://dtc_data_lake_taxi-rides-ny-137/code/06_spark_sql_big_query.py  \nhe BigQuery schema we already have is trips_data_all.\nSo, we slightly modify the script created previously to create the report in BigQuery by indicating the schema name for the report:\n--output=trips_data_all.reports-2020\nWe also need to specify the connector jar :\n--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\nAnd run the following from the command line :\n\ngcloud dataproc jobs submit pyspark \\\n    --cluster=de-zoomcamp-cluster \\\n    --region=europe-central2 \\\n    --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar \\\n    gs://dtc_data_lake_taxi-rides-ny-137/code/06_spark_sql_big_query.py \\\n    -- \\\n        --input_green=gs://dtc_data_lake_taxi-rides-ny-137/pq/green/2020/*/ \\\n        --input_yellow=gs://dtc_data_lake_taxi-rides-ny-137/pq/yellow/2020/*/ \\\n        --output=trips_data_all.reports-2020\n\n\n\n\ngcloud_BQ.PNG\n\n\nThat seems to have run successfully. Let’s check BigQuery :\n\n\n\ngcloud_cluster_BQ_reports_2020.PNG\n\n\nOur table has been created and we can see the first 10 rows."
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html",
    "title": "Image Processing with Python",
    "section": "",
    "text": "As computer systems have become faster and more powerful, and cameras and other imaging systems have become commonplace in many other areas of life, the need has grown for researchers to be able to process and analyse image data. Considering the large volumes of data that can be involved - high-resolution images that take up a lot of disk space/virtual memory, and/or collections of many images that must be processed together - and the time-consuming and error-prone nature of manual processing, it can be advantageous or even necessary for this processing and analysis to be automated as a computer program.\nThis blog introduces an open source toolkit for processing image data: the Python programming language and the scikit-image (skimage) library. With careful experimental design, Python code can be a powerful instrument in answering many different kinds of questions.\n\n\nAutomated processing can be used to analyse many different properties of an image, including the distribution and change in colours in the image, the number, size, position, orientation, and shape of objects in the image, and even - when combined with machine learning techniques for object recognition - the type of objects in the image.\nSome examples of image processing methods applied in research include:\n\nimaging a Black Hole\nestimating the population of Emperor Penguins\nthe global-scale analysis of marine plankton diversity\nsegmentation of liver and vessels from CT images\n\nThis blog aims to provide a thorough grounding in the fundamental concepts and skills of working with image data in Python. Most of the examples used focus on one particular class of image processing technique, morphometrics, but what we will learn can be used to solve a much wider range of problems.\n\n\n\nMorphometrics involves counting the number of objects in an image, analyzing the size of the objects, or analyzing the shape of the objects. For example, we might be interested in automatically counting the number of bacterial colonies growing in a Petri dish.\n\nWe could use image processing to find the colonies, count them, and then highlight their locations on the original image.\n\nNote that we can easily manually count the number of bacteria colonies shown in the morphometric example above. So, why should we learn how to write a Python program to do a task we could easily perform with our own eyes? There are at least two reasons to learn how to perform tasks like these with Python and skimage:\n\nWhat if there are many more bacteria colonies in the Petri dish? For example, suppose the image looked like this: Manually counting the colonies in that image would present more of a challenge. A Python program using skimage could count the number of colonies more accurately, and much more quickly, than a human could.\nWhat if we have hundreds, or thousands, of images to consider? Imagine having to manually count colonies on several thousand images like those above. A Python program using skimage could move through all of the images in seconds; how long would a graduate student require to do the task? Which process would be more accurate and repeatable?\n\nAs we can see, the simple image processing / computer vision techniques we will learn during this blog can be very valuable tools for scientific research. As we progress, we will learn image analysis methods useful for many different scientific problems. Let’s get started, by learning some basics about how images are represented and stored digitally.\n\n\n\n\n\n\nKey Points:\n\n\n\n\n\nSimple Python and skimage (scikit-image) techniques can be used to solve genuine image analysis problems\n\nMorphometric problems involve the number, shape, and / or size of the objects in an image"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#image-basics",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#image-basics",
    "title": "Image Processing with Python",
    "section": "1. Image Basics",
    "text": "1. Image Basics\nThe images we see on hard copy, view with our electronic devices, or process with our programs are represented and stored in the computer as numeric abstractions, approximations of what we see with our eyes in the real world. Before we begin to learn how to process images with Python programs, we need to spend some time understanding how these abstractions work.\n\nPixels\nIt is important to realise that images are stored as rectangular arrays of hundreds, thousands, or millions of discrete “picture elements,” otherwise known as pixels. Each pixel can be thought of as a single square point of coloured light. For example, consider this image of a maize seedling, with a square area designated by a red box:\n\nNow, if we zoomed in close enough to see the pixels in the red box, we would see something like this:\n\n\n\nmaize-seedling-enlarged.jpg\n\n\nNote that each square in the enlarged image area - each pixel - is all one colour, but that each pixel can have a different colour from its neighbours. Viewed from a distance, these pixels seem to blend together to form the image we see.\n\n\nWorking with Pixels\nAs noted, in practice, real world images will typically be made up of a vast number of pixels, and each of these pixels will be one of potentially millions of colours. While we will deal with pictures of such complexity shortly, let’s start our exploration with 15 pixels in a 5 X 3 matrix with 2 colours and work our way up to that complexity.\n\n\n\n\n\n\nMatrices, arrays, images and pixels\n\n\n\nThe matrix is mathematical concept - numbers evenly arranged in a rectangle. This can be a two dimensional rectangle, like the shape of the screen you’re looking at now. Or it could be a three dimensional equivalent, a cuboid, or have even more dimensions, but always keeping the evenly spaced arrangement of numbers. In computing, array refers to a structure in the computer’s memory where data is stored in evenly-spaced elements. This is strongly analogous to a matrix. A numpy array is a type of variable (a simpler example of a type is an integer). For our purposes, the distinction between matrices and arrays is not important, we don’t really care how the computer arranges our data in its memory. The important thing is that the computer stores values describing the pixels in images, as arrays. And the terms matrix and array can be used interchangeably.\n\n\nFirst, the necessary imports:\n\n# Python libraries for learning and performing image processing\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipympl\nimport imageio.v3 as iio\nimport skimage\n\n\n\n\n\n\n\nImport Statements in Python\n\n\n\nIn Python, the import statement is used to load additional functionality into a program. This is necessary when we want our code to do something more specialised, which cannot easily be achieved with the limited set of basic tools and data structures available in the default Python environment. Additional functionality can be loaded as a single function or object, a module defining several of these, or a library containing many modules. We will encounter several different forms of import statement.\n\n\n\n# form 1, load whole skimage library\nimport skimage  \n\n# form 2, load skimage.draw module only\nimport skimage.draw  \n\n# form 3, load only the disk function\nfrom skimage.draw import disk\n\n# form 4, load all of numpy into an object called np\nimport numpy as np           \n\nNow that we have our libraries loaded, we will run a Jupyter Magic Command that will ensure our images display in our Jupyter document with pixel information that will help us more efficiently run commands later in the session.\n\n%matplotlib inline\n\nWith that taken care of, let’s load our image data from disk using the imread function from the imageio.v3 module and display it using the imshow function from the matplotlib.pyplot module. imageio is a Python library for reading and writing image data. imageio.v3 is specifying that we want to use version 3 of imageio. This version has the benefit of supporting nD (multidimensional) image data natively (think of volumes, movies).\n\n\n\n\n\n\nWhy not use skimage.io.imread()\n\n\n\nThe skimage library has its own function to read an image, so you might be asking why we don’t use it here. Actually, skimage.io.imread() uses iio.imread() internally when loading an image into Python. It is certainly something you may use as you see fit in your own code. In this lesson, we use the imageio library to read or write (save) images, while skimage is dedicated to performing operations on the images. Using imageio gives us more flexibility, especially when it comes to handling metadata.\n\n\n\nfrom skimage import data, io\nfrom matplotlib import pyplot as plt\n\nimage = iio.imread(uri=\"Images/eight.tif\")\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f29490f2fe0>\n\n\n\n\n\nYou might be thinking, “That does look vaguely like an eight, and I see two colours but how can that be only 15 pixels”. The display of the eight you see does use a lot more screen pixels to display our eight so large, but that does not mean there is information for all those screen pixels in the file. All those extra pixels are a consequence of our viewer creating additional pixels through interpolation. It could have just displayed it as a tiny image using only 15 screen pixels if the viewer was designed differently.\nWhile many image file formats contain descriptive metadata that can be essential, the bulk of a picture file is just arrays of numeric information that, when interpreted according to a certain rule set, become recognizable as an image to us. Our image of an eight is no exception, and imageio.v3 stored that image data in an array of arrays making a 5 x 3 matrix of 15 pixels. We can demonstrate that by calling on the shape property of our image variable and see the matrix by printing our image variable to the screen.\n\nprint(image.shape)\nprint(image)\n\n(5, 3)\n[[0. 0. 0.]\n [0. 1. 0.]\n [0. 0. 0.]\n [0. 1. 0.]\n [0. 0. 0.]]\n\n\nThus if we have tools that will allow us to manipulate these arrays of numbers, we can manipulate the image. The numpy library can be particularly useful here, so let’s try that out using numpy array slicing. Notice that the default behavior of the imshow function appended row and column numbers that will be helpful to us as we try to address individual or groups of pixels. First let’s load another copy of our eight, and then make it look like a zero.\nTo make it look like a zero, we need to change the number underlying the centremost pixel to be 1. With the help of those row and column headers, at this small scale we can determine the centre pixel is in row labeled 2 and column labeled 1. Using array slicing, we can then address and assign a new value to that position.\nzero = iio.imread(uri=“Images/eight.tif”) zero[2,1]= 1.0 ““” The following line of code creates a new figure for imshow to use in displaying our output. Without it, plt.imshow() would overwrite our previous image in the cell above ““” fig, ax = plt.subplots() plt.imshow(zero) print(zero)\n\n\nCo-ordinate system\nWhen we process images, we can access, examine, and / or change the colour of any pixel we wish. To do this, we need some convention on how to access pixels individually; a way to give each one a name, or an address of a sort. The most common manner to do this, and the one we will use in our programs, is to assign a modified Cartesian coordinate system to the image. The coordinate system we usually see in mathematics has a horizontal x-axis and a vertical y-axis, like this:\n\n\n\ncartesian-coordinates.png\n\n\nThe modified coordinate system used for our images will have only positive coordinates, the origin will be in the upper left corner instead of the centre, and y coordinate values will get larger as they go down instead of up, like this:\n\n\n\nmodified-coordinates.png\n\n\nThis is called a left-hand coordinate system. If you hold your left hand in front of your face and point your thumb at the floor, your extended index finger will correspond to the x-axis while your thumb represents the y-axis.\n\n\n\nleft-hand-coordinates.png\n\n\n\n\n\n\n\n\nThe most common mistake made with coordinates is to forget that y coordinates get larger as they go down instead of up as in a normal Cartesian coordinate system.\n\n\n\nConsequently, it may be helpful to think in terms of counting down rows (r) for the y-axis and across columns (c) for the x-axis. This can be especially helpful in cases where you need to transpose image viewer data provided in x,y format to y,x format. Thus, we will use cx and ry where appropriate to help bridge these two approaches.\n\n\n\n\nChanging pixel values\nLet’s load another copy of eight named five, and then change the value of pixels so you have what looks like a 5 instead of an 8. There are many possible solutions, but one method would be . . .\n\nfive = iio.imread(uri=\"Images/eight.tif\")\nfive[1,2]= 1.0\nfive[3,0]= 1.0\nfig, ax = plt.subplots()\nplt.imshow(five)\nprint(five)\n\n[[0. 0. 0.]\n [0. 1. 1.]\n [0. 0. 0.]\n [1. 1. 0.]\n [0. 0. 0.]]\n\n\n\n\n\n\n\nMore colours\nUp to now, we only had a 2 colour matrix, but we can have more if we use other numbers or fractions. One common way is to use the numbers between 0 and 255 to allow for 256 different colours or 256 different levels of grey. Let’s try that out:\n\n# make a copy of eight\nthree_colours = iio.imread(uri=\"Images/eight.tif\")\n\n# multiply the whole matrix by 128\nthree_colours = three_colours * 128\n\n# set the middle row (index 2) to the value of 255.,\n# so you end up with the values 0., 128., and 255.\nthree_colours[2,:] = 255.\nfig, ax = plt.subplots()\nplt.imshow(three_colours)\nprint(three_colours)\n\n[[  0.   0.   0.]\n [  0. 128.   0.]\n [255. 255. 255.]\n [  0. 128.   0.]\n [  0.   0.   0.]]\n\n\n\n\n\nWe now have 3 colours, but are they the three colours you expected? They all appear to be on a continuum of dark purple on the low end and yellow on the high end. This is a consequence of the default colour map (cmap) in this library. You can think of a colour map as an association or mapping of numbers to a specific colour. However, the goal here is not to have one number for every possible colour, but rather to have a continuum of colours that demonstrate relative intensity. In our specific case here for example, 255 or the highest intensity is mapped to yellow, and 0 or the lowest intensity is mapped to a dark purple. The best colour map for your data will vary and there are many options built in, but this default selection was not arbitrary. A lot of science went into making this the default due to its robustness when it comes to how the human mind interprets relative colour values, grey-scale printability, and colour-blind friendliness (You can read more about this default colour map in a Matplotlib tutorial and an explanatory article by the authors). Thus it is a good place to start, and you should change it only with purpose and forethought. For now, let’s see how you can do that using an alternative map you have likely seen before where it will be even easier to see it as a mapped continuum of intensities: greyscale.\n\nfig, ax = plt.subplots()\nplt.imshow(three_colours,cmap=plt.cm.gray)\n\n<matplotlib.image.AxesImage at 0x7f294bdd79d0>\n\n\n\n\n\nAbove we have exactly the same underying data matrix, but in greyscale. Zero maps to black, 255 maps to white, and 128 maps to medium grey. Here we only have a single channel in the data and utilize a grayscale color map to represent the luminance, or intensity of the data and correspondingly this channel is referred to as the luminance channel.\n\n\nEven More Colours\nThis is all well and good at this scale, but what happens when we instead have a picture of a natural landscape that contains millions of colours. Having a one to one mapping of number to colour like this would be inefficient and make adjustments and building tools to do so very difficult. Rather than larger numbers, the solution is to have more numbers in more dimensions. Storing the numbers in a multi-dimensional matrix where each colour or property like transparency is associated with its own dimension allows for individual contributions to a pixel to be adjusted independently. This ability to manipulate properties of groups of pixels separately will be key to certain techniques explored in later chapters of this lesson.\nTo get started let’s see an example of how different dimensions of information combine to produce a set of pixels using a 4 X 4 matrix with 3 dimensions for the colours red, green, and blue. Rather than loading it from a file, we will generate this example using NumPy.\n\n# set the random seed so we all get the same matrix\npseudorandomizer = np.random.RandomState(2021)\n# create a 4 × 4 checkerboard of random colours\ncheckerboard = pseudorandomizer.randint(0, 255, size=(4, 4, 3))\n# restore the default map as you show the image\nfig, ax = plt.subplots()\nplt.imshow(checkerboard)\n# display the arrays\nprint(checkerboard)\n\n[[[116  85  57]\n  [128 109  94]\n  [214  44  62]\n  [219 157  21]]\n\n [[ 93 152 140]\n  [246 198 102]\n  [ 70  33 101]\n  [  7   1 110]]\n\n [[225 124 229]\n  [154 194 176]\n  [227  63  49]\n  [144 178  54]]\n\n [[123 180  93]\n  [120   5  49]\n  [166 234 142]\n  [ 71  85  70]]]\n\n\n\n\n\nPreviously we had one number being mapped to one colour or intensity. Now we are combining the effect of 3 numbers to arrive at a single colour value. Let’s see an example of that using the blue square at the end of the second row, which has the index [row, column][1, 3]. Note that in Python indexing starts at zero.\n\n# extract all the colour information for the blue square\nupper_right_square = checkerboard[1, 3, :]\nupper_right_square\n\narray([  7,   1, 110])\n\n\nThe integers in order represent\n\nRed: 7\nGreen: 1\nBlue: 3\n\nLooking at theses three values and knowing how they map, can help us understand why it is blue. If we divide each value by 255, which is the maximum, we can determine how much it is contributing relative to its maximum potential. Effectively, the red is at 7/255 or 2.8 percent of its potential, the green is at 1/255 or 0.4 percent, and blue is 110/255 or 43.1 percent of its potential. So when you mix those three intensities of colour, blue is winning by a wide margin, but the red and green still contribute to make it a slightly different shade of blue than 0,0,110 would be on its own.\nThese colours mapped to dimensions of the matrix may be referred to as channels. It may be helpful to display each of these channels independently, to help us understand what is happening. We can do that by multiplying our image array representation with a 1d matrix that has a one for the channel we want to keep and zeros for the rest.\n\n# transform array to focus only on the red channel (green and blue channels effectively removed by multiplying by zero)\nred_channel = checkerboard * [1, 0, 0]\nfig, ax = plt.subplots()\nplt.imshow(red_channel)\n\n<matplotlib.image.AxesImage at 0x7f2949357580>\n\n\n\n\n\n\n# transform array to focus only on the green channel (red and blue channels effectively removed by multiplying by zero)\ngreen_channel = checkerboard * [0, 1, 0]\nfig, ax = plt.subplots()\nplt.imshow(green_channel)\n\n<matplotlib.image.AxesImage at 0x7f29491dae60>\n\n\n\n\n\n\n# transform array to focus only on the blue channel (red and green channels effectively removed by multiplying by zero)\nblue_channel = checkerboard * [0, 0, 1]\nfig, ax = plt.subplots()\nplt.imshow(blue_channel)\n\n<matplotlib.image.AxesImage at 0x7f2949248700>\n\n\n\n\n\nIf we look at the upper [1, 3] square in all three figures, we can see each of those colour contributions in action. Notice that there are several squares in the blue figure that look even more intensely blue than square [1, 3]. When all three channels are combined though, the blue light of those squares is being diluted by the relative strength of red and green being mixed in with them.\n\n\n24-bit RGB Colour\nThis last colour model we used, known as the RGB (Red, Green, Blue) model, is the most common. As we saw, the RGB model is an additive colour model, which means that the primary colours are mixed together to form other colours. Most frequently, the amount of the primary colour added is represented as an integer in the closed range [0, 255] as seen in the example. Therefore, there are 256 discrete amounts of each primary colour that can be added to produce another colour. The number of discrete amounts of each colour, 256, corresponds to the number of bits used to hold the colour channel value, which is eight (28=256). Since we have three channels with 8 bits for each (8+8+8=24), this is called 24-bit colour depth.\nAny particular colour in the RGB model can be expressed by a triplet of integers in [0, 255], representing the red, green, and blue channels, respectively. A larger number in a channel means that more of that primary colour is present.\n\n\n\n\n\n\nSuppose that we represent colours as triples (r, g, b), where each of r, g, and b is an integer in [0, 255]. What colours are represented by each of these triples? 1.(255, 0, 0)  2.(0, 255, 0)  3.(0, 0, 255)  4.(255, 255, 255)  5.(0, 0, 0)  6.(128, 128, 128)\n\n\n\n\n\n\n(255, 0, 0) represents red, because the red channel is maximised, while the other two channels have the minimum values\n\n(0, 255, 0) represents green\n\n(0, 0, 255) represents blue\n\n(255, 255, 255) is a little harder. When we mix the maximum value of all three colour channels, we see the colour white\n\n(0, 0, 0) represents the absence of all colour, or black\n\n(128, 128, 128) represents a medium shade of gray. Note that the 24-bit RGB colour model provides at least 254 shades of gray, rather than only fifty\n\n\n\n\nNote that the RGB colour model may run contrary to your experience, especially if you have mixed primary colours of paint to create new colours. In the RGB model, the lack of any colour is black, while the maximum amount of each of the primary colours is white. With physical paint, we might start with a white base, and then add differing amounts of other paints to produce a darker shade.\nWe can look at some further examples of 24-bit RGB colours, in a visual way. The image below shows some colour names, their 24-bit RGB triplet values, and the colour itself:\n\n\n\n\n\n\nWe cannot really provide a complete table. To see why, answer this question: How many possible colours can be represented with the 24-bit RGB model?\n\n\n\n\n\nThere are 24 total bits in an RGB colour of this type, and each bit can be on or off, and so there are \\(2^{24}\\) = 16,777,216 possible colours with our additive, 24-bit RGB colour model.\n\n\n\nAlthough 24-bit colour depth is common, there are other options. We might have 8-bit colour (3 bits for red and green, but only 2 for blue, providing 8 × 8 × 4 = 256 colours) or 16-bit colour (4 bits for red, green, and blue, plus 4 more for transparency, providing 16 × 16 × 16 = 4096 colours), for example. There are colour depths with more than eight bits per channel, but as the human eye can only discern approximately 10 million different colours, these are not often used.\nIf you are using an older or inexpensive laptop screen or LCD monitor to view images, it may only support 18-bit colour, capable of displaying 64 × 64 × 64 = 262,144 colours. 24-bit colour images will be converted in some manner to 18-bit, and thus the colour quality you see will not match what is actually in the image.\nWe can combine our coordinate system with the 24-bit RGB colour model to gain a conceptual understanding of the images we will be working with. An image is a rectangular array of pixels, each with its own coordinate. Each pixel in the image is a square point of coloured light, where the colour is specified by a 24-bit RGB triplet. Such an image is an example of raster graphics.\n\n\nImage formats\nAlthough the images we will manipulate in our programs are conceptualised as rectangular arrays of RGB triplets, they are not necessarily created, stored, or transmitted in that format. There are several image formats we might encounter, and we should know the basics of at least of few of them. Some formats we might encounter, and their file extensions, are shown in this table:\n\n\n\nimage_formats.JPG\n\n\n\n\nDevice-Independent Bitmap (BMP)\nThe file format that comes closest to our preceding conceptualisation of images is the Device-Independent Bitmap, or BMP, file format. BMP files store raster graphics images as long sequences of binary-encoded numbers that specify the colour of each pixel in the image. Since computer files are one-dimensional structures, the pixel colours are stored one row at a time. That is, the first row of pixels (those with y-coordinate 0) are stored first, followed by the second row (those with y-coordinate 1), and so on. Depending on how it was created, a BMP image might have 8-bit, 16-bit, or 24-bit colour depth.\n24-bit BMP images have a relatively simple file format, can be viewed and loaded across a wide variety of operating systems, and have high quality. However, BMP images are not compressed, resulting in very large file sizes for any useful image resolutions.\nThe idea of image compression is important to us for two reasons: first, compressed images have smaller file sizes, and are therefore easier to store and transmit; and second, compressed images may not have as much detail as their uncompressed counterparts, and so our programs may not be able to detect some important aspect if we are working with compressed images. Since compression is important to us, we should take a brief detour and discuss the concept.\n\n\nImage compression\nBefore we talk specifically about images, we first need to understand how numbers are stored in a modern digital computer. When we think of a number, we do so using a decimal, or base-10 place-value number system. For example, a number like 659 is 6 × 102 + 5 × 101 + 9 × 100. Each digit in the number is multiplied by a power of 10, based on where it occurs, and there are 10 digits that can occur in each position (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\nIn principle, computers could be constructed to represent numbers in exactly the same way. But, the electronic circuits inside a computer are much easier to construct if we restrict the numeric base to only two, instead of 10. (It is easier for circuitry to tell the difference between two voltage levels than it is to differentiate among 10 levels.) So, values in a computer are stored using a binary, or base-2 place-value number system.\nIn this system, each symbol in a number is called a bit instead of a digit, and there are only two values for each bit (0 and 1). We might imagine a four-bit binary number, 1101. Using the same kind of place-value expansion as we did above for 659, we see that 1101 = 1 × 23 + 1 × 22 + 0 × 21 + 1 × 20, which if we do the math is 8 + 4 + 0 + 1, or 13 in decimal.\nInternally, computers have a minimum number of bits that they work with at a given time: eight. A group of eight bits is called a byte. The amount of memory (RAM) and drive space our computers have is quantified by terms like Megabytes (MB), Gigabytes (GB), and Terabytes (TB). The following table provides more formal definitions for these terms:\n\n\n\nbits.JPG\n\n\n\n\n\n\n\n\nImagine that we have a fairly large, but very boring image: a 5,000 × 5,000 pixel image composed of nothing but white pixels. If we used an uncompressed image format such as BMP, with the 24-bit RGB colour model, how much storage would be required for the file?\n\n\n\n\n\nIn such an image, there are 5,000 × 5,000 = 25,000,000 pixels, and 24 bits for each pixel, leading to 25,000,000 × 24 = 600,000,000 bits, or 75,000,000 bytes (71.5MB). That is quite a lot of space for a very uninteresting image!\n\n\n\nSince image files can be very large, various compression schemes exist for saving (approximately) the same information while using less space. These compression techniques can be categorised as lossless or lossy.\n\n\nLossless compression\nIn lossless image compression, we apply some algorithm (i.e., a computerised procedure) to the image, resulting in a file that is significantly smaller than the uncompressed BMP file equivalent would be. Then, when we wish to load and view or process the image, our program reads the compressed file, and reverses the compression process, resulting in an image that is identical to the original. Nothing is lost in the process – hence the term “lossless.”\nThe general idea of lossless compression is to somehow detect long patterns of bytes in a file that are repeated over and over, and then assign a smaller bit pattern to represent the longer sample. Then, the compressed file is made up of the smaller patterns, rather than the larger ones, thus reducing the number of bytes required to save the file. The compressed file also contains a table of the substituted patterns and the originals, so when the file is decompressed it can be made identical to the original before compression.\nTo provide you with a concrete example, consider the 71.5 MB white BMP image discussed above. When put through the zip compression utility on Microsoft Windows, the resulting .zip file is only 72 KB in size! That is, the .zip version of the image is three orders of magnitude smaller than the original, and it can be decompressed into a file that is byte-for-byte the same as the original. Since the original is so repetitious - simply the same colour triplet repeated 25,000,000 times - the compression algorithm can dramatically reduce the size of the file.\nIf you work with .zip or .gz archives, you are dealing with lossless compression.\n\n\nLossy compression\nLossy compression takes the original image and discards some of the detail in it, resulting in a smaller file format. The goal is to only throw away detail that someone viewing the image would not notice. Many lossy compression schemes have adjustable levels of compression, so that the image creator can choose the amount of detail that is lost. The more detail that is sacrificed, the smaller the image files will be - but of course, the detail and richness of the image will be lower as well.\nThis is probably fine for images that are shown on Web pages or printed off on 4 × 6 photo paper, but may or may not be fine for scientific work. You will have to decide whether the loss of image quality and detail are important to your work, versus the space savings afforded by a lossy compression format.\n\n\n\n\n\n\nIt is important to understand that once an image is saved in a lossy compression format, the lost detail is just that - lost.\n\n\n\nUnlike lossless formats, given an image saved in a lossy format, there is no way to reconstruct the original image in a byte-by-byte manner.\n\n\n\n\nJPEG\nJPEG images are perhaps the most commonly encountered digital images today. JPEG uses lossy compression, and the degree of compression can be tuned to your liking. It supports 24-bit colour depth, and since the format is so widely used, JPEG images can be viewed and manipulated easily on all computing platforms.\nLet us see the effects of image compression on image size with actual images. The following script creates a square white image 5000 X 5000 pixels, and then saves it as a BMP and as a JPEG image.\n\nimport imageio.v3 as iio\nimport numpy as np\n\ndim = 5000\n\nimg = np.zeros((dim, dim, 3), dtype=\"uint8\")\nimg.fill(255)\n\niio.imwrite(uri=\"Images/ws.bmp\", image=img)\niio.imwrite(uri=\"Images/ws.jpg\", image=img)\n\n\n\n\ndownload.jpg\n\n\nThe BMP file, ws.bmp, is 75,000,054 bytes, which matches our prediction very nicely. The JPEG file, ws.jpg, is 392,503 bytes, two orders of magnitude smaller than the bitmap version.\nLet us see a hands-on example of lossless versus lossy compression. Once again, open a terminal and navigate to the data/ directory. The two output images, ws.bmp and ws.jpg, should still be in the directory, along with another image, tree.jpg.\nWe can apply lossless compression to any file by using the zip command. Recall that the ws.bmp file contains 75,000,054 bytes. Apply lossless compression to this image by executing the following command:\nzip ws.zip ws.bmp. \nThis command tells the computer to create a new compressed file, ws.zip, from the original bitmap image. Execute a similar command on the tree JPEG file:\nzip tree.zip tree.jpg.\nHaving created the compressed file, use the ls -al command to display the contents of the directory.\n\nHow big are the compressed files?\nHow do those compare to the size of ws.bmp and tree.jpg?\nWhat can you conclude from the relative sizes?\n\n\n\n\nzipped.JPG\n\n\nThe tree.jpg has not reduced in size as this was already in a compressed format. However the regularity of the bitmap image (remember, it is a 5,000 x 5,000 pixel image containing only white pixels) allows the lossless compression scheme to compress the ws.bmp file quite effectivelym reducing the size from 7,500,054 bytes (71.52Mb) to 72,998 bytes (71.29kb).\nHere is an example showing how JPEG compression might impact image quality. Consider this image of several maize seedlings (scaled down here from 11,339 × 11,336 pixels in order to fit the display):\n\n\n\nquality-original.jpg\n\n\nNow, let us zoom in and look at a small section of the label in the original, first in the uncompressed format:\n\n\n\nquality-tif.jpg\n\n\nHere is the same area of the image, but in JPEG format. We used a fairly aggressive compression parameter to make the JPEG, in order to illustrate the problems you might encounter with the format.\n\n\n\nquality-jpg.jpg\n\n\nThe JPEG image is of clearly inferior quality. It has less colour variation and noticeable pixelation. Quality differences become even more marked when one examines the colour histograms for each image. A histogram shows how often each colour value appears in an image. The histograms for the uncompressed (left) and compressed (right) images are shown below:\n\n\n\nquality-histogram.jpg\n\n\nWe learn how to make histograms such as these later on in the workshop. The differences in the colour histograms are even more apparent than in the images themselves; clearly the colours in the JPEG image are different from the uncompressed version.\nIf the quality settings for your JPEG images are high (and the compression rate therefore relatively low), the images may be of sufficient quality for your work. It all depends on how much quality you need, and what restrictions you have on image storage space. Another consideration may be where the images are stored. For example,if your images are stored in the cloud and therefore must be downloaded to your system before you use them, you may wish to use a compressed image format to speed up file transfer time.\n\n\nPNG\nPNG images are well suited for storing diagrams. It uses a lossless compression and is hence often used in web applications for non-photographic images. The format is able to store RGB and plain luminance (single channel, without an associated color) data, among others. Image data is stored row-wise and then, per row, a simple filter, like taking the difference of adjacent pixels, can be applied to increase the compressability of the data. The filtered data is then compressed in the next step and written out to the disk.\n\n\nTIFF\nTIFF images are popular with publishers, graphics designers, and photographers. TIFF images can be uncompressed, or compressed using either lossless or lossy compression schemes, depending on the settings used, and so TIFF images seem to have the benefits of both the BMP and JPEG formats. The main disadvantage of TIFF images (other than the size of images in the uncompressed version of the format) is that they are not universally readable by image viewing and manipulation software.\n\n\nMetadata\nJPEG and TIFF images support the inclusion of metadata in images. Metadata is textual information that is contained within an image file. Metadata holds information about the image itself, such as when the image was captured, where it was captured, what type of camera was used and with what settings, etc. We normally don’t see this metadata when we view an image, but we can view it independently if we wish to (see Accessing Metadata, below). The important thing to be aware of at this stage is that you cannot rely on the metadata of an image being fully preserved when you use software to process that image.\n\n\n\n\n\n\nThe image reader/writer library that we use throughout this blog, imageio.v3, includes metadata when saving new images but may fail to keep certain metadata fields.\n\n\n\nIf metadata is important to you, take precautions to always preserve the original files.\n\n\nMetadata is served independently from pixel data. imageio.v3 provides a way to display or explore the metadata associated with an image:\n\n# read metadata\nmetadata = iio.immeta(uri='Images/eight.tif')\n\n# display the format-specific metadata\nmetadata\n\n{'is_fluoview': False,\n 'is_nih': False,\n 'is_micromanager': False,\n 'is_ome': False,\n 'is_lsm': False,\n 'is_reduced': False,\n 'is_shaped': True,\n 'is_stk': False,\n 'is_tiled': False,\n 'is_mdgel': False,\n 'compression': <COMPRESSION.NONE: 1>,\n 'predictor': 1,\n 'is_mediacy': False,\n 'description': '{\"shape\": [5, 3]}',\n 'description1': '',\n 'is_imagej': False,\n 'software': 'tifffile.py',\n 'resolution_unit': 1,\n 'resolution': (1.0, 1.0, 'NONE')}\n\n\nOther software exists that can help you handle metadata, such as Fiji and ImageMagick. You may want to explore these options if you need to work with the metadata of your images.\n\n\nSummary of image formats used in this blog\nThe following table summarises the characteristics of the BMP, JPEG, and TIFF image formats:\n\n\n\nformats.JPG\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\ndigital images are represented as rectangular arrays of square pixels.\ndigital images use a left-hand coordinate system, with the origin in the upper left corner, the x-axis running to the right, and the y-axis running down. Some learners may prefer to think in terms of counting down rows for the y-axis and across columns for the x-axis. Thus, we will make an effort to allow for both approaches in our lesson presentation.\nmost frequently, digital images use an additive RGB model, with eight bits for the red, green, and blue channels.\nskimage images are stored as multi-dimensional NumPy arrays.\nin skimage images, the red channel is specified first, then the green, then the blue, i.e., RGB.\nlossless compression retains all the details in an image, but lossy compression results in loss of some of the original image detail.\nBMP images are uncompressed, meaning they have high quality but also that their file sizes are large.\nJPEG images use lossy compression, meaning that their file sizes are smaller, but image quality may suffer.\nTIFF images can be uncompressed or compressed with lossy or lossless compression.\ndepending on the camera or sensor, various useful pieces of information may be stored in an image file, in the image metadata."
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#working-with-skimage",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#working-with-skimage",
    "title": "Image Processing with Python",
    "section": "2. Working with skimage",
    "text": "2. Working with skimage\nIn the previous section we covered much of how images are represented in computer software. In this section we will learn some more methods for accessing and changing digital images.\n\nReading, displaying, and saving images\nImageio provides intuitive functions for reading and writing (saving) images. All of the popular image formats, such as BMP, PNG, JPEG, and TIFF are supported, along with several more esoteric formats. Check the Supported Formats docs for a list of all formats. Matplotlib provides a large collection of plotting utilities.\nLet us examine a simple Python program to load, display, and save an image to a different format. First, we import the v3 module of imageio (imageio.v3) as iio so we can read and write images. Then, we use the iio.imread() function to read a JPEG image entitled chair.jpg. Here are the first few lines:\n\n# Python program to open, display, and save an image.\nimport imageio.v3 as iio\n\n# read image\nimage = iio.imread(uri='Images/chair.jpg')\n\nNext, we will do something with the image. Once we have the image in the program, we first call plt.subplots() so that we will have a fresh figure with a set of axis independent from our previous calls. Next we call plt.imshow() in order to display the image:\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f0839e0da80>\n\n\n\n\n\nNow, we will save the image in another format:\n\n# save a new version in .tif format\niio.imwrite(uri=\"Images/chair.tif\", image=image)\n\nThe final statement in the program, iio.imwrite(uri=“data/chair.tif”, image=image), writes the image to a file named chair.tif in our Images/ directory. The imwrite() function automatically determines the type of the file, based on the file extension we provide. In this case, the .tif extension causes the image to be saved as a TIFF.\n\n\n\n\n\n\nThe iio.imwrite() function automatically uses the file type we specify in the file name parameter’s extension.\n\n\n\nNote that this is not always the case. For example, if we are editing a document in Microsoft Word, and we save the document as paper.pdf instead of paper.docx, the file is not saved as a PDF document.\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen we call functions in Python, there are two ways we can specify the necessary arguments. We can specify the arguments positionally, i.e., in the order the parameters appear in the function definition, or we can use named arguments.\nFor example, the iio.imwrite() function definition specifies two parameters, the resource to save the image to (e.g., a file name, an http address) and the image to write to disk. So, we could save the chair image in the sample code above using positional arguments like this:\niio.imwrite(\"data/chair.tif\", image)\nSince the function expects the first argument to be the file name, there is no confusion about what “data/chair.jpg” means. The same goes for the second argument.\nThe style we will use in this workshop is to name each argument, like this:\niio.imwrite(uri=\"data/chair.tif\", image=image)\nThis style will make it easier for you to learn how to use the variety of functions we will cover in this blog.\n\n\n\n\nResizing an image\nAdd import skimage.transform and import skimage.util to your list of imports. Using the chair.jpg image located in the data folder, write a Python script to read your image into a variable named image. Then, resize the image to 10 percent of its current size using these lines of code:\n\nimport skimage.transform, skimage.util\n\nimage = iio.imread(uri='Images/chair.jpg')\n\n# resize to 10% of original size\nnew_shape = (image.shape[0] // 10, image.shape[1] // 10, image.shape[2])\n\n# image file stored as whole numbers for space efficiency\nsmall = skimage.transform.resize(image=image, output_shape=new_shape)\n\n# converts image back to whole numbers before saving to disk\nsmall = skimage.util.img_as_ubyte(small)\n\nAs it is used here, the parameters to the skimage.transform.resize() function are the image to transform, image, the dimensions we want the new image to have, new_shape.\nNote that the pixel values in the new image are an approximation of the original values and should not be confused with actual, observed data. This is because skimage interpolates the pixel values when reducing or increasing the size of an image. skimage.transform.resize has a number of optional parameters that allow the user to control this interpolation. You can find more details in the scikit-image documentation.\nImage files on disk are normally stored as whole numbers for space efficiency, but transformations and other math operations often result in conversion to floating point numbers. Using the skimage.util.img_as_ubyte() method converts it back to whole numbers before we save it back to disk. If we don’t convert it before saving, iio.imwrite() may not recognise it as image data.\nNext, write the resized image out to a new file named resized.jpg in your data directory. Finally, use plt.imshow() with each of your image variables to display both images in your notebook. Don’t forget to use fig, ax = plt.subplots() so you don’t overwrite the first image with the second. Images may appear the same size in jupyter, but you can see the size difference by comparing the scales for each. You can also see the differnce in file storage size on disk by hovering your mouse cursor over the original and the new file in the jupyter file browser, using ls -l in your shell, or the OS file browser if it is configured to show file sizes.\n\n# write the resized image out to a new file named resized.jpg\niio.imwrite(uri=\"Images/resized.jpg\", image=small)\n\n# plot original image\nfig, ax = plt.subplots()\nplt.imshow(image)\nplt.title(\"Original image\")\n\n# plot resized image\nfig, ax = plt.subplots()\nplt.imshow(small)\nplt.title(\"Reduced to 10% of size of original image\")\n\nText(0.5, 1.0, 'Reduced to 10% of size of original image')\n\n\n\n\n\n\n\n\nWe can see from the above that our reduction to 10% of the original size appears to have been successsful. The axes of the reduced image are 0-300 and 0-400 as against 0-3000 and 0-4000.\nThe script has: - resized the data/chair.jpg image by a factor of 10 in both dimensions - saved the result to the data/resized.jpg file, and - displays original and resized for comparision\n\n\nManipulating pixels\n\nRetaining only high intensity pixels\nIn section 1, we individually manipulated the colours of pixels by changing the numbers stored in the image’s NumPy array. Let’s apply the principles learned there along with some new principles to a real world example. Suppose we are interested in this maize root cluster image.\n\nWe want to be able to focus our program’s attention on the roots themselves, while ignoring the black background.\nSince the image is stored as an array of numbers, we can simply look through the array for pixel colour values that are less than some threshold value. This process is called thresholding, and we will see more powerful methods to perform the thresholding task in the Thresholding section.\nHere, though, we will look at a simple and elegant NumPy method for thresholding. Let us develop a program that keeps only the pixel colour values in an image that have value greater than or equal to 128. This will keep the pixels that are brighter than half of “full brightness”, i.e., pixels that do not belong to the black background. We will start by reading the image and displaying it.\n\nimport imageio.v3 as iio\n\n# read input image\nimage = iio.imread(uri=\"Images/maize_cluster.jpg\")\n\n# display original image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f4d204fe320>\n\n\n\n\n\nNow we can threshold the image and display the result:\n\n# keep only high-intensity pixels\nimage[image < 128] = 0\n\n# display modified image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\nValueError: assignment destination is read-only\n\n\n\n\n\n\n\n\nCheck if the array is writeable with image.flags\n\n\n\n\nIf WRITEABLE is false, change it with img.setflags(write=1)\nIf after doing this you receive ValueError: cannot set WRITEABLE flag to True of this array, then as a workaround create a copy of the image using image_copy = image.copy()\n\n\n\nimage.flags\n\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : False\n  WRITEABLE : False\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n\n\n\nimage.setflags(write=1)\n\nValueError: cannot set WRITEABLE flag to True of this array\n\n\n\n# workaround to address the read-only issues\nimage_copy = image.copy()\n\n# keep only high-intensity pixels, by setting all low-intensity pixels < 128 to zero\nimage_copy[image_copy < 128] = 0\n\n# display modified image\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f4d11f9c4c0>\n\n\n\n\n\nThe result is an image in which the extraneous background detail has been removed.\n\n\n\nConverting colour images to grayscale\nIt is often easier to work with grayscale images, which have a single channel, instead of colour images, which have three channels. Skimage offers the function skimage.color.rgb2gray() to achieve this. This function adds up the three colour channels in a way that matches human colour perception, see the skimage documentation for details. It returns a grayscale image with floating point values in the range from 0 to 1. We can use the function skimage.util.img_as_ubyte() in order to convert it back to the original data type and the data range back 0 to 255. Note that it is often better to use image values represented by floating point values, because using floating point numbers is numerically more stable.\n\n\n\n\n\n\nskimage contains many modules and functions that include the US English spelling, color.\n\n\n\nThe exact spelling matters here, e.g. you will encounter an error if you try to run skimage.colour.rgb2gray(). To account for this, we will use the US English spelling, color, in example Python code throughout this blog. We will adopt a similar approach with “centre” and center.\n\n\n\nimport imageio.v3 as iio\nimport skimage.color\n\n# read input image\nimage = iio.imread(uri=\"Images/chair.jpg\")\n\n# display original image\nfig, ax = plt.subplots()\nplt.imshow(image)\nplt.title(\"Original image\")\n\n# convert to grayscale and display\ngray_image = skimage.color.rgb2gray(image)\nfig, ax = plt.subplots()\nplt.imshow(gray_image, cmap=\"gray\")\nplt.title(\"Grayscale image\")\n\nText(0.5, 1.0, 'Grayscale image')\n\n\n\n\n\n\n\n\nWe also load colour images as grayscale directly by passing the argument mode=“L”  to iio.imread():\n\nimport imageio.v3 as iio\nimport skimage.color\n\n# read input image, based on filename parameter\nimage = iio.imread(uri=\"Images/chair.jpg\", mode=\"L\")\n\n# display grayscale image\nfig, ax = plt.subplots()\nplt.imshow(image, cmap=\"gray\")\nplt.title(\"Grayscale image\")\n\nText(0.5, 1.0, 'Grayscale image')\n\n\n\n\n\n\nRetaining only low intensity pixels\nA little earlier, we showed how we could use Python and skimage to turn on only the high intensity pixels from an image, while turning all the low intensity pixels off. Now, let’s practice doing the opposite - keeping all the low intensity pixels while changing the high intensity ones, by turning all of the white pixels in the image to a light gray colour, say with the intensity of each formerly white pixel set to 0.75:\n\n# read input image, based on filename parameter\nsudoku = iio.imread(uri=\"Images/sudoku.png\", mode=\"L\")\n\n# display grayscale image\nfig, ax = plt.subplots()\nplt.imshow(sudoku, cmap=\"gray\")\nplt.title(\"Grayscale image\")\n\nText(0.5, 1.0, 'Grayscale image')\n\n\n\n\n\n\n# Clip all high intensity pixels > 0.75 to 0.75\nsudoku[sudoku > 0.75] = 0.75\n\n# display modified image\nfig, ax = plt.subplots()\nplt.imshow(sudoku, cmap=\"gray\", vmin=0, vmax=1)\n\n\n\n\n\nPlotting single channel images (cmap, vmin, vmax)\nCompared to a colour image, a grayscale image contains only a single intensity value per pixel. When we plot such an image with plt.imshow, matplotlib uses a colour map, to assign each intensity value a colour. The default colour map is called “viridis” and maps low values to purple and high values to yellow. We can instruct matplotlib to map low values to black and high values to white instead, by calling plt.imshow with cmap=“gray”. The documentation contains an overview of pre-defined colour maps.\nFurthermore, matplotlib determines the minimum and maximum values of the colour map dynamically from the image, by default. That means, that in an image, where the minimum is 0.25 and the maximum is 0.75, those values will be mapped to black and white respectively (and not dark gray and light gray as you might expect). If there are defined minimum and maximum vales, you can specify them via vmin and vmax to get the desired output. If you forget about this, it can lead to unexpected results.\n\n\nAccess via slicing\nAs noted in the previous section skimage images are stored as NumPy arrays, so we can use array slicing to select rectangular areas of an image. Then, we can save the selection as a new image, change the pixels in the image, and so on. It is important to remember that coordinates are specified in (ry, cx) order and that colour values are specified in (r, g, b) order when doing these manipulations.\nConsider this image of a whiteboard, and suppose that we want to create a sub-image with just the portion that says “odd + even = odd,” along with the red box that is drawn around the words.\n\n\n\nboard.jpg\n\n\nUsing the same display technique we have used throughout this blog, we can determine the coordinates of the corners of the area we wish to extract by hovering the mouse near the points of interest and noting the coordinates. If we do that, we might settle on a rectangular area with an upper-left coordinate of (135, 60) and a lower-right coordinate of (480, 150), as shown in this version of the whiteboard picture:\n\n\n\nboard-coordinates.jpg\n\n\nNote that the coordinates in the preceding image are specified in (cx, ry) order. Now if our entire whiteboard image is stored as an skimage image named image, we can create a new image of the selected region with a statement like this:\nclip = image[60:151, 135:481, :]\nOur array slicing specifies the range of y-coordinates or rows first, 60:151, and then the range of x-coordinates or columns, 135:481. Note we go one beyond the maximum value in each dimension, so that the entire desired area is selected. The third part of the slice, :, indicates that we want all three colour channels in our new image.\nA script to create the subimage would start by loading the image:\n\nimport imageio.v3 as iio\n\n# load and display original image\nimage = iio.imread(uri=\"Images/board.jpg\")\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f93077220>\n\n\n\n\n\nThen we use array slicing to create a new image with our selected area and then display the new image:\n\n# extract, display, and save sub-image\nclip = image[60:151, 135:481, :]\nfig, ax = plt.subplots()\nplt.imshow(clip)\niio.imwrite(uri=\"Images/clip.tif\", image=clip)\n\n\n\n\nWe can also change the values in an image.\n\nFirst, we sample a single pixel’s colour at a particular location of the image, saving it in a variable named color, which creates a 1 × 1 × 3 NumPy array with the blue, green, and red colour values for the pixel located at (ry = 330, cx = 90).\nThen, with the img[60:151, 135:481] = color command, we modify the image in the specified area. From a NumPy perspective, this changes all the pixel values within that range to array saved in the color variable. In this case, the command “erases” that area of the whiteboard, replacing the words with a beige colour, as shown in the final image produced by the program:\n\n\n# replace clipped area with sampled color\nimage_copy = image.copy()\ncolor = image_copy[330, 90]\nimage_copy[60:151, 135:481] = color\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f2f9102ffd0>\n\n\n\n\n\n\n::: {.callout-important}\n## Key Points:\n- images are read from disk with the iio.imread() function\n- we create a window that automatically scales the displayed image with matplotlib and calling show() on the global figure object\n- colour images can be transformed to grayscale using skimage.color.rgb2gray() or, in many cases, be read as grayscale directly by passing the argument mode=\"L\" to iio.imread()\n- we can resize images with the skimage.transform.resize() function\n- NumPy array commands, such as image[image < 128] = 0, can be used to manipulate the pixels of an image.\n- array slicing can be used to extract sub-images or modify areas of images, e.g., clip = image[60:150, 135:480, :]\n- metadata is not retained when images are loaded as skimage images\n:::\n\n\nimport imageio.v3 as iio\n\n# load and display original image\nimage = iio.imread(uri=\"Images/maize_cluster.jpg\")\nfig, ax = plt.subplots()\nplt.imshow(image)\nplt.title(\"Original image\")\n\n# extract, display, and save sub-image\n# WRITE YOUR CODE TO SELECT THE SUBIMAGE NAME clip HERE:\nclip = image[0:1750, 1500:2500, :]\nfig, ax = plt.subplots()\nplt.imshow(clip)\nplt.title(\"Clipped image\")\n\n# WRITE YOUR CODE TO SAVE clip HERE\niio.imwrite(uri=\"Images/clip.jpg\", image=clip)"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#drawing-and-bitwise-operations",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#drawing-and-bitwise-operations",
    "title": "Image Processing with Python",
    "section": "3. Drawing and Bitwise Operations",
    "text": "3. Drawing and Bitwise Operations\nThe next sections cover a basic toolkit of skimage operators. With these tools, we will be able to create programs to perform simple analyses of images based on changes in colour or shape.\n\nDrawing on images\nOften we wish to select only a portion of an image to analyze, and ignore the rest. Creating a rectangular sub-image with slicing, as we did in section 2 is one option for simple cases. Another option is to create another special image, of the same size as the original, with white pixels indicating the region to save and black pixels everywhere else. Such an image is called a mask. In preparing a mask, we sometimes need to be able to draw a shape - a circle or a rectangle, say - on a black image. skimage provides tools to do that.\nConsider this image of maize seedlings:\n\nNow, suppose we want to analyze only the area of the image containing the roots themselves; we do not care to look at the kernels, or anything else about the plants. Further, we wish to exclude the frame of the container holding the seedlings as well. Hovering over the image with our mouse, could tell us that the upper-left coordinate of the sub-area we are interested in is (44, 357), while the lower-right coordinate is (720, 740). These coordinates are shown in (x, y) order.\nA Python program to create a mask to select only that area of the image would start with a now-familiar section of code to open and display the original image.As before, we first import the v3 submodule of imageio (imageio.v3). We also import the NumPy library, which we need to create the initial mask image. Then, we import the draw submodule of skimage. We load and display the initial image in the same way we have done before.\n\nimport imageio.v3 as iio\nimport skimage.draw\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load and display the original image\nimage = iio.imread(uri=\"Images/maize-seedlings.tif\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f9073cd30>\n\n\n\n\n\n\n\nMasking\n\nNumPy allows indexing of images/arrays with “boolean” arrays of the same size. Indexing with a boolean array is also called mask indexing. The “pixels” in such a mask array can only take two values: True or False. When indexing an image with such a mask, only pixel values at positions where the mask is True are accessed. But first, we need to generate a mask array of the same size as the image. Luckily, the NumPy library provides a function to create just such an array. The next section of code shows how:\n\n# Create the basic mask\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\ncolor = \n\nThe first argument to the ones() function is the shape of the original image, so that our mask will be exactly the same size as the original. Notice, that we have only used the first two indices of our shape. We omitted the channel dimension. Indexing with such a mask will change all channel values simultaneously. The second argument, dtype = “bool”, indicates that the elements in the array should be booleans* - i.e., values are either True or False. Thus, even though we use np.ones() to create the mask, its pixel values are in fact not 1 but True. You could check this, e.g., by print(mask[0, 0]).\nNext, we draw a filled, rectangle on the mask. The parameters of the rectangle() function (357, 44) and (740, 720), are the coordinates of the upper-left (start) and lower-right (end) corners of a rectangle in (ry, cx) order. The function returns the rectangle as row (rr) and column (cc) coordinate arrays:\n\n# Draw filled rectangle on the mask image\n# co-ordinates are (row, column)\n# first co-ord is top-left, second co-ord is bottom-right\nrr, cc = skimage.draw.rectangle(start=(357, 44), end=(740, 720))\nmask[rr, cc] = False\n\n# Display mask image\nfig, ax = plt.subplots()\nplt.imshow(mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f90de33a0>\n\n\n\n\n\n\n\n\n\n\n\nCheck the documentation!\n\n\n\nWhen using an skimage function for the first time - or the fifth time - it is wise to check how the function is used, via the skimage documentation or other usage examples on programming-related sites such as Stack Overflow. Basic information about skimage functions can be found interactively in Python, via commands like help(skimage) or help(skimage.draw.rectangle). Take notes in your lab notebook. And, it is always wise to run some test code to verify that the functions your program uses are behaving in the manner you intend.\n\n\n\n\n\n\n\n\nVariable naming conventions\n\n\n\nYou may have wondered why we called the return values of the rectangle function rr and cc?! You may have guessed that r is short for row and c is short for column. However, the rectangle function returns mutiple rows and columns; thus we used a convention of doubling the letter r to rr (and c to cc) to indicate that those are multiple values. In fact it may have even been clearer to name those variables rows and columns; however this would have been also much longer. Whatever you decide to do, try to stick to some already existing conventions, such that it is easier for other people to understand your code.\n\n\n\n\nOther drawing operations\nThere are other functions for drawing on images, in addition to the skimage.draw.rectangle() function. We can draw circles, lines, text, and other shapes as well. These drawing functions may be useful later on, to help annotate images that our programs produce. Practice some of these functions here.\nCircles can be drawn with the skimage.draw.disk() function, which takes two parameters: the (ry, cx) point of the centre of the circle, and the radius of the circle. There is an optional shape parameter that can be supplied to this function. It will limit the output coordinates for cases where the circle dimensions exceed the ones of the image.\nLines can be drawn with the skimage.draw.line() function, which takes four parameters: the (ry, cx) coordinate of one end of the line, and the (ry, cx) coordinate of the other end of the line.\nOther drawing functions supported by skimage can be found in the skimage reference pages.\nFirst let’s make an empty, black image with a size of 800x600 pixels:\n\n# create the black canvas\nimage = np.zeros(shape=(600, 800, 3), dtype=\"uint8\")\n\n\n# Draw a blue circle with centre (200, 300) in (ry, cx) coordinates, and radius 100\nrr, cc = skimage.draw.disk(center=(200, 300), radius=100, shape=image.shape[0:2])\nimage[rr, cc] = (0, 0, 255)\n\n\n# Draw a green line from (400, 200) to (500, 700) in (ry, cx) coordinates\nrr, cc = skimage.draw.line(r0=400, c0=200, r1=500, c1=700)\nimage[rr, cc] = (0, 255, 0)\n\n\n# Display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f9064a6e0>\n\n\n\n\n\nWe could expand this solution, if we wanted, to draw rectangles, circles and lines at random positions within our black canvas. To do this, we could use the random python module, and the function random.randrange, which can produce random numbers within a certain range.\nLet’s draw 15 randomly placed circles:\n\nimport random\n\n# create the black canvas\nimage = np.zeros(shape=(600, 800, 3), dtype=\"uint8\")\n\n# draw a blue circle at a random location 15 times\nfor i in range(15):\n    rr, cc = skimage.draw.disk(center=(\n         random.randrange(600),\n         random.randrange(800)),\n         radius=50,\n         shape=image.shape[0:2],\n        )\n    image[rr, cc] = (0, 0, 255)\n\n# display the results\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f906bbaf0>\n\n\n\n\n\nWe could expand this even further to also randomly choose whether to plot a rectangle, a circle, or a square. Again, we do this with the random module, now using the function random.random that returns a random number between 0.0 and 1.0.\n\n# Draw 15 random shapes (rectangle, circle or line) at random positions\nfor i in range(15):\n    # generate a random number between 0.0 and 1.0 and use this to decide if we\n    # want a circle, a line or a sphere\n    x = random.random()\n    if x < 0.33:\n        # draw a blue circle at a random location\n        rr, cc = skimage.draw.disk(center=(\n            random.randrange(600),\n            random.randrange(800)),\n            radius=50,\n            shape=image.shape[0:2],\n        )\n        color = (0, 0, 255)\n    elif x < 0.66:\n        # draw a green line at a random location\n        rr, cc = skimage.draw.line(\n            r0=random.randrange(600),\n            c0=random.randrange(800),\n            r1=random.randrange(600),\n            c1=random.randrange(800),\n        )\n        color = (0, 255, 0)\n    else:\n        # draw a red rectangle at a random location\n        rr, cc = skimage.draw.rectangle(\n            start=(random.randrange(600), random.randrange(800)),\n            extent=(50, 50),\n            shape=image.shape[0:2],\n        )\n        color = (255, 0, 0)\n\n    image[rr, cc] = color\n\n# display the results\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f906fb5e0>\n\n\n\n\n\n\n\nImage modification\nAll that remains is the task of modifying the image using our mask in such a way that the areas with True pixels in the mask are not shown in the image any more.\n\n\n\n\n\n\nNow, consider the mask image we created above. The values of the mask that corresponds to the portion of the image we are interested in are all False, while the values of the mask that corresponds to the portion of the image we want to remove are all True. How do we change the original image using the mask?\n\n\n\n\n\nWhen indexing the image using the mask, we access only those pixels at positions where the mask is True. So, when indexing with the mask, one can set those values to 0, and effectively remove them from the image.\n\n\n\nNow we can write a Python program to use a mask to retain only the portions of our maize roots image that actually contains the seedling roots. We load the original image and create the mask in the same way as before:\n\n# Load the original image\nimage = iio.imread(uri=\"Images/maize-seedlings.tif\")\n\n# Create the basic mask\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n# Draw a filled rectangle on the mask image\nrr, cc = skimage.draw.rectangle(start=(357, 44), end=(740, 720))\nmask[rr, cc] = False\n\nThen, we use numpy indexing to remove the portions of the image, where the mask is True:\n\n# Apply the mask\nimage[mask] = 0\n\nThen, we display the masked image.\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f90c853f0>\n\n\n\n\n\n\n\nMasked monster truck\nI’ll now try to mask an image of 32 Degrees from my son’s monster truck collection!\n\n# Load and display the original image\nimage = iio.imread(uri=\"Images/monster_truck.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f90c86470>\n\n\n\n\n\n\n# Create the basic mask\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n\n# Draw filled rectangle on the mask image\nrr, cc = skimage.draw.rectangle(start=(900, 900), end=(2200, 3000))\nmask[rr, cc] = False\n\n# Display mask image\nfig, ax = plt.subplots()\nplt.imshow(mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f90c85810>\n\n\n\n\n\n\nimage_copy = image.copy()\n\n# Apply the mask\nimage_copy[mask] = 0\n\n\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f2f910be050>\n\n\n\n\n\n\n\nMasking - iterating through a co-ordinates file\nConsider this image of a 96-well plate that has been scanned on a flatbed scanner:\n\n# Load the image\nimage = iio.imread(uri=\"Images/wellplate-01.jpg\")\n\n# Display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f90fe3d90>\n\n\n\n\n\nSuppose that we are interested in the colours of the solutions in each of the wells. We do not care about the colour of the rest of the image, i.e., the plastic that makes up the well plate itself.\nYour task is to write some code that will produce a mask that will mask out everything except for the wells. To help with this, you should use the text file data/centers.txt that contains the (cx, ry) coordinates of the centre of each of the 96 wells in this image. You may assume that each of the wells has a radius of 16 pixels. Your program should produce output that looks like this:\n\n\n# read in original image\nimage = iio.imread(uri=\"Images/wellplate-01.jpg\")\n\n# create the mask image\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n# open and iterate through the centers file...\nwith open(\"Images/centers.txt\", \"r\") as center_file:\n    for line in center_file:\n        # ... getting the coordinates of each well...\n        coordinates = line.split()\n        cx = int(coordinates[0])\n        ry = int(coordinates[1])\n\n        # ... and drawing a circle on the mask\n        rr, cc = skimage.draw.disk(center=(ry, cx), radius=16, shape=image.shape[0:2])\n        mask[rr, cc] = False\n\n# apply the mask\nimage_copy = image.copy()\nimage_copy[mask] = 0\n\n# display the result\nfig, ax = plt.subplots()\nplt.imshow(image_copy)\n\n<matplotlib.image.AxesImage at 0x7f2f905eabc0>\n\n\n\n\n\n\n\nMasking - using nested for loops\n\n# read in original image\nimage = iio.imread(uri=\"Images/wellplate-01.jpg\")\n\n# create the mask image\nmask = np.ones(shape=image.shape[0:2], dtype=\"bool\")\n\n# upper left well coordinates\ncx0 = 91\nry0 = 108\n\n# spaces between wells\ndeltaCX = 70\ndeltaRY = 72\n\ncx = cx0\nry = ry0\n\n# iterate each row and column\nfor row in range(12):\n    # reset cx to leftmost well in the row\n    cx = cx0\n    for col in range(8):\n\n        # ... and drawing a circle on the mask\n        rr, cc = skimage.draw.disk(center=(ry, cx), radius=16, shape=image.shape[0:2])\n        mask[rr, cc] = False\n        cx += deltaCX\n    # after one complete row, move to next row\n    ry += deltaRY\n\n# apply the mask\ncopy_image=image.copy()\ncopy_image[mask] = 0\n\n# display the result\nfig, ax = plt.subplots()\nplt.imshow(copy_image)\n\n<matplotlib.image.AxesImage at 0x7f2f905ea080>\n\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nwe can use the NumPy zeros() function to create a blank, black image\nwe can draw on skimage images with functions such as skimage.draw.rectangle(), skimage.draw.disk(), skimage.draw.line(), and more\nthe drawing functions return indices to pixels that can be set directly"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#creating-histograms",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#creating-histograms",
    "title": "Image Processing with Python",
    "section": "4. Creating Histograms",
    "text": "4. Creating Histograms\nIn this section, we will learn how to use skimage functions to create and display histograms for images.\n\nIntroduction to Histograms\nAs it pertains to images, a histogram is a graphical representation showing how frequently various colour values occur in the image. We saw in section 1 that we could use a histogram to visualise the differences in uncompressed and compressed image formats. If our project involves detecting colour changes between images, histograms will prove to be very useful, and histograms are also quite handy as a preparatory step before performing thresholding.\n\n\nGrayscale Histograms\nWe will start with grayscale images, and then move on to colour images. We will use an image of a plant seedling as an example. ere we load the image in grayscale instead of full colour, and display it.\nAgain, we use the iio.imread() function to load our image. The first argument to iio.imread() is the filename of the image. The second argument mode=“L” defines the type and depth of a pixel in the image (e.g., an 8-bit pixel has a range of 0-255). This argument is forwarded to the pillow backend, for which mode “L” means 8-bit pixels and single-channel (i.e., grayscale). pillow is a Python imaging library; which backend is used by iio.imread() may be specified (to use pillow, you would pass this argument: plugin=“pillow”); if unspecified, iio.imread() determines the backend to use based on the image type.\nThen, we convert the grayscale image of integer dtype, with 0-255 range, into a floating-point one with 0-1 range, by calling the function skimage.util.img_as_float. We will keep working with images in the value range 0 to 1 in this section.\n\nimport imageio.v3 as iio\nimport numpy as np\nimport skimage.color\nimport skimage.util\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# read the image of a plant seedling as grayscale from the outset\nimage = iio.imread(uri=\"Images/plant-seedling.jpg\", mode=\"L\")\n\n# convert the image to float dtype with a value range from 0 to 1\nimage = skimage.util.img_as_float(image)\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f90393a60>\n\n\n\n\n\nWe now use the function np.histogram to compute the histogram of our image which, after all, is a NumPy array.\nThe parameter bins determines the number of “bins” to use for the histogram. We pass in 256 because we want to see the pixel count for each of the 256 possible values in the grayscale image.\nThe parameter range is the range of values each of the pixels in the image can have. Here, we pass 0 and 1, which is the value range of our input image after transforming it to grayscale.\nThe first output of the np.histogram function is a one-dimensional NumPy array, with 256 rows and one column, representing the number of pixels with the intensity value corresponding to the index. I.e., the first number in the array is the number of pixels found with intensity value 0, and the final number in the array is the number of pixels found with intensity value 255.\nThe second output of np.histogram is an array with the bin edges and one column and 257 rows (one more than the histogram itself). There are no gaps between the bins, which means that the end of the first bin, is the start of the second and so on. For the last bin, the array also has to contain the stop, so it has one more element, than the histogram.\n\n# create the histogram\nhistogram, bin_edges = np.histogram(image, bins=256, range=(0, 1))\n\nNext, we turn our attention to displaying the histogram, by taking advantage of the plotting facilities of the matplotlib library. We create the plot with plt.figure(), then label the figure and the coordinate axes with plt.title(), plt.xlabel(), and plt.ylabel() functions. We then set the limits on the values on the x-axis with the plt.xlim([0.0, 1.0]) function call, before creating the histogram plot itself with plt.plot(bin_edges[0:-1], histogram).\nWe use the left bin edges as x-positions for the histogram values by indexing the bin_edges array to ignore the last value (the right edge of the last bin). When we run the program on this image of a plant seedling, it produces this histogram:\n\n# configure and draw the histogram figure\nplt.figure()\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixel count\")\nplt.xlim([0.0, 1.0])  # <- named arguments do not work here\n\nplt.plot(bin_edges[0:-1], histogram)  # <- or here\n\n\n\n\n\n\n\n\n\n\nNote that we cannot used named parameters for the plt.xlim() or plt.plot() functions.\n\n\n\nThis is because these functions are defined to take an arbitrary number of unnamed arguments. The designers wrote the functions this way because they are very versatile, and creating named parameters for all of the possible ways to use them would be complicated.\n\n\n\n\n\n\n\n\nMatplotlib provides a dedicated function to compute and display histograms: plt.hist().\n\n\n\nWe will not use it in this section in order to understand how to calculate histograms in more detail. In practice, it is a good idea to use this function, because it visualises histograms more appropriately than plt.plot(). Here, we could use it by calling:\n**plt.hist(image.flatten(), bins=256, range=(0, 1))** \ninstead of np.histogram() and plt.plot()\n.flatten() is a numpy function that converts our two-dimensional image into a one-dimensional array.This is because these functions are defined to take an arbitrary number of unnamed arguments. The designers wrote the functions this way because they are very versatile, and creating named parameters for all of the possible ways to use them would be complicated.\n\n\nLooking at the histogram above, you will notice that there is a large number of very dark pixels, as indicated in the chart by the spike around the grayscale value 0.12. That is not so surprising, since the original image is mostly black background. What if we want to focus more closely on the leaf of the seedling? That is where a mask enters the picture!\nLet’s hover over the plant seedling image with your mouse to determine the (x, y) coordinates of a bounding box around the leaf of the seedling. Then, using techniques from section 3, we can create a mask with a white rectangle covering that bounding box. After we have created the mask, we can apply it to the input image before passing it to the np.histogram function.\n\n# Load and display the original image\nimage = iio.imread(uri=\"Images/plant-seedling.jpg\", mode = \"L\")\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f8ba21de0>\n\n\n\n\n\n\ncopy_image = image.copy()\n\n# create mask here, using np.zeros() and skimage.draw.rectangle()\nmask = np.zeros(shape=copy_image.shape, dtype=\"bool\")\n\n# Draw filled rectangle on the mask image\n# co-ordinates are (row, column)\n# first co-ord is top-left, second co-ord is bottom-right\nrr, cc = skimage.draw.rectangle(start=(199,410), end=(384,485))\nmask[rr, cc] = True\n\n\n# Display the mask\nfig, ax = plt.subplots()\nplt.imshow(mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f2f8b83b700>\n\n\n\n\n\n\n# mask the image and create the new histogram\nhistogram, bin_edges = np.histogram(copy_image[mask], bins=256, range=(0.0, 1.0))\n\n# configure and draw the histogram figure\nplt.figure()\n\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixel count\")\nplt.xlim([0.0, 1.0])\nplt.plot(bin_edges[0:-1], histogram)\n\nYour histogram of the masked area should look something like this:\n\n\n\nColour Histograms\nWe can also create histograms for full colour images, in addition to grayscale histograms. We have seen colour histograms before, in the first section. A program to create colour histograms starts in a familiar way. We read the original image, now in full colour, and display it:\n\n# read original image, in full color\nimage = iio.imread(uri=\"Images/plant-seedling.jpg\")\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f2f8b772980>\n\n\n\n\n\nNext, we create the histogram, by calling the np.histogram function three times, once for each of the channels. We obtain the individual channels, by slicing the image along the last axis. For example, we can obtain the red colour channel by calling:\nr_chan = image[:, :, 0]\nWe will draw the histogram line for each channel in a different colour, and so we create a tuple of the colours to use for the three lines with the:\ncolors = (\"red\", \"green\", \"blue\")\nline of code. Then, we limit the range of the x-axis with the plt.xlim() function call.\nNext, we use the for control structure to iterate through the three channels, plotting an appropriately-coloured histogram line for each. This may be new Python syntax for you, so we will take a moment to discuss what is happening in the for statement.\nThe Python built-in enumerate() function takes a list and returns an iterator of tuples, where the first element of the tuple is the index and the second element is the element of the list.\n\n# tuple to select colors of each channel line\ncolors = (\"red\", \"green\", \"blue\")\n\n# create the histogram plot, with three lines, one for\n# each color\nplt.figure()\nplt.xlim([0, 256])\n\n# using a tuple, (channel_id, color), as the for variable\n\nfor channel_id, color in enumerate(colors):\n    histogram, bin_edges = np.histogram(\n        image[:, :, channel_id], bins=256, range=(0, 256)\n    )\n    plt.plot(bin_edges[0:-1], histogram, color=color)\n    \n# label our axes and display the histogram\nplt.title(\"Color Histogram\")\nplt.xlabel(\"Color value\")\nplt.ylabel(\"Pixel count\")\n\nText(0, 0.5, 'Pixel count')\n\n\n\n\n\nIn our colour histogram program, we are using a tuple, (channel_id, color), as the for variable. The first time through the loop, the channel_id variable takes the value 0, referring to the position of the red colour channel, and the color variable contains the string “red”. The second time through the loop the values are the green channels index 1 and “green”, and the third time they are the blue channel index 2 and “blue”.\nInside the for loop, our code looks much like it did for the grayscale example. We calculate the histogram for the current channel with the:\nhistogram, bin_edges = np.histogram(image[:, :, channel_id], bins=256, range=(0, 256))\nfunction call, and then add a histogram line of the correct colour to the plot with the:\nplt.plot(bin_edges[0:-1], histogram, color=color)\nfunction call.\n\n\n\n\n\n\nIterators, tuples, and enumerate()\n\n\n\nIn Python, an iterator, or an iterable object, is something that can be iterated over with the for control structure. A tuple is a sequence of objects, just like a list. However, a tuple cannot be changed, and a tuple is indicated by (parentheses) instead of [square brackets]. The enumerate() function takes an iterable object, and returns an iterator of tuples consisting of the 0-based index and the corresponding object.\n\n\nFor example, consider this small Python program:\n\nlist = (\"a\", \"b\", \"c\", \"d\", \"e\")\n\nfor x in enumerate(list):\n    print(x)\n\n(0, 'a')\n(1, 'b')\n(2, 'c')\n(3, 'd')\n(4, 'e')\n\n\n\n\nColour histogram with a mask\nWe can also apply a mask to the images we apply the colour histogram process to, in the same way we did for grayscale histograms. Consider this image of a well plate, where various chemical sensors have been applied to water and various concentrations of hydrochloric acid and sodium hydroxide:\n\nSuppose we are interested in the colour histogram of one of the sensors in the well plate image, specifically, the seventh well from the left in the topmost row, which shows Erythrosin B reacting with water.Hover over the image with your mouse to find the centre of that well and the radius (in pixels) of the well. Then create a circular mask to select only the desired well. Then, use that mask to apply the colour histogram operation to that well.\nYour masked image should look like this:\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nin many cases, we can load images in grayscale by passing the mode=“L” argument to the iio.imread() function.\nwe can create histograms of images with the np.histogram function.\nwe can separate the RGB channels of an image using slicing operations.\nwe can display histograms using the matplotlib pyplot figure(), title(), xlabel(), ylabel(), xlim(), plot(), and show() functions."
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#blurring-images",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#blurring-images",
    "title": "Image Processing with Python",
    "section": "5. Blurring Images",
    "text": "5. Blurring Images\nIn this section, we will learn how to use skimage functions to blur images.\nWhen processing an image, we are often interested in identifying objects represented within it so that we can perform some further analysis of these objects e.g. by counting them, measuring their sizes, etc. An important concept associated with the identification of objects in an image is that of edges: the lines that represent a transition from one group of similar pixels in the image to another different group. One example of an edge is the pixels that represent the boundaries of an object in an image, where the background of the image ends and the object begins.\nWhen we blur an image, we make the colour transition from one side of an edge in the image to another smooth rather than sudden. The effect is to average out rapid changes in pixel intensity. A blur is a very common operation we need to perform before other tasks such as thresholding.\nThere are several different blurring functions in the skimage.filters module, so we will focus on just one here, the Gaussian blur.\n\nFilters\nIn the day-to-day, macroscopic world, we have physical filters which separate out objects by size. A filter with small holes allows only small objects through, leaving larger objects behind. This is a good analogy for image filters. A high-pass filter will retain the smaller details in an image, filtering out the larger ones. A low-pass filter retains the larger features, analogous to what’s left behind by a physical filter mesh. High- and low-pass, here, refer to high and low spatial frequencies in the image. Details associated with high spatial frequencies are small, a lot of these features would fit across an image. Features associated with low spatial frequencies are large - maybe a couple of big features per image.\n\n\nBlurring\nBlurring is to make something less clear or distinct. This could be interpreted quite broadly in the context of image analysis - anything that reduces or distorts the detail of an image might apply. Applying a low pass filter, which removes detail occurring at high spatial frequencies, is perceived as a blurring effect. A Gaussian blur is a filter that makes use of a Gaussian kernel.\n\n\nKernels\nA kernel can be used to implement a filter on an image. A kernel, in this context, is a small matrix which is combined with the image using a mathematical technique: convolution. Different sizes, shapes and contents of kernel produce different effects. The kernel can be thought of as a little image in itself, and will favour features of a similar size and shape in the main image. On convolution with an image, a big, blobby kernel will retain big, blobby, low spatial frequency features.\n\n\nGaussian blur\nConsider this image of a cat, in particular the area of the image outlined by the white square.\n\n\n\ncat.jpg\n\n\nNow, zoom in on the area of the cat’s eye, as shown in the left-hand image below. When we apply a filter, we consider each pixel in the image, one at a time. In this example, the pixel we are currently working on is highlighted in red, as shown in the right-hand image.\n\n\n\ncat-eye-pixels.jpg\n\n\nWhen we apply a filter, we consider rectangular groups of pixels surrounding each pixel in the image, in turn. The kernel is another group of pixels (a separate matrix / small image), of the same dimensions as the rectangular group of pixels in the image, that moves along with the pixel being worked on by the filter. The width and height of the kernel must be an odd number, so that the pixel being worked on is always in its centre. In the example shown above, the kernel is square, with a dimension of seven pixels.\nTo apply the kernel to the current pixel, an average of the the colour values of the pixels surrounding it is calculated, weighted by the values in the kernel. In a Gaussian blur, the pixels nearest the centre of the kernel are given more weight than those far away from the centre. The rate at which this weight diminishes is determined by a Gaussian function, hence the name Gaussian blur.\nA Gaussian function maps random variables into a normal distribution or “Bell Curve”.\n\nhttps://en.wikipedia.org/wiki/Gaussian_function#/media/File:Normal_Distribution_PDF.svg\nThe shape of the function is described by a mean value μ, and a variance value σ². The mean determines the central point of the bell curve on the x axis, and the variance describes the spread of the curve. In fact, when using Gaussian functions in Gaussian blurring, we use a 2D Gaussian function to account for X and Y dimensions, but the same rules apply. The mean μ is always 0, and represents the middle of the 2D kernel. Increasing values of σ² in either dimension increases the amount of blurring in that dimension.\n\n\n\nGaussian_2D.png\n\n\nThe averaging is done on a channel-by-channel basis, and the average channel values become the new value for the pixel in the filtered image. Larger kernels have more values factored into the average, and this implies that a larger kernel will blur the image more than a smaller kernel. To get an idea of how this works, consider this plot of the two-dimensional Gaussian function:\n\n\n\ngaussian-kernel.png\n\n\nImagine that plot laid over the kernel for the Gaussian blur filter. The height of the plot corresponds to the weight given to the underlying pixel in the kernel. I.e., the pixels close to the centre become more important to the filtered pixel colour than the pixels close to the outer limits of the kernel. The shape of the Gaussian function is controlled via its standard deviation, or sigma:\n\na large sigma value results in a flatter shape, while\na smaller sigma value results in a more pronounced peak.\n\nThe mathematics involved in the Gaussian blur filter are not quite that simple, but this explanation gives you the basic idea. To illustrate the blur process, consider the blue channel colour values from the seven-by-seven region of the cat image above:\n\n\n\ncat-corner-blue.png\n\n\nThe filter is going to determine the new blue channel value for the centre pixel – the one that currently has the value 86. The filter calculates a weighted average of all the blue channel values in the kernel giving higher weight to the pixels near the centre of the kernel.\n\n\n\ncombination.png\n\n\nThis weighted average, the sum of the multiplications, becomes the new value for the centre pixel (3, 3). The same process would be used to determine the green and red channel values, and then the kernel would be moved over to apply the filter to the next pixel in the image.\n\n\nImage edges\nSomething different needs to happen for pixels near the outer limits of the image, since the kernel for the filter may be partially off the image. For example, what happens when the filter is applied to the upper-left pixel of the image? Here are the blue channel pixel values for the upper-left pixel of the cat image, again assuming a seven-by-seven kernel:\n\n\n\nedges.JPG\n\n\nThe upper-left pixel is the one with value 4. Since the pixel is at the upper-left corner, there are no pixels underneath much of the kernel; here, this is represented by x’s. So, what does the filter do in that situation? The default mode is to fill in the nearest pixel value from the image. For each of the missing x’s the image value closest to the x is used. If we fill in a few of the missing pixels, you will see how this works:\n\n\n\nedges_2.JPG\n\n\nAnother strategy to fill those missing values is to reflect the pixels that are in the image to fill in for the pixels that are missing from the kernel.\n\n\n\nreflect.JPG\n\n\nA similar process would be used to fill in all of the other missing pixels from the kernel. Other border modes are available; you can learn more about them in the skimage documentation. This animation shows how the blur kernel moves along in the original image in order to calculate the colour channel values for the blurred image.\n\n\n\nblur-demo.gif\n\n\nskimage has built-in functions to perform blurring for us, so we do not have to perform all of these mathematical operations ourselves. Let’s work through an example of blurring an image with the skimage Gaussian blur function.\nFirst, we load the image, and display it:\n\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\nimport skimage.filters\n\nimage = iio.imread(uri=\"Images/gaussian-original.png\")\n\n# display the image\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7fa014b526e0>\n\n\n\n\n\nNext, we apply the gaussian blur:\n\nsigma = 3.0\n\n# apply Gaussian blur, creating a new image\nblurred = skimage.filters.gaussian(\n    image, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n\nThe first two parameters to skimage.filters.gaussian() are the image to blur, image, and a tuple defining the sigma to use in ry- and cx-direction, (sigma, sigma). The third parameter truncate gives the radius of the kernel in terms of sigmas. A Gaussian function is defined from -infinity to +infinity, but our kernel (which must have a finite, smaller size) can only approximate the real function. Therefore, we must choose a certain distance from the centre of the function where we stop this approximation, and set the final size of our kernel. In the above example, we set truncate to 3.5, which means the kernel size will be 2 * sigma * 3.5. For example, for a sigma of 1.0 the resulting kernel size would be 7, while for a sigma of 2.0 the kernel size would be 14. The default value for truncate in scikit-image is 4.0.\nThe last parameter to skimage.filters.gaussian() tells skimage to interpret our image, that has three dimensions, as a multichannel colour image.\nFinally, we display the blurred image:\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n<matplotlib.image.AxesImage at 0x7f9fdadf2500>\n\n\n\n\n\nThe size and shape of the kernel used to blur an image can have a significant effect on the result of the blurring and any downstream analysis carried out on the blurred image. Let’s now experiment with the sigma values of the kernel, as this is a good way to develop our understanding of how the choice of kernel can influence the result of blurring.\n\n\n\n\n\n\nGenerally speaking, what effect does the sigma value have on the blurred image?\n\n\n\n\n\nGenerally speaking, the larger the sigma value, the more blurry the result. A larger sigma will tend to get rid of more noise in the image, which will help for other operations we will cover soon, such as thresholding. However, a larger sigma also tends to eliminate some of the detail from the image. So, we must strike a balance with the sigma value used for blur filters.\n\n\n\n\nsigma = 1.0\n\n# apply Gaussian blur, creating a new image\nblurred = skimage.filters.gaussian(\n    image, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n<matplotlib.image.AxesImage at 0x7f9fda93f7f0>\n\n\n\n\n\n\nsigma = 5.0\n\n# apply Gaussian blur, creating a new image\nblurred = skimage.filters.gaussian(\n    image, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n<matplotlib.image.AxesImage at 0x7f9fda9b6470>\n\n\n\n\n\n\n\n\n\n\n\nKernel shape - what is the effect of applying an asymmetric kernel to blurring an image?\n\n\n\n\n\nThese unequal sigma values produce a kernel that is rectangular instead of square. The result is an image that is much more blurred in the x direction than the y direction or vice versa. For most use cases, a uniform blurring effect is desirable and this kind of asymmetric blurring should be avoided. However, it can be helpful in specific circumstances e.g. when noise is present in your image in a particular pattern or orientation, such as vertical lines, or when you want to remove uniform noise without blurring edges present in the image in a particular orientation.\n\n\n\n\n# apply Gaussian blur, with a sigma of 1.0 in the ry direction, and 6.0 in the cx direction\nblurred = skimage.filters.gaussian(\n    image, sigma=(1.0, 6.0), truncate=3.5, multichannel=True\n)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n/tmp/ipykernel_2635/3991388852.py:2: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n  blurred = skimage.filters.gaussian(\n\n\n<matplotlib.image.AxesImage at 0x7f9fda82f160>\n\n\n\n\n\n\n# apply Gaussian blur, with a sigma of 6.0 in the ry direction, and 1.0 in the cx direction\nblurred = skimage.filters.gaussian(\n    image, sigma=(6.0, 1.0), truncate=3.5, multichannel=True\n)\n\n# display blurred image\nfig, ax = plt.subplots()\nplt.imshow(blurred)\n\n/tmp/ipykernel_2635/1775552283.py:2: FutureWarning: `multichannel` is a deprecated argument name for `gaussian`. It will be removed in version 1.0. Please use `channel_axis` instead.\n  blurred = skimage.filters.gaussian(\n\n\n<matplotlib.image.AxesImage at 0x7f9fda8a3ca0>\n\n\n\n\n\n\n\nOther methods of blurring\nThe Gaussian blur is a way to apply a low-pass filter in skimage. It is often used to remove Gaussian (i. e., random) noise from the image. For other kinds of noise, e.g. “salt and pepper” or “static” noise, a median filter is typically used. See the skimage.filters documentation for a list of available filters.\n\n\n\n\n\n\nKey Points:\n\n\n\n\napplying a low-pass blurring filter smooths edges and removes noise from an image\nblurring is often used as a first step before we perform thresholding or edge detection\nthe Gaussian blur can be applied to an image with the skimage.filters.gaussian() function\nlarger sigma values may remove more noise, but they will also remove detail from an image"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#thresholding",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#thresholding",
    "title": "Image Processing with Python",
    "section": "6. Thresholding",
    "text": "6. Thresholding\nIn this section, we will learn how to use skimage functions to apply thresholding to an image. Thresholding is a type of image segmentation, where we change the pixels of an image to make the image easier to analyze. In thresholding, we convert an image from colour or grayscale into a binary image, i.e., one that is simply black and white. Most frequently, we use thresholding as a way to select areas of interest of an image, while ignoring the parts we are not concerned with. We have already done some simple thresholding, in section 2 Working with skimage. In that case, we used a simple NumPy array manipulation to separate the pixels belonging to the root system of a plant from the black background. In this section, we will learn how to use skimage functions to perform thresholding. Then, we will use the masks returned by these functions to select the parts of an image we are interested in.\n\nSimple thresholding\nConsider the image Images/shapes-01.jpg with a series of crudely cut shapes set against a white background.\n\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport skimage.color\nimport skimage.filters\n\n%matplotlib inline\n\n# load the image\nimage = iio.imread(uri=\"Images/shapes-01.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f9fdae61720>\n\n\n\n\n\nNow suppose we want to select only the shapes from the image. In other words, we want to leave the pixels belonging to the shapes “on,” while turning the rest of the pixels “off,” by setting their colour channel values to zeros. The skimage library has several different methods of thresholding. We will start with the simplest version, which involves an important step of human input. Specifically, in this simple, fixed-level thresholding, we have to provide a threshold value t.\nThe process works like this. First, we will load the original image, convert it to grayscale, and de-noise it as in the Blurring Images section:\n\n# convert the image to grayscale\ngray_image = skimage.color.rgb2gray(image)\n\n# blur the image to denoise\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\n\nfig, ax = plt.subplots()\nplt.imshow(blurred_image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9c5f850>\n\n\n\n\n\nNext, we would like to apply the threshold t such that pixels with grayscale values on one side of t will be turned “on”, while pixels with grayscale values on the other side will be turned “off”. How might we do that? Remember that grayscale images contain pixel values in the range from 0 to 1, so we are looking for a threshold t in the closed range [0.0, 1.0]. We see in the image that the geometric shapes are “darker” than the white background but there is also some light gray noise on the background. One way to determine a “good” value for t is to look at the grayscale histogram of the image and try to identify what grayscale ranges correspond to the shapes in the image or the background.\nThe histogram for the shapes image shown above can be produced as in the Creating Histograms section.\n\n# create a histogram of the blurred grayscale image\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\n\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixels\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nSince the image has a white background, most of the pixels in the image are white. This corresponds nicely to what we see in the histogram: there is a peak near the value of 1.0. If we want to select the shapes and not the background, we want to turn off the white background pixels, while leaving the pixels for the shapes turned on. So, we should choose a value of t somewhere before the large peak and turn pixels above that value “off”. Let us choose t=0.8.\nTo apply the threshold t, we can use the numpy comparison operators to create a mask. Here, we want to turn “on” all pixels which have values smaller than the threshold, so we use the less operator < to compare the blurred_image to the threshold t. The operator returns a mask, that we capture in the variable binary_mask. It has only one channel, and each of its values is either 0 or 1. The binary mask created by the thresholding operation can be shown with plt.imshow, where the False entries are shown as black pixels (0-valued) and the True entries are shown as white pixels (1-valued).\n\n# create a mask based on the threshold\nt = 0.8\nbinary_mask = blurred_image < t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fda7add20>\n\n\n\n\n\nYou can see that the areas where the shapes were in the original area are now white, while the rest of the mask image is black.\n\n\nWhat makes a good threshold?\nAs is often the case, the answer to this question is “it depends”. In the example above, we could have just switched off all the white background pixels by choosing t=1.0, but this would leave us with some background noise in the mask image. On the other hand, if we choose too low a value for the threshold, we could lose some of the shapes that are too bright. We can experiment with the threshold by re-running the above code lines with different values for t.\nIn practice, it is a matter of domain knowledge and experience to interpret the peaks in the histogram so to determine an appropriate threshold. The process often involves trial and error, which is a drawback of the simple thresholding method. Below we will introduce automatic thresholding, which uses a quantitative, mathematical definition for a good threshold that allows us to determine the value of t automatically. It is worth noting that the principle for simple and automatic thresholding can also be used for images with pixel ranges other than [0.0, 1.0]. For example, we could perform thresholding on pixel intensity values in the range [0, 255] as we have already seen in the Image Representation in skimage section.\nWe can now apply the binary_mask to the original coloured image as we learned in the Drawing and Bitwise Operations section. What we are left with is only the coloured shapes from the original.\n\n# use the binary_mask to select the \"interesting\" part of the image\nselection = image.copy()\nselection[~binary_mask] = 0\n\nfig, ax = plt.subplots()\nplt.imshow(selection)\n\n<matplotlib.image.AxesImage at 0x7f9fd99cffa0>\n\n\n\n\n\nSuppose we want to use simple thresholding to select only the coloured shapes (in this particular case we consider grayish to be a colour, too) from the image data/shapes-02.jpg:\n\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport skimage.color\nimport skimage.filters\n\n%matplotlib inline\n\n# load the image\nimage = iio.imread(uri=\"Images/shapes-02.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f9fd9a56770>\n\n\n\n\n\n\n# convert the image to grayscale\ngray_image = skimage.color.rgb2gray(image)\n\n# blur the image to denoise\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\n\nfig, ax = plt.subplots()\nplt.imshow(blurred_image, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9b598a0>\n\n\n\n\n\n\n# create a histogram of the blurred grayscale image\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\n\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Grayscale Histogram\")\nplt.xlabel(\"grayscale value\")\nplt.ylabel(\"pixels\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nWe can see a large spike around 0.3, and a smaller spike around 0.7. The spike near 0.3 represents the darker background, so it seems like a value close to t=0.5 would be a good choice.\n\n\n\n\n\n\nNote that unlike the image with a white background we used above, here the peak for the background colour is at a lower gray level than the shapes. Therefore, change the comparison operator less < to greater > to create the appropriate mask. Then apply the mask to the image and view the thresholded image. If everything works as it should, our output should show only the coloured shapes on a black background.\n\n\n\n\n\n\n\n# create a mask based on the threshold\nt = 0.5\nbinary_mask = blurred_image > t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9845300>\n\n\n\n\n\nAnd here are the commands to apply the mask and view the thresholded image:\n\nimage = iio.imread(uri=\"Images/shapes-02.jpg\")\nselection = image.copy()\nselection[~binary_mask] = 0\n\nfig, ax = plt.subplots()\nplt.imshow(selection)\n\n<matplotlib.image.AxesImage at 0x7f9fd971ad40>\n\n\n\n\n\n\n\nAutomatic thresholding\nThe downside of the simple thresholding technique is that we have to make an educated guess about the threshold t by inspecting the histogram. There are also automatic thresholding methods that can determine the threshold automatically for us. One such method is **Otsu’s method. It is particularly useful for situations where the grayscale histogram of an image has two peaks that correspond to background and objects of interest.\n\n\nDenoising an image before thresholding\nIn practice, it is often necessary to denoise the image before thresholding, which can be done with one of the methods from the Blurring Images section. Consider the image data/maize-root-cluster.jpg of a maize root system which we saw before in the Image Representation in skimage section:\n\nimage = iio.imread(uri=\"Images/maize_cluster.jpg\")\n\nfig, ax = plt.subplots()\nplt.imshow(image)\n\n<matplotlib.image.AxesImage at 0x7f9fda4de470>\n\n\n\n\n\nWe use Gaussian blur with a sigma of 1.0 to denoise the root image. Let us look at the grayscale histogram of the denoised image:\n\n# convert the image to grayscale\ngray_image = skimage.color.rgb2gray(image)\n\n# blur the image to denoise\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\n\n# show the histogram of the blurred image\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Graylevel histogram\")\nplt.xlabel(\"gray value\")\nplt.ylabel(\"pixel count\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nThe histogram has a significant peak around 0.2, and a second, smaller peak very near 1.0. Thus, this image is a good candidate for thresholding with Otsu’s method. The mathematical details of how this works are complicated (see the skimage documentation if you are interested), but the outcome is that Otsu’s method finds a threshold value between the two peaks of a grayscale histogram.\nThe skimage.filters.threshold_otsu() function can be used to determine the threshold automatically via Otsu’s method. Then numpy comparison operators can be used to apply it as before. Here are the Python commands to determine the threshold t with Otsu’s method:\n\n# perform automatic thresholding\nt = skimage.filters.threshold_otsu(blurred_image)\nprint(\"Found automatic threshold t = {}.\".format(t))\n\nFound automatic threshold t = 0.43393258215217667.\n\n\nFor this root image and a Gaussian blur with the chosen sigma of 1.0, the computed threshold value is 0.43. No we can create a binary mask with the comparison operator >. As we have seen before, pixels above the threshold value will be turned on, those below the threshold will be turned off.\n\n# create a binary mask with the threshold found by Otsu's method\nbinary_mask = blurred_image > t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd9b58cd0>\n\n\n\n\n\nFinally, we use the mask to select the foreground:\n\n# apply the binary mask to select the foreground\nselection = image.copy()\nselection[~binary_mask] = 0\n\nfig, ax = plt.subplots()\nplt.imshow(selection)\n\n<matplotlib.image.AxesImage at 0x7f9fcfa9b550>\n\n\n\n\n\n\n\nApplication: measuring root mass\nLet us now turn to an application where we can apply thresholding and other techniques we have learned to this point. Consider these four maize root system images, which we can find in the files data/trial-016.jpg, data/trial-020.jpg, data/trial-216.jpg, and data/trial-293.jpg.\n\n\n\nfour-maize-roots.jpg\n\n\nSuppose we are interested in the amount of plant material in each image, and in particular how that amount changes from image to image. Perhaps the images represent the growth of the plant over time, or perhaps the images show four different maize varieties at the same phase of their growth. The question we would like to answer is, “how much root mass is in each image?”\nWe will first construct a Python program to measure this value for a single image. Our strategy will be this:\n\nRead the image, converting it to grayscale as it is read. For this application we do not need the colour image.\nBlur the image.\nUse Otsu’s method of thresholding to create a binary image, where the pixels that were part of the maize plant are white, and everything else is black.\nSave the binary image so it can be examined later.\nCount the white pixels in the binary image, and divide by the number of pixels in the image. This ratio will be a measure of the root mass of the plant in the image.\nOutput the name of the image processed and the root mass ratio.\n\nOur intent is to perform these steps and produce the numeric result - a measure of the root mass in the image - without human intervention. Implementing the steps within a Python function will enable us to call this function for different images.\nHere is a Python function that implements this root-mass-measuring strategy. Since the function is intended to produce numeric output without human interaction, it does not display any of the images. Almost all of the commands should be familiar, and in fact, it may seem simpler than the code we have worked on thus far, because we are not displaying any of the images.\n\ndef measure_root_mass(filename, sigma=1.0):\n\n    # read the original image, converting to grayscale on the fly\n    image = iio.imread(uri=filename, mode=\"L\")\n\n    # blur before thresholding\n    blurred_image = skimage.filters.gaussian(image, sigma=sigma)\n\n    # perform automatic thresholding to produce a binary image\n    t = skimage.filters.threshold_otsu(blurred_image)\n    binary_mask = blurred_image > t\n\n    # determine root mass ratio\n    rootPixels = np.count_nonzero(binary_mask)\n    w = binary_mask.shape[1]\n    h = binary_mask.shape[0]\n    density = rootPixels / (w * h)\n\n    return density\n\nThe function begins with reading the original image from the file filename. We use iio.imread() with the optional argument mode=“L” to automatically convert it to grayscale. Next, the grayscale image is blurred with a Gaussian filter with the value of sigma that is passed to the function. Then we determine the threshold t with Otsu’s method and create a binary mask just as we did in the previous section. Up to this point, everything should be familiar.\nThe final part of the function determines the root mass ratio in the image. Recall that in the binary_mask, every pixel has either a value of zero (black/background) or one (white/foreground). We want to count the number of white pixels, which can be accomplished with a call to the numpy function np.count_nonzero. Then we determine the width and height of the image by using the elements of binary_mask.shape (that is, the dimensions of the numpy array that stores the image). Finally, the density ratio is calculated by dividing the number of white pixels by the total number of pixels wh* in the image. The function returns then root density of the image.\nWe can call this function with any filename and provide a sigma value for the blurring. If no sigma value is provided, the default value 1.0 will be used. For example, for the file data/trial-016.jpg and a sigma value of 1.5, we would call the function like this:\n\nmeasure_root_mass(filename=\"Images/trial-016.jpg\", sigma=1.5)\n\n0.04907413563829787\n\n\nNow we can use the function to process the series of four images shown above. In a real-world scientific situation, there might be dozens, hundreds, or even thousands of images to process. To save us the tedium of calling the function for each image by hand, we can write a loop that processes all files automatically. The following code block assumes that the files are located in the same directory and the filenames all start with the trial- prefix and end with the .jpg suffix.\n\nall_files = glob.glob(\"Images/trial-*.jpg\")\nfor filename in all_files:\n    density = measure_root_mass(filename=filename, sigma=1.5)\n    # output in format suitable for .csv\n    print(filename, density, sep=\",\")\n\nImages/trial-216.jpg,0.1420516954787234\nImages/trial-016.jpg,0.04907413563829787\nImages/trial-293.jpg,0.13665458776595746\nImages/trial-020.jpg,0.06381349734042553\n\n\n\n\nIgnoring more of the images\nLet us take a closer look at the binary masks produced by the measure_root_mass function:\n\n\n\nfour-maize-roots-binary.jpg\n\n\nYou may have noticed in the section on automatic thresholding that the thresholded image does include regions of the image aside of the plant root: the numbered labels and the white circles in each image are preserved during the thresholding, because their grayscale values are above the threshold. Therefore, our calculated root mass ratios include the white pixels of the label and white circle that are not part of the plant root. Those extra pixels affect how accurate the root mass calculation is!\n\n\n\n\n\n\nHow might we remove the labels and circles before calculating the ratio, so that our results are more accurate? Think about some options given what we have learned so far.\n\n\n\n\n\nOne approach we might take is to try to completely mask out a region from each image, particularly, the area containing the white circle and the numbered label. If we had coordinates for a rectangular area on the image that contained the circle and the label, we could mask the area out easily by using techniques we learned in the Drawing and Bitwise Operations section.\nHowever, a closer inspection of the binary images raises some issues with that approach. Since the roots are not always constrained to a certain area in the image, and since the circles and labels are in different locations each time, we would have difficulties coming up with a single rectangle that would work for every image. We could create a different masking rectangle for each image, but that is not a practicable approach if we have hundreds or thousands of images to process.\nAnother approach we could take is to apply two thresholding steps to the image. Look at the graylevel histogram of the file data/trial-016.jpg shown above again. Notice the peak near 1.0? Recall that a grayscale value of 1.0 corresponds to white pixels: the peak corresponds to the white label and circle. So, we could use simple binary thresholding to mask the white circle and label from the image, and then we could use Otsu’s method to select the pixels in the plant portion of the image.\nNote that most of this extra work in processing the image could have been avoided during the experimental design stage, with some careful consideration of how the resulting images would be used. For example, all of the following measures could have made the images easier to process, by helping us predict and/or detect where the label is in the image and subsequently mask it from further processing:\n\nusing labels with a consistent size and shape\nplacing all the labels in the same position, relative to the sample\nusing a non-white label, with non-black writing\n\n\n\n\nLet’s now Implement an enhanced version of the function measure_root_mass that applies simple binary thresholding to remove the white circle and label from the image before applying Otsu’s method. We can apply a simple binary thresholding with a threshold t=0.95 to remove the label and circle from the image. We use the binary mask to set the pixels in the blurred image to zero (black):\n\ndef enhanced_root_mass(filename, sigma):\n\n    # read the original image, converting to grayscale on the fly\n    image = iio.imread(uri=filename, mode=\"L\")\n\n    # blur before thresholding\n    blurred_image = skimage.filters.gaussian(image, sigma=sigma)\n\n    # perform binary thresholding to mask the white label and circle\n    binary_mask = blurred_image < 0.95\n    # use the mask to remove the circle and label from the blurred image\n    blurred_image[~binary_mask] = 0\n\n    # perform automatic thresholding to produce a binary image\n    t = skimage.filters.threshold_otsu(blurred_image)\n    binary_mask = blurred_image > t\n\n    # determine root mass ratio\n    rootPixels = np.count_nonzero(binary_mask)\n    w = binary_mask.shape[1]\n    h = binary_mask.shape[0]\n    density = rootPixels / (w * h)\n\n    return density\n\nall_files = glob.glob(\"Images/trial-*.jpg\")\nfor filename in all_files:\n    density = enhanced_root_mass(filename=filename, sigma=1.5)\n    # output in format suitable for .csv\n    print(filename, density, sep=\",\")\n\nImages/trial-216.jpg,0.13761419547872342\nImages/trial-016.jpg,0.04632878989361702\nImages/trial-293.jpg,0.1323479055851064\nImages/trial-020.jpg,0.05924468085106383\n\n\nThe output of the improved program does illustrate that the white circles and labels were skewing our root mass ratios. The values generated by the enhanced function are lower.\nHere are the binary images produced by the additional thresholding. Note that we have not completely removed the offending white pixels. Outlines still remain. However, we have reduced the number of extraneous pixels, which should make the output more accurate.\n\n\n\nfour-maize-roots-binary-improved.jpg\n\n\n\n\nThresholding a bacteria colony image\nIn the images directory Images/, you will find an image named colonies-01.tif:\n\n\n\ncolonies-01.jpg\n\n\nThis is one of the images we will be working with in the morphometric challenge at the end of this blog. Let’s first plot and inspect the grayscale histogram of the image to determine a good threshold value for the image:\n\nimage = iio.imread(uri=\"Images/colonies-01.tif\")\ngray_image = skimage.color.rgb2gray(image)\nblurred_image = skimage.filters.gaussian(gray_image, sigma=1.0)\nhistogram, bin_edges = np.histogram(blurred_image, bins=256, range=(0.0, 1.0))\nfig, ax = plt.subplots()\nplt.plot(bin_edges[0:-1], histogram)\nplt.title(\"Graylevel histogram\")\nplt.xlabel(\"gray value\")\nplt.ylabel(\"pixel count\")\nplt.xlim(0, 1.0)\n\n(0.0, 1.0)\n\n\n\n\n\nThe peak near one corresponds to the white image background, and the broader peak around 0.5 corresponds to the yellow/brown culture medium in the dish. The small peak near zero is what we are after: the dark bacteria colonies. A reasonable choice thus might be to leave pixels below t=0.2 on.\nNow let’s create the binary mask that leaves the pixels in the bacteria colonies “on” while turning the rest of the pixels in the image “off”. Here is the code to create and show the binarized image using the < operator with a threshold t=0.2:\n\nt = 0.2\nbinary_mask = blurred_image < t\n\nfig, ax = plt.subplots()\nplt.imshow(binary_mask, cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x7f9fd2f0dae0>\n\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nthresholding produces a binary image, where all pixels with intensities above (or below) a threshold value are turned on, while all other pixels are turned off\nthe binary images produced by thresholding are held in two-dimensional NumPy arrays, since they have only one colour value channel. They are boolean, hence they contain the values 0 (off) and 1 (on)\nthresholding can be used to create masks that select only the interesting parts of an image, or as the first step before edge detection or finding contours"
  },
  {
    "objectID": "posts/Image_Processing_with_Python/Image_Processing.html#connected-component-analysis",
    "href": "posts/Image_Processing_with_Python/Image_Processing.html#connected-component-analysis",
    "title": "Image Processing with Python",
    "section": "7. Connected Component Analysis",
    "text": "7. Connected Component Analysis\nIn the Thresholding section we covered dividing an image into foreground and background pixels. In the shapes example image, we considered the coloured shapes as foreground objects on a white background:\n\n\n\nshapes-01.jpg\n\n\nIn thresholding we went from the original image to this version:\n\n\n\nshapes-01-mask.png\n\n\nHere, we created a mask that only highlights the parts of the image that we find interesting, the objects. All objects have pixel value of True while the background pixels are False.\nBy looking at the mask image, we can count the objects that are present in the image (7). But how did we actually do that, how did we decide which lump of pixels constitutes a single object?\n\nPixel Neighborhoods\nIn order to decide which pixels belong to the same object, one can exploit their neighborhood: pixels that are directly next to each other and belong to the foreground class can be considered to belong to the same object.\nLet’s discuss the concept of pixel neighborhoods in more detail. Consider the following mask “image” with 8 rows, and 8 columns. For the purpose of illustration, the digit 0 is used to represent background pixels, and the letter X is used to represent object pixels foreground).\n0 0 0 0 0 0 0 0\n0 X X 0 0 0 0 0\n0 X X 0 0 0 0 0\n0 0 0 X X X 0 0\n0 0 0 X X X X 0\n0 0 0 0 0 0 0 0\nThe pixels are organised in a rectangular grid. In order to understand pixel neighborhoods we will introduce the concept of “jumps” between pixels. The jumps follow two rules:\n\nOnly one jump is allowed along the column, or the row. Diagonal jumps are not allowed. So, from a centre pixel, denoted with o, only the pixels indicated with a 1 are reachable\n\n\n\n\none_jump.JPG\n\n\nThe pixels on the diagonal (from o) are not reachable with a single jump, which is denoted by the -. The pixels reachable with a single jump form the 1-jump neighborhood.\n\nIn a sequence of jumps, one may only jump in row and column direction once -> they have to be orthogonal. An example of a sequence of orthogonal jumps is shown below. Starting from o the first jump goes along the row to the right. The second jump then goes along the column direction up. After this, the sequence cannot be continued as a jump has already been made in both row and column direction:\n\n\n\n\ntwo_jump.JPG\n\n\nAll pixels reachable with one, or two jumps form the 2-jump neighborhood. The grid below illustrates the pixels reachable from the centre pixel o with a single jump, highlighted with a 1, and the pixels reachable with 2 jumps with a 2.\n2 1 2\n1 o 1\n2 1 2\nIn the 1-jump version, only pixels that have direct neighbors along rows or columns are considered connected. Diagonal connections are not included in the 1-jump neighborhood. With two jumps, however, we only get a single object A because pixels are also considered connected along the diagonals.\n0 0 0 0 0 0 0 0\n0 A A 0 0 0 0 0\n0 A A 0 0 0 0 0\n0 0 0 A A A 0 0\n0 0 0 A A A A 0\n0 0 0 0 0 0 0 0\n\n\nObject counting\n\n\n\n\n\n\nConsider the mask below. How many objects with 1 orthogonal jump?\n\n\n\n\n\nFive.\n\n\n\n0 0 0 0 0 0 0 0\n0 X 0 0 0 X X 0\n0 0 X 0 0 0 0 0\n0 X 0 X X X 0 0\n0 X 0 X X 0 0 0\n0 0 0 0 0 0 0 0\n\n\n\n\n\n\nConsider the mask above. How many objects with 2 orthogonal jump?\n\n\n\n\n\nTwo.\n\n\n\n\n\nJumps and neighborhoods\nWe have just introduced how you can reach different neighboring pixels by performing one or more orthogonal jumps. We have used the terms 1-jump and 2-jump neighborhood. There is also a different way of referring to these neighborhoods: the 4- and 8-neighborhood.\nWith a single jump you can reach four pixels from a given starting pixel. Hence, the 1-jump neighborhood corresponds to the 4-neighborhood. When two orthogonal jumps are allowed, eight pixels can be reached, so the 2-jump neighborhood corresponds to the 8-neighborhood.\n\n\nConnected Component Analysis\nIn order to find the objects in an image, we want to employ an operation that is called Connected Component Analysis (CCA). This operation takes a binary image as an input. Usually, the False value in this image is associated with background pixels, and the True value indicates foreground, or object pixels. Such an image can be produced, e.g., with thresholding. Given a thresholded image, the connected component analysis produces a new labeled image with integer pixel values. Pixels with the same value, belong to the same object. Skimage provides connected component analysis in the function skimage.measure.label(). Let us add this function to the already familiar steps of thresholding an image. Here we define a reusable Python function connected_components:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio.v3 as iio\nimport skimage.color\nimport skimage.filters\nimport skimage.measure\n\ndef connected_components(filename, sigma=1.0, t=0.5, connectivity=2):\n    # load the image\n    image = iio.imread(filename)\n    # convert the image to grayscale\n    gray_image = skimage.color.rgb2gray(image)\n    # denoise the image with a Gaussian filter\n    blurred_image = skimage.filters.gaussian(gray_image, sigma=sigma)\n    # mask the image according to threshold\n    binary_mask = blurred_image < t\n    # perform connected component analysis\n    labeled_image, count = skimage.measure.label(binary_mask,\n                                                 connectivity=connectivity, return_num=True)\n    return labeled_image, count\n\nNote the new import of skimage.measure in order to use the skimage.measure.label function that performs the CCA. The first four lines of code are familiar from the Thresholding section.\nThen we call the skimage.measure.label function. This function has one positional argument where we pass the binary_mask, i.e., the binary image to work on. With the optional argument connectivity, we specify the neighborhood in units of orthogonal jumps. For example, by setting connectivity=2 we will consider the 2-jump neighborhood introduced above. The function returns a labeled_image where each pixel has a unique value corresponding to the object it belongs to. In addition, we pass the optional parameter return_num=True to return the maximum label index as count.\n\n\nOptional parameters and return values\nThe optional parameter return_num changes the data type that is returned by the function skimage.measure.label. The number of labels is only returned if return_num is True. Otherwise, the function only returns the labeled image. This means that we have to pay attention when assigning the return value to a variable. If we omit the optional parameter return_num or pass return_num=False, we can call the function as:\n\nlabeled_image = skimage.measure.label(binary_mask)\n\nIf we pass return_num=True, the function returns a tuple and we can assign it as:\n\nlabeled_image, count = skimage.measure.label(binary_mask, return_num=True)\n\nIf we used the same assignment as in the first case, the variable labeled_image would become a tuple, in which labeled_image[0] is the image and labeled_image[1] is the number of labels. This could cause confusion if we assume that labeled_image only contains the image and pass it to other functions. If you get an AttributeError: ‘tuple’ object has no attribute ‘shape’ or similar, check if you have assigned the return values consistently with the optional parameters.\nWe can call the above function connected_components and display the labeled image like so:\n\nlabeled_image, count = connected_components(filename=\"Images/shapes-01.jpg\", sigma=2.0, t=0.9, connectivity=2)\n\nfig, ax = plt.subplots()\nplt.imshow(labeled_image)\nplt.axis(\"off\");\n\n\n\n\n\n\n\n\n\n\nColor mappings\n\n\n\nHere you might get a warning UserWarning: Low image data range; displaying image with stretched contrast. or just see an all black image (Note: this behavior might change in future versions or not occur with a different image viewer).\nWhat went wrong? When we hover over the black image, the pixel values are shown as numbers in the lower corner of the viewer. We an see that some pixels have values different from 0, so they are not actually pure black.\n\n\nLet’s find out more by examining labeled_image. Properties that might be interesting in this context are dtype, the minimum and maximum value. We can print them with the following lines:\n\nprint(\"dtype:\", labeled_image.dtype)\nprint(\"min:\", np.min(labeled_image))\nprint(\"max:\", np.max(labeled_image))\n\ndtype: int32\nmin: 0\nmax: 11\n\n\n\n\n\n\n\n\nint64\n\n\n\nIf the dtype of labeled_image is int64, this means that values in this image range from -2 ** 63 to 2 ** 63 - 1. Those are really big numbers. From this available space we only use the range from 0 to 11. When showing this image in the viewer, it squeezes the complete range into 256 gray values. Therefore, the range of our numbers will not produce any visible change.\n\n\nFortunately, the skimage library has tools to cope with this situation.\nWe can use the function skimage.color.label2rgb() to convert the colours in the image (recall that we already used the skimage.color.rgb2gray() function to convert to grayscale). With skimage.color.label2rgb(), all objects are coloured according to a list of colours that can be customised. We can use the following commands to convert and show the image:\n\n# convert the label image to color image\ncolored_label_image = skimage.color.label2rgb(labeled_image, bg_label=0)\n\nfig, ax = plt.subplots()\nplt.imshow(colored_label_image)\nplt.axis(\"off\");\n\n\n\n\n\n\nHow many objects are in that image?\nIt looks fairly obvious right? Seven. But, let’s practice. Using the function connected_components, we can find two ways of printing out the number of objects found in the image. As you might have guessed, the return value count already contains the number of found images. So it can simply be printed with:\n\nprint(\"Found\", count, \"objects in the image.\")\n\nFound 11 objects in the image.\n\n\nBut there is also a way to obtain the number of found objects from the labeled image itself. Recall that all pixels that belong to a single object are assigned the same integer value. The connected component algorithm produces consecutive numbers. The background gets the value 0, the first object gets the value 1, the second object the value 2, and so on. This means that by finding the object with the maximum value, we also know how many objects there are in the image. We can thus use the np.max function from Numpy to find the maximum value that equals the number of found objects:\n\nnum_objects = np.max(labeled_image)\nprint(\"Found\", num_objects, \"objects in the image.\")\n\nFound 11 objects in the image.\n\n\nInvoking the function with sigma=2.0, and threshold=0.9, both methods will print Found 11 objects in the image.\nYou might wonder why the connected component analysis with sigma=2.0, and threshold=0.9 finds 11 objects, whereas we would expect only 7 objects. Where are the four additional objects? With a bit of detective work, we can spot some small objects in the image, for example, near the left border:\n\n\n\nshapes-01-cca-detail.png\n\n\nFor us it is clear that these small spots are artifacts and not objects we are interested in. But how can we tell the computer? One way to calibrate the algorithm is to adjust the parameters for blurring (sigma) and thresholding (t), but you may have noticed during the above exercise that it is quite hard to find a combination that produces the right output number. In some cases, background noise gets picked up as an object. And with other parameters, some of the foreground objects get broken up or disappear completely. Therefore, we need other criteria to describe desired properties of the objects that are found.\n\n\nMorphometrics - Describe object features with numbers\nMorphometrics is concerned with the quantitative analysis of objects and considers properties such as size and shape. For the example of the images with the shapes, our intuition tells us that the objects should be of a certain size or area. So we could use a minimum area as a criterion for when an object should be detected. To apply such a criterion, we need a way to calculate the area of objects found by connected components. Recall how we determined the root mass in the Thresholding section by counting the pixels in the binary mask. But here we want to calculate the area of several objects in the labeled image. The skimage library provides the function skimage.measure.regionprops to measure the properties of labeled regions. It returns a list of RegionProperties that describe each connected region in the images. The properties can be accessed using the attributes of the RegionProperties data type. Here we will use the properties “area” and “label”. You can explore the skimage documentation to learn about other properties available.\nWe can get a list of areas of the labeled objects as follows:\n\n# compute object features and extract object areas\nobject_features = skimage.measure.regionprops(labeled_image)\nobject_areas = [objf[\"area\"] for objf in object_features]\nobject_areas\n\n[318542, 1, 523204, 496613, 517331, 143, 256215, 1, 68, 338784, 265755]\n\n\n\n\nPlot a histogram of the object area distribution\nit is often helpful to inspect the histogram of an object property. For example, we want to look at the distribution of the object areas.\n\nCreate and examine a histogram of the object areas obtained with skimage.measure.regionprops\nWhat does the histogram tell us about the objects?\n\n\n# plot the histogram\nfig, ax = plt.subplots()\nplt.hist(object_areas)\nplt.xlabel(\"Area (pixels)\")\nplt.ylabel(\"Number of objects\");\n\n\n\n\nThe histogram shows the number of objects (vertical axis) whose area is within a certain range (horizontal axis). The height of the bars in the histogram indicates the prevalence of objects with a certain area. The whole histogram tells us about the distribution of object sizes in the image. It is often possible to identify gaps between groups of bars (or peaks if we draw the histogram as a continuous curve) that tell us about certain groups in the image.\nIn this example, we can see that there are four small objects that contain less than 50000 pixels. Then there is a group of four (1+1+2) objects in the range between 200000 and 400000, and three objects with a size around 500000. For our object count, we might want to disregard the small objects as artifacts, i.e, we want to ignore the leftmost bar of the histogram. We could use a threshold of 50000 as the minimum area to count. In fact, the object_areas list already tells us that there are fewer than 200 pixels in these objects. Therefore, it is reasonable to require a minimum area of at least 200 pixels for a detected object. In practice, finding the “right” threshold can be tricky and usually involves an educated guess based on domain knowledge.\n\n\nFilter objects by area\nNow we would like to use a minimum area criterion to obtain a more accurate count of the objects in the image.\n\nOne way to count only objects above a certain area is to first create a list of those objects, and then take the length of that list as the object count. This can be done as follows:\n\n\nmin_area = 200\nlarge_objects = []\nfor objf in object_features:\n    if objf[\"area\"] > min_area:\n        large_objects.append(objf[\"label\"])\nprint(\"Found\", len(large_objects), \"objects!\")\n\nFound 7 objects!\n\n\n\nAnother option is to use Numpy arrays to create the list of large objects. We first create an array object_areas containing the object areas, and an array object_labels containing the object labels. The labels of the objects are also returned by skimage.measure.regionprops. We have already seen that we can create boolean arrays using comparison operators. Here we can use object_areas > min_area to produce an array that has the same dimension as object_labels. It can then used to select the labels of objects whose area is greater than min_area by indexing:\n\n\nobject_areas = np.array([objf[\"area\"] for objf in object_features])\nobject_labels = np.array([objf[\"label\"] for objf in object_features])\nlarge_objects = object_labels[object_areas > min_area]\nprint(\"Found\", len(large_objects), \"objects!\")\n\nFound 7 objects!\n\n\n\n\n\n\n\n\nNumPy vs for loops and if statements\n\n\n\nThe advantage of using Numpy arrays is that for loops and if statements in Python can be slow, and in practice the first approach may not be feasible if the image contains a large number of objects. In that case, Numpy array functions turn out to be very useful because they are much faster.\n\n\n\nIn this example, we can also use the np.count_nonzero function that we saw earlier together with the > operator to count the objects whose area is above min_area:\n\n\nn = np.count_nonzero(object_areas > min_area)\nprint(\"Found\", n, \"objects!\")\n\nFound 7 objects!\n\n\nFor all three alternatives, the output is the same and gives the expected count of 7 objects.\n\n\n\n\n\n\nUsing functions from Numpy and other Python packages\n\n\n\nFunctions from Python packages such as Numpy are often more efficient and require less code to write. It is a good idea to browse the reference pages of NumPy and skimage to look for an availabe function that can solve a given task.\n\n\n\n\nRemove small objects\nWe might also want to exclude (mask) the small objects when plotting the labeled image. Enhance the connected_components function such that it automatically removes objects that are below a certain area that is passed to the function as an optional parameter.\n\nTo remove the small objects from the labeled image, we change the value of all pixels that belong to the small objects to the background label 0. One way to do this is to loop over all objects and set the pixels that match the label of the object to :\n\n\nfor object_id, objf in enumerate(object_features, start=1):\n    if objf[\"area\"] < min_area:\n        labeled_image[labeled_image == objf[\"label\"]] = 0\n\n\nHere NumPy functions can also be used to eliminate for loops and if statements. Like above, we can create an array of the small object labels with the comparison object_areas < min_area. We can use another Numpy function, np.isin, to set the pixels of all small objects to 0. np.isin takes two arrays and returns a boolean array with values True if the entry of the first array is found in the second array, and False otherwise. This array can then be used to index the labeled_image and set the entries that belong to small objects to 0 :\n\n\nobject_areas = np.array([objf[\"area\"] for objf in object_features])\nobject_labels = np.array([objf[\"label\"] for objf in object_features])\nsmall_objects = object_labels[object_areas < min_area]\nlabeled_image[np.isin(labeled_image,small_objects)] = 0\n\n\nAn even more elegant way to remove small objects from the image is to leverage the skimage.morphology module. It provides a function skimage.morphology.remove_small_objects that does exactly what we are looking for. It can be applied to a binary image and returns a mask in which all objects smaller than min_area are excluded, i.e, their pixel values are set to False. We can then apply skimage.measure.label to the masked image:\n\n\nobject_mask = skimage.morphology.remove_small_objects(binary_mask,min_area)\nlabeled_image, n = skimage.measure.label(object_mask,\n                                         connectivity=2, return_num=True)\n\nUsing the skimage features, we can implement the enhanced_connected_component as follows:\n\ndef enhanced_connected_components(filename, sigma=1.0, t=0.5, connectivity=2, min_area=0):\n    image = iio.imread(filename)\n    gray_image = skimage.color.rgb2gray(image)\n    blurred_image = skimage.filters.gaussian(gray_image, sigma=sigma)\n    binary_mask = blurred_image < t\n    object_mask = skimage.morphology.remove_small_objects(binary_mask,min_area)\n    labeled_image, count = skimage.measure.label(object_mask,\n                                                 connectivity=connectivity, return_num=True)\n    return labeled_image, count\n\nWe can now call the function with a chosen min_area and display the resulting labeled image:\n\nlabeled_image, count = enhanced_connected_components(filename=\"Images/shapes-01.jpg\", sigma=2.0, t=0.9,\n                                                     connectivity=2, min_area=min_area)\ncolored_label_image = skimage.color.label2rgb(labeled_image, bg_label=0)\n\nfig, ax = plt.subplots()\nplt.imshow(colored_label_image)\nplt.axis(\"off\");\n\nprint(\"Found\", count, \"objects in the image.\")\n\nFound 7 objects in the image.\n\n\n\n\n\nNote that the small objects are “gone” and we obtain the correct number of 7 objects in the image.\n\n\nColour objects by area\nFinally, we would like to display the image with the objects coloured according to the magnitude of their area. In practice, this can be used with other properties to give visual cues of the object properties.\nWe already know how to get the areas of the objects from the regionprops. We just need to insert a zero area value for the background (to colour it like a zero size object). The background is also labeled 0 in the labeled_image, so we insert the zero area value in front of the first element of object_areas with np.insert. Then we can create a colored_area_image where we assign each pixel value the area by indexing the object_areas with the label values in labeled_image.\n\nobject_areas = np.array([objf[\"area\"] for objf in skimage.measure.regionprops(labeled_image)])\nobject_areas = np.insert(0,1,object_areas)\ncolored_area_image = object_areas[labeled_image]\n\nfig, ax = plt.subplots()\nim = plt.imshow(colored_area_image)\ncbar = fig.colorbar(im, ax=ax, shrink=0.85)\ncbar.ax.set_title(\"Area\")\nplt.axis(\"off\");\n\n\n\n\nYou may have noticed that in the solution, we have used the labeled_image to index the array object_areas. This is an example of advanced indexing in NumPy. The result is an array of the same shape as the labeled_image whose pixel values are selected from object_areas according to the object label. Hence the objects will be colored by area when the result is displayed. Note that advanced indexing with an integer array works slightly different than the indexing with a Boolean array that we have used for masking. While Boolean array indexing returns only the entries corresponding to the True values of the index, integer array indexing returns an array with the same shape as the index.\n\n\n\n\n\n\nKey Points:\n\n\n\n\nwe can use skimage.measure.label to find and label connected objects in an image\nwe can use skimage.measure.regionprops to measure properties of labeled objects\nwe can use skimage.morphology.remove_small_objects to mask small objects and remove artifacts from an image\nwe can display the labeled image to view the objects coloured by label"
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html",
    "title": "Audio Feature Extraction",
    "section": "",
    "text": "In this blog, I’ll be sharing how we can extract some prominent features from an audio file for further processing and analysis using Python, in particular the librosa, pydub and wave libraries."
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#audio-files-and-concepts",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#audio-files-and-concepts",
    "title": "Audio Feature Extraction",
    "section": "Audio files and concepts",
    "text": "Audio files and concepts\nIn audio data analysis, we process and transform audio signals captured by digital devices. Depending on how they’re captured, they can come in many different formats such as wav, mp3, m4a, aiff, and flac.\nQuoting Izotope.com, Waveform (wav) is one of the most popular digital audio formats. It is a lossless file format — which means it captures the closest mathematical representation of the original audio with no noticeable audio quality loss. In mp3 or m4a (Apple’s mp3 format) the data is compressed in such a way so it can be more easily distributed although in lower quality. In audio data analytics, most libraries support wav file processing.\nAs a form of a wave, sound/audio signal has the generic properties of:\n\nFrequency: occurrences of vibrations per unit of time\nAmplitude: maximum displacement or distance moved by a point on a wave measured from its equilibrium position; impacting the sound intensity\nSpeed of sound: distance traveled per unit of time by a soundwave\n\nThe information to be extracted from audio files are just transformations of the main properties above.\n\nExploratory analysis on audio files\nFor this analysis, I’m going to compare two demo tracks that our band Thirteen-Seven produced.\nThe files will be analyzed mainly with these Python packages:\n\nlibrosa for audio signal extraction and visualization\npydub for audio file manipulation\nwave for reading wav files"
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#general-audio-parameters",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#general-audio-parameters",
    "title": "Audio Feature Extraction",
    "section": "General audio parameters",
    "text": "General audio parameters\nJust like how we usually start evaluating tabular data by getting the statistical summary of the data (i.e using “Dataframe.describe” method), in the audio analysis we can start by getting the audio metadata summary. We can do so by utilizing the audiosegment module in pydub.\nBelow are some generic features that can be extracted:\n\nChannels: number of channels; 1 for mono, 2 for stereo audio\nSample width: number of bytes per sample; 1 means 8-bit, 2 means 16-bit, 3 means 24-bit, 4 means 32-bit\nFrame rate(sample rate): frequency of samples used (in Hertz)\nFrame width: Number of bytes for each “frame”. One frame contains a sample for each channel.\nLength: audio file length (in milliseconds)\nFrame count: the number of frames from the sample\nIntensity: loudness in dBFS (dB relative to the maximum possible loudness)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pydub import AudioSegment\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\n\n\n# Load in the track and create widget to listen\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\nipd.Audio(all_the_excuses, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Load files\nall_the_excuses = AudioSegment.from_file('Audio/all_the_excuses.wav')\n\n# Print attributes\nprint(f\"***All The Excuses - metadata***\")\nprint(f\"Channels:  {all_the_excuses.channels}\")\nprint(f\"Sample width: {all_the_excuses.sample_width}\")\nprint(f\"Frame rate (sample rate): {all_the_excuses.frame_rate}\")\nprint(f\"Frame width:  {all_the_excuses.frame_width}\")\nprint(f\"Length (ms): {len(all_the_excuses)}\")\nprint(f\"Frame count:  {all_the_excuses.frame_count()}\")\nprint(f\"Intensity: {all_the_excuses.dBFS}\")\n\n***All The Excuses - metadata***\nChannels:  2\nSample width: 2\nFrame rate (sample rate): 44100\nFrame width:  4\nLength (ms): 252891\nFrame count:  11152512.0\nIntensity: -10.902991191150802\n\n\n\n# Load in the track and create widget to listen\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\nipd.Audio(all_or_nothing, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nall_or_nothing = AudioSegment.from_file('Audio/all_or_nothing.wav')\n\n# Print attributes\nprint(f\"***All or Nothing - metadata***\")\nprint(f\"Channels:  {all_or_nothing.channels}\")\nprint(f\"Sample width: {all_or_nothing.sample_width}\")\nprint(f\"Frame rate (sample rate): {all_or_nothing.frame_rate}\")\nprint(f\"Frame width:  {all_or_nothing.frame_width}\")\nprint(f\"Length (ms): {len(all_or_nothing)}\")\nprint(f\"Frame count:  {all_or_nothing.frame_count()}\")\nprint(f\"Intensity: {all_or_nothing.dBFS}\")\n\n***All or Nothing - metadata***\nChannels:  2\nSample width: 2\nFrame rate (sample rate): 44100\nFrame width:  4\nLength (ms): 239099\nFrame count:  10544256.0\nIntensity: -10.614852809540894"
  },
  {
    "objectID": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#feature-extraction",
    "href": "posts/Audio_Feature_Extraction/Audio_Feature_Extraction.html#feature-extraction",
    "title": "Audio Feature Extraction",
    "section": "Feature extraction",
    "text": "Feature extraction\nNumerous advanced features can be extracted and visualized using librosa to analyze audio characteristics.\n\nAmplitude envelope\nWe can visualize the amplitude over time of an audio file to get an idea of the wave movement using librosa:\n\n# Import required module\nimport librosa.display\nimport matplotlib.pyplot as plt\n\n\n# Load in our track\nall_the_excuses = 'Audio/all_the_excuses.wav'\nx , sr = librosa.load(all_the_excuses, sr=None)\n    \n# Plot the signal\nplt.figure(figsize=(15, 3))\nplt.title(\"Thirteen-Seven | All The Excuses - waveplot\")\nlibrosa.display.waveshow(x, sr=sr)\n\n<librosa.display.AdaptiveWaveplot at 0x7fbc37a9a260>\n\n\n\n\n\n\n# Load in our track\nall_or_nothing = 'Audio/all_or_nothing.wav'\nx , sr = librosa.load(all_or_nothing, sr=None)\n    \n# Import required module\nimport librosa.display\n\n# Plot the signal\nplt.figure(figsize=(15, 3))\nplt.title(\"Thirteen-Seven | All or Nothing - waveplot\")\nlibrosa.display.waveshow(x, sr=sr)\n\n<librosa.display.AdaptiveWaveplot at 0x7fbc2b8dcd60>\n\n\n\n\n\n\n\nSpectrogram\nThe extracted audio features can be visualized on a spectrogram. Quoting Wikipedia, a spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. It is usually depicted as a heat map, with the intensity shown on varying color gradients.\n\nimport librosa.display\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\nX = librosa.stft(all_the_excuses)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(15, 3))\nplt.title('Thirteen-Seven | All The Excuses - spectrogram')\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fbc29d14c40>\n\n\n\n\n\n\nimport librosa.display\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\nX = librosa.stft(all_or_nothing)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(15, 3))\nplt.title('Thirteen-Seven | All or Nothing - spectrogram')\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fbc29c08dc0>\n\n\n\n\n\nThe vertical axis shows frequency, the horizontal axis shows the time of the clip, and the color variation shows the intensity of the audio wave.\n\n\nRoot-mean-square (RMS)\nThe root-mean-square here refers to the total magnitude of the signal, which in layman terms can be interpreted as the loudness or energy parameter of the audio file.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\n# Get RMS value from each frame's magnitude value\nS, phase = librosa.magphase(librosa.stft(all_the_excuses))\nrms = librosa.feature.rms(S=S)\n\n\n# Plot the RMS energy\nfig, ax = plt.subplots(figsize=(15, 6), nrows=2, sharex=True)\ntimes = librosa.times_like(rms)\nax[0].semilogy(times, rms[0], label='RMS Energy')\nax[0].set(xticks=[])\nax[0].legend()\nax[0].label_outer()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[1])\nax[1].set(title='Thirteen-Seven | All The Excuses - log Power spectrogram')\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All The Excuses - log Power spectrogram')]\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\n# Get RMS value from each frame's magnitude value\nS, phase = librosa.magphase(librosa.stft(all_or_nothing))\nrms = librosa.feature.rms(S=S)\n\n\n# Plot the RMS energy\nfig, ax = plt.subplots(figsize=(15, 6), nrows=2, sharex=True)\ntimes = librosa.times_like(rms)\nax[0].semilogy(times, rms[0], label='RMS Energy')\nax[0].set(xticks=[])\nax[0].legend()\nax[0].label_outer()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[1])\nax[1].set(title='Thirteen-Seven | All or Nothing - log Power spectrogram')\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All or Nothing - log Power spectrogram')]\n\n\n\n\n\nHere we can see the RMS values are consistently high (until the very end of the tracks) as this rock music is loud and intense throughout.\n\n\nZero crossing rate\nQuoting Wikipedia, zero-crossing rate (ZCR) is the rate at which a signal changes from positive to zero to negative or from negative to zero to positive. Its value has been widely used in both speech recognition and music information retrieval, being a key feature to classify percussive sounds. Highly percussive sounds like rock, metal, emo, or punk music tend to have higher zero-crossing rate values.\nWe can get this data manually by zooming into a certain frame in the amplitude time series, counting the times it passes zero value in the y-axis and extrapolating for the whole audio. Alternatively, there is a function in librosa that we can use to get the zero-crossing state and rate.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\nzcrs = librosa.feature.zero_crossing_rate(all_the_excuses)\n                     \nprint(f\"Zero crossing rate: {sum(librosa.zero_crossings(all_the_excuses))}\")\nplt.figure(figsize=(15, 3))\nplt.plot(zcrs[0])\nplt.title('Thirteen-Seven | All The Excuses - zero-crossing rate (ZCR)')\n\nZero crossing rate: 706615\n\n\nText(0.5, 1.0, 'Thirteen-Seven | All The Excuses - zero-crossing rate (ZCR)')\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\nzcrs = librosa.feature.zero_crossing_rate(all_or_nothing)\n                     \nprint(f\"Zero crossing rate: {sum(librosa.zero_crossings(all_or_nothing))}\")\nplt.figure(figsize=(15, 3))\nplt.plot(zcrs[0])\nplt.title('Thirteen-Seven | All or Nothing - zero-crossing rate (ZCR)')\n\nZero crossing rate: 679083\n\n\nText(0.5, 1.0, 'Thirteen-Seven | All or Nothing - zero-crossing rate (ZCR)')\n\n\n\n\n\nAbove is the zero crossing value and rate for the track. Here we can see the zero-crossing rate is high as it is a highly percussive rock song.\n\n\nMel-Frequency Cepstral Coefficients (MFCCs)\nQuoting Analytics Vidhya, humans do not perceive frequencies on a linear scale. We are better at detecting differences in lower frequencies than higher frequencies, even if the gap is the same (i.e 50 and 1,000 Hz vs 10,000 and 10,500 Hz). In Mel-scale, equal distances in pitch sounded equally distant to the listener.\nMel-Frequency Cepstral Coefficients (MFCCs) is a representation of the short-term power spectrum of a sound, based on some transformation in a Mel-scale. It is commonly used in speech recognition as people’s voices are usually on a certain range of frequency and different from one to another. Getting and displaying MFCCs is quite straightforward in Librosa.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\nmfccs = librosa.feature.mfcc(all_the_excuses, sr=sr)\n\n#Displaying  the MFCCs:\nfig,ax = plt.subplots(figsize=(15, 3))\nimg = librosa.display.specshow(mfccs, sr=sr, x_axis='time')\nfig.colorbar(img, ax=ax)\n                     \nax.set(title='Thirteen-Seven | All The Excuses - Mel-Frequency Cepstral Coefficients (MFCCs')\n\n/tmp/ipykernel_64/1341239866.py:2: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  mfccs = librosa.feature.mfcc(all_the_excuses, sr=sr)\n\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All The Excuses - Mel-Frequency Cepstral Coefficients (MFCCs')]\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\nmfccs = librosa.feature.mfcc(all_or_nothing, sr=sr)\n\n#Displaying  the MFCCs:\nfig,ax = plt.subplots(figsize=(15, 3))\nimg = librosa.display.specshow(mfccs, sr=sr, x_axis='time')\nfig.colorbar(img, ax=ax)\n                     \nax.set(title='Thirteen-Seven | All or Nothing - Mel-Frequency Cepstral Coefficients (MFCCs')\n\n/tmp/ipykernel_64/2838893102.py:2: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  mfccs = librosa.feature.mfcc(all_or_nothing, sr=sr)\n\n\n[Text(0.5, 1.0, 'Thirteen-Seven | All or Nothing - Mel-Frequency Cepstral Coefficients (MFCCs')]\n\n\n\n\n\n\n\nChroma\nWe can use Chroma feature visualization to know how dominant the characteristics of a certain pitch {C, C♯, D, D♯, E, F, F♯, G, G♯, A, A♯, B} is present in the sampled frame.\n\nall_the_excuses, sr = librosa.load('Audio/all_the_excuses.wav')\n\nhop_length = 512\n\nchromagram = librosa.feature.chroma_stft(all_the_excuses, sr=sr, hop_length=hop_length)\n\nplt.figure(figsize=(15, 5))\nplt.title('Thirteen-Seven | All The Excuses - chromagram')\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\n\n/tmp/ipykernel_64/4136034188.py:5: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  chromagram = librosa.feature.chroma_stft(all_the_excuses, sr=sr, hop_length=hop_length)\n\n\n<matplotlib.collections.QuadMesh at 0x7fbc29847760>\n\n\n\n\n\n\nall_or_nothing, sr = librosa.load('Audio/all_or_nothing.wav')\n\nhop_length = 512\n\nchromagram = librosa.feature.chroma_stft(all_or_nothing, sr=sr, hop_length=hop_length)\n\nplt.figure(figsize=(15, 5))\nplt.title('Thirteen-Seven | All or Nothing - chromagram')\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\n\n/tmp/ipykernel_64/626002655.py:5: FutureWarning: Pass y=[0. 0. 0. ... 0. 0. 0.] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  chromagram = librosa.feature.chroma_stft(all_or_nothing, sr=sr, hop_length=hop_length)\n\n\n<matplotlib.collections.QuadMesh at 0x7fbc298ddfc0>\n\n\n\n\n\n\n\nTempogram\nTempo refers to the rate of the musical beat and is given by the reciprocal of the beat period. Tempo is often defined in units of beats per minute (BPM). Tempo can vary locally within a piece. Therefore, we introduce the tempogram (FMP, p. 317) as a feature matrix which indicates the prevalence of certain tempo at each moment in time.\n\n# Estimate the tempo:\ntempo = librosa.beat.tempo(all_the_excuses, sr=sr)\ntempo\n\n/tmp/ipykernel_64/3447807976.py:2: FutureWarning: Pass y=[ 0.          0.          0.         ... -0.15874578 -0.19955122\n -0.38175374] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempo = librosa.beat.tempo(all_the_excuses, sr=sr)\n\n\narray([172.265625])\n\n\n\n# Visualize the tempo estimate on top of the input signal\nT = len(all_the_excuses)/float(sr)\nseconds_per_beat = 60.0/tempo[0]\nbeat_times = np.arange(0, T, seconds_per_beat)\n\nlibrosa.display.waveshow(all_the_excuses)\nplt.vlines(beat_times, -1, 1, color='r')\nplt.title(\"Thirteen-Seven | All The Excuses - estimated tempo plot\")\n\nText(0.5, 1.0, 'Thirteen-Seven | All The Excuses - estimated tempo plot')\n\n\n\n\n\n\n# Listen to the input signal with a click track using the tempo estimate:\nclicks = librosa.clicks(beat_times, sr, length=len(all_the_excuses))\nipd.Audio(all_the_excuses + clicks, rate=sr)\n\n/tmp/ipykernel_64/625036389.py:1: FutureWarning: Pass times=[ 0.          0.53405896  1.06811791  1.60217687  2.13623583  2.67029478\n  3.20435374  3.7384127   4.27247166  4.80653061  5.34058957  5.87464853\n  6.40870748  6.94276644  7.4768254   8.01088435  8.54494331  9.07900227\n  9.61306122 10.14712018 10.68117914 11.2152381  11.74929705 12.28335601\n 12.81741497 13.35147392 13.88553288 14.41959184 14.95365079 15.48770975\n 16.02176871 16.55582766 17.08988662 17.62394558 18.15800454 18.69206349\n 19.22612245 19.76018141 20.29424036 20.82829932 21.36235828 21.89641723\n 22.43047619 22.96453515 23.4985941  24.03265306 24.56671202 25.10077098\n 25.63482993 26.16888889 26.70294785 27.2370068  27.77106576 28.30512472\n 28.83918367 29.37324263 29.90730159], frames=22050 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  clicks = librosa.clicks(beat_times, sr, length=len(all_the_excuses))\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Estimate the tempo:\ntempo = librosa.beat.tempo(all_or_nothing, sr=sr)\ntempo\n\n/tmp/ipykernel_64/3362745988.py:2: FutureWarning: Pass y=[0.         0.         0.         ... 0.3365013  0.3631341  0.37732217] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  tempo = librosa.beat.tempo(all_or_nothing, sr=sr)\n\n\narray([112.34714674])\n\n\n\n# Visualize the tempo estimate on top of the input signal\nT = len(all_or_nothing)/float(sr)\nseconds_per_beat = 60.0/tempo[0]\nbeat_times = np.arange(0, T, seconds_per_beat)\n\nlibrosa.display.waveshow(all_or_nothing)\nplt.vlines(beat_times, -1, 1, color='r')\nplt.title(\"Thirteen-Seven | All or Nothing - estimated tempo plot\")\n\nText(0.5, 1.0, 'Thirteen-Seven | All or Nothing - estimated tempo plot')\n\n\n\n\n\n\n# Listen to the input signal with a click track using the tempo estimate:\nclicks = librosa.clicks(beat_times, sr, length=len(all_or_nothing))\nipd.Audio(all_or_nothing + clicks, rate=sr)\n\n/tmp/ipykernel_64/1680592788.py:2: FutureWarning: Pass times=[ 0.          0.53405896  1.06811791  1.60217687  2.13623583  2.67029478\n  3.20435374  3.7384127   4.27247166  4.80653061  5.34058957  5.87464853\n  6.40870748  6.94276644  7.4768254   8.01088435  8.54494331  9.07900227\n  9.61306122 10.14712018 10.68117914 11.2152381  11.74929705 12.28335601\n 12.81741497 13.35147392 13.88553288 14.41959184 14.95365079 15.48770975\n 16.02176871 16.55582766 17.08988662 17.62394558 18.15800454 18.69206349\n 19.22612245 19.76018141 20.29424036 20.82829932 21.36235828 21.89641723\n 22.43047619 22.96453515 23.4985941  24.03265306 24.56671202 25.10077098\n 25.63482993 26.16888889 26.70294785 27.2370068  27.77106576 28.30512472\n 28.83918367 29.37324263 29.90730159], frames=22050 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n  clicks = librosa.clicks(beat_times, sr, length=len(all_or_nothing))\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "",
    "text": "This blog has been produced after working through the Introduction to Geospatial Raster and Vector data with Python lesson provided by Data Carpentry."
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-data",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-data",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "1. Raster data",
    "text": "1. Raster data\nRaster data is any pixelated (or gridded) data where each pixel is associated with a specific geographic location. The value of a pixel can be continuous (e.g. elevation) or categorical (e.g. land use). If this sounds familiar, it is because this data structure is very common: it’s how we represent any digital image. A geospatial raster is only different from a digital photo in that it is accompanied by spatial information that connects the data to a particular location. This includes the raster’s extent and cell size, the number of rows and columns, and its coordinate reference system (or CRS).\n\nSome examples of continuous rasters include:\n\nPrecipitation maps\nMaps of tree height derived from LiDAR data\nElevation values for a region\n\nA map of elevation for Harvard Forest derived from the NEON AOP LiDAR sensor is below. Elevation is represented as a continuous numeric variable in this map. The legend shows the continuous range of values in the data from around 300 to 420 meters:\n\nSome rasters contain categorical data where each pixel represents a discrete class such as a landcover type (e.g., “forest” or “grassland”) rather than a continuous value such as elevation or temperature. Some examples of classified maps include:\n\nLandcover / land-use maps\nTree height maps classified as short, medium, and tall trees\nElevation maps classified as low, medium, and high elevation\n\n\nThe map above shows the contiguous United States with landcover as categorical data. Each color is a different landcover category. (Source: Homer, C.G., et al., 2015, Completion of the 2011 National Land Cover Database for the conterminous United States-Representing a decade of land cover change information. Photogrammetric Engineering and Remote Sensing, v. 81, no. 5, p. 345-354)\n\n\n\n\n\n\nCan you think of potential advantages and disadvantages of storing data in raster format?\n\n\n\n\n\nRaster data has some important advantages:\n\nrepresentation of continuous surfaces\npotentially very high levels of detail\ndata is ‘unweighted’ across its extent - the geometry doesn’t implicitly highlight features\ncell-by-cell calculations can be very fast and efficient\n\nThe downsides of raster data are:\n\nvery large file sizes as cell size gets smaller\ncurrently popular formats don’t embed metadata well (more on this later!)\ncan be difficult to represent complex information\n\n\n\n\n\nExtent\nThe spatial extent is the geographic area that the raster data covers. The spatial extent of an object represents the geographic edge or location that is the furthest north, south, east and west. In other words, extent represents the overall geographic coverage of the spatial object.\n\n\n\n\n\n\n\nIn the image above, the dashed boxes around each set of objects seems to imply that the three objects have the same extent. Is this accurate? If not, which object(s) have a different extent?\n\n\n\n\n\nThe lines and polygon objects have the same extent. The extent for the points object is smaller in the vertical direction than the other two because there are no points on the line at y = 8.\n\n\n\n\n\nResolution\nA resolution of a raster represents the area on the ground that each pixel of the raster covers. The image below illustrates the effect of changes in resolution:\n\n\n\nRaster data format\nRaster data can come in many different formats. In this blog, we will use the GeoTIFF format which has the extension .tif. A .tif file stores metadata or attributes about the file as embedded tif tags. For instance, your camera might store a tag that describes the make and model of the camera or the date the photo was taken when it saves a .tif. A GeoTIFF is a standard .tif image format with additional spatial (georeferencing) information embedded in the file as tags. These tags should include the following raster metadata:\n\nextent\nresolution\nCoordinate Reference System (CRS) - this concept will be introduced later\nvalues that represent missing data (NoDataValue) - this concept will be introduced later\n\n\n\n\n\n\n\nMore resources on the .tif format\n\n\n\n\nGeoTIFF on Wikipedia\n\n\n\n\n\nMulti-band Raster data\nA raster can contain one or more bands. One type of multi-band raster dataset that is familiar to many of us is a colour image. A basic colour image consists of three bands: red, green, and blue. Each band represents light reflected from the red, green or blue portions of the electromagnetic spectrum. The pixel brightness for each band, when composited creates the colours that we see in an image.\n\nWe can plot each band of a multi-band image individually. Or we can composite all three bands together to make a colour image. In a multi-band dataset, the rasters will always have the same extent, resolution, and CRS.\n\n\nOther Types of Multi-band Raster Data\nMulti-band raster data might also contain:\n\nTime series: the same variable, over the same area, over time\nMultispectral imagery: image rasters that have 4 or more bands\nhyperspectral imagery: image rasters with more than 10-15 bands\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- raster data is pixelated data where each pixel is associated with a specific location\n- raster data always has an extent and a resolution\n- the extent is the geographical area covered by a raster\n- the resolution is the area covered by each pixel of a raster"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "2. Vector data",
    "text": "2. Vector data\nVector data structures represent specific features on the Earth’s surface, and assign attributes to those features. Vectors are composed of discrete geometric locations (x, y values) known as vertices that define the shape of the spatial object. The organization of the vertices determines the type of vector that we are working with:\n\npoint;\nline; or\npolygon\n\nVector datasets are in use in many industries besides geospatial fields. For instance, computer graphics are largely vector-based, although the data structures in use tend to join points using arcs and complex curves rather than straight lines. Computer-aided design (CAD) is also vector- based. The difference is that geospatial datasets are accompanied by information tying their features to real-world locations.\n\n\nPoints\nEach point is defined by a single x, y coordinate. There can be many points in a vector point file. Examples of point data include: sampling locations, the location of individual trees, or the location of survey plots.\n\n\nLines\nLines are composed of many (at least 2) points that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has a defined x, y location.\n\n\nPolygons\nA polygon consists of 3 or more vertices that are connected and closed. The outlines of survey plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\n\n\n\nData tip\n\n\n\nSometimes, boundary layers such as states and countries, are stored as lines rather than polygons. However, these boundaries, when represented as a line, will not create a closed object with a defined area that can be filled.\n\n\n\n\n\n\n\n\nThe plot below includes examples of two of the three types of vector objects. Use the definitions above to identify which features are represented by which vector type.\n\n\n\n\n\nState boundaries are polygons. The Fisher Tower location is a point. There are no line features shown.\n\n\n\n\n\n\nAdvantages of vector data\n\nthe geometry itself contains information about what the dataset creator thought was important\nthe geometry structures hold information in themselves - why choose point over polygon, for instance?\neach geometry feature can carry multiple attributes instead of just one, e.g. a database of cities can have attributes for name, country, population, etc\ndata storage can be very efficient compared to rasters\n\n\n\nDisadvantages of vector data\n\npotential loss of detail compared to raster\npotential bias in datasets - what didn’t get recorded?\ncalculations involving multiple vector layers need to do math on the geometry as well as the attributes, so can be slow compared to raster math\n\n\n\nVector data format\nLike raster data, vector data can also come in many different formats. For this blog, we will use the Shapefile format. A Shapefile format consists of multiple files in the same directory, of which .shp, .shx, and .dbf files are mandatory. Other non-mandatory but very important files are .prj and shp.xml files.\n\nthe .shp file stores the feature geometry itself\n.shx is a positional index of the feature geometry to allow quickly searching forwards and backwards the geographic coordinates of each vertex in the vector\n.dbf contains the tabular attributes for each shape.\n.prj file indicates the Coordinate reference system (CRS)\n.shp.xml contains the Shapefile metadata.\n\nTogether, the Shapefile includes the following information:\n\nextent - the spatial extent of the shapefile (i.e. geographic area that the shapefile covers). The spatial extent for a shapefile represents the combined extent for all spatial objects in the shapefile.\nobject type - whether the shapefile includes points, lines, or polygons.\nCoordinate reference system (CRS)\nother attributes - for example, a line shapefile that contains the locations of streams, might contain the name of each stream.\n\nBecause the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\n\n\n\n\n\n\nMore resources on Shapefiles\n\n\n\nMore about shapefiles can be found on Wikipedia. Shapefiles are often publicly available from government services, such as this page from the US Census Bureau or this one from Australia’s Data.gov.au website.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- vector data structures represent specific features on the Earth’s surface along with attributes of those features\n- vector objects are either points, lines, or polygons"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#coordinate-reference-systems",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#coordinate-reference-systems",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "3. Coordinate Reference Systems",
    "text": "3. Coordinate Reference Systems\nA data structure cannot be considered geospatial unless it is accompanied by coordinate reference system (CRS) information, in a format that geospatial applications can use to display and manipulate the data correctly. CRS information connects data to the Earth’s surface using a mathematical model. The CRS associated with a dataset tells your mapping software (for example Python) where the raster is located in geographic space. It also tells the mapping software what method should be used to flatten or project the raster in geographic space.\nCRS and SRS (spatial reference system) are synonyms and are commonly interchanged. We will use only the term CRS throughout this blog.\n\nThe above image shows maps of the United States in different projections. Notice the differences in shape associated with each projection. These differences are a direct result of the calculations used to flatten the data onto a 2-dimensional map. (Source: opennews.org)\nThere are lots of great resources that describe coordinate reference systems and projections in greater detail.\n\n\n\n\n\n\nData from the same location but saved in different projections will not line up in any Geographic Information System(GIS) or other program.\n\n\n\nIt’s important when working with spatial data to identify the coordinate reference system applied to the data and retain it throughout data processing and analysis.\n\n\n\nComponents of a CRS\nCRS information has three components:\n\nDatum: A model of the shape of the earth. It has angular units (i.e. degrees) and defines the starting point (i.e. where is [0,0]?) so the angles reference a meaningful spot on the earth. Common global datums are WGS84 and NAD83. Datums can also be local - fit to a particular area of the globe, but ill-fitting outside the area of intended use. In this blog, we will use the WGS84 datum.\nProjection: A mathematical transformation of the angular measurements on a round earth to a flat surface (i.e. paper or a computer screen). The units associated with a given projection are usually linear (feet, meters, etc.). In this workshop, we will see data in two different projections.\nAdditional Parameters: Additional parameters are often necessary to create the full coordinate reference system. One common additional parameter is a definition of the center of the map. The number of required additional parameters depends on what is needed by each specific projection.\n\n\n\n\n\n\n\nOrange peel analogy\n\n\n\nA common analogy employed to teach projections is the orange peel analogy. If you imagine that the Earth is an orange, how you peel it and then flatten the peel is similar to how projections get made.\n\n\n\n\n\nWhich projection should we use?\nTo decide if a projection is right for our data, answer these questions:\n\nWhat is the area of minimal distortion?\nWhat aspect of the data does it preserve?\n\nPeter Dana from the University of Colorado at Boulder and the Department of Geo-Information Processing has a good discussion of these aspects of projections. Online tools like Projection Wizard can also help discover projections that might be a good fit for our data.\n\n\nDescribing Coordinate Reference Systems\nThere are several common systems in use for storing and transmitting CRS information, as well as translating among different CRSs. These systems generally comply with ISO 19111. Common systems for describing CRSs include EPSG, OGC WKT, and PROJ strings.\n\n\nEPSG\nThe EPSG system is a database of CRS information maintained by the International Association of Oil and Gas Producers. The dataset contains both CRS definitions and information on how to safely convert data from one CRS to another. Using EPSG is easy as every CRS has an integer identifier, e.g. WGS84 is EPSG:4326. The downside is that you can only use the CRSs defined by EPSG and cannot customise them (some datasets do not have EPSG codes). epsg.io is an excellent website for finding suitable projections by location or for finding information about a particular EPSG code.\n\n\nWell-Known Text\nThe Open Geospatial Consortium WKT standard is used by a number of important geospatial apps and software libraries. WKT is a nested list of geodetic parameters. The structure of the information is defined on their website. WKT is valuable in that the CRS information is more transparent than in EPSG, but can be more difficult to read and compare than PROJ since it is meant to necessarily represent more complex CRS information. Additionally, the WKT standard is implemented inconsistently across various software platforms, and the spec itself has some known issues.\n\n\nPROJ\nPROJ is an open-source library for storing, representing and transforming CRS information.\n\n\n\n\n\n\nPROJ strings continue to be used, but the format is deprecated by the PROJ C maintainers due to inaccuracies when converting to the WKT format.\n\n\n\nCRS information can still be represented with EPSG, WKT, or PROJ strings without consequence, but it is best to only use PROJ strings as a format for viewing CRS information, not for reprojecting data.\n\n\nThe data and python libraries we will be working with in this blog use different underlying representations of CRSs under the hood for reprojecting. PROJ represents CRS information as a text string of key-value pairs, which makes it easy to read and interpret.\nA PROJ4 string includes the following information:\n\nproj: the projection of the data\nzone: the zone of the data (this is specific to the UTM projection)\ndatum: the datum used\nunits: the units for the coordinates of the data\nellps: the ellipsoid (how the earth’s roundness is calculated) for the data\n\nNote that the zone is unique to the UTM projection. Not all CRSs will have a zone.\n Image source: Chrismurf at English Wikipedia, via Wikimedia Commons (CC-BY).\nHere is a PROJ4 string for one of the datasets we will use in this blog:\n+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0\n\n\n\n\n\n\nWhat projection, zone, datum, and ellipsoid are used for this data? What are the units of the data? Using the map above, what part of the United States was this data collected from?\n\n\n\n\n\n\nProjection is UTM, zone 18, datum is WGS84, ellipsoid is WGS84.\nThe data is in meters.\nThe data comes from the eastern US seaboard.\n\n\n\n\n\n\nFormat interoperability\nMany existing file formats were invented by GIS software developers, often in a closed-source environment. This led to the large number of formats on offer today, and considerable problems transferring data between software environments. The Geospatial Data Abstraction Library (GDAL) is an open-source answer to this issue.\nGDAL is a set of software tools that translate between almost any geospatial format in common use today (and some not so common ones). GDAL also contains tools for editing and manipulating both raster and vector files, including reprojecting data to different CRSs. GDAL can be used as a standalone command-line tool, or built in to other GIS software. Several open-source GIS programs use GDAL for all file import/export operations.\n\n\nMetadata\nSpatial data is useless without metadata. Essential metadata includes the CRS information, but proper spatial metadata encompasses more than that. History and provenance of a dataset (how it was made), who is in charge of maintaining it, and appropriate (and inappropriate!) use cases should also be documented in metadata. This information should accompany a spatial dataset wherever it goes. In practice this can be difficult, as many spatial data formats don’t have a built-in place to hold this kind of information. Metadata often has to be stored in a companion file, and generated and maintained manually.\n\n\n\n\n\n\nMore Resources on CRS\n\n\n\n\nspatialreference.org - A comprehensive online library of CRS information.\nQGIS Documentation - CRS Overview.\nChoosing the Right Map Projection.\nVideo highlighting how map projections can make continents seems proportionally larger or smaller than they actually are.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- All geospatial datasets (raster and vector) are associated with a specific coordinate reference system\n- A coordinate reference system includes datum, projection, and additional parameters specific to the dataset"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#the-geospatial-landscape",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#the-geospatial-landscape",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "4. The Geospatial Landscape",
    "text": "4. The Geospatial Landscape\nMost traditional GIS work is carried out in standalone applications that aim to provide end-to-end geospatial solutions. These applications are available under a wide range of licenses and price points. Some of the most common are listed below.\n\nOpen-source software\nThe Open Source Geospatial Foundation (OSGEO) supports several actively managed GIS platforms:\n\nQGIS is a professional GIS application that is built on top of and proud to be itself Free and Open Source Software (FOSS). QGIS is written in Python, has a python console interface, and has several interfaces written in R including RQGIS.\nGRASS GIS, commonly referred to as GRASS (Geographic Resources Analysis Support System), is a FOSS-GIS software suite used for geospatial data management and analysis, image processing, graphics and maps production, spatial modeling, and visualization. GRASS GIS is currently used in academic and commercial settings around the world, as well as by many governmental agencies and environmental consulting companies. It is a founding member of the Open Source Geospatial Foundation (OSGeo). GRASS GIS can be installed along with and made accessible within QGIS 3.\nGDAL is a multiplatform set of tools for translating between geospatial data formats. It can also handle reprojection and a variety of geoprocessing tasks. GDAL is built in to many applications both FOSS and commercial, including GRASS and QGIS.\nSAGA-GIS, or System for Automated Geoscientific Analyses, is a FOSS-GIS application developed by a small team of researchers from the Dept. of Physical Geography, Göttingen, and the Dept. of Physical Geography, Hamburg. SAGA has been designed for an easy and effective implementation of spatial algorithms, offers a comprehensive, growing set of geoscientific methods, provides an easily approachable user interface with many visualisation options, and runs under Windows and Linux operating systems. Like GRASS GIS, it can also be installed and made accessible in QGIS3.\nPostGIS is a geospatial extension to the PostGreSQL relational database.\n\n\n\nOnline + Cloud computing\n\nPANGEO is a community organization dedicated to open and reproducible data science with python. They focus on the Pangeo software ecosystem for working with big data in the geosciences. This community organization also supports python libraries like xarray, iris, dask, jupyter, and many other packages.\nGoogle has created Google Earth Engine which combines a multi-petabyte catalog of satellite imagery and geospatial datasets with planetary-scale analysis capabilities and makes it available for scientists, researchers, and developers to detect changes, map trends, and quantify differences on the Earth’s surface. Earth Engine API runs in both Python and JavaScript.\nArcGIS Online provides access to thousands of maps and base layers.\nKepler.gl is Uber’s toolkit for handling large datasets (i.e. Uber’s data archive).\nSepal.io by FAO Open Foris utilizing EOS satellite imagery and cloud resources for global forest monitoring.\n\n\n\nGUI vs CLI\nThe earliest computer systems operated without a graphical user interface (GUI), relying only on the command-line interface (CLI). Since mapping and spatial analysis are strongly visual tasks, GIS applications benefited greatly from the emergence of GUIs and quickly came to rely heavily on them. Most modern GIS applications have very complex GUIs, with all common tools and procedures accessed via buttons and menus.\nBenefits of using a GUI include:\n\nTools are all laid out in front of you\nComplex commands are easy to build\nDon’t need to learn a coding language\nCartography and visualisation is more intuitive and flexible\n\nDownsides of using a GUI include:\n\nLow reproducibility - you can’t record your actions and replay\nMost are not designed for batch-processing files\nLimited ability to customise functions or write your own\nIntimidating interface for new users - so many buttons!\n\nIn scientific computing, the lack of reproducibility in point-and-click software has come to be viewed as a critical weakness. As such, scripted CLI-style workflows are again becoming popular, which leads us to another approach to doing GIS — via a programming language. This is the approach we will be using throughout this blog.\n\n\nGIS in programming languages\nA number of powerful geospatial processing libraries exist for general-purpose programming languages like Java and C++. However, the learning curve for these languages is steep and the effort required is excessive for users who only need a subset of their functionality.\nHigher-level scripting languages like Python and R are easier to learn and use. Both now have their own packages that wrap up those geospatial processing libraries and make them easy to access and use safely. A key example is the Java Topology Suite (JTS), which is implemented in C++ as GEOS. GEOS is accessible in Python via the shapely package (and geopandas, which makes use of shapely) and in R via sf. R and Python also have interface packages for GDAL, and for specific GIS apps.\nThis last point is a huge advantage for GIS-by-programming; these interface packages give you the ability to access functions unique to particular programs, but have your entire workflow recorded in a central document - a document that can be re-run at will. Below are lists of some of the key spatial packages for Python, which we will be using in the remainder of this workshop.\n\ngeopandas and geocube for working with vector data\nrasterio and rioxarray for working with raster data\n\nThese packages along with the matplotlib package are all we need for spatial data visualisation. Python also has many fundamental scientific packages that are relevant in the geospatial domain. Below is a list of particularly fundamental packages:\n\nNumPy\nscipy\nscikit-image\n\nThese are all excellent options for working with rasters, as arrays. An overview of these and other Python spatial packages can be accessed here.\nAs a programming language, Python can be a CLI tool. However, using Python together with an Integrated Development Environment (IDE) application allows some GUI features to become part of your workflow. IDEs allow the best of both worlds. They provide a place to visually examine data and other software objects, interact with your file system, and draw plots and maps, but your activities are still command-driven: recordable and reproducible. There are several IDEs available for Python. JupyterLab is well-developed and the most widely used option for data science in Python. VSCode and Spyder are other popular options for data science.\nTraditional GIS apps are also moving back towards providing a scripting environment for users, further blurring the CLI/GUI divide. ESRI have adopted Python into their software, and QGIS is both Python and R-friendly.\n\n\nGIS File Types\nThere are a variety of file types that are used in GIS analysis. Depending on the program you choose to use some file types can be used while others are not readable. Below is a brief table describing some of the most common vector and raster file types.\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- Many software packages exist for working with geospatial data\n- Command-line programs allow you to automate and reproduce your work\n- JupyterLab provides a user-friendly interface for working with Python"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#access-satellite-imagery-using-python",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#access-satellite-imagery-using-python",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "5. Access satellite imagery using Python",
    "text": "5. Access satellite imagery using Python\nA number of satellites take snapshots of the Earth’s surface from space. The images recorded by these remote sensors represent a very precious data source for any activity that involves monitoring changes on Earth. Satellite imagery is typically provided in the form of geospatial raster data, with the measurements in each grid cell (“pixel”) being associated to accurate geographic coordinate information.\nIn this section we will explore how to access open satellite data using Python. In particular, we will consider the Sentinel-2 data collection that is hosted on AWS. This dataset consists of multi-band optical images acquired by the two satellites of the Sentinel-2 mission and it is continuously updated with new images.\n\n\n\n\n\n\nSatellite data sources\n\n\n\n\nLandsat\nSentinel\n\n\n\n\nSearch for satellite imagery - Sentinel\nCurrent sensor resolutions and satellite revisit periods are such that terabytes of data products are added daily to the corresponding collections. Such datasets cannot be made accessible to users via full-catalog download. Space agencies and other data providers often offer access to their data catalogs through interactive Graphical User Interfaces (GUIs), see for instance the Copernicus Open Access Hub portal for the Sentinel missions. Accessing data via a GUI is a nice way to explore a catalog and get familiar with its content, but it represents a heavy and error-prone task that should be avoided if carried out systematically to retrieve data.\nA service that offers programmatic access to the data enables users to reach the desired data in a more reliable, scalable and reproducible manner. An important element in the software interface exposed to the users, which is generally called the Application Programming Interface (API), is the use of standards. Standards, in fact, can significantly facilitate the reusability of tools and scripts across datasets and applications.\nThe SpatioTemporal Asset Catalog (STAC) specification is an emerging standard for describing geospatial data. By organizing metadata in a form that adheres to the STAC specifications, data providers make it possible for users to access data from different missions, instruments and collections using the same set of tools.\n\n\nSearch a STAC catalog\nThe STAC browser is a good starting point to discover available datasets, as it provides an up-to-date list of existing STAC catalogs. From the list, let’s click on the “Earth Search” catalog, i.e. the access point to search the archive of Sentinel-2 images hosted on AWS.\nLet’s take a moment to explore the Earth Search STAC catalog, which is the catalog indexing the Sentinel-2 collection that is hosted on AWS. We can interactively browse this catalog using the STAC browser at this link :\n\nOpen the link in your web browser. Which (sub-) catalogs are available?\n\nFour subcatalogs are available, including both Sentinel 2 and Landsat 8 images\n\n\n\ncatalogs.PNG\n\n\n\nOpen the Sentinel-2 L2A COGs collection, and select one item from the list. Each item corresponds to a satellite “scene”, i.e. a portion of the footage recorded by the satellite at a given time. Have a look at the metadata fields and the list of assets. What kind of data do the assets represent?\n\n\nWhen you select the Sentinel-2 L2A COGs collection, and randomly choose one of the items from the list, you should find yourself on a page similar to the screenshot above. On the left side you will find a list of the available assets: overview images (thumbnail and true color images), metadata files and the “real” satellite images, one for each band captured by the Multispectral Instrument on board Sentinel-2.\nWhen opening a catalog with the STAC browser, you can access the API URL by clicking on the “Source” button on the top right of the page. By using this URL, we have access to the catalog content and, if supported by the catalog, to the functionality of searching its items. For the Earth Search STAC catalog the API URL is:\n\napi_url = \"https://earth-search.aws.element84.com/v0\"\n\nWe can query a STAC API endpoint from Python using the pystac_client library:\n\nfrom pystac_client import Client\nclient = Client.open(api_url)\n\nIn the following, we ask for scenes belonging to the sentinel-s2-l2a-cogs collection. This dataset includes Sentinel-2 data products pre-processed at level 2A (bottom-of-atmosphere reflectance) and saved in Cloud Optimized GeoTIFF (COG) format:\n\n# Sentinel-2, Level 2A, COGs\ncollection = \"sentinel-s2-l2a-cogs\" \n\nhttps://earth-search.aws.element84.com/v0/collections/sentinel-s2-l2a-cogs\n\n\nCloud Optimized GeoTIFFs\nCloud Optimized GeoTIFFs (COGs) are regular GeoTIFF files with some additional features that make them ideal to be employed in the context of cloud computing and other web-based services. This format builds on the widely-employed GeoTIFF format, introduced in section 1: Raster Data.\nIn essence, COGs are regular GeoTIFF files with a special internal structure. One of the features of COGs is that data is organized in “blocks” that can be accessed remotely via independent HTTP requests. Data users can thus access the only blocks of a GeoTIFF that are relevant for their analysis, without having to download the full file. In addition, COGs typically include multiple lower-resolution versions of the original image, called “overviews”, which can also be accessed independently. By providing this “pyramidal” structure, users that are not interested in the details provided by a high-resolution raster can directly access the lower-resolution versions of the same image, significantly saving on the downloading time. More information on the COG format can be found here.\nWe also ask for scenes intersecting a geometry defined using the shapely library (in this case, a point):\n\n# AMS coordinates\nfrom shapely.geometry import Point\npoint = Point(4.89, 52.37)  \n\nNote: at this stage, we are only dealing with metadata, so no image is going to be downloaded yet. But even metadata can be quite bulky if a large number of scenes match our search! For this reason, we limit the search result to 10 items:\n\n# Search for scenes which include the point(4.89, 52.37)\nsearch = client.search(\n    collections=[collection],\n    intersects=point,\n    max_items=10,\n)\n\nWe submit the query and find out how many scenes match our search criteria (please note that this output can be different as more data is added to the catalog):\n\nprint(search.matched())\n\n736\n\n\nFinally, we retrieve the metadata of the search results:\n\nitems = search.get_all_items()\nprint(len(items))\n\n10\n\n\nThis is consistent with the maximum number of items that we set in the search criteria. We can iterate over the returned items and print these to show their IDs:\n\n# Iterate over our 10 items to show unique IDs\nfor item in items:\n    print(item)\n\n<Item id=S2B_31UFU_20221211_0_L2A>\n<Item id=S2B_31UFU_20221208_0_L2A>\n<Item id=S2A_31UFU_20221206_0_L2A>\n<Item id=S2A_31UFU_20221203_0_L2A>\n<Item id=S2B_31UFU_20221201_0_L2A>\n<Item id=S2B_31UFU_20221128_0_L2A>\n<Item id=S2A_31UFU_20221126_0_L2A>\n<Item id=S2A_31UFU_20221123_0_L2A>\n<Item id=S2B_31UFU_20221121_0_L2A>\n<Item id=S2B_31UFU_20221118_0_L2A>\n\n\nEach of the items contains information about the scene geometry, its acquisition time, and other metadata that can be accessed as a dictionary from the properties attribute. Let’s inspect the metadata associated with the first item of the search results:\n\n# extract some metadata from our first search item\nitem = items[0]\nprint(item.datetime)\nprint(item.geometry)\nprint(item.properties)\n\n2022-12-11 10:56:20+00:00\n{'type': 'Polygon', 'coordinates': [[[6.071664488869862, 52.22257539160586], [4.464995307918359, 52.25346561204129], [4.498475093400055, 53.24019917467795], [6.1417542968794585, 53.20819279121764], [6.071664488869862, 52.22257539160586]]]}\n{'datetime': '2022-12-11T10:56:20Z', 'platform': 'sentinel-2b', 'constellation': 'sentinel-2', 'instruments': ['msi'], 'gsd': 10, 'view:off_nadir': 0, 'proj:epsg': 32631, 'sentinel:utm_zone': 31, 'sentinel:latitude_band': 'U', 'sentinel:grid_square': 'FU', 'sentinel:sequence': '0', 'sentinel:product_id': 'S2B_MSIL2A_20221211T105339_N0509_R051_T31UFU_20221211T122517', 'sentinel:data_coverage': 100, 'eo:cloud_cover': 53.32, 'sentinel:valid_cloud_cover': True, 'sentinel:processing_baseline': '05.09', 'sentinel:boa_offset_applied': True, 'created': '2022-12-11T19:02:22.484Z', 'updated': '2022-12-11T19:02:22.484Z'}\n\n\nThe above metadata e.g. datetime, eo:cloud_cover are the arguments which can be fed into our search to acess specific information.\nLet’s try another scene search from the sentinel-s2-l2a-cogs collection using different criteria:\n-intersect a provided bounding box (use ±0.01 deg in lat/lon from the previously defined point);\n-have been recorded between 20 March 2020 and 30 March 2020;\n-have a cloud coverage smaller than 10% (hint: use the query input argument of client.search).\n\nbound_box = point.buffer(0.01).bounds\n\n\n# Search for scenes which intersect bounding box +- 0.01 deg in lat/lon from (4.89, 52.37) as prev defined\n# between 20/3/20 and 30/3/20 where cloud cover < 10%\nsearch = client.search(\n    collections=[collection],\n    bbox=bound_box,\n    datetime=\"2020-03-20/2020-03-30\",\n    query=[\"eo:cloud_cover<10\"]\n)\n\nprint(search.matched())\n\n4\n\n\n\n# Grab search items and save as a JSON file\nitems = search.get_all_items()\nitems.save_object(\"search.json\")\n\nAn extract of the JSON file is included below. As we can see the file contains a lot of information for each of our 4 search items, which are indexed 0 to 3, such as properties, geometry, links etc:\n\n\n\njson.PNG\n\n\n\n\nAccess the images(assets)\nSo far we have only discussed metadata - but how can one get to the actual images of a satellite scene (the “assets” in the STAC nomenclature)? These can be reached via links that are made available through the item’s attribute assets. The JSON file extract is included below, followed by how to access this info using Python.\n\n\n\njson_image_thumbnail.PNG\n\n\n\n# first item's asset dictionary\nassets = items[0].assets  \nprint(assets.keys())\n\ndict_keys(['thumbnail', 'overview', 'info', 'metadata', 'visual', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'AOT', 'WVP', 'SCL'])\n\n\nAs we can see these dictionary keys match the headings included in the JSON image above.\nWe can print a minimal description of the available assets:\n\nfor key, asset in assets.items():\n    print(f\"{key}: {asset.title}\")\n\nthumbnail: Thumbnail\noverview: True color image\ninfo: Original JSON metadata\nmetadata: Original XML metadata\nvisual: True color image\nB01: Band 1 (coastal)\nB02: Band 2 (blue)\nB03: Band 3 (green)\nB04: Band 4 (red)\nB05: Band 5\nB06: Band 6\nB07: Band 7\nB08: Band 8 (nir)\nB8A: Band 8A\nB09: Band 9\nB11: Band 11 (swir16)\nB12: Band 12 (swir22)\nAOT: Aerosol Optical Thickness (AOT)\nWVP: Water Vapour (WVP)\nSCL: Scene Classification Map (SCL)\n\n\nAmong the others, assets include multiple raster data files B01 through B12 (one per optical band, as acquired by the multi-spectral instrument), a thumbnail, a true-color image (“visual”), instrument metadata and scene-classification information (“SCL”).\n\n# Let’s get the URL links to the actual image:\nprint(assets[\"thumbnail\"].href)\n\nhttps://roda.sentinel-hub.com/sentinel-s2-l1c/tiles/31/U/FU/2020/3/28/0/preview.jpg\n\n\n\n\n\nsentinel_image_download.jpg\n\n\nRemote raster data can be directly opened via the rioxarray library. We will learn more about this library in the next sections.\n\n# Open raster B01 Band 1 (coastal)\nimport rioxarray\nb01_href = assets[\"B01\"].href\nb01 = rioxarray.open_rasterio(b01_href)\nprint(b01)\n\n<xarray.DataArray (band: 1, y: 1830, x: 1830)>\n[3348900 values with dtype=uint16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6.001e+05 6.002e+05 ... 7.097e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0\n\n\nWe can then save the data to disk:\n\n# save image to disk\nb01.rio.to_raster(\"B01.tif\")\n\n\n\nSearch for satellite imagery - Landsat 8\nLet’s now put into practice all the skills we have learned in this section to retrieve images from a different mission: Landsat 8. In particular, we browse images from the Harmonized Landsat Sentinel-2 (HLS) project, which provides images from NASA’s Landsat 8 and ESA’s Sentinel-2 that have been made consistent with each other. The HLS catalog is indexed in the NASA Common Metadata Repository (CMR) and it can be accessed from the STAC API endpoint at the following URL: https://cmr.earthdata.nasa.gov/stac/LPCLOUD.\n\nusing pystac_client, search for all assets of the Landsat 8 collection (HLSL30.v2.0) from February to March 2021\nintersecting the point with longitude/latitute coordinates (-73.97, 40.78) deg.\nsort by cloud cover\n\nVisualize an item’s thumbnail (asset key “browse”).\n\n# Connect to the STAC endpoint\napi_url = \"https://cmr.earthdata.nasa.gov/stac/LPCLOUD\"\nclient = Client.open(api_url)\n\n\n# Search for scenes which include point (-73.97, 40.78) between Feb and March 2021\n# Note we can enter collections and co_ors directly into search argument\nsearch = client.search(\n    collections=[\"HLSL30.v2.0\"],\n    intersects=Point(-73.97, 40.78),  \n    datetime=\"2021-02-01/2021-03-31\"\n )\n\n# retrieve search results\nitems = search.get_all_items()\n\n# save as JSON file\nitems.save_object(\"landsat.search.json\")\nprint(len(items))\n\n5\n\n\n\n# sort by cloud cover and select first item\nitems_sorted = sorted(items, key=lambda x: x.properties[\"eo:cloud_cover\"]) \nitem = items_sorted[0]\nprint(item)\n\n<Item id=HLS.L30.T18TWL.2021039T153324.v2.0>\n\n\n\n# Let’s get the URL links to the actual image:\nprint(item.assets[\"browse\"].href)\n\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-public/HLSL30.020/HLS.L30.T18TWL.2021039T153324.v2.0/HLS.L30.T18TWL.2021039T153324.v2.0.jpg\n\n\n\n\n\n\n\n\n\nPublic catalogs, protected data\n\n\n\nPublicly accessible catalogs and STAC endpoints do not necessarily imply publicly accessible data. Data providers, in fact, may limit data access to specific infrastructures and/or require authentication. For instance, the NASA CMR STAC endpoint considered in the last exercise offers publicly accessible metadata for the HLS collection, but most of the linked assets are available only for registered users (the thumbnail is publicly accessible).\nThe authentication procedure for dataset with restricted access might differ depending on the data provider. For the NASA CMR, follow these steps in order to access data using Python:\n\ncreate a NASA Earthdata login account here;\nset up a netrc file with your credentials, e.g. by using this script;\ndefine the following environment variables:\n\n\n\nimport os\nos.environ[\"GDAL_HTTP_COOKIEFILE\"] = \"./cookies.txt\"\nos.environ[\"GDAL_HTTP_COOKIEJAR\"] = \"./cookies.txt\"\n\n\n\n\n\n\nKey Points\n\n\n\n\naccessing satellite images via the providers’ API enables a more reliable and scalable data retrieval\nSTAC catalogs can be browsed and searched using the same tools and scripts\nrioxarray allows you to open and download remote raster files"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#read-and-visualize-raster-data",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#read-and-visualize-raster-data",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "6. Read and visualize raster data",
    "text": "6. Read and visualize raster data\nRaster datasets were introduced in section 1. Here, we introduce the fundamental principles, packages and metadata/raster attributes for working with raster data in Python. We will also explore how Python handles missing and bad data values.\nrioxarray is the Python package we will use throughout this blog to work with raster data. It is based on the popular rasterio package for working with rasters and xarray for working with multi-dimensional arrays. rioxarray extends xarray by providing top-level functions (e.g. the open_rasterio function to open raster datasets) and by adding a set of methods to the main objects of the xarray package (the Dataset and the DataArray). These additional methods are made available via the rio accessor and become available from xarray objects after importing rioxarray.\nWe will also use the pystac package to load rasters from the search results we created in the previous section.\nWe’ll continue from the results of the satellite image search that we have carried out in the previous section. We will load data starting from the search.json file, using one scene from the search results as an example to demonstrate data loading and visualization. You can download the raster data using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started with this lesson.\n\nLoad a Raster and View Attributes\nIn the previous section, we searched for Sentinel-2 images, and then saved the search results to a file named search.json. This contains the information on where and how to access the target images from a remote repository. We can use the function pystac.ItemCollection.from_file() to load the search results as an Item list.\n\nimport pystac\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\nIn the search results, we have 2 Item type objects, corresponding to 4 Sentinel-2 scenes from March 26th and 28th in 2020. We will focus on the first scene S2A_31UFU_20200328_0_L2A, and load band B09 (central wavelength 945 nm). We can load this band using the function rioxarray.open_rasterio(), via the Hypertext Reference href (commonly referred to as a URL):\n\nimport rioxarray\n\n# load band B09 (central wavelength 945 nanometres(nm) - 1 nm = 10^-9m\nraster_ams_b9 = rioxarray.open_rasterio(items[0].assets[\"B09\"].href)\n\n# Call the variable name to get a quick look at the shape and attributes\nraster_ams_b9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1830, x: 1830)>\n[3348900 values with dtype=uint16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6.001e+05 6.002e+05 ... 7.097e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 1830x: 1830...[3348900 values with dtype=uint16]Coordinates: (4)band(band)int641array([1])x(x)float646e+05 6.001e+05 ... 7.098e+05array([600030., 600090., 600150., ..., 709650., 709710., 709770.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900010., 5899950., 5899890., ..., 5790390., 5790330., 5790270.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 60.0 0.0 5900040.0 0.0 -60.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600030.0, 600090.0, 600150.0, 600210.0, 600270.0, 600330.0,\n              600390.0, 600450.0, 600510.0, 600570.0,\n              ...\n              709230.0, 709290.0, 709350.0, 709410.0, 709470.0, 709530.0,\n              709590.0, 709650.0, 709710.0, 709770.0],\n             dtype='float64', name='x', length=1830))yPandasIndexPandasIndex(Float64Index([5900010.0, 5899950.0, 5899890.0, 5899830.0, 5899770.0, 5899710.0,\n              5899650.0, 5899590.0, 5899530.0, 5899470.0,\n              ...\n              5790810.0, 5790750.0, 5790690.0, 5790630.0, 5790570.0, 5790510.0,\n              5790450.0, 5790390.0, 5790330.0, 5790270.0],\n             dtype='float64', name='y', length=1830))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\nThe first call to rioxarray.open_rasterio() opens the file from remote or local storage, and then returns a xarray.DataArray object. The object is stored in a variable, i.e. raster_ams_b9. Reading in the data with xarray instead of rioxarray also returns a xarray.DataArray, but the output will not contain the geospatial metadata (such as projection information). We can use numpy functions or built-in Python math operators on a xarray.DataArray just like a numpy array. Calling the variable name of the DataArray also prints out all of its metadata information.\nThe output tells us that we are looking at an xarray.DataArray, with:\n\n1 band;\n1830 rows(y); and\n1830 columns(x)\n\nWe can also see the number of pixel values in the DataArray (1,830 x 1,830 = 3,348,900) and the type of those pixel values, which is unsigned integer (or uint16). The DataArray also stores different values for the coordinates of the DataArray. When using rioxarray, the term coordinates refers to spatial coordinates like x and y but also the band coordinate. Each of these sequences of values has its own data type, like float64 for the spatial coordinates and int64 for the band coordinate.\nThis DataArray object also has a couple of attributes that are accessed like .rio.crs, .rio.nodata, and .rio.bounds(), which contain the metadata for the file we opened.\n\n\n\n\n\n\nNote\n\n\n\nNote that many of the metadata are accessed as attributes using .attribute_name, but bounds() is a method (i.e. a function in an object) and needs parentheses.\n\n\n\n# Co_ordinate Reference System\nprint(raster_ams_b9.rio.crs)\n# Nodata value encoded as...\nprint(raster_ams_b9.rio.nodata)\n# Bounding box corners\nprint(raster_ams_b9.rio.bounds())\n# Width \nprint(raster_ams_b9.rio.width)\n# Height\nprint(raster_ams_b9.rio.height)\n\nEPSG:32631\n0\n(600000.0, 5790240.0, 709800.0, 5900040.0)\n1830\n1830\n\n\nThe Coordinate Reference System, or raster_ams_b9.rio.crs, is reported as the string EPSG:32631. The nodata value is encoded as 0 and the bounding box corners of our raster are represented by the output of .bounds() as a tuple (like a list but we can’t edit it). The height and width match what we saw when we printed the DataArray, but by using .rio.width and .rio.height we can access these values if we need them in calculations.\nWe will be exploring this data throughout this section. By the end of this section we will be able to understand and explain the metadata output.\n\n\nVisualize a Raster\nAfter viewing the attributes of our raster, we can examine the raw values of the array with .values:\n\nraster_ams_b9.values\n\narray([[[    0,     0,     0, ...,  8888,  9075,  8139],\n        [    0,     0,     0, ..., 10444, 10358,  8669],\n        [    0,     0,     0, ..., 10346, 10659,  9168],\n        ...,\n        [    0,     0,     0, ...,  4295,  4289,  4320],\n        [    0,     0,     0, ...,  4291,  4269,  4179],\n        [    0,     0,     0, ...,  3944,  3503,  3862]]], dtype=uint16)\n\n\nThis can give us a quick view of the values of our array, but only at the corners. Since our raster is loaded in python as a DataArray type, we can plot this in one line similar to a pandas DataFrame with DataArray.plot() :\n\nraster_ams_b9.plot()\n\n<matplotlib.collections.QuadMesh at 0x7fa254cc0d30>\n\n\n\n\n\nNice plot! Notice that rioxarray helpfully allows us to plot this raster with spatial coordinates on the x and y axis (this is not the default in many cases with other functions or libraries).\nThis plot shows the satellite measurement of the spectral band B09 for an area that covers part of the Netherlands. According to the Sentinel-2 documentaion, this is a band with the central wavelength of 945nm, which is sensitive to water vapour. It has a spatial resolution of 60m. Note that the band=1 in the image title refers to the ordering of all the bands in the DataArray, not the Sentinel-2 band number B09 that we saw in the pystac search results.\n\n\n\n\n\n\nWith a quick view of the image, we notice that half of the image is blank, no data is captured. We also see that the cloudy pixels at the top have high reflectance values, while the contrast of everything else is quite low. This is expected because this band is sensitive to the water vapour.\n\n\n\nTo obtain a better colour contrast, we can add the option robust=True, which displays values between the 2nd and 98th percentile.\n\n\n\n# restrict display to values within the 2nd and 98th quartile\nraster_ams_b9.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa255063370>\n\n\n\n\n\nNow that the colour limit is set in a way fitting most of the values in the image, we have a much better view of the ground pixels.\n\n\n\n\n\n\nThe option robust=True defaults to displaying values between the 2nd and 98th percentile.\n\n\n\nFor a customized displaying range, you can also manually specifying the keywords vmin and vmax.\n\n\n\nraster_ams_b9.plot(vmin=100, vmax=7000)\n\n<matplotlib.collections.QuadMesh at 0x7fa254f4e680>\n\n\n\n\n\n\n\nView Raster Coordinate Reference System (CRS) in Python\nAnother feature that we’re interested in is the CRS, and it can be accessed with .rio.crs. We introduced the concept of a CRS in an earlier section. Now we will see how features of the CRS appear in our data file and what meanings they have. We can view the CRS string associated with our DataArray’s rio object using the crs attribute.\n\n# view the co_ordinate reference system string\nprint(raster_ams_b9.rio.crs)\n\nEPSG:32631\n\n\nTo print the EPSG code number as an int, we use the .to_epsg() method:\n\n# print the ESPG code as an int\nraster_ams_b9.rio.crs.to_epsg()\n\n32631\n\n\nEPSG codes are great for succinctly representing a particular coordinate reference system. But what if we want to see more details about the CRS, like the units? For that, we can use pyproj, a library for representing and working with coordinate reference systems.\n\nfrom pyproj import CRS\nepsg = raster_ams_b9.rio.crs.to_epsg()\ncrs = CRS(epsg)\ncrs\n\n<Derived Projected CRS: EPSG:32631>\nName: WGS 84 / UTM zone 31N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 0°E and 6°E, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Andorra. Belgium. Benin. Burkina Faso. Denmark - North Sea. France. Germany - North Sea. Ghana. Luxembourg. Mali. Netherlands. Niger. Nigeria. Norway. Spain. Togo. United Kingdom (UK) - North Sea.\n- bounds: (0.0, 0.0, 6.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 31N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nUnderstanding pyproj CRS Summary\nLet’s break down the pieces of the pyproj CRS summary. The string contains all of the individual CRS elements that Python or another GIS might need, separated into distinct sections, and datum.\n\nName: of the projection is UTM zone 31N (UTM has 60 zones, each 6-degrees of longitude in width). The underlying datum is WGS84.\nAxis Info: the CRS shows a Cartesian system with two axes, easting and northing, in meter units.\nArea of Use: the projection is used for a particular range of longitudes 0°E to 6°E in the northern hemisphere (0.0°N to 84.0°N)\nCoordinate Operation: the operation to project the coordinates (if it is projected) onto a cartesian (x, y) plane. Transverse Mercator is accurate for areas with longitudinal widths of a few degrees, hence the distinct UTM zones.\nDatum: Details about the datum, or the reference point for coordinates. WGS 84 and NAD 1983 are common datums. NAD 1983 is set to be replaced in 2022.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the zone is unique to the UTM projection. Not all CRSs will have a zone.\n\n\nThe CRS class from the pyproj library allows us to create a CRS object with methods and attributes for accessing specific information about a CRS, or the detailed summary shown above. A particularly useful attribute is area_of_use, which shows the geographic bounds that the CRS is intended to be used.\n\ncrs.area_of_use\n\nAreaOfUse(west=0.0, south=0.0, east=6.0, north=84.0, name='Between 0°E and 6°E, northern hemisphere between equator and 84°N, onshore and offshore. Algeria. Andorra. Belgium. Benin. Burkina Faso. Denmark - North Sea. France. Germany - North Sea. Ghana. Luxembourg. Mali. Netherlands. Niger. Nigeria. Norway. Spain. Togo. United Kingdom (UK) - North Sea.')\n\n\n\n\n\n\n\n\nWhat units are our data in? See if you can find a method to examine this information using help(crs) or dir(crs)\n\n\n\n\n\ncrs.axis_info tells us that the CRS for our raster has two axis and both are in meters. We could also get this information from the attribute raster_ams_b9.rio.crs.linear_units.\n\n\n\n\ncrs.axis_info\n\n[Axis(name=Easting, abbrev=E, direction=east, unit_auth_code=EPSG, unit_code=9001, unit_name=metre),\n Axis(name=Northing, abbrev=N, direction=north, unit_auth_code=EPSG, unit_code=9001, unit_name=metre)]\n\n\n\nraster_ams_b9.rio.crs.linear_units\n\n'metre'\n\n\n\n\nCalculate Raster Statistics\nIt is useful to know the minimum or maximum values of a raster dataset. We can compute these and other descriptive statistics with min, max, mean, and std.\n\nprint(raster_ams_b9.min())\nprint(raster_ams_b9.max())\nprint(raster_ams_b9.mean())\nprint(raster_ams_b9.std())\n\n<xarray.DataArray ()>\narray(0, dtype=uint16)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(15497, dtype=uint16)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(1652.44009944)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(2049.16447495)\nCoordinates:\n    spatial_ref  int64 0\n\n\nThe information above includes a report of the min, max, mean, and standard deviation values, along with the data type. If we want to see specific quantiles, we can use xarray’s .quantile() method. For example for the 25% and 75% quantiles:\n\nprint(raster_ams_b9.quantile([0.25, 0.75]))\n\n<xarray.DataArray (quantile: 2)>\narray([   0., 2911.])\nCoordinates:\n  * quantile  (quantile) float64 0.25 0.75\n\n\nWe could also get each of these values one by one using NumPy:\n\nimport numpy\nprint(numpy.percentile(raster_ams_b9, 25))\nprint(numpy.percentile(raster_ams_b9, 75))\n\n0.0\n2911.0\n\n\n\n\n\n\n\n\nYou may notice that raster_ams_b9.quantile and numpy.percentile didn’t require an argument specifying the axis or dimension along which to compute the quantile. This is because axis=None is the default for most numpy functions, and therefore dim=None is the default for most xarray methods.\n\n\n\nit’s always good to check out the docs on a function to see what the default arguments are, particularly when working with multi-dimensional image data. To do so, we can use help(raster_ams_b9.quantile) or ?raster_ams_b9.quantile if you are using jupyter notebook or jupyter lab.\n\n\n\n\nDealing with Missing Data\nSo far, we have visualized a band of a Sentinel-2 scene and calculated its statistics. However, we need to take missing data into account. Raster data often has a “no data value” associated with it and for raster datasets read in by rioxarray. This value is referred to as nodata. This is a value assigned to pixels where data is missing or no data were collected. There can be different cases that cause missing data, and it’s common for other values in a raster to represent different cases. The most common example is missing data at the edges of rasters.\nBy default the shape of a raster is always rectangular. So if we have a dataset that has a shape that isn’t rectangular, some pixels at the edge of the raster will have no data values. This often happens when the data were collected by a sensor which only flew over some part of a defined region. As we have seen above, the nodata value of this dataset (raster_ams_b9.rio.nodata) is 0. When we have plotted the band data, or calculated statistics, the missing value was not distinguished from other values.\n\n\n\n\n\n\nMissing data may cause some unexpected results. For example, the 25th percentile we just calculated was 0, probably reflecting the presence of a lot of missing data in the raster.\n\n\n\nTo distinguish missing data from real data, one possible way is to use NaN to represent them. This can be done by specifying masked=True when loading the raster.\n\n\n\n# use NaN to represent missing data\nraster_ams_b9 = rioxarray.open_rasterio(items[0].assets[\"B09\"].href, masked=True)\n\nOr, we can also use the where function to select all the pixels which are different from the nodata value of the raster:\n\nraster_ams_b9.where(raster_ams_b9!=raster_ams_b9.rio.nodata)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 1830, x: 1830)>\narray([[[   nan,    nan,    nan, ...,  8888.,  9075.,  8139.],\n        [   nan,    nan,    nan, ..., 10444., 10358.,  8669.],\n        [   nan,    nan,    nan, ..., 10346., 10659.,  9168.],\n        ...,\n        [   nan,    nan,    nan, ...,  4295.,  4289.,  4320.],\n        [   nan,    nan,    nan, ...,  4291.,  4269.,  4179.],\n        [   nan,    nan,    nan, ...,  3944.,  3503.,  3862.]]],\n      dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6.001e+05 6.002e+05 ... 7.097e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 1830x: 1830nan nan nan nan nan ... 3.996e+03 3.944e+03 3.503e+03 3.862e+03array([[[   nan,    nan,    nan, ...,  8888.,  9075.,  8139.],\n        [   nan,    nan,    nan, ..., 10444., 10358.,  8669.],\n        [   nan,    nan,    nan, ..., 10346., 10659.,  9168.],\n        ...,\n        [   nan,    nan,    nan, ...,  4295.,  4289.,  4320.],\n        [   nan,    nan,    nan, ...,  4291.,  4269.,  4179.],\n        [   nan,    nan,    nan, ...,  3944.,  3503.,  3862.]]],\n      dtype=float32)Coordinates: (4)band(band)int641array([1])x(x)float646e+05 6.001e+05 ... 7.098e+05array([600030., 600090., 600150., ..., 709650., 709710., 709770.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900010., 5899950., 5899890., ..., 5790390., 5790330., 5790270.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 60.0 0.0 5900040.0 0.0 -60.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600030.0, 600090.0, 600150.0, 600210.0, 600270.0, 600330.0,\n              600390.0, 600450.0, 600510.0, 600570.0,\n              ...\n              709230.0, 709290.0, 709350.0, 709410.0, 709470.0, 709530.0,\n              709590.0, 709650.0, 709710.0, 709770.0],\n             dtype='float64', name='x', length=1830))yPandasIndexPandasIndex(Float64Index([5900010.0, 5899950.0, 5899890.0, 5899830.0, 5899770.0, 5899710.0,\n              5899650.0, 5899590.0, 5899530.0, 5899470.0,\n              ...\n              5790810.0, 5790750.0, 5790690.0, 5790630.0, 5790570.0, 5790510.0,\n              5790450.0, 5790390.0, 5790330.0, 5790270.0],\n             dtype='float64', name='y', length=1830))Attributes: (4)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGEscale_factor :1.0add_offset :0.0\n\n\nEither way will change the nodata value from 0 to nan. Now if we compute the statistics again, the missing data will not be considered:\n\nprint(raster_ams_b9.min())\nprint(raster_ams_b9.max())\nprint(raster_ams_b9.mean())\nprint(raster_ams_b9.std())\n\n<xarray.DataArray ()>\narray(8., dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(15497., dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(2477.405, dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n<xarray.DataArray ()>\narray(2061.9539, dtype=float32)\nCoordinates:\n    spatial_ref  int64 0\n\n\nAnd if we plot the image, the nodata pixels are not shown because they are not 0 anymore:\n\nraster_ams_b9.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa254eac670>\n\n\n\n\n\n\n\n\n\n\n\nNotice that there is a side effect of using NaN instead of 0 to represent missing data: the data type of the DataArray was changed from integers to float\n\n\n\nThis need to be taken into consideration when the data type matters in our application.\n\n\n\n\nRaster Bands\nSo far we looked into a single band raster, i.e. the B09 band of a Sentinel-2 scene. However, to get an overview of the scene, we may also want to visualize the true-colour thumbnail of the region. This is provided as a multi-band raster – a raster dataset that contains more than one band.\n\nThe overview asset in the Sentinel-2 scene is a multiband asset. Similar to B09, we can load it by:\n\nraster_ams_overview = rioxarray.open_rasterio(items[0].assets['overview'].href)\nraster_ams_overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 3, y: 343, x: 343)>\n[352947 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3\n  * x            (x) float64 6.002e+05 6.005e+05 ... 7.093e+05 7.096e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.899e+06 ... 5.791e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 3y: 343x: 343...[352947 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3array([1, 2, 3])x(x)float646.002e+05 6.005e+05 ... 7.096e+05array([600160., 600480., 600800., ..., 708960., 709280., 709600.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5899880., 5899560., 5899240., ..., 5791080., 5790760., 5790440.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 320.0 0.0 5900040.0 0.0 -320.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1, 2, 3], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600160.0, 600480.0, 600800.0, 601120.0, 601440.0, 601760.0,\n              602080.0, 602400.0, 602720.0, 603040.0,\n              ...\n              706720.0, 707040.0, 707360.0, 707680.0, 708000.0, 708320.0,\n              708640.0, 708960.0, 709280.0, 709600.0],\n             dtype='float64', name='x', length=343))yPandasIndexPandasIndex(Float64Index([5899880.0, 5899560.0, 5899240.0, 5898920.0, 5898600.0, 5898280.0,\n              5897960.0, 5897640.0, 5897320.0, 5897000.0,\n              ...\n              5793320.0, 5793000.0, 5792680.0, 5792360.0, 5792040.0, 5791720.0,\n              5791400.0, 5791080.0, 5790760.0, 5790440.0],\n             dtype='float64', name='y', length=343))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\nThe band number comes first when GeoTiffs are read with the .open_rasterio() function. As we can see in the xarray.DataArray object, the shape is now (band: 3, y: 343, x: 343), with three bands in the band dimension. It’s always a good idea to examine the shape of the raster array you are working with and make sure it’s what you expect. Many functions, especially the ones that plot images, expect a raster array to have a particular shape. We can also check the shape using the .shape attribute:\n\nraster_ams_overview.shape\n\n(3, 343, 343)\n\n\nWe can visualize the multi-band data with the DataArray.plot.imshow() function:\n\nraster_ams_overview.plot.imshow()\n\n<matplotlib.image.AxesImage at 0x7fa254c55600>\n\n\n\n\n\nNote that the DataArray.plot.imshow() function makes assumptions about the shape of the input DataArray, that since it has three channels, the correct colormap for these channels is RGB. It does not work directly on image arrays with more than 3 channels. We can replace one of the RGB channels with another band, to make a false-colour image.\nAs seen in the figure above, the true-colour image is stretched. Let’s visualize it with the right aspect ratio. Since we know the height/width ratio is 1:1 (check the rio.height and rio.width attributes), we can set the aspect ratio to be 1. For example, we can choose the size to be 5 inches, and set aspect=1. Note that according to the documentation of DataArray.plot.imshow(), when specifying the aspect argument, size also needs to be provided.\n\nraster_ams_overview.plot.imshow(size=5, aspect=1)\n\n<matplotlib.image.AxesImage at 0x7fa2545752a0>\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- rioxarray and xarray are for working with multidimensional arrays like pandas is for working with tabular data\n- rioxarray stores CRS information as a CRS object that can be converted to an EPSG code or PROJ4 string\n- missing raster data are filled with nodata values, which should be handled with care for statistics and visualization"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data-in-python",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#vector-data-in-python",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "7. Vector data in Python",
    "text": "7. Vector data in Python\nAs covered in section 2, vector data represents specific features on the Earth’s surface using points, lines and polygons. These geographic elements can then have one or more attributes assigned to them, such as ‘name’ and ‘population’ for a city, or crop type for a field. Vector data can be much smaller in (file) size than raster data, while being very rich in terms of the information captured.\nIn this section, we will be moving from working with raster data to working with vector data. We will use Python to open and plot point, line and polygon vector data. In particular, we will make use of the geopandas package to open, manipulate and write vector datasets. geopandas extends the popular pandas library for data analysis to geospatial applications. The main pandas objects (the Series and the DataFrame) are expanded by including geometric types, represented in Python using the **shapely library, and by providing dedicated methods for spatial operations (union, intersection, etc.).\n\nIntroducing the vector data\nThe data we will work with comes from the Dutch government’s open geodata sets, obtained from the PDOK platform. It provides open data for various applications, e.g. real estate, infrastructure, agriculture, etc. In this episode we will use three data sets:\n\ncrop fields (polygons)\nwater ways (lines)\nground water monitoring wells (points)\n\nIn later sections, we will learn how to work with raster and vector data together and combine them into a single plot.\n\nimport geopandas as gpd\n\nWe will use the geopandas module to load the crop field vector data we downloaded at: data/brpgewaspercelen_definitief_2020_small.gpkg. This file contains data for the entirety of the European portion of the Netherlands, resulting in a very large number of crop field parcels. Directly loading the whole file to memory can be slow. Let’s consider as Area of Interest (AoI) northern Amsterdam, which is a small portion of the Netherlands. We only need to load this part.\nWe define a bounding box, and will only read the data within the extent of the bounding box:\n\n# Define bounding box\nxmin, xmax = (110_000, 140_000)\nymin, ymax = (470_000, 510_000)\nbbox = (xmin, ymin, xmax, ymax)\n\n\n\n\n\n\n\nHow should I define my bounding box?\n\n\n\nFor simplicity, here we assume the Coordinate Reference System (CRS) and extent of the vector file are known (for instance they are provided in the dataset documentation). Some Python tools, e.g. fiona(which is also the backend of geopandas), provides the file inspection functionality without actually the need to read the full data set into memory. An example can be found in the documentation of fiona.\n\n\n\n# Partially load data within the bounding box\ncropfield = gpd.read_file(\"Data/brpgewaspercelen_definitief_2020_small.gpkg\", bbox=bbox)\n\n\n\nVector Metadata & Attributes\nWhen we import the vector dataset to Python (as our cropfield object) it comes in as a DataFrame, specifically a GeoDataFrame. The read_file() function also automatically stores geospatial information about the data. We are particularly interested in describing the format, CRS, extent, and other components of the vector data, and the attributes which describe properties associated with each individual vector object.\n\n\nSpatial Metadata\nKey metadata includes:\n\nObject Type: the class of the imported object.\nCoordinate Reference System (CRS): the projection of the data.\nExtent: the spatial extent (i.e. geographic area that the data covers). Note that the spatial extent for a vector dataset represents the combined extent for all spatial objects in the dataset.\n\nEach GeoDataFrame has a “geometry” column that contains geometries. In the case of our cropfield object, this geometry is represented by a shapely.geometry.Polygon object. geopandas uses the shapely library to represent polygons, lines, and points, so the types are inherited from shapely.\nWe can view the metadata using the .crs, .bounds and .type attributes. First, let’s view the geometry type for our crop field dataset:\n\n# view the geometry type using the pandas method .type on the GeoDataFrame object, cropfield\ncropfield.type\n\n0        Polygon\n1        Polygon\n2        Polygon\n3        Polygon\n4        Polygon\n          ...   \n22026    Polygon\n22027    Polygon\n22028    Polygon\n22029    Polygon\n22030    Polygon\nLength: 22031, dtype: object\n\n\n\n# view the CRS metadata\ncropfield.crs\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\n- bounds: (3.2, 50.75, 7.22, 53.7)\nCoordinate Operation:\n- name: RD New\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\nOur data is in the CRS RD New.\n\n\n\n\n\n\nImportant\n\n\n\nThe CRS is critical to interpreting the object’s extent values as it specifies units. To find the extent of our dataset in the projected coordinates, we can use the .total_bounds attribute.\n\n\n\n# find the extent of our dataset in the projected coordinates\ncropfield.total_bounds\n\narray([109222.03325 , 469461.512625, 140295.122125, 510939.997875])\n\n\nThis array contains, in order, the values for the overall dataset:\n\nminx, miny, maxx, maxy\n\nThe spatial extent of a GeoDataFrame represents the geographic “edge” or location that is the furthest north, south, east, and west. Thus, it is represents the overall geographic coverage of the spatial object. We can convert these coordinates to a bounding box or acquire the index of the dataframe to access the geometry. Either of these polygons can be used to clip rasters (more on that later).\n\n\nSelecting spatial features\nSometimes, the loaded data can still be too large. We can cut it is to a even smaller extent using the .cx indexer (note the use of square brackets instead of round brackets, which are used instead with functions and methods):\n\n# Define a Boundingbox in RD\nxmin, xmax = (120_000, 135_000)\nymin, ymax = (485_000, 500_000)\ncropfield_crop = cropfield.cx[xmin:xmax, ymin:ymax]\n\nThis will cut out a smaller area, defined by a box in units of the projection, discarding the rest of the data. The resultant GeoDataframe, which includes all the features intersecting the box, is found in the cropfield_crop object. Note that the edge elements are not ‘cropped’ themselves. We can check the total bounds of this new data as before:\n\ncropfield_crop.total_bounds\n\narray([119594.384   , 484949.292625, 135375.77025 , 500782.531   ])\n\n\nWe can then save this cropped dataset for use in future, using the to_file() method of our GeoDataFrame object:\n\ncropfield_crop.to_file('cropped_field.shp')\n\nThis will write it to disk (in this case, in ‘shapefile’ format), containing only the data from our cropped area. It can be read in again at a later time using the read_file() method we have been using above. Note that this actually writes multiple files to disk (cropped_field.cpg, cropped_field.dbf, cropped_field.prj, cropped_field.shp, cropped_field.shx). All these files should ideally be present in order to re-read the dataset later, although only the .shp, .shx, and .dbf files are mandatory. See section 2 for more information.\n\n\nPlotting a vector dataset\nWe can now plot this data. Any GeoDataFrame can be plotted in CRS units to view the shape of the object with .plot().\n\ncropfield_crop.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nWe can customize our boundary plot by setting the figsize, edgecolor, and color. Making some polygons transparent will come in handy when we need to add multiple spatial datasets to a single plot.\n\ncropfield_crop.plot(figsize=(5,5), edgecolor=\"purple\", facecolor=\"None\")\n\n<AxesSubplot: >\n\n\n\n\n\nUnder the hood, geopandas is using matplotlib to generate this plot. In the next section we will see how we can add DataArrays and other vector datasets to this plot to start building an informative map of our area of interest.\n\n\nSpatial Data Attributes\nWe introduced the idea of spatial data attributes in an earlier section. Now we will explore how to use spatial data attributes stored in our data to plot different features.\n\nWaterways\n\n# load data \nwaterways_nl = gpd.read_file(\"Data/status_vaarweg.zip\")\n\n\n# Type of features\nwaterways_nl.type\n\n0     LineString\n1     LineString\n2     LineString\n3     LineString\n4     LineString\n         ...    \n86    LineString\n87    LineString\n88    LineString\n89    LineString\n90    LineString\nLength: 91, dtype: object\n\n\n\n# Co_ord reference system\nwaterways_nl.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Bounds\nwaterways_nl.total_bounds\n\narray([50.7916,  3.1626, 53.6161,  7.0121])\n\n\n\n# How many spatial features\nprint(len(waterways_nl))\n\n91\n\n\nOur waterways dataset includes 91 lines.\nNow let’s take a deeper look at the Dutch waterway lines: waterways_nl. Let’s visualize it with the plot function:\n\nwaterways_nl.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\n\n\n\nAxis ordering - we can tell that the latitude and longitude of the file are flipped.\n\n\n\nAccording to the standards, the axis ordering for a CRS should follow the definition provided by the competent authority. For the commonly used EPSG:4326 geographic coordinate system, the EPSG defines the ordering as first latitude then longitude. However, in the GIS world, it is custom to work with coordinate tuples where the first component is aligned with the east/west direction and the second component is aligned with the north/south direction. Multiple software packages thus implement this convention also when dealing with EPSG:4326. As a result, one can encounter vector files that implement either convention - keep this in mind and always check your datasets!\n\n\n\n\nGround water monitoring wells\n\n# load data \nwells_nl = gpd.read_file(\"Data/brogmwvolledigeset.zip\")\n\n\n# Type of features\nwells_nl.type\n\n0        Point\n1        Point\n2        Point\n3        Point\n4        Point\n         ...  \n54654    Point\n54655    Point\n54656    Point\n54657    Point\n54658    Point\nLength: 54659, dtype: object\n\n\n\n# Co_ord reference system\nwells_nl.crs\n\n<Geographic 2D CRS: EPSG:4258>\nName: ETRS89\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: Europe - onshore and offshore: Albania; Andorra; Austria; Belgium; Bosnia and Herzegovina; Bulgaria; Croatia; Cyprus; Czechia; Denmark; Estonia; Faroe Islands; Finland; France; Germany; Gibraltar; Greece; Hungary; Ireland; Italy; Kosovo; Latvia; Liechtenstein; Lithuania; Luxembourg; Malta; Moldova; Monaco; Montenegro; Netherlands; North Macedonia; Norway including Svalbard and Jan Mayen; Poland; Portugal; Romania; San Marino; Serbia; Slovakia; Slovenia; Spain; Sweden; Switzerland; United Kingdom (UK) including Channel Islands and Isle of Man; Vatican City State.\n- bounds: (-16.1, 32.88, 40.18, 84.73)\nDatum: European Terrestrial Reference System 1989 ensemble\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\n\n# Bounds\nwells_nl.total_bounds\n\narray([ 3.37982412, 50.75590464,  7.21010667, 53.49457587])\n\n\n\n# How many spatial features\nprint(len(wells_nl))\n\n54659\n\n\nOur wells dataset includes 54659 points.\n\nwells_nl.plot()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\n\nModify the geometry of a GeoDataFrame\nSometimes we need to modify the geometry of a GeoDataFrame. For example, as we saw previously, the latitude and longitude are flipped in the vector data waterways_nl. This error needs to be fixed before performing further analysis. Let’s first take a look at what makes up the geometry column of waterways_nl:\n\nwaterways_nl['geometry']\n\n0     LINESTRING (52.41810 4.84060, 52.42070 4.84090...\n1     LINESTRING (52.11910 4.67450, 52.11930 4.67340...\n2     LINESTRING (52.10090 4.25730, 52.10390 4.25530...\n3     LINESTRING (53.47250 6.84550, 53.47740 6.83840...\n4     LINESTRING (52.32270 5.14300, 52.32100 5.14640...\n                            ...                        \n86    LINESTRING (51.49270 5.39100, 51.48050 5.39160...\n87    LINESTRING (52.15900 5.38510, 52.16010 5.38340...\n88    LINESTRING (51.97340 4.12420, 51.97110 4.12220...\n89    LINESTRING (52.11910 4.67450, 52.11850 4.67430...\n90    LINESTRING (51.88940 4.61900, 51.89040 4.61350...\nName: geometry, Length: 91, dtype: geometry\n\n\nEach row is a LINESTRING object. We can further zoom into one of the rows, for example, the 13th row:\n\nprint(waterways_nl['geometry'][12])\nprint(type(waterways_nl['geometry'][12]))\n\nLINESTRING (51.714200001 4.620299999, 51.7203 4.62279999999998, 51.7212 4.62319999900001, 51.73 4.62759999899998, 51.736000001 4.62959999999998, 51.7434 4.63139999999999, 51.7489 4.63139999999999, 51.753600001 4.63019999900001, 51.759799998 4.62700000000001, 51.764699999 4.62630000000001, 51.769200001 4.62680000099999, 51.771699999 4.62720000000002, 51.773699999 4.62740000000002, 51.775800001 4.62740000000002, 51.780099999 4.62740000000002, 51.782699998 4.62709999800001, 51.785700001 4.62640000099998, 51.791900001 4.62359999900002, 51.7962 4.62229999900001, 51.800000001 4.62130000100001)\n<class 'shapely.geometry.linestring.LineString'>\n\n\nAs we can see in the output, the LINESTRING object contains a list of coordinates of the vertices. In our situation, we would like to find a way to flip the x and y of every coordinates set. A good way to look for the solution is to use the documentation of the shapely package, since we are seeking to modify the LINESTRING object. Here we are going to use the shapely.ops.transform function, which applies a self-defined function to all coordinates of a geometry.\n\nimport shapely\n\n# Define a function flipping the x and y coordinate values\ndef flip(geometry):\n    return shapely.ops.transform(lambda x, y: (y, x), geometry)\n\n# Apply this function to all coordinates and all lines\ngeom_corrected = waterways_nl['geometry'].apply(flip)\n\nThen we can update the geometry column with the corrected geometry geom_corrected, and visualize it to check:\n\n# Update geometry\nwaterways_nl['geometry'] = geom_corrected\n\n# Visualization\nwaterways_nl.plot()\n\n<AxesSubplot: >\n\n\n\n\n\nNow the waterways look good! We can save the vector data for later usage:\n\n# Save plot as .shp file\nwaterways_nl.to_file('waterways_nl_corrected.shp')\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- vector dataset metadata include geometry type, CRS, and extent\n- load spatial objects into Python with geopandas.read_file() function\n- spatial objects can be plotted directly with GeoDataFrame’s .plot() method"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#crop-raster-data-with-rioxarray-and-geopandas",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#crop-raster-data-with-rioxarray-and-geopandas",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "8. Crop raster data with rioxarray and geopandas",
    "text": "8. Crop raster data with rioxarray and geopandas\nIt is quite common that the raster data we have in hand is too large to process, or not all the pixels are relevant to our area of interest (AoI). In both situations, we should consider cropping our raster data before performing data analysis. In this section, we will introduce how to crop raster data into the desired area. We will use one Sentinel-2 image over Amsterdam as the example raster data, and introduce how to crop our data to different types of AoIs.\n\nIntroduce the Data\nWe’ll continue from the results of the satellite image search that we carried out in a previous section. We will load data starting from the search.json file.\nThe rasta data can be downloaded using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started. We also use the vector data that was introduced in the previous section.\n\n\nCrop raster data with a bounding box\nLet’s load a true colour image using pystac and rioxarray and check the shape of the raster:\n\nimport pystac\nimport rioxarray\n\n# Load image and inspect the shape\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\n# Select a true colour image\ntrue_color_image = rioxarray.open_rasterio(items[1].assets[\"visual\"].href) \nprint(true_color_image.shape)\n\n(3, 10980, 10980)\n\n\nThe large size of the raster data makes it time and memory consuming to visualise in its entirety. Instead, we can plot the “overview” asset, to investigate the coverage of the image:\n\n# Get the overview asset\noverview_image = rioxarray.open_rasterio(items[1].assets[\"overview\"].href)\nprint(overview_image.shape)\n\n# Visualize it\noverview_image.plot.imshow(figsize=(8,8))\n\n(3, 343, 343)\n\n\n<matplotlib.image.AxesImage at 0x7fa21341cc70>\n\n\n\n\n\nAs we can see, the overview image is much smaller compared to the original true colour image. Therefore the visualization is much faster. If we are interested in the crop fields, then we would like to know where these are located in the image. To compare its coverage with the raster data, we first check the coordinate systems of both raster and vector data.\nFor raster data, we use pyproj.CRS:\n\nfrom pyproj import CRS\n\n# Check the coordinate system\nCRS(true_color_image.rio.crs)\n\n<Derived Projected CRS: EPSG:32631>\nName: WGS 84 / UTM zone 31N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 31N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nTo open and check the coordinate system of vector data, we use geopandas:\n\nimport geopandas as gpd\n\n# Load the polygons of the crop fields\ncf_boundary_crop = gpd.read_file(\"cropped_field.shp\")\n\n# Check the coordinate system\ncf_boundary_crop.crs\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\n- bounds: (3.2, 50.75, 7.22, 53.7)\nCoordinate Operation:\n- name: RD New\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\n\n\n\n\n\n\nAs we can see, the coordinate systems differ.\n\n\n\nTo crop the raster using the shapefile, we first convert the coordinate system of cf_boundary_crop to the coordinate system of true_color_image, using .to_crs and then check the coverage:\n\n\n\nfrom matplotlib import pyplot as plt\n\n# Convert the coordinate system\ncf_boundary_crop = cf_boundary_crop.to_crs(true_color_image.rio.crs)\n\n# Plot\nfig, ax = plt.subplots()\nfig.set_size_inches((8,8))\n\n# Plot image\noverview_image.plot.imshow(ax=ax)\n\n# Plot crop fields\ncf_boundary_crop.plot(\n    ax=ax,\n    edgecolor=\"red\",\n)\n\n<AxesSubplot: title={'center': 'spatial_ref = 0'}, xlabel='x', ylabel='y'>\n\n\n\n\n\nSeeing from the location of the polygons, the crop fields (red) only takes a small part of the raster. Therefore, before actual processing, we can first crop the raster to our area of interest. The clip_box function allows us to crop a raster by the min/max of the x and y coordinates. Note that we are cropping the original image true_color_image now, and not the overview image overview_image.\n\n# Crop the raster with the bounding box\nraster_clip_box = true_color_image.rio.clip_box(*cf_boundary_crop.total_bounds)\nprint(raster_clip_box.shape)\n\n(3, 1574, 1584)\n\n\nWe successfully cropped the raster to a much smaller piece. We can visualize it now:\n\n# view the image\nraster_clip_box.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa213d2ab90>\n\n\n\n\n\nThis cropped image can be saved for later use:\n\n# save cropped image as .tif file\nraster_clip_box.rio.to_raster(\"raster_clip.tif\")\n\n\n\nCrop raster data with polygons\nWe have a cropped image around the fields. To further analyze the fields, we might want to crop the image to the exact field boundaries. This can be done with the clip function:\n\n# crop image to exact field boundaries\nraster_clip_fields = raster_clip_box.rio.clip(cf_boundary_crop['geometry'])\n\nAnd we can visualize the results:\n\n# view the image\nraster_clip_fields.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa2132929b0>\n\n\n\n\n\nWe can save this image for later use:\n\n# save image as .tif file\nraster_clip_fields.rio.to_raster(\"crop_fields.tif\")\n\n\n\nCrop raster data with a geometry buffer\nIt is not always the case that the AoI comes in polygon format. Sometimes we would like to perform analysis around a (set of) point(s), or polyline(s). For example, in our AoI, there are also some groundwater monitoring wells available as point vector data. We may also want to perform analysis around these wells. The location of the wells is stored in data/groundwater_monitoring_well.\nWe can first load the wells vector data, and select wells within the coverage of the image:\n\n# Load wells\nwells = gpd.read_file(\"Data/brogmwvolledigeset.zip\")\nwells = wells.to_crs(raster_clip_box.rio.crs)\n\n# Crop the wells to the image extent\nxmin, ymin, xmax, ymax = raster_clip_box.rio.bounds()\nwells = wells.cx[xmin:xmax, ymin:ymax]\n\nThen we can check the location of the wells:\n\n# Plot the wells over raster\nfig, ax = plt.subplots()\nfig.set_size_inches((8,8))\nraster_clip_box.plot.imshow(ax=ax)\nwells.plot(ax=ax, color='red', markersize=2)\n\n<AxesSubplot: title={'center': 'spatial_ref = 0'}, xlabel='x coordinate of projection\\n[metre]', ylabel='y coordinate of projection\\n[metre]'>\n\n\n\n\n\nTo select pixels around the geometries, we need to first define a region including the geometries. This region is called a “buffer” and it is defined in the units of the projection. The size of the buffer depends on the analysis in our research. A buffer is also a polygon, which can be used to crop the raster data. geopandas’ objects have a buffer method to generate buffer polygons.\n\n# Create 200m buffer around the wells\nwells_buffer = wells.buffer(200)\n\n\n# Visualize buffer on raster\nfig, ax = plt.subplots()\nfig.set_size_inches((8,8))\nraster_clip_box.plot.imshow(ax=ax)\nwells_buffer.plot(ax=ax, color='red')\n\n<AxesSubplot: title={'center': 'spatial_ref = 0'}, xlabel='x coordinate of projection\\n[metre]', ylabel='y coordinate of projection\\n[metre]'>\n\n\n\n\n\nThe red dots have grown larger indicating the conversion from points to buffer polygons.\n\n\nSelect the raster data around the wells\nNow we have the buffer polygons around the groudwater monitoring wells, i.e. wells_buffer. Let’s now crop the image raster_clip_box to the buffer polygons, and visualize the results of cropping:\n\n# Crop the image raster_clip_box to the buffer polygons\nraster_clip_wells = raster_clip_box.rio.clip(wells_buffer)\n\n# Visualize cropped buffer\nraster_clip_wells.plot.imshow()\n\nNameError: name 'raster_clip_box' is not defined\n\n\n\n\nSelect the raster data around the waterways\nLet’s now attempt to select all the raster data within 100m around the waterways, and visualize the results:\n\n# Load waterways\nwaterways_nl = gpd.read_file(\"Data/waterways_nl_corrected.shp\")\nwaterways_nl = waterways_nl.to_crs(raster_clip_box.rio.crs)\n\n# Crop the waterways to the image extent\nwaterways_nl = waterways_nl.cx[xmin:xmax, ymin:ymax]\n\n# Create 100m buffer around the waterways\nwaterways_nl_buffer = waterways_nl.buffer(100)\n\n# Crop\nraster_clip_waterways = raster_clip_box.rio.clip(waterways_nl_buffer)\n\n# Visualize\nraster_clip_waterways.plot.imshow(figsize=(8,8))\n\nNameError: name 'raster_clip_box' is not defined\n\n\n\n\nCrop raster data using another raster dataset\nSo far we have learned how to crop raster image with vector data. We can also crop a raster with another raster data. Let’s demonstrate how to crop the true_color_image image using the crop_fields.tif image. that was produced in the sub-section “Crop raster data with polygon”.\n\n# Read crop_fields\ncrop_fields = rioxarray.open_rasterio(\"Data/crop_fields.tif\")\n\n# Reproject to RD to make the CRS different from the \"true_color_image\"\ncrop_fields = crop_fields.rio.reproject(\"EPSG:28992\")\nCRS(crop_fields.rio.crs)\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: unnamed\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\nAnd let’s check again the CRS of true_color_image:\n\n# Get CRS of true_color_image\nCRS(true_color_image.rio.crs)\n\n<Derived Projected CRS: EPSG:32631>\nName: WGS 84 / UTM zone 31N\nAxis Info [cartesian]:\n- [east]: Easting (metre)\n- [north]: Northing (metre)\nArea of Use:\n- undefined\nCoordinate Operation:\n- name: UTM zone 31N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nNow the two images are in different coordinate systems. We can use rioxarray.reproject_match() function to crop true_color_image image. It will perform both the reprojection and the cropping operation. This might take a few minutes, because the true_color_image image is large:\n\n# Crop and reproject\ncropped_raster = true_color_image.rio.reproject_match(crop_fields)\n\n# Visualize\ncropped_raster.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa20b696b90>\n\n\n\n\n\n\n\n\n\n\n\nIn one line reproject_match does a lot of helpful things:\n\n\n\n\nit reprojects\nit matches the extent using nodata values or by clipping the data\nit sets nodata values. This means we can run calculations on those two images\n\n\n\n\n\n\n\n\n\nIf you want more control over how rasters are resampled, clipped, and/or reprojected\n\n\n\nUse the reproject() method and other rioxarray methods individually\n\n\nThis time let’s do it the other way around by cropping the crop_fields image using the true_color_image image:\n\n# Crop and reproject\ncropped_raster = crop_fields.rio.reproject_match(true_color_image)\n\n# Visualize\ncropped_raster.plot.imshow(figsize=(8,8))\n\n<matplotlib.image.AxesImage at 0x7fa20b647c10>\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- use clip_box in DataArray.rio to crop a raster with a bounding box\n- use clip in DataArray.rio to crop a raster with a given polygon\n- use buffer in geopandas to make a buffer polygon of a (multi)point or a polyline. This polygon can be used to crop data\n- use reproject_match function in DataArray.rio to reproject and crop a raster data using another raster data"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-calculations-in-python",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#raster-calculations-in-python",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "9. Raster Calculations in Python",
    "text": "9. Raster Calculations in Python\nWe often want to combine values of and perform calculations on rasters to create a new output raster. This episode covers how to perform basic math operations using raster datasets. It also illustrates how to match rasters with different resolutions so that they can be used in the same calculation. As an example, we will calculate a vegetation index over one of the satellite scenes.\n\nNormalized Difference Vegetation Index (NDVI)\nSuppose we are interested in monitoring vegetation fluctuations using satellite remote sensors. Scientists have defined a vegetation index to quantify the amount of green leaf vegetation using the light reflected in different wavelengths. This index, named Normalized Difference Vegetation Index (NDVI), exploits the fact that healthy green leaves strongly absorb red visible light while they mostly reflect light in the near infrared (NIR). The NDVI is computed as:\nNDVI = \\(\\frac{NIR - red}{NIR + red}\\)\nwhere NIR and red label the reflectance values of the corresponding wavelengths. NDVI values range from -1 to +1.\nValues close to one indicate high density of green leaves. Poorly vegetated areas typically have NDVI values close to zero. Negative NDVI values often indicate cloud and water bodies.\n\nSource: Wu C-D, McNeely E, Cedeño-Laurent JG, Pan W-C, Adamkiewicz G, Dominici F, et al. (2014) Linking Student Performance in Massachusetts Elementary Schools with the “Greenness” of School Surroundings Using Remote Sensing. PLoS ONE 9(10): e108548. https://doi.org/10.1371/journal.pone.0108548\nCheck out more on NDVI in the NASA Earth Observatory portal: Measuring Vegetation.\n\n\nLoad and crop the data\nWe’ll continue from the results of the satellite image search that we have carried out in Section 5 previously. We will load data starting from the search.json file. You can download the raster data using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started.\nLet’s load the results of our initial imagery search using pystac:\n\nimport pystac\n# load in our imagery search\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\nWe then select the second item, and extract the URIs of the red and NIR bands (“B04” and “B8A”, respectively):\n\n# extract URIs of second items of red and NIR bands\nred_uri = items[1].assets[\"B04\"].href\nnir_uri = items[1].assets[\"B8A\"].href\n\nLet’s load the rasters with open_rasterio using the argument masked=True.\n\nimport rioxarray\n# load in the rasters with argument masked=True which \nred = rioxarray.open_rasterio(red_uri, masked=True)\nnir = rioxarray.open_rasterio(nir_uri, masked=True)\n\nLet’s also restrict our analysis to the same crop field area defined in the previous section by clipping the rasters using a bounding box:\n\n# clip the rasters using a bounding box\nbbox = (629_000, 5_804_000, 639_000, 5_814_000)\nred_clip = red.rio.clip_box(*bbox)\nnir_clip = nir.rio.clip_box(*bbox)\n\nWe can now plot the two rasters. Using robust=True color values are stretched between the 2nd and 98th percentiles of the data, which results in clearer distinctions between high and low reflectances:\n\n# plot the red visible light wavelength raster\nred_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20b6031c0>\n\n\n\n\n\n\n\n\n\n\n\nThe crop fields (rectangular shapes in the central part of the figure) appear as dark spots in the red-visible wavelength, suggesting the presence of leafy crop at the time of observation (end of March).\n\n\n\nThe same fields would instead appear as bright spots in the off season.\n\n\n\n# plot the near infra red wavelength raster\nnir_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20ca6ff70>\n\n\n\n\n\n\n\n\n\n\n\nThe crop fields (rectangular shapes in the central part of the figure) appear as bright spots in the NIR wavelength, suggesting the presence of leafy crop at the time of observation (end of March).\n\n\n\nThe same fields would instead appear as dark spots in the off season.\n\n\nLet’s check this by grabbing scenes for an ‘off-season’ period\n\napi_url = \"https://earth-search.aws.element84.com/v0\"\n\nfrom pystac_client import Client\nclient = Client.open(api_url)\n\n# Sentinel-2, Level 2A, COGs\ncollection = \"sentinel-s2-l2a-cogs\" \n\n# AMS coordinates\nfrom shapely.geometry import Point\npoint = Point(4.89, 52.37)  \n\nbound_box = point.buffer(0.01).bounds\n\n# Search for scenes which intersect bounding box +- 0.01 deg in lat/lon from (4.89, 52.37) as prev defined\n# between 09/13/20 and 09/30/20 where cloud cover < 10%\nsearch = client.search(\n    collections=[collection],\n    bbox=bound_box,\n    datetime=\"2020-09-13/2020-09-30\",\n    query=[\"eo:cloud_cover<10\"]\n)\n\nprint(search.matched())\n\n# Grab search items and save as a JSON file\nitems_off_season = search.get_all_items()\nitems_off_season.save_object(\"search_off_season.json\")\n\n3\n\n\n\nimport pystac\nitems_off_season = pystac.ItemCollection.from_file(\"search_off_season.json\")\n\n# extract URIs of second items of red and NIR bands\nred_uri_off_season = items_off_season[1].assets[\"B04\"].href\nnir_uri_off_season = items_off_season[1].assets[\"B8A\"].href\n\nimport rioxarray\n\nred_off_season = rioxarray.open_rasterio(red_uri_off_season, masked=True)\nnir_off_season = rioxarray.open_rasterio(nir_uri_off_season, masked=True)\n\n# clip the rasters using a bounding box\nbbox = (629_000, 5_804_000, 639_000, 5_814_000)\nred_off_season_clip = red_off_season.rio.clip_box(*bbox)\nnir_off_season_clip = nir_off_season.rio.clip_box(*bbox)\n\n\n# plot the OFF SEASON red visible light wavelength raster\nred_off_season_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20c4d8130>\n\n\n\n\n\n\n# plot the OFF SEASON near infra red wavelength raster\nnir_off_season_clip.plot(robust=True)\n\n<matplotlib.collections.QuadMesh at 0x7fa20c4787c0>\n\n\n\n\n\n\n\nRaster Math\nWe can perform raster calculations by subtracting (or adding, multiplying, etc.) two rasters. In the geospatial world, we call this “raster math”, and typically it refers to operations on rasters that have the same width and height (including nodata pixels). We can check the shapes of the two rasters in the following way:\n\nprint(red_clip.shape, nir_clip.shape)\n\n(1, 1000, 1000) (1, 500, 500)\n\n\nBoth rasters include a single band, but their width and height do not match. We can now use the reproject_match function, which both reprojects and clips a raster to the CRS and extent of another raster.\n\nred_clip_matched = red_clip.rio.reproject_match(nir_clip)\nprint(red_clip_matched.shape)\n\n(1, 500, 500)\n\n\nLet’s now compute the NDVI as a new raster using the formula\nNDVI = $\\frac{NIR - red}{NIR + red}$\nWe’ll use rioxarray objects so that we can easily plot our result and keep track of the metadata.\n\nndvi = (nir_clip - red_clip_matched)/ (nir_clip + red_clip_matched)\nprint(ndvi)\n\n<xarray.DataArray (band: 1, y: 500, x: 500)>\narray([[[ 0.7379576 ,  0.77153456,  0.54531944, ...,  0.39254385,\n          0.49227372,  0.4465174 ],\n        [ 0.7024894 ,  0.7074668 ,  0.3903298 , ...,  0.423283  ,\n          0.4706971 ,  0.45964912],\n        [ 0.6557818 ,  0.5610572 ,  0.46742022, ...,  0.4510345 ,\n          0.43815723,  0.6005133 ],\n        ...,\n        [ 0.02391171,  0.21843003,  0.02479339, ..., -0.50923485,\n         -0.53367877, -0.4955414 ],\n        [ 0.11376493,  0.17681159, -0.1673566 , ..., -0.5221932 ,\n         -0.5271318 , -0.4852753 ],\n        [ 0.45398772, -0.00518135,  0.03346133, ..., -0.5019455 ,\n         -0.4987013 , -0.49081364]]], dtype=float32)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6.29e+05 6.29e+05 6.29e+05 ... 6.39e+05 6.39e+05\n  * y            (y) float64 5.814e+06 5.814e+06 ... 5.804e+06 5.804e+06\n    spatial_ref  int64 0\n\n\nWe can now plot the output NDVI:\n\nndvi.plot()\n\n<matplotlib.collections.QuadMesh at 0x7fa20b9cfdc0>\n\n\n\n\n\n\n\n\n\n\n\nNotice that the range of values for the output NDVI is between -1 and 1. This makes sense for the selected region?\n\n\n\n\n\nYes. Remember that NDVI values close to one indicate high density of green leaves, and negative NDVI values often indicate water bodies. This is consistent with our region.\n\n\n\nMaps are great, but it can also be informative to plot histograms of values to better understand the distribution. We can accomplish this using a built-in xarray method we have already been using: .plot\n\nndvi.plot.hist()\n\n(array([2.1000e+01, 1.2800e+02, 1.6076e+04, 1.3546e+04, 6.4030e+03,\n        1.5811e+04, 2.4608e+04, 4.0979e+04, 8.2569e+04, 4.9858e+04]),\n array([-9.98647749e-01, -7.98825085e-01, -5.99002421e-01, -3.99179786e-01,\n        -1.99357137e-01,  4.65512276e-04,  2.00288162e-01,  4.00110811e-01,\n         5.99933445e-01,  7.99756110e-01,  9.99578774e-01]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n\n\nExplore NDVI Raster Values\nIt’s often a good idea to explore the range of values in a raster dataset just like we might explore a dataset that we collected in the field. The histogram we just made is a good start but there’s more we can do to improve our understanding of the data.\n\n# What is the min and maximum value for the NDVI raster (ndvi) that we just created?\n# Are there missing values?\n\nprint(ndvi.min().values)\nprint(ndvi.max().values)\nprint(ndvi.isnull().sum().values)\n\n-0.99864775\n0.9995788\n1\n\n\n\n# Plot a histogram with 50 bins instead of 8. What do you notice that wasn’t clear before?\nndvi.plot.hist(bins=50)\n\n(array([1.2000e+01, 2.0000e+00, 2.0000e+00, 3.0000e+00, 2.0000e+00,\n        8.0000e+00, 7.0000e+00, 4.0000e+00, 2.9000e+01, 8.0000e+01,\n        5.5800e+02, 2.0930e+03, 3.1920e+03, 4.6700e+03, 5.5630e+03,\n        7.6140e+03, 2.9020e+03, 1.3340e+03, 9.5700e+02, 7.3900e+02,\n        8.1800e+02, 1.0210e+03, 1.2840e+03, 1.4820e+03, 1.7980e+03,\n        2.1740e+03, 2.8470e+03, 3.2340e+03, 3.7030e+03, 3.8530e+03,\n        4.2410e+03, 4.6100e+03, 4.8320e+03, 5.3200e+03, 5.6050e+03,\n        6.2230e+03, 6.9630e+03, 7.9370e+03, 9.0210e+03, 1.0835e+04,\n        1.2352e+04, 1.3883e+04, 1.5219e+04, 1.8287e+04, 2.2828e+04,\n        2.5534e+04, 1.9329e+04, 4.9850e+03, 8.0000e+00, 2.0000e+00]),\n array([-9.98647749e-01, -9.58683193e-01, -9.18718696e-01, -8.78754139e-01,\n        -8.38789642e-01, -7.98825085e-01, -7.58860588e-01, -7.18896031e-01,\n        -6.78931534e-01, -6.38966978e-01, -5.99002421e-01, -5.59037924e-01,\n        -5.19073367e-01, -4.79108840e-01, -4.39144313e-01, -3.99179786e-01,\n        -3.59215260e-01, -3.19250733e-01, -2.79286206e-01, -2.39321664e-01,\n        -1.99357137e-01, -1.59392610e-01, -1.19428076e-01, -7.94635490e-02,\n        -3.94990183e-02,  4.65512276e-04,  4.04300429e-02,  8.03945735e-02,\n         1.20359100e-01,  1.60323635e-01,  2.00288162e-01,  2.40252689e-01,\n         2.80217230e-01,  3.20181757e-01,  3.60146284e-01,  4.00110811e-01,\n         4.40075338e-01,  4.80039865e-01,  5.20004392e-01,  5.59968948e-01,\n         5.99933445e-01,  6.39898002e-01,  6.79862559e-01,  7.19827056e-01,\n         7.59791613e-01,  7.99756110e-01,  8.39720666e-01,  8.79685163e-01,\n         9.19649720e-01,  9.59614217e-01,  9.99578774e-01]),\n <BarContainer object of 50 artists>)\n\n\n\n\n\nIncreasing the number of bins gives us a much clearer view of the distribution. Also, there seem to be very few NDVI values larger than ~ 0.9.\n\n# Plot the ndvi raster using breaks that make sense for the data\nclass_bins = (-1, 0., 0.2, 0.7, 1)\nndvi.plot(levels=class_bins)\n\n<matplotlib.collections.QuadMesh at 0x7fa20b8f6e90>\n\n\n\n\n\nWe can discretize the colour bar by specifying the intervals via the level argument to plot(). Suppose we want to bin our data in the following intervals:\n\n-1 \\(\\le\\) NDVI \\(\\lt\\) 0 for water\n0 \\(\\le\\) NDVI \\(\\lt\\) 0.2 for no vegetation\n0.2 \\(\\le\\) NDVI \\(\\lt\\) 0.7 for sparse vegetation\n0.7 \\(\\le\\) NDVI \\(\\lt\\) 1 for dense vegetation\n\nMissing values can be interpolated from the values of neighbouring grid cells using the .interpolate_na method. We then save ndvi as a GeoTiff file:\n\nndvi_nonan = ndvi.interpolate_na(dim=\"x\")\nndvi_nonan.rio.to_raster(\"NDVI.tif\")\n\n\n\nClassifying Continuous Rasters in Python\nNow that we have a sense of the distribution of our NDVI raster, we can reduce the complexity of our map by classifying it. Classification involves assigning each pixel in the raster to a class based on its value. In Python, we can accomplish this using the numpy.digitize function.\nFirst, we define NDVI classes based on a list of values, as defined in the last exercise: [-1, 0., 0.2, 0.7, 1]. When bins are ordered from low to high, as here, numpy.digitize assigns classes like so:\n\n\n\nNDVI-classes.jpg\n\n\nSource: Image (license)\nNote that, by default, each class includes the left but not the right bound. This is not an issue here, since the computed range of NDVI values is fully contained in the open interval (-1; 1).\n\nimport numpy as np\nimport xarray\n\n# Defines the bins for pixel values\nclass_bins = (-1, 0., 0.2, 0.7, 1)\n\n# The numpy.digitize function returns an unlabeled array, in this case, a\n# classified array without any metadata. That doesn't work--we need the\n# coordinates and other spatial metadata. We can get around this using\n# xarray.apply_ufunc, which can run the function across the data array while\n# preserving metadata.\nndvi_classified = xarray.apply_ufunc(\n    np.digitize,\n    ndvi_nonan,\n    class_bins\n)\n\nLet’s now visualize the classified NDVI, customizing the plot with proper title and legend. We can use EarthPy to assist. We then export the figure in PNG format:\n\nimport earthpy.plot as ep\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.colors import ListedColormap\n\n# Define color map of the map legend\nndvi_colors = [\"blue\", \"gray\", \"green\", \"darkgreen\"]\nndvi_cmap = ListedColormap(ndvi_colors)\n\n# Define class names for the legend\ncategory_names = [\n    \"Water\",\n    \"No Vegetation\",\n    \"Sparse Vegetation\",\n    \"Dense Vegetation\"\n]\n\n# We need to know in what order the legend items should be arranged\ncategory_indices = list(range(len(category_names)))\n\n# Make the plot\nim = ndvi_classified.plot(cmap=ndvi_cmap, add_colorbar=False)\nplt.title(\"Classified NDVI\")\n\n# earthpy helps us by drawing a legend given an existing image plot and legend items, plus indices\nep.draw_legend(im_ax=im, classes=category_indices, titles=category_names)\n\n# Save the figure\nplt.savefig(\"NDVI_classified.png\", bbox_inches=\"tight\", dpi=300)\n\n\n\n\nWe can finally export the classified NDVI raster object to a GeoTiff file. The to_raster() function by default writes the output file to your working directory unless you specify a full file path.\n\nndvi_classified.rio.to_raster(\"NDVI_classified.tif\", dtype=\"int32\")\n\n\n\nCompute the NDVI for the Texel island\nData are often more interesting and powerful when we compare them across various locations. Let’s compare the computed NDVI map with the one of another region in the same Sentinel-2 scene: the Texel island, located in the North Sea. You should have the red- and the NIR-band rasters already loaded (red and nir variables, respectively).\n\n# Crop the two rasters using the following bounding box: (610000, 5870000, 630000, 5900000)\n# Don’t forget to check the shape of the data, and make sure the cropped areas have the same CRSs, heights and widths\n\nbbox_texel = (610_000, 5_870_000, 630_000, 5_900_000)\n\n# We crop the area of interest using clip_box\nred_texel = red.rio.clip_box(*bbox_texel)\nnir_texel = nir.rio.clip_box(*bbox_texel)\n\n\n# Reproject and clip one raster to the extent of the smaller raster using reproject_match. \n# The lines of code below assign a variable to the reprojected raster and calculate the NDVI\nred_texel_matched = red_texel.rio.reproject_match(nir_texel)\nndvi_texel = (nir_texel - red_texel_matched)/ (nir_texel + red_texel_matched)\n\n\n# Plot the NDVI and save the raster data as a GeoTIFF file.\nndvi_texel.plot()\nndvi_texel.rio.to_raster(\"NDVI_Texel.tif\")\n\n\n\n\n\n# Compute the NDVI histogram and compare it with the region that we have previously investigated. \nndvi_texel.plot.hist(bins=50)\n\n(array([1.00000e+00, 1.00000e+00, 2.00000e+00, 5.40000e+01, 4.69000e+02,\n        2.57300e+03, 1.62910e+04, 8.02330e+04, 1.78238e+05, 1.53825e+05,\n        1.01914e+05, 9.36200e+04, 9.87920e+04, 1.01029e+05, 9.09090e+04,\n        5.97100e+04, 2.73410e+04, 9.47800e+03, 3.04700e+03, 1.62600e+03,\n        1.55700e+03, 1.65400e+03, 2.29600e+03, 3.89600e+03, 5.28800e+03,\n        1.36810e+04, 4.09490e+04, 2.19440e+04, 1.86000e+04, 1.45780e+04,\n        1.44360e+04, 1.21830e+04, 1.17350e+04, 1.19370e+04, 1.27390e+04,\n        1.38190e+04, 1.53210e+04, 1.83800e+04, 2.38000e+04, 2.65840e+04,\n        2.62420e+04, 2.41790e+04, 2.32460e+04, 2.33390e+04, 2.34600e+04,\n        2.27600e+04, 2.10470e+04, 1.70200e+04, 1.20100e+04, 2.16700e+03]),\n array([-0.9191919 , -0.88229859, -0.84540534, -0.80851203, -0.77161872,\n        -0.73472542, -0.69783217, -0.66093886, -0.62404555, -0.58715224,\n        -0.55025899, -0.51336569, -0.47647238, -0.4395791 , -0.40268579,\n        -0.36579251, -0.3288992 , -0.29200593, -0.25511262, -0.21821934,\n        -0.18132605, -0.14443275, -0.10753946, -0.07064617, -0.03375287,\n         0.00314042,  0.04003371,  0.07692701,  0.1138203 ,  0.15071359,\n         0.18760689,  0.22450018,  0.26139346,  0.29828677,  0.33518004,\n         0.37207335,  0.40896663,  0.44585994,  0.48275322,  0.51964653,\n         0.55653983,  0.59343308,  0.63032639,  0.6672197 ,  0.70411301,\n         0.74100626,  0.77789956,  0.81479287,  0.85168618,  0.88857943,\n         0.92547274]),\n <BarContainer object of 50 artists>)\n\n\n\n\n\n\nmany more grid cells have negative NDVI values, since the area of interest includes much more water\nalso, NDVI values close to zero are more abundant, indicating the presence of bare ground (sand) regions\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- Python’s built-in math operators are fast and simple options for raster math\n- numpy.digitize can be used to classify raster values in order to generate a less complicated map"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#calculating-zonal-statistics-on-rasters",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#calculating-zonal-statistics-on-rasters",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "10. Calculating Zonal Statistics on Rasters",
    "text": "10. Calculating Zonal Statistics on Rasters\nStatistics on predefined zones of the raster data are commonly used for analysis and to better understand the data. These zones are often provided within a single vector dataset, identified by certain vector attributes. For example, in the previous sections, we used the crop field polygon dataset. The fields with the same crop type can be identified as a “zone”, resulting in multiple zones in one vector dataset. We might be interested in performing statistical analysis over these crop zones.\nIn this section, we will explore how to calculate zonal statistics based on the types of crops in *cropped_field.shp . To do this, we will first identify zones from the vector data, then rasterize these vector zones. Finally the zonal statistics for ndvi will be calculated over the rasterized zones.\n\nMaking vector and raster data compatible\nFirst, let’s load the NDVI.tif file saved in the previous episode to obtained our calculated raster ndvi data. We also use the squeeze() function in order to reduce our raster data ndvi dimension to 2D by removing the singular band dimension - this is necessary for use with the rasterize and zonal_stats functions:\n\nimport rioxarray\n\n# load in raster data from .tif file\nndvi = rioxarray.open_rasterio('Data/NDVI.tif')\n\n# reduce raster data dimension to 2D\nndvi_sq = ndvi.squeeze()\n\nLet’s also read the crop fields vector data from our saved cropped_field.shp file and view the CRS information.\n\nimport geopandas as gpd\n\n# read in the vector data from the .shp file\nfield = gpd.read_file('Data/cropped_field.shp')\nfield.crs\n\n<Derived Projected CRS: EPSG:28992>\nName: Amersfoort / RD New\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: Netherlands - onshore, including Waddenzee, Dutch Wadden Islands and 12-mile offshore coastal zone.\n- bounds: (3.2, 50.75, 7.22, 53.7)\nCoordinate Operation:\n- name: RD New\n- method: Oblique Stereographic\nDatum: Amersfoort\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich\n\n\n\n\n\n\n\n\nLoading a .shp file\n\n\n\nAll the files which come with the .shp file MUST be in the same folder (.cp, .dbf, .prj, .shx)\n\n\nIn order to use the vector data as a classifier for our raster, we need to convert the vector data to the appropriate CRS. We can perform the CRS conversion from the vector CRS (EPSG:28992) to our raster ndvi CRS (EPSG:32631) and view the data with:\n\n# convert vector data to appropriate CRS\nfield_to_raster_crs = field.to_crs(ndvi.rio.crs)\nfield_to_raster_crs\n\n\n\n\n\n  \n    \n      \n      category\n      gewas\n      gewascode\n      jaar\n      status\n      geometry\n    \n  \n  \n    \n      0\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627394.386 5812746.538, 627393.681 5...\n    \n    \n      1\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627326.170 5813192.913, 627324.734 5...\n    \n    \n      2\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627093.646 5812863.661, 627090.972 5...\n    \n    \n      3\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((627384.004 5813467.330, 627376.549 5...\n    \n    \n      4\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((627179.293 5812870.376, 627160.747 5...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4867\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640710.959 5809885.718, 640703.030 5...\n    \n    \n      4868\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640753.587 5810143.151, 640751.454 5...\n    \n    \n      4869\n      Grasland\n      Grasland, blijvend\n      265\n      2020\n      Definitief\n      POLYGON ((640890.832 5809998.743, 640892.271 5...\n    \n    \n      4870\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640770.878 5810005.390, 640766.767 5...\n    \n    \n      4871\n      Grasland\n      Grasland, natuurlijk. Hoofdfunctie landbouw.\n      331\n      2020\n      Definitief\n      POLYGON ((640704.702 5809791.384, 640698.357 5...\n    \n  \n\n4872 rows × 6 columns\n\n\n\n\n\nRasterizing our vector data\nBefore calculating zonal statistics, we first need to rasterize our field_to_raster_crs vector geodataframe with the rasterio.features.rasterize function. With this function, we aim to produce a grid with numerical values representing the types of crop as defined by the column gewascode from field_cropped. gewascode stands for the crop codes as defined by the Netherlands Enterprise Agency (RVO) for different types of crops or gewas (Grassland, permanent; Grassland, temporary; corn fields; etc.). This grid of values thus defines the zones for the xrspatial.zonal_stats function, where each pixel in the zone grid overlaps with a corresponding pixel in our NDVI raster.\nWe can generate the geometry, gewascode pairs for each vector feature to be used as the first argument to rasterio.features.rasterize as:\n\n# generate geometty,gewascode pairings\ngeom = field_to_raster_crs[['geometry', 'gewascode']].values.tolist()\n\n# Limit output to firat 10 items\ngeom[:10]\n\n[[<POLYGON ((627394.386 5812746.538, 627393.681 5812749.022, 627384.063 581279...>,\n  265],\n [<POLYGON ((627326.17 5813192.913, 627324.734 5813196.828, 627324.11 5813203....>,\n  265],\n [<POLYGON ((627093.646 5812863.661, 627090.972 5812870.702, 627076.762 581286...>,\n  265],\n [<POLYGON ((627384.004 5813467.33, 627376.549 5813470.117, 627385.233 5813495...>,\n  265],\n [<POLYGON ((627179.293 5812870.376, 627160.747 5812846.868, 627159.505 581284...>,\n  331],\n [<POLYGON ((627419.816 5812765.254, 627416.356 5812766.811, 627413.1 5812769....>,\n  265],\n [<POLYGON ((627358.683 5813335.376, 627362.871 5813352.853, 627370.175 581337...>,\n  265],\n [<POLYGON ((627423.06 5813550.358, 627421.409 5813550.392, 627427.293 5813568...>,\n  265],\n [<POLYGON ((627191.219 5813028.547, 627189.596 5813028.866, 627186.64 5813029...>,\n  265],\n [<POLYGON ((627349.601 5813329.836, 627343.37 5813330.159, 627341.775 5813330...>,\n  265]]\n\n\nWe can now rasterize our vector data using rasterio.features.rasterize:\n\nfrom rasterio import features\nfield_cropped_raster = features.rasterize(geom, out_shape=ndvi_sq.shape, fill=0, transform=ndvi.rio.transform())\n\nThe argument out_shape specifies the shape of the output grid in pixel units, while transform represents the projection from pixel space to the projected coordinate space. We also need to specify the fill value for pixels that are not contained within a polygon in our shapefile, which we do with fill = 0. It’s important to pick a fill value that is not the same as any value already defined in gewascode or else we won’t distinguish between this zone and the background.\nWe convert the output of the rasterio.features.rasterize function, which generates a numpy array np.ndarray, to xarray.DataArray which will be used further:\n\nimport xarray as xr\nfield_cropped_raster_xarr = xr.DataArray(field_cropped_raster)\n\n\n\nCalculate zonal statistics\nIn order to calculate the statistics for each crop zone, we call the function, xrspatial.zonal_stats. The xrspatial.zonal_stats function takes as input zones, a 2D xarray.DataArray, that defines different zones, and values, a 2D xarray.DataArray providing input values for calculating statistics.\nWe call the zonal_stats function with field_cropped_raster_xarr as our classifier and the 2D raster with our values of interest ndvi_sq to obtain the NDVI statistics for each crop type:\n\nfrom xrspatial import zonal_stats\nzonal_stats(field_cropped_raster_xarr, ndvi_sq)\n\n\n\n\n\n  \n    \n      \n      zone\n      mean\n      max\n      min\n      sum\n      std\n      var\n      count\n    \n  \n  \n    \n      0\n      0\n      0.266528\n      0.999579\n      -0.998648\n      38887.554688\n      0.409970\n      0.168075\n      145904.0\n    \n    \n      1\n      259\n      0.520282\n      0.885242\n      0.289196\n      449.003052\n      0.111205\n      0.012366\n      863.0\n    \n    \n      2\n      265\n      0.775609\n      0.925955\n      0.060755\n      66478.976562\n      0.091089\n      0.008297\n      85712.0\n    \n    \n      3\n      266\n      0.794128\n      0.918048\n      0.544686\n      1037.925781\n      0.074009\n      0.005477\n      1307.0\n    \n    \n      4\n      331\n      0.703056\n      0.905304\n      0.142226\n      10725.819336\n      0.102255\n      0.010456\n      15256.0\n    \n    \n      5\n      332\n      0.681699\n      0.849158\n      0.178113\n      321.080261\n      0.123633\n      0.015285\n      471.0\n    \n    \n      6\n      335\n      0.648063\n      0.865804\n      0.239661\n      313.662598\n      0.146582\n      0.021486\n      484.0\n    \n    \n      7\n      863\n      0.388575\n      0.510572\n      0.185987\n      1.165724\n      0.144245\n      0.020807\n      3.0\n    \n  \n\n\n\n\nThe zonal_stats function calculates the minimum, maximum, and sum for each zone along with statistical measures such as the mean, variance and standard deviation for each rasterized vector zone.\n\nIn our raster data-set zone = 0, corresponding to non-crop areas, has the highest count followed by zone = 265 which corresponds to ‘Grasland, blijvend’ (‘Grassland, permanent’).\nThe highest mean NDVI is observed for zone = 266 for ‘Grasslands, temporary’ with the lowest mean, aside from non-crop area, going to zone = 863 representing ‘Forest without replanting obligation’.\n\nThus, the zonal_stats function can be used to analyse and understand different sections of our raster data. The definition of the zones can be derived from vector data or from classified raster data as presented in the challenge below:\n\n\nCalculate zonal statistics for zones defined by ndvi_classified\nTo apply what we have just learned, let’s now calculate NDVI zonal statistics for the different zones as classified by ndvi_classified in the previous section.\nFirst we’ll load both raster data-sets and convert into 2D xarray.DataArray. Then, we’ll calculate zonal statistics for each class_bins, and inspect the output of the zonal_stats function.\n\n# load both raster data-sets and convert into 2D xarray.DataArray\nndvi = rioxarray.open_rasterio('Data/NDVI.tif')\nndvi_classified = rioxarray.open_rasterio('Data/NDVI_classified.tif')\n\n\n# reduce raster data dimension to 2D\nndvi_classified_sq = ndvi_classified.squeeze()\n\n# calculate zonal statistics for each class_bins\nzonal_stats(ndvi_classified_sq, ndvi_sq)\n\n\n\n\n\n  \n    \n      \n      zone\n      mean\n      max\n      min\n      sum\n      std\n      var\n      count\n    \n  \n  \n    \n      0\n      1\n      -0.355660\n      -0.000257\n      -0.998648\n      -12838.253906\n      0.145916\n      0.021291\n      36097.0\n    \n    \n      1\n      2\n      0.110731\n      0.199839\n      0.000000\n      1754.752441\n      0.055864\n      0.003121\n      15847.0\n    \n    \n      2\n      3\n      0.507998\n      0.700000\n      0.200000\n      50410.167969\n      0.140193\n      0.019654\n      99233.0\n    \n    \n      3\n      4\n      0.798281\n      0.999579\n      0.700025\n      78888.523438\n      0.051730\n      0.002676\n      98823.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- zones can be extracted by attribute columns of a vector dataset\n- zones can be rasterized using rasterio.features.rasterize\n- calculate zonal statistics with xrspatial.zonal_stats over the rasterized zones"
  },
  {
    "objectID": "posts/Geospatial/Working with Geospatial Data in Python.html#parallel-raster-computations-using-dask",
    "href": "posts/Geospatial/Working with Geospatial Data in Python.html#parallel-raster-computations-using-dask",
    "title": "Introduction to Geospatial Raster and Vector data with Python",
    "section": "11. Parallel raster computations using Dask",
    "text": "11. Parallel raster computations using Dask\nVery often raster computations involve applying the same operation to different pieces of data. Think, for instance, to the “pixel”-wise sum of two raster datasets, where the same sum operation is applied to all the matching grid-cells of the two rasters. This class of tasks can benefit from chunking the input raster(s) into smaller pieces: operations on different pieces can be run in parallel using multiple computing units (e.g., multi-core CPUs), thus potentially speeding up calculations. In addition, working on chunked data can lead to smaller memory footprints, since one may bypass the need to store the full dataset in memory by processing it chunk by chunk.\nIn this section, we will introduce the use of Dask in the context of raster calculations. Dask is a Python library for parallel and distributed computing. It provides a framework to work with different data structures, including chunked arrays (Dask Arrays). Dask is well integrated with (rio)xarray objects, which can use Dask arrays as underlying data structures.\nThis section shows how Dask can be used to parallelize operations on local CPUs. However, the same library can be configured to run tasks on large compute clusters. More resources on Dask:\nDask Array\nXarray with Dask\nIt is important to realize, however, that many details determine the extent to which using Dask’s chunked arrays instead of regular NumPy arrays leads to faster calculations (and lower memory requirements). The actual operations to carry out, the size of the dataset, and parameters such as the chunks’ shape and size, all affects the performance of our computations. Depending on the specifics of the calculations, serial calculations might actually turn out to be faster! Being able to profile the computational time is thus essential, and we will see how to do that in a Jupyter environment in the next section.\n\nTime profiling in Jupyter\n\nWe’ll continue from the results of the satellite image search that we carried out in the previous sections. We will load data starting from the search.json file. You can download the raster data using this link. Save the geospatial-python-raster-dataset.tar.gz file in your current working directory, and extract the archive file by double-clicking on it or by running the following command in your terminal tar -zxvf geospatial-python-raster-dataset.tar.gz. Use the file geospatial-python-raster-dataset/search.json (instead of search.json) to get started.\nLet’s set up a raster calculation using assets from our previous search of satellite scenes. We first load the item collection using the pystac library:\n\nimport pystac\nitems = pystac.ItemCollection.from_file(\"search.json\")\n\nLet’s select the last scene, and extract the URLs of two assets: - the true-color image (“visual”) and - the scene classification layer (“SCL”). The latter is a mask where each grid cell is assigned a label that represents a specific class e.g. “4” for vegetation, “6” for water, etc. (all classes and labels are reported in the Sentinel-2 documentation.\n\n# last item's assets\nassets = items[-1].assets\n\n# true color image\nvisual_href = assets[\"visual\"].href  \n\n# scene classification layer\nscl_href = assets[\"SCL\"].href \n\nOpening the two assets with rioxarray shows that the true-color image is available as a raster file with 10 m resolution, while the scene classification layer has a lower resolution (20 m):\n\nimport rioxarray\nscl = rioxarray.open_rasterio(scl_href)\nvisual = rioxarray.open_rasterio(visual_href)\nprint(scl.rio.resolution(), visual.rio.resolution())\n\n(20.0, -20.0) (10.0, -10.0)\n\n\nIn order to match the image and the mask pixels, we take advantage of a feature of the cloud-optimized GeoTIFF (COG) format, which is used to store these raster files. COGs typically include multiple lower-resolution versions of the original image, called “overviews”, in the same file. This allows to avoid downloading high-resolution images when only quick previews are required.\nOverviews are often computed using powers of 2 as down-sampling (or zoom) factors (e.g. 2, 4, 8, 16). For the true-color image we thus open the first level overview (zoom factor 2) and check that the resolution is now also 20 m:\n\nvisual = rioxarray.open_rasterio(visual_href, overview_level=0)\nprint(visual.rio.resolution())\n\n(20.0, -20.0)\n\n\nWe can now time profile the first step of our raster calculation: the (down)loading of the rasters’ content. We do it by using the Jupyter magic %%time, which returns the time required to run the content of a cell:\n\n%%time\nscl = scl.load()\nvisual = visual.load()\n\nCPU times: user 614 ms, sys: 241 ms, total: 855 ms\nWall time: 9.31 s\n\n\n\nvisual.plot.imshow(figsize=(10,10))\nscl.squeeze().plot.imshow(levels=range(13), figsize=(12,10))\n\n<matplotlib.image.AxesImage at 0x7f655d6225c0>\n\n\n\n\n\n\n\n\nAfter loading the raster files into memory, we run the following steps:\n\nwe create a mask of the grid cells that are labeled as “cloud” in the scene classification layer (values “8” and “9”, standing for medium- and high-cloud probability, respectively)\nwe use this mask to set the corresponding grid cells in the true-color image to null values\nwe save the masked image to disk as in COG format\n\nAgain, we measure the cell execution time using %%time:\n\n%%time\nmask = scl.squeeze().isin([8, 9])\nvisual_masked = visual.where(~mask, other=visual.rio.nodata)\nvisual_masked.rio.to_raster(\"band_masked.tif\")\n\nCPU times: user 164 ms, sys: 154 ms, total: 318 ms\nWall time: 317 ms\n\n\nWe can inspect the masked image as:\n\nvisual_masked.plot.imshow(figsize=(10, 10))\n\n<matplotlib.image.AxesImage at 0x7f655d3e7640>\n\n\n\n\n\nIn the following sub-section we will see how to parallelize these raster calculations, and we will compare timings to the serial calculations that we just ran.\n\n\nDask-powered rasters\n\nChunked arrays\nAs we have mentioned, rioxarray supports the use of Dask’s chunked arrays as underlying data structure. When opening a raster file with open_rasterio and providing the chunks argument, Dask arrays are employed instead of regular Numpy arrays. chunks describes the shape of the blocks which the data will be split in. As an example, we open the blue band raster (“B02”) using a chunk shape of (1, 4000, 4000) (block size of 1 in the first dimension and of 4000 in the second and third dimensions):\n\nblue_band_href = assets[\"B02\"].href\nblue_band = rioxarray.open_rasterio(blue_band_href, chunks=(1, 4000, 4000))\n\nXarray and Dask also provide a graphical representation of the raster data array and of its blocked structure:\n\nblue_band\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 10980, x: 10980)>\ndask.array<open_rasterio-bb9b2ce30f7d0b7c18c953045067cb21<this-array>, shape=(1, 10980, 10980), dtype=uint16, chunksize=(1, 4000, 4000), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6e+05 6e+05 ... 7.098e+05 7.098e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 10980x: 10980dask.array<chunksize=(1, 4000, 4000), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         229.95 MiB \n                         30.52 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 10980, 10980) \n                         (1, 4000, 4000) \n                    \n                    \n                         Count \n                         2 Graph Layers \n                         9 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  10980\n  10980\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05array([600005., 600015., 600025., ..., 709775., 709785., 709795.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900035., 5900025., 5900015., ..., 5790265., 5790255., 5790245.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 10.0 0.0 5900040.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600005.0, 600015.0, 600025.0, 600035.0, 600045.0, 600055.0,\n              600065.0, 600075.0, 600085.0, 600095.0,\n              ...\n              709705.0, 709715.0, 709725.0, 709735.0, 709745.0, 709755.0,\n              709765.0, 709775.0, 709785.0, 709795.0],\n             dtype='float64', name='x', length=10980))yPandasIndexPandasIndex(Float64Index([5900035.0, 5900025.0, 5900015.0, 5900005.0, 5899995.0, 5899985.0,\n              5899975.0, 5899965.0, 5899955.0, 5899945.0,\n              ...\n              5790335.0, 5790325.0, 5790315.0, 5790305.0, 5790295.0, 5790285.0,\n              5790275.0, 5790265.0, 5790255.0, 5790245.0],\n             dtype='float64', name='y', length=10980))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\nWe have already seen how COGs are regular GeoTIFF files with a special internal structure. Another feature of COGs is that data is organized in “blocks” that can be accessed remotely via independent HTTP requests, enabling partial file readings. This is useful if you want to access only a portion of your raster file, but it also allows for efficient parallel reading. You can check the blocksize employed in a COG file with the following code snippet:\n\nimport rasterio\nwith rasterio.open(visual_href) as r:\n    if r.is_tiled:\n        print(f\"Chunk size: {r.block_shapes}\")\n\nChunk size: [(1024, 1024), (1024, 1024), (1024, 1024)]\n\n\nIn order to optimally access COGs it is best to align the blocksize of the file with the chunks employed when loading the file.\nLet’s open the blue-band asset (“B02”) of a Sentinel-2 scene as a chunked DataArray object using a suitable chunk size:\n\nimport rasterio\nwith rasterio.open(blue_band_href) as r:\n    if r.is_tiled:\n        print(f\"Chunk size: {r.block_shapes}\")\n\nChunk size: [(1024, 1024)]\n\n\ndeal chunk size values for this raster are thus multiples of 1024. An element to consider is the number of resulting chunks and their size. Chunks should not be too big nor too small (i.e. too many). As a rule of thumb, chunk sizes of 100 MB typically work well with Dask (see, e.g., this blog post. Also, the shape might be relevant, depending on the application! Here, we might select a chunks shape of (1, 6144, 6144):\n\nband = rioxarray.open_rasterio(blue_band_href, chunks=(1, 6144, 6144))\n\nwhich leads to chunks 72 MB large: ((1 x 6144 x 6144) x 2 bytes / 2^20 = 72 MB). Also, we can let rioxarray and Dask figure out appropriate chunk shapes by setting chunks=“auto”:\n\nband = rioxarray.open_rasterio(blue_band_href, chunks=\"auto\")\n\nwhich leads to (1, 8192, 8192) chunks (128 MB) as illustrated below:\n\nband\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray (band: 1, y: 10980, x: 10980)>\ndask.array<open_rasterio-e0482a83b59aad29d7a36b8e8d497165<this-array>, shape=(1, 10980, 10980), dtype=uint16, chunksize=(1, 8192, 8192), chunktype=numpy.ndarray>\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6e+05 6e+05 ... 7.098e+05 7.098e+05 7.098e+05\n  * y            (y) float64 5.9e+06 5.9e+06 5.9e+06 ... 5.79e+06 5.79e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:       Area\n    OVR_RESAMPLING_ALG:  AVERAGE\n    _FillValue:          0\n    scale_factor:        1.0\n    add_offset:          0.0xarray.DataArrayband: 1y: 10980x: 10980dask.array<chunksize=(1, 8192, 8192), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         229.95 MiB \n                         128.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 10980, 10980) \n                         (1, 8192, 8192) \n                    \n                    \n                         Count \n                         2 Graph Layers \n                         4 Chunks \n                    \n                    \n                     Type \n                     uint16 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n\n  \n  \n  \n  \n\n  \n  \n\n  \n  10980\n  10980\n  1\n\n        \n    \nCoordinates: (4)band(band)int641array([1])x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05array([600005., 600015., 600025., ..., 709775., 709785., 709795.])y(y)float645.9e+06 5.9e+06 ... 5.79e+06array([5900035., 5900025., 5900015., ..., 5790265., 5790255., 5790245.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 31Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :3.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 31N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",3],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32631\"]]GeoTransform :600000.0 10.0 0.0 5900040.0 0.0 -10.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Int64Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Float64Index([600005.0, 600015.0, 600025.0, 600035.0, 600045.0, 600055.0,\n              600065.0, 600075.0, 600085.0, 600095.0,\n              ...\n              709705.0, 709715.0, 709725.0, 709735.0, 709745.0, 709755.0,\n              709765.0, 709775.0, 709785.0, 709795.0],\n             dtype='float64', name='x', length=10980))yPandasIndexPandasIndex(Float64Index([5900035.0, 5900025.0, 5900015.0, 5900005.0, 5899995.0, 5899985.0,\n              5899975.0, 5899965.0, 5899955.0, 5899945.0,\n              ...\n              5790335.0, 5790325.0, 5790315.0, 5790305.0, 5790295.0, 5790285.0,\n              5790275.0, 5790265.0, 5790255.0, 5790245.0],\n             dtype='float64', name='y', length=10980))Attributes: (5)AREA_OR_POINT :AreaOVR_RESAMPLING_ALG :AVERAGE_FillValue :0scale_factor :1.0add_offset :0.0\n\n\n\n\n\nParallel computations\nOperations performed on a DataArray that has been opened as a chunked Dask array are executed using Dask. Dask coordinates how the operations should be executed on the individual chunks of data, and runs these tasks in parallel as much as possible.\nLet’s now repeat the raster calculations that we have carried out in the previous section, but running calculations in parallel over a multi-core CPU. We first open the relevant rasters as chunked arrays:\n\nscl = rioxarray.open_rasterio(scl_href, lock=False, chunks=(1, 2048, 2048))\nvisual = rioxarray.open_rasterio(visual_href, overview_level=0, lock=False, chunks=(3, 2048, 2048))\n\nSetting lock=False tells rioxarray that the individual data chunks can be loaded simultaneously from the source by the Dask workers.\nAs the next step, we trigger the download of the data using the .persist() method, see below. This makes sure that the downloaded chunks are stored in the form of a chunked Dask array (calling .load() would instead merge the chunks in a single Numpy array).\nWe explicitly tell Dask to parallelize the required workload over 4 threads. Don’t forget to add the Jupyter magic to record the timing!\n\n%%time\nscl = scl.persist(scheduler=\"threads\", num_workers=4)\nvisual = visual.persist(scheduler=\"threads\", num_workers=4)\n\nCPU times: user 2.04 s, sys: 168 ms, total: 2.21 s\nWall time: 13.5 s\n\n\nSo downloading chunks of data using 4 workers was actually slower (13.5 s vs 9.31 s).\nLet’s now continue to the second step of the calculation. Note how the same syntax as for its serial version is employed for creating and applying the cloud mask. Only the raster saving includes additional arguments:\n\ntiled=True: write raster as a chunked GeoTIFF.\nlock=threading.Lock(): the threads which are splitting the workload must “synchronise” when writing to the same file (they might otherwise overwrite each other’s output).\ncompute=False: do not immediately run the calculation, more on this later.\n\n\nfrom threading import Lock\n\n\n%%time\nmask = scl.squeeze().isin([8, 9])\nvisual_masked = visual.where(~mask, other=0)\nvisual_store = visual_masked.rio.to_raster(\"band_masked.tif\", tiled=True, lock=Lock(), compute=False)\n\nCPU times: user 13.1 ms, sys: 12 ms, total: 25.1 ms\nWall time: 21.1 ms\n\n\nDid we just observe a 15x speed-up when comparing to the serial calculation (317 ms vs 21.1 ms)? Actually, no calculation has run yet. This is because operations performed on Dask arrays are executed “lazily”, i.e. they are not immediately run.\n\n\nDask graph\nThe sequence of operations to carry out is stored in a task graph, which can be visualized with:\n\nimport dask\ndask.visualize(visual_store)\n\n\n\n\nThe task graph gives Dask the complete “overview” of the calculation, thus enabling a better management of tasks and resources when dispatching calculations to be run in parallel.\nWhile most methods of DataArray’s run operations lazily when Dask arrays are employed, some methods by default trigger immediate calculations, like the method to_raster() (we have changed this behaviour by specifying compute=False). In order to trigger calculations, we can use the .compute() method. Again, we explicitly tell Dask to run tasks on 4 threads. Let’s time the cell execution:\n\n%%time\nvisual_store.compute(scheduler=\"threads\", num_workers=4)\n\nCPU times: user 292 ms, sys: 39.3 ms, total: 331 ms\nWall time: 176 ms\n\n\n[None, None, None, None, None, None, None, None, None]\n\n\nThe timing that we have recorded for this step is about half the speed of the one recorded for the serial calculation, despite the overhead that Dask introduces to manage the tasks in the Dask graph. This overhead, which is typically of the order of milliseconds per task, can sometimes be larger than the parallelization gain, and this is typically the case for calculations with small chunks (note that here we have used chunks that are only 4 to 32 MB large).\n\n\n\n\n\n\nImportant\n\n\n\nKey Points:\n- The %%time Jupyter magic command can be used to profile calculations\n- data ‘chunks’ are the unit of parallelization in raster calculations\n- (rio)xarray can open raster files as chunked arrays\n- the chunk shape and size can significantly affect the calculation performance\n- cloud-optimized GeoTIFFs have an internal structure that enables performant parallel read"
  },
  {
    "objectID": "posts/Hello World/HelloWorld.html",
    "href": "posts/Hello World/HelloWorld.html",
    "title": "Hello, World!",
    "section": "",
    "text": "A chartered accountant by trade, I now find myself transitioning into Data Science, and Machine Learning in particular. The voice in my head just wouldn’t be silenced and I had no option but to take its advice.\nI set about changing track in the ‘traditional’ way. I started building the foundations. I signed up for a Maths and Statistics degree with the Open University. It turns out my aptitude had not left me despite a 20-year gap, but I soon found that picking away at this in the evenings whilst working full time was not the quickest route. My progress has been ‘frozen’ at the half-way mark, and I may decide to pick this up again at some stage. I feel like I owe it to my high school maths teacher! In any event, the mathematical and statistical concepts covered certainly gives me a solid foothold for my new path.\nI also signed up for the Google Data Analytics certificate, which gave me an introduction to R, SQL and Tableau. The combination of the power and speed of computer programming languages and the beauty of the visualisation tools had me hooked! It reminded me of my very first computer, an Amstrad CPC 464. Seems like yesterday but it was during the times of the BASIC programming language! I can still remember staring proudly at the screen as a flashing purple circle winked back at me 😉\n\nI then quickly followed this up with some Python courses via Data Camp, and was in the middle of yet another course, Machine Learning Specialization taught by Andrew Ng, when I had the fortune to stumble upon Radek Osmulski’s book Meta Learning. He too, had followed a traditional model of learning but found that this wasn’t working out. That resonated with me as, despite the apparent ‘progress’ I was making via all these courses, it where was the tangible output? What did I have to show for all those hours of lectures?\nIt was Radek’s book that changed my life trajectory, by introducing me to Jeremy Howard and the fast.ai Practical Deep Learning for Coders. The ‘getting your hands dirty’ approach is the complete antithesis of my career path to date and is just what I needed to springboard my transition.\nThe initial purpose of this blog is to finally have a platform to showcase my achievements, immature and naïve as they are in these first steps. As I dive deeper hopefully I can start to give something back to the community and do some troubleshooting. The road is bumpy out there.\nCome join me… Into the Unknown!"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html",
    "title": "Numpy Tutorial",
    "section": "",
    "text": "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\nAt the core of the NumPy package, is the ndarray object. This encapsulates n-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance. There are several important differences between NumPy arrays and the standard Python sequences.\nSee the documentation for further information.\n\n\n\narrays.JPG"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#numpy-v-lists",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#numpy-v-lists",
    "title": "Numpy Tutorial",
    "section": "NumPy [ ] v Lists ( )",
    "text": "NumPy [ ] v Lists ( )\n\nWhy is NumPy faster?\nNumPy has a significant speed advantage over Lists.\n\n\n\nspeed.JPG\n\n\nLet’s find out why…\n\nLess memory is used to represent data\n\nSay for example the number 5. This is represented in binary form as follows:\n\nint8 which takes up 8 bits (or 1 byte) of memory and is represented in binary form as 00000101\n\nNumPy can cast this to:\n\nint16 which takes up 16 bits (or 2 bytes) of memory and is represented in binary form as 00000000 00000101\nint32 which takes up 32 bits (or 4 bytes) of memory and is represented in binary form as 00000000 00000000 00000000 00000101\nint64 which takes up 64 bits (or 8 bytes) of memory and is represented in binary form as 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101\n\nLists use a built in int type for Python which consists of four different things:\n\nobject value\nobject type\nreference count\nsize of value\n\neach of which use up memory:\n\nobject value: 8 bytes 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000101\nobject type: 8 bytes 11001010 10111110 01100001 01000100 11111100 00000000 11001100 01011111\nreference count: 8 bytes 00000001 00111101 11111110 10111100 00011010 11011101 10100100 11011000\nsize of value: 4 bytes 00000000 00000000 00000000 00011100\n\n\nNo type checking when iterating through objects\nNumPy utilizes contiguous memory which enables Single Instruction Multiple Data (SIMD) Vector Processing to be harnessed, and effective cache utilization\n\n\n\nHow do Lists differ from NumPy?\n\n\n\ndifferences.JPG\n\n\nNumPy allows itemwise computation:\n\n\n\nmultiply.JPG"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#applications-of-numpy",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#applications-of-numpy",
    "title": "Numpy Tutorial",
    "section": "Applications of NumPy",
    "text": "Applications of NumPy\n\n\n\napplications.JPG\n\n\n\n\n\n\n\n\nSciPy\n\n\n\nNumPy can perform complex mathematical operations, although the SciPy library allows even more advanced computation.\n\n\n\n# load in \nimport numpy as np\n\n\n# which version?\nnp.__version__\n\n'1.22.4'"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#the-basics",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#the-basics",
    "title": "Numpy Tutorial",
    "section": "The Basics",
    "text": "The Basics\n\n# create an array of integers\na = np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\n# create an array of floating point numbers\nb = np.array([[9.0,8.0,7.0],[6.0,5.0,4.0]])\nb\n\narray([[9., 8., 7.],\n       [6., 5., 4.]])\n\n\n\nShape of a NumPy array\nEvery NumPy array can be queried for its shape. A shape is a tuple of the format (n_rows, n_cols).\n\n# Grab the array shape\na.shape\n\n(3,)\n\n\nSince there is no value after the comma, this is a one-dimensional array.\n\n# Grab the array dimension\na.ndim\n\n1\n\n\n\n# Grab the array shape\nb.shape\n\n(2, 3)\n\n\n(2, 3) means that the matrix has 2 rows and 3 columns.\n\n# Grab the array dimension\nb.ndim\n\n2\n\n\n\n# Get type\na.dtype\n\ndtype('int64')\n\n\nThis is the default size, but we can assign a lower int value to save memory:\n\na = np.array([1,2,3], dtype='int16')\na\n\narray([1, 2, 3], dtype=int16)\n\n\n\n# Get size - how many bytes?\na.itemsize\n\n2\n\n\n\n# Get size - how many bytes?\nb.itemsize\n\n8\n\n\n\n# Get total number of elements\na.size\n\n3\n\n\n\n# Get total size\na.size * a.itemsize\n\n6\n\n\n\n# Get total size\na.nbytes\n\n6"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---accessing-specific-elements-rows-columns-etc",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---accessing-specific-elements-rows-columns-etc",
    "title": "Numpy Tutorial",
    "section": "Indexing - accessing specific elements, rows, columns etc",
    "text": "Indexing - accessing specific elements, rows, columns etc\n\n\n\n\n\n\nIndexing\n\n\n\nRemember - the first index of a NumPy array is zero! and not one\n\n\n\nc = np.array([[1,2,3,4,5,6,7],[8,9,10,11,12,13,14]])\nprint(c)\n\n[[ 1  2  3  4  5  6  7]\n [ 8  9 10 11 12 13 14]]\n\n\n\nc.shape\n\n(2, 7)\n\n\n\n# Get a specific element from row, column (r,c)\nc[1,5]\n\n13\n\n\n\n# Get a specific element from row, column (r,c)\nc[1,-2]\n\n13\n\n\n\n# Get a specific ROW\nc[0, :]\n\narray([1, 2, 3, 4, 5, 6, 7])\n\n\n\n# Get a specific COLUMN\nc[:, 2]\n\narray([ 3, 10])\n\n\n\n# Getting a little more fancy [start_index : end_index : step_size]\n# Row 0, then columns 1 to 6 (excluding 6) in steps of 2\nc [0, 1:6:2]\n\narray([2, 4, 6])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---changing-specific-elements",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#indexing---changing-specific-elements",
    "title": "Numpy Tutorial",
    "section": "Indexing - changing specific elements",
    "text": "Indexing - changing specific elements\n\n# change row 1, column 5 value (13) to 20\nc[1,5] = 20\nprint(c)\n\n[[ 1  2  3  4  5  6  7]\n [ 8  9 10 11 12 20 14]]\n\n\n\n# change column 2 [3,10] to [21,28]\nc[:,2] = [21,28]\nprint(c)\n\n[[ 1  2 21  4  5  6  7]\n [ 8  9 28 11 12 20 14]]\n\n\n\n# 3-d example\nd = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\nprint(d)\n\n[[[1 2]\n  [3 4]]\n\n [[5 6]\n  [7 8]]]\n\n\n\n# Get specific example (work from outside in)\n\n# the first index reference [0] relates to the first grouping\n# [1, 2]\n# [3, 4]\n\n# the second index reference [1] relates to the second row within that grouping [3, 4]\n\n# the third index reference [1] specifies the second column from the second row of the grouping [3,4] i.e 4\n\nd[0,1,1]\n\n4\n\n\n\n# Get specific example (work from outside in)\n\n# the first index reference : means we are selecting from both groupings\n# [1 2]\n# [3 4]\n\n# [5 6]\n# [7 8]\n\n# the second index reference [1] relates to the second row from each of the groupings i.e. [3,4] and [7,8]\n\n# the third index reference [:] specifies all values within those rows \n\nd[:,1,:]\n\narray([[3, 4],\n       [7, 8]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#initializing-different-array-types",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#initializing-different-array-types",
    "title": "Numpy Tutorial",
    "section": "Initializing Different Array Types",
    "text": "Initializing Different Array Types\n\n# All 0s matrix specifying shape\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\n\n# All 0s 2 x 3 matrix specifying shape\n\nnp.zeros((2,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# All 0s 2 x 3 x 4 matrix \n\n# first reference = number of groupings\n# second reference = number of rows\n# third reference = number of columns\nnp.zeros((2,3,4))\n\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])\n\n\n\n# All 1s matrix\n\nnp.ones((4,2,2), dtype='int16')\n\narray([[[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]],\n\n       [[1, 1],\n        [1, 1]]], dtype=int16)\n\n\n\n# Any other number .full\n\nnp.full((2,2), 137, dtype='float32')\n\narray([[137., 137.],\n       [137., 137.]], dtype=float32)\n\n\n\n# Any other number .full_like\n# creates a new array full of 4s matching the shape of the d array we previously created\n\nnp.full_like(d,4)\n\narray([[[4, 4],\n        [4, 4]],\n\n       [[4, 4],\n        [4, 4]]])\n\n\n\n# random decimals\n# using random.rand\n\nnp.random.rand(1,3,7)\n\narray([[[0.16735136, 0.86937755, 0.30866395, 0.05841447, 0.28817268,\n         0.55635487, 0.87087044],\n        [0.95692978, 0.45277212, 0.87002198, 0.59516086, 0.56308885,\n         0.71476549, 0.64600732],\n        [0.06479773, 0.83108022, 0.0321547 , 0.3054754 , 0.72857438,\n         0.56460774, 0.72935517]]])\n\n\n\n# random decimals following shape of previously defined array\n# using random.random_sample\n\nnp.random.random_sample(d.shape)\n\narray([[[0.04212123, 0.39748958],\n        [0.63778666, 0.3396428 ]],\n\n       [[0.33967012, 0.49291645],\n        [0.97990323, 0.45722717]]])\n\n\n\n# random integers\n# first argument is range of integers to sample from (upper limit is EXclusive)\n# second argument is shape - established by size=\n\nnp.random.randint(7,13, size=(3,3))\n\narray([[10, 10, 10],\n       [12, 12, 12],\n       [10,  7,  9]])\n\n\n\n# identity matrix\n# ones on main diagonal and zeros elsewhere\n# only one pararmeter as this is a square matrix\n\nnp.identity(7)\n\narray([[1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 1.]])\n\n\n\n# repeat a matrix using .repeat\n# first argument is array to be repeated\n# second argument is how many times to be repeated\n# axis 0 refers to rows\n\narr = np.array([[1,3,7]])\nr1 = np.repeat(arr,3,axis=0)\nprint(r1)\n\n[[1 3 7]\n [1 3 7]\n [1 3 7]]\n\n\n\n# repeat a matrix using .repeat\n# first argument is array to be repeated\n# second argument is how many times to be repeated\n# axis 1 refers to columns\n\narr = np.array([[1,3,7]])\nr1 = np.repeat(arr,3,axis=1)\nprint(r1)\n\n[[1 1 1 3 3 3 7 7 7]]\n\n\nHow might we go about initializing the matrix below?\n\n\n\nmatrix.JPG\n\n\n\n# create a 5 x 5 1s matrix for the outer layer\noutputs =np.ones((5,5))\noutputs\n\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\n\n\n# create a 3 x 3 zero matric for the middle layer\nz = np.zeros ((3,3))\nz\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n# fil in middle element i.e row 1 column 1 with a 9\nz[1,1] = 9\nz\n\narray([[0., 0., 0.],\n       [0., 9., 0.],\n       [0., 0., 0.]])\n\n\n\n# replace outer layer with inner layers\noutputs[1:4,1:4] = z\noutputs\n\narray([[1., 1., 1., 1., 1.],\n       [1., 0., 0., 0., 1.],\n       [1., 0., 9., 0., 1.],\n       [1., 0., 0., 0., 1.],\n       [1., 1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#be-careful-when-copying-arrays",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#be-careful-when-copying-arrays",
    "title": "Numpy Tutorial",
    "section": "Be careful when copying arrays",
    "text": "Be careful when copying arrays\n\na = np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\n# if you equate two arrays then any changes impact BOTH\n# this is NOT making a copy!\nb = a\nb\n\narray([1, 2, 3])\n\n\n\n# To illustrate let's change the first element of b\nb[0] = 137\nb\n\narray([137,   2,   3])\n\n\n\n# note the first element in a has also changed!\na\n\narray([137,   2,   3])\n\n\nIn order to make a copy we have to use .copy:\n\na = np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\nb = a.copy()\nb\n\narray([1, 2, 3])\n\n\n\nb[0] = 137\nb\n\narray([137,   2,   3])\n\n\n\na\n\narray([1, 2, 3])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#mathematics",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#mathematics",
    "title": "Numpy Tutorial",
    "section": "Mathematics",
    "text": "Mathematics\n\nWe can carry out element wise arithmetic using NumPy\n\n\na = np.array([1,2,3,4])\na\n\narray([1, 2, 3, 4])\n\n\n\n# add 2 to every element within the array\na + 2\n\narray([3, 4, 5, 6])\n\n\n\n# deduct 2 from every element within the array\na - 2\n\narray([-1,  0,  1,  2])\n\n\n\n# multiply every element within the array by 2\na * 2\n\narray([2, 4, 6, 8])\n\n\n\n# divide every element within the array by 2\na / 2\n\narray([0.5, 1. , 1.5, 2. ])\n\n\n\nb = np.array([1,0,1,0])\na + b\n\narray([2, 2, 4, 4])\n\n\n\na ** 3\n\narray([ 1,  8, 27, 64])\n\n\n\n# Take the sin \nnp.sin(a)\n\narray([ 0.84147098,  0.90929743,  0.14112001, -0.7568025 ])\n\n\n\n# Take the cos\nnp.cos(a)\n\narray([ 0.54030231, -0.41614684, -0.9899925 , -0.65364362])\n\n\nFor a comprehensive outline of the mathematical operations possible using NumPy see the documentation.\n\nMatrix Multiplication\nNote that for matrix multiplication the number of rows of one of the matrices needs to match the number of columns of the other matrix:\n\n# 2 rows x 3 columns\na = np.ones ((2,3))\na\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\n# 3 rows x 2 columns\nb = np.full((3,2), 2)\nb\n\narray([[2, 2],\n       [2, 2],\n       [2, 2]])\n\n\n\n\n\n\n\n\nMatrix Multiplication\n\n\n\nMatrix multiplication can only be performed where the number of rows in Matrix A match the number of columns in Matrix B. For example we can multiply a 2 row x [3] column matrix by a [3] row x 2 column matrix. The resulting matrixc is a 2 x 2 matrix - the [3]s cancel out leaving the outer 2s.\n\n\n\n# Let's try to multiply together\na * b\n\nValueError: operands could not be broadcast together with shapes (2,3) (3,2) \n\n\nHold on - why is this not working?! Our matrix multiplication criteria is satisfied - the number of rows in matrix a = 2 which matches the number of columns in matrix b. Don’t panic! We just have to use the .matmul() function instead:\n\n# Let's try again using .matmul\nnp.matmul(a,b)\n\narray([[6., 6.],\n       [6., 6.]])\n\n\n\n\nDeterminant of a matrix\nThe determinant of a 2 × 2 matrix is:\n\nFor simplicity let’s use an identity matrix to illustrate. Recall that the identity matric has 1s on the leading diagonal and 0s elsewhere:\n\ni= np.identity(2)\ni \n\narray([[1., 0.],\n       [0., 1.]])\n\n\nAn identity matrix using the above formula should have a determinant of 1:\n\n# calc the determinant of identity matrix i\nnp.linalg.det(i)\n\n1.0\n\n\nThere are many other linear algebra operations that can be performed. See the documentation for more detail."
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#summary-statistics",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#summary-statistics",
    "title": "Numpy Tutorial",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nstats = np.array([[1,2,3],[4,5,6]])\nstats\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\nnp.min(stats)\n\n1\n\n\n\nnp.max(stats)\n\n6\n\n\n\nnp.mean(stats)\n\n3.5\n\n\n\nnp.std(stats)\n\n1.707825127659933\n\n\n\nnp.sum(stats)\n\n21\n\n\n\n# sum by row going downwards (axis = 0)\nnp.sum(stats, axis = 0)\n\narray([5, 7, 9])\n\n\n\n# sum by column going across (axis = 1)\nnp.sum(stats, axis = 1)\n\narray([ 6, 15])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#reorganizing-arrays",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#reorganizing-arrays",
    "title": "Numpy Tutorial",
    "section": "Reorganizing arrays",
    "text": "Reorganizing arrays\n\nbefore = np.array([[1,2,3,4],\n                   [5,6,7,8]])\nbefore\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nbefore.shape\n\n(2, 4)\n\n\nIn some cases we might want to change the shape of the array:\n\nafter = before.reshape(8,1)\nafter\n\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6],\n       [7],\n       [8]])\n\n\n\nafter = before.reshape(2,2,2)\nafter\n\narray([[[1, 2],\n        [3, 4]],\n\n       [[5, 6],\n        [7, 8]]])\n\n\n\nVertcally stacking vectors\n\nv1 = np.array([1,2,3,4])\nv2 = np.array([5,6,7,8])\n\n\nnp.vstack([v1,v2])\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\n# we can stack as much as we want any in any order\nnp.vstack([v1,v2,v2,v2,v1,v1,v2])\n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8],\n       [5, 6, 7, 8],\n       [5, 6, 7, 8],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\n\nHorizontally stacking vectors\n\nh1 = np.ones((2,4))\nh2 = np.zeros((2,2))\n\nh1\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\n\nh2\n\narray([[0., 0.],\n       [0., 0.]])\n\n\n\nnp.hstack([h1,h2])\n\narray([[1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 0.]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#other-use-cases",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#other-use-cases",
    "title": "Numpy Tutorial",
    "section": "Other use cases",
    "text": "Other use cases\n\n# Load data from file using .genfromtxt\n# automatically cast as float type\n\nfiledata = np.genfromtxt('NumPy.txt', delimiter=',')\nfiledata\n\narray([[  1.,  13.,  21.,  11., 196.,  75.,   4.,   3.,  34.,   6.,   7.,\n          8.,   0.,   1.,   2.,   3.,   4.,   5.],\n       [  3.,  42.,  12.,  33., 766.,  75.,   4.,  55.,   6.,   4.,   3.,\n          4.,   5.,   6.,   7.,   0.,  11.,  12.],\n       [  1.,  22.,  33.,  11., 999.,  11.,   2.,   1.,  78.,   0.,   1.,\n          2.,   9.,   8.,   7.,   1.,  76.,  88.]])\n\n\n\n# cast the data as int32\nfiledata = filedata.astype('int32')\nfiledata\n\narray([[  1,  13,  21,  11, 196,  75,   4,   3,  34,   6,   7,   8,   0,\n          1,   2,   3,   4,   5],\n       [  3,  42,  12,  33, 766,  75,   4,  55,   6,   4,   3,   4,   5,\n          6,   7,   0,  11,  12],\n       [  1,  22,  33,  11, 999,  11,   2,   1,  78,   0,   1,   2,   9,\n          8,   7,   1,  76,  88]], dtype=int32)\n\n\n\nBoolean masking and advanced indexing\n\n# this returns a boolean for every vlaue based on our condition\nfiledata > 50\n\narray([[False, False, False, False,  True,  True, False, False, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False,  True,  True, False,  True, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False,  True, False, False, False,  True,\n        False, False, False, False, False, False, False,  True,  True]])\n\n\nThis is good but not very helpful. We really want to extract the values:\n\n# this is better as this returns the values that meet our condition\nfiledata[filedata > 50]\n\narray([196,  75, 766,  75,  55, 999,  78,  76,  88], dtype=int32)\n\n\n\n\nIndexing with a List\n\na = np.array([1,2,3,4,5,6,7,8,9])\n\n# Let's grab 2, 3 and 9\na[[1,2,-1]]\n\nNameError: name 'np' is not defined\n\n\n\nnp.any(filedata > 50, axis = 0)\n\narray([False, False, False, False,  True,  True, False,  True,  True,\n       False, False, False, False, False, False, False,  True,  True])\n\n\n\nnp.any(filedata > 50, axis = 1)\n\narray([ True,  True,  True])\n\n\n\nnp.all(filedata > 50, axis = 0)\n\narray([False, False, False, False,  True, False, False, False, False,\n       False, False, False, False, False, False, False, False, False])\n\n\n\nnp.all(filedata > 50, axis = 1)\n\narray([False, False, False])\n\n\n\n(filedata > 50) & (filedata < 100)\n\narray([[False, False, False, False, False,  True, False, False, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False, False,  True, False,  True, False,\n        False, False, False, False, False, False, False, False, False],\n       [False, False, False, False, False, False, False, False,  True,\n        False, False, False, False, False, False, False,  True,  True]])\n\n\n\n# ~ means NOT and negates the condition specified\n(~((filedata > 50) & (filedata < 100)))\n\narray([[ True,  True,  True,  True,  True, False,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True, False,  True, False,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True],\n       [ True,  True,  True,  True,  True,  True,  True,  True, False,\n         True,  True,  True,  True,  True,  True,  True, False, False]])"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#wrap-up",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#wrap-up",
    "title": "Numpy Tutorial",
    "section": "Wrap up",
    "text": "Wrap up\nHow would we index the blue highlighted section of the matrix below:\n\n\n\nindexing_blue.JPG\n\n\nFirst we index the row range, and then the column range:\n\na[2:4, 0:2]\n\nHow would we index the green highlighted section of the matrix below:\n\n\n\nindexing_green.JPG\n\n\nWe can do this by using two different lists within our indexing. The first list contains the row indices and the second list contains the column indices:\n\na[ [0,1,2,3], [1,2,3,4] ]\n\nHow would we index the red highlighted section of the matrix below:\n\n\n\nindexing_red.JPG\n\n\nAgain, we can do this by using two different lists. The first list contains the required rows, and the second list contains the required column range:\n\na [[0,4,5], 3:]"
  },
  {
    "objectID": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#acknowledgements",
    "href": "posts/NumPy/Complete_Python_NumPy_Tutorial.html#acknowledgements",
    "title": "Numpy Tutorial",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks once again to Santiago for signposting this video posted by Keith Galli. This blog was written after interactively working through it.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QUT1VHiLmmI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
  },
  {
    "objectID": "posts/Paddy_Doctor/Paddy_Doctor.html",
    "href": "posts/Paddy_Doctor/Paddy_Doctor.html",
    "title": "Paddy Doctor: Paddy Disease Classification",
    "section": "",
    "text": "Problem Statement - identify the type of disease present in paddy leaf images\nRice (Oryza sativa) is one of the staple foods worldwide. Paddy, the raw grain before removal of husk, is cultivated in tropical climates, mainly in Asian countries. Paddy cultivation requires consistent supervision because several diseases and pests might affect the paddy crops, leading to up to 70% yield loss. Expert supervision is usually necessary to mitigate these diseases and prevent crop loss. With the limited availability of crop protection experts, manual disease diagnosis is tedious and expensive. Thus, it is increasingly important to automate the disease identification process by leveraging computer vision-based techniques that achieved promising results in various domains.\n\n\nObjective\nThe main objective of the competition is to develop a machine or deep learning-based model to classify the given paddy leaf images accurately. A training dataset of 10,407 (75%) labeled images across ten classes (nine disease categories and normal leaf) is provided. Moreover, also provided is additional metadata for each image, such as the paddy variety and age. Our task is to classify each paddy image in the given test dataset of 3,469 (25%) images into one of the nine disease categories or a normal leaf.\n\n\nApproach\nIn Iterate Like a Grandmaster Jeremy Howard explained that when working on a Kaggle project:\n\n…the focus generally should be two things:\n\nCreating an effective validation set\nIterating rapidly to find changes which improve results on the validation set\n\n\nHere we’re going to go further, showing the process he used to tackle the Paddy Doctor competition, leading to four submissions in a row which all were (at the time of submission) in 1st place, each one more accurate than the last. You might be surprised to discover that the process of doing this was nearly entirely mechanistic and didn’t involve any consideration of the actual data or evaluation details at all.\nThis notebook shows every step of the process. At the start of this notebook we’ll make a basic submission; by the end we’ll see how he got to the top of the table!:\n\nAs a special extra, also included is a selection of “walkthru” videos that were prepared for the new fast.ai course, and cover this competition:\n\nWalkthru 8\nWalkthru 9\nWalkthru 10\nWalkthru 11\nWalkthru 12\nWalkthru 13\n\n\n\nGetting set up\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\nFirst, we’ll get the data. There’s a new library called fastkaggle which has a few handy features, including getting the data for a competition correctly regardless of whether we’re running on Kaggle or elsewhere. Note we’ll need to first accept the competition rules and join the competition, and we’ll need our kaggle API key file kaggle.json downloaded if you’re running this somewhere other than on Kaggle. setup_comp is the function we use in fastkaggle to grab the data, and install or upgrade our needed python modules when we’re running on Kaggle:\n::: {.cell _kg_hide-output=‘true’ execution_count=10}\ncomp = 'paddy-disease-classification'\n\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n:::\n\npath\n\nPath('paddy-disease-classification')\n\n\nNow we can import the stuff we’ll need from fastai, set a seed (for reproducibility – just for the purposes of making this notebook easier to write; It’s not recommended to do that in your own analysis however) and check what’s in the data:\n\nfrom fastai.vision.all import *\nset_seed(42)\n\npath.ls()\n\n(#4) [Path('paddy-disease-classification/sample_submission.csv'),Path('paddy-disease-classification/test_images'),Path('paddy-disease-classification/train_images'),Path('paddy-disease-classification/train.csv')]\n\n\n\n\nLooking at the data\nThe images are in train_images, so let’s grab a list of all of them:\n\ntrn_path = path/'train_images'\nfiles = get_image_files(trn_path)\n\n…and take a look at one:\n\nimg = PILImage.create(files[0])\nprint(img.size)\nimg.to_thumb(128)\n\n(480, 640)\n\n\n\n\n\nLooks like the images might be 480x640 – let’s check all their sizes. This is faster if we do it in parallel, so we’ll use fastcore’s parallel for this:\nWatch out! In the imaging world images are represented by (columns, rows) however in the array/tensor world images are represented as (rows, columns). Pytorch would say size is (640, 480)!!\n\nfrom fastcore.parallel import *\n\n# create function to create a PILLOW image and get its size\n# speed up process using parallel \ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files, n_workers=8)\npd.Series(sizes).value_counts()\n\n(480, 640)    10403\n(640, 480)        4\ndtype: int64\n\n\nThey’re nearly all the same size, except for a few. Because of those few, however, we’ll need to make sure we always resize each image to common dimensions first, otherwise fastai won’t be able to create batches. For now, we’ll just squish them to 480x480 images, and then once they’re in batches we do a random resized crop down to a smaller size, along with the other default fastai augmentations provided by aug_transforms. We’ll start out with small resized images, since we want to be able to iterate quickly:\n\n# create our dataloader\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize(480, method='squish'), # resize to a 480 x 480 square using squish - change aspect ratio\n    batch_tfms=aug_transforms(size=128, min_scale=0.75))\n\n#  show_batch allows us to see or hear our data\ndls.show_batch(max_n=6)\n\n\n\n\n\n\nOur first model\nLet’s create a model. To pick an architecture, we should look at the options in The best vision models for fine-tuning. resnet26d is the fastest resolution-independent model which gets into the top-15 lists there.\n\nlearn = vision_learner(dls, 'resnet26d', metrics=error_rate, path='.').to_fp16()\n\nDownloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26d-69e92c46.pth\" to /home/stephen137/.cache/torch/hub/checkpoints/resnet26d-69e92c46.pth\n\n\nLet’s see what the learning rate finder shows:\n\n# puts through one mini-batch at a time, starting at a very low learning rate\n# gradually increase learning rate, see improvement, then once lr gets bigger worsens\nlearn.lr_find(suggest_funcs=(valley, slide))\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083, slide=0.0030199517495930195)\n\n\n\n\n\nlr_find generally recommends rather conservative learning rates, to ensure that your model will train successfully. I generally like to push it a bit higher if I can. Let’s train a few epochs and see how it looks:\n\n# let's fine tune for 3 epochs with a selected learning rate of 0.01 (10 ^ -2)\nlearn.fine_tune(3, 0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.774996\n      1.171467\n      0.378664\n      03:57\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.074707\n      0.791964\n      0.265257\n      04:52\n    \n    \n      1\n      0.786653\n      0.482838\n      0.144161\n      04:59\n    \n    \n      2\n      0.534015\n      0.414971\n      0.129265\n      05:00\n    \n  \n\n\n\nWe’re now ready to build our first submission!!! Let’s take a look at the sample Kaggle provided to see what it needs to look like:\n\n\nSubmitting to Kaggle\n\n# lets's have a look at the sample Kaggle submisison file\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n    \n  \n  \n    \n      0\n      200001.jpg\n      NaN\n    \n    \n      1\n      200002.jpg\n      NaN\n    \n    \n      2\n      200003.jpg\n      NaN\n    \n    \n      3\n      200004.jpg\n      NaN\n    \n    \n      4\n      200005.jpg\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3464\n      203465.jpg\n      NaN\n    \n    \n      3465\n      203466.jpg\n      NaN\n    \n    \n      3466\n      203467.jpg\n      NaN\n    \n    \n      3467\n      203468.jpg\n      NaN\n    \n    \n      3468\n      203469.jpg\n      NaN\n    \n  \n\n3469 rows × 2 columns\n\n\n\nOK so we need a CSV containing all the test images, in alphabetical order, and the predicted label for each one. We can create the needed test set using fastai like so:\n\n# create our test set\ntst_files = get_image_files(path/'test_images').sorted()\n\n# create a dataloader pointing at the test set - use dls.test_dl\n# key difference from normal dataloader is that it does not have any labels\ntst_dl = dls.test_dl(tst_files)\n\nWe can now get the probabilities of each class, and the index of the most likely class, from this test set (the 2nd thing returned by get_preds are the targets, which are blank for a test set, so we discard them):\n\n# get our precitions and indexes from our learner\n# decoded means rather than just get probability will get indexes of 0 to 9\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n\nTensorBase([4, 3, 3,  ..., 4, 8, 3])\n\n\nThese need to be mapped to the names of each of these diseases, these names are stored by fastai automatically in the vocab:\n\n# grab names of the diseases from the index vocab\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\nWe can create an apply this mapping using pandas:\n\n# map disease name to indexes\nmapping = dict(enumerate(dls.vocab)) # create a dictionary of the indexes and vocab\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping) # looks up the dictionary and returns the indexes, and name of indexes. Passing .map to a dictionary (mapping) is much fasster than passing to a function\nresults\n\n0       brown_spot\n1            blast\n2            blast\n3            blast\n4            blast\n           ...    \n3464         blast\n3465         blast\n3466    brown_spot\n3467        normal\n3468         blast\nName: idxs, Length: 3469, dtype: object\n\n\nKaggle expects the submission as a CSV file, so let’s save it, and check the first few lines:\n\n# replace 'label' column with our results\nss['label'] = results\nss.to_csv('subm.csv', index=False) \n!head subm.csv\n\nimage_id,label\n200001.jpg,brown_spot\n200002.jpg,blast\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,normal\n200007.jpg,blast\n200008.jpg,blast\n200009.jpg,hispa\n\n\nLet’s submit this to kaggle. We can do it from the notebook if we’re running on Kaggle, otherwise we can use the API:\n\n# function to submit to Kaggle\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial rn26d 128px', comp)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 62.9k/62.9k [00:01<00:00, 41.0kB/s]\n\n\nSuccess! We successfully created a submission, although it’s not very good (top 80% - or bottoms 20%!) but it only took a short time to train. The important thing is that we have a good starting point to iterate from, and we can do rapid iterations. Every step from loading the data to creating the model to submitting to Kaggle is all automated and runs quickly. Therefore, we can now try lots of things quickly and easily and use those experiments to improve our results.\n\n\nGoing faster\nI have noticed often when using Kaggle that the “GPU” indicator in the top right is nearly empty, and the “CPU” one is full. This strongly suggests that Kaggle’s notebook is CPU bound by decoding and resizing the images. This is a common problem on machines with poor CPU performance.\nWe really need to fix this, since we need to be able to iterate much more quickly. What we can do is to simply resize all the images to 40% of their height and width – which reduces their number of pixels 6.25x. This should mean an around 6.25x increase in performance for training small models.\nLuckily, fastai has a function which does exactly this, whilst maintaining the folder structure of the data: resize_images.\n\ntrn_path = Path('sml')\n\n\nresize_images(path/'train_images', dest=trn_path, max_size=256, recurse=True)\n\nThis will give us 192x256px images. Let’s take a look:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize((256,192)))\n\ndls.show_batch(max_n=3)\n\n\n\n\nIn this section we’ll be experimenting with a few different architectures and image processing approaches (item and batch transforms). In order to make this easier, we’ll put our modeling steps together into a little function which we can pass the architecture, item transforms, and batch transforms to:\n\ndef train(arch, item, batch, epochs=5):\n    dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)\n    learn = vision_learner(dls, arch, metrics=error_rate)\n    learn.fine_tune(epochs, 0.01)\n    return learn\n\nOur item_tfms already resize our images to small sizes, so this shouldn’t impact the accuracy of our models much, if at all. Let’s re-run our resnet26d to test.\n\nlearn = train('resnet26d', item=Resize(192),\n              batch=aug_transforms(size=128, min_scale=0.75))\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.915986\n      1.551140\n      0.477174\n      03:13\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.242299\n      1.098648\n      0.353676\n      04:09\n    \n    \n      1\n      0.969338\n      0.703203\n      0.231619\n      04:05\n    \n    \n      2\n      0.744738\n      0.554062\n      0.181643\n      04:04\n    \n    \n      3\n      0.532851\n      0.422054\n      0.135031\n      04:15\n    \n    \n      4\n      0.423329\n      0.404017\n      0.123979\n      04:10\n    \n  \n\n\n\nThat’s a big improvement in speed, and the accuracy looks fine.\n\n\nPyTorch Image Models (timm)\nPyTorch Image Models (timm) is a wonderful library by Ross Wightman which provides state-of-the-art pre-trained computer vision models. It’s like Hugging Face Transformers, but for computer vision instead of NLP (and it’s not restricted to transformers-based models)!\nRoss regularly benchmarks new models as they are added to timm, and puts the results in a CSV in the project’s GitHub repo. To analyse the data, we’ll first clone the repo:\n\n! git clone --depth 1 https://github.com/rwightman/pytorch-image-models.git\n%cd pytorch-image-models/results\n\nCloning into 'pytorch-image-models'...\nremote: Enumerating objects: 532, done.\nremote: Counting objects: 100% (532/532), done.\nremote: Compressing objects: 100% (367/367), done.\nremote: Total 532 (delta 222), reused 340 (delta 156), pack-reused 0\nReceiving objects: 100% (532/532), 1.30 MiB | 1.21 MiB/s, done.\nResolving deltas: 100% (222/222), done.\n/home/stephen137/Kaggle_Comp/pytorch-image-models/results\n\n\nUsing Pandas, we can read the two CSV files we need, and merge them together:\n\nimport pandas as pd\ndf_results = pd.read_csv('results-imagenet.csv')\n\nWe’ll also add a “family” column that will allow us to group architectures into categories with similar characteristics. Ross told Jeremy Howard which models he’s found the most usable in practice, so we’ll limit the charts to just look at these. (Also include is VGG, not because it’s good, but as a comparison to show how far things have come in the last few years.)\n\ndef get_data(part, col):\n    df = pd.read_csv(f'benchmark-{part}-amp-nhwc-pt111-cu113-rtx3090.csv').merge(df_results, on='model')\n    df['secs'] = 1. / df[col]\n    df['family'] = df.model.str.extract('^([a-z]+?(?:v2)?)(?:\\d|_|$)')\n    df = df[~df.model.str.endswith('gn')]\n    df.loc[df.model.str.contains('in22'),'family'] = df.loc[df.model.str.contains('in22'),'family'] + '_in22'\n    df.loc[df.model.str.contains('resnet.*d'),'family'] = df.loc[df.model.str.contains('resnet.*d'),'family'] + 'd'\n    return df[df.family.str.contains('^re[sg]netd?|beit|convnext|levit|efficient|vit|vgg|swin')]\n\n\ndf = get_data('infer', 'infer_samples_per_sec')\n\n\nInference results\nHere’s the results for inference performance (see the last section for training performance). In this chart:\n\nthe x axis shows how many seconds it takes to process one image (note: it’s a log scale)\nthe y axis is the accuracy on Imagenet\nthe size of each bubble is proportional to the size of images used in testing\nthe color shows what “family” the architecture is from.\n\nHover your mouse over a marker to see details about the model. Double-click in the legend to display just one family. Single-click in the legend to show or hide a family.\nNote: on my screen, Kaggle cuts off the family selector and some plotly functionality – to see the whole thing, collapse the table of contents on the right by clicking the little arrow to the right of “Contents”.\n\nimport plotly.express as px\nw,h = 1000,800\n\ndef show_all(df, title, size):\n    return px.scatter(df, width=w, height=h, size=df[size]**2, title=title,\n        x='secs',  y='top1', log_x=True, color='family', hover_name='model', hover_data=[size])\n\n\nshow_all(df, 'Inference', 'infer_img_size')\n\n\n                                                \n\n\nI noticed that the GPU usage bar in Kaggle was still nearly empty, so we’re still CPU bound. That means we should be able to use a more capable model with little if any speed impact. convnext_small tops the performance/accuracy tradeoff score there, so let’s give it a go!\n\n\n\nConvNeXT\nThe ConvNeXT model was proposed in A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them.\nThe abstract from the paper is the following:\n\nThe “Roaring 20s” of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.\n\nLet’s take a look at one of them…\n\n# choose our vision model architecture\narch = 'convnext_small_in22k'\n\n\n# feed chosen model into our learner\nlearn = train(arch, item=Resize(192, method='squish'),\n              batch=aug_transforms(size=128, min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.288349\n      0.913078\n      0.279673\n      05:54\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.646190\n      0.435760\n      0.138395\n      34:20\n    \n    \n      1\n      0.492490\n      0.374682\n      0.122057\n      34:16\n    \n    \n      2\n      0.316289\n      0.239387\n      0.075444\n      34:23\n    \n    \n      3\n      0.200733\n      0.164755\n      0.053340\n      34:13\n    \n    \n      4\n      0.134689\n      0.158538\n      0.050937\n      34:10\n    \n  \n\n\n\n\n# create our test set\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = learn.dls.test_dl(tst_files)\n\n\n# grab our predictions\nprobs,_,idxs = learn.get_preds(dl=tst_dl, with_decoded=True)\nidxs\n\n\n\n\n\n\n\n\nTensorBase([7, 8, 3,  ..., 8, 1, 5])\n\n\n\n# grab disease names from vocab\ndls.vocab\n\n['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n\n\n\n# map disease names to indexes\nmapping = dict(enumerate(dls.vocab)) # create a dictionary of the indexes and vocab\nresults = pd.Series(idxs.numpy(), name=\"idxs\").map(mapping) # looks up the dictionary and returns the indexes, and name of indexes. Passing .map to a dictionary (mapping) is much fasster than passing to a function\nresults\n\n0                       hispa\n1                      normal\n2                       blast\n3                       blast\n4                       blast\n                ...          \n3464               dead_heart\n3465                    hispa\n3466                   normal\n3467    bacterial_leaf_streak\n3468               dead_heart\nName: idxs, Length: 3469, dtype: object\n\n\n\n# lets's have a look at the sample Kaggle submisison file\nss = pd.read_csv(path/'sample_submission.csv')\nss\n\n\n\n\n\n  \n    \n      \n      image_id\n      label\n    \n  \n  \n    \n      0\n      200001.jpg\n      NaN\n    \n    \n      1\n      200002.jpg\n      NaN\n    \n    \n      2\n      200003.jpg\n      NaN\n    \n    \n      3\n      200004.jpg\n      NaN\n    \n    \n      4\n      200005.jpg\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3464\n      203465.jpg\n      NaN\n    \n    \n      3465\n      203466.jpg\n      NaN\n    \n    \n      3466\n      203467.jpg\n      NaN\n    \n    \n      3467\n      203468.jpg\n      NaN\n    \n    \n      3468\n      203469.jpg\n      NaN\n    \n  \n\n3469 rows × 2 columns\n\n\n\n\n# replace 'label' column with our results\nss['label'] = results\nss.to_csv('subm.csv', index=False) \n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\n# function to submit to Kaggle\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'initial convnext small in22k', comp)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 70.5k/70.5k [00:01<00:00, 50.3kB/s]\n\n\nExcellent. This improved model achiveved a public score of 0.95617, comfortably mid table. But, we can do even better:\n\n\nPre-processing experiments\nSo, what shall we try first? One thing which can make a difference is whether we “squish” a rectangular image into a square shape by changing it’s aspect ratio, or randomly crop out a square from it, or whether we add black padding to the edges to make it a square. In the previous version we “squished”.\nWe can also try padding, which keeps all the original image without transforming it – here’s what that looks like:\n\n# data augmentation using padding\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,\n    item_tfms=Resize(192, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros))\ndls.show_batch(max_n=3)\n\n\n\n\n\n# feed our learner \nlearn = train(arch, item=Resize((256,192), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n      batch=aug_transforms(size=(171,128), min_scale=0.75))\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.263865\n      0.892569\n      0.281115\n      07:24\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.659388\n      0.440261\n      0.138395\n      44:51\n    \n    \n      1\n      0.513566\n      0.397354\n      0.131667\n      44:49\n    \n    \n      2\n      0.339301\n      0.231382\n      0.067756\n      44:36\n    \n    \n      3\n      0.204870\n      0.158647\n      0.047093\n      44:34\n    \n    \n      4\n      0.134242\n      0.140719\n      0.044690\n      44:33\n    \n  \n\n\n\nThat’s looking like a pretty good improvement - an error_rate of 0.044690 against 0.050937.\n\n\nTest time augmentation\nTo make the predictions even better, we can try test time augmentation (TTA), which our book defines as:\n\nDuring inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image.\n\nBefore trying that out, we’ll first see how to check the predictions and error rate of our model without TTA:\n\nvalid = learn.dls.valid\npreds,targs = learn.get_preds(dl=valid)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n\n\n\n\n\n\n\n\n\n\nerror_rate(preds, targs)\n\nTensorBase(0.0509)\n\n\nThat’s the same error rate we saw at the end of training, above, so we know that we’re doing that correctly. Here’s what our data augmentation is doing – if you look carefully, you can see that each image is a bit lighter or darker, sometimes flipped, zoomed, rotated, warped, and/or zoomed:\n\nlearn.dls.train.show_batch(max_n=6, unique=True)\n\n\n\n\nIf we call tta() then we’ll get the average of predictions made for multiple different augmented versions of each image, along with the unaugmented original:\n\ntta_preds,_ = learn.tta(dl=valid)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nLet’s check the error rate of this:\n\nerror_rate(tta_preds, targs)\n\nTensorBase(0.0375)\n\n\nThat’s a huge improvement! We’re now ready to get our Kaggle submission sorted. First, we’ll grab the test set like we did in the last notebook:\n\ntst_files = get_image_files(path/'test_images').sorted()\ntst_dl = learn.dls.test_dl(tst_files)\n\nNext, do TTA on that test set:\n\npreds,_ = learn.tta(dl=tst_dl)\n\n\n\n\n\n\n    \n      \n      \n    \n    \n\n\n\n\n\n\n\n\n\nWe need to indices of the largest probability prediction in each row, since that’s the index of the predicted disease. argmax in PyTorch gives us exactly that:\n\nidxs = preds.argmax(dim=1)\n\nNow we need to look up those indices in the vocab. Last time we did that using pandas, although since then I realised there’s an even easier way!:\n\nvocab = np.array(learn.dls.vocab)\nresults = pd.Series(vocab[idxs], name=\"idxs\")\n\n\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = results\nss.to_csv('subm.csv', index=False)\n!head subm.csv\n\nimage_id,label\n200001.jpg,hispa\n200002.jpg,normal\n200003.jpg,blast\n200004.jpg,blast\n200005.jpg,blast\n200006.jpg,brown_spot\n200007.jpg,dead_heart\n200008.jpg,brown_spot\n200009.jpg,hispa\n\n\n\n# submit to Kaggle\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'convnext small 256x192 tta', comp)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 70.4k/70.4k [00:01<00:00, 44.9kB/s]\n\n\nThis submission scored 0.96309, improving on our previous submission score of 0.95617.\n\n\nIterative approach\nIt took a long time to train the ConvNeXT model (due to GPU constraints on my machine and also on Paperspace - it’s very rare for there to be any GPU’s available on the free subscription. I’ve since upgraded to ‘Pro’ which costs $8 pm at the time of writing). However you can see the significant improvements made by iterating, and the latest submission of 0.96309 would have been further improved by using larger images and more epochs.\n\n\n\nkaggle_submissions.JPG\n\n\n\n\nKey takeaways\nMost importantly, we have learned the importance of making an early submission to Kaggle, in order to obtain a baseline for rapid improvement through iterating. We’ve also learned some powerful data augmentation techniques, in particular test time augmentation (TTA), how to handle CPU bound environments by resizing images, and discovered the vision model playground that is timm."
  },
  {
    "objectID": "posts/Scaling up/Scaling_up.html",
    "href": "posts/Scaling up/Scaling_up.html",
    "title": "Scaling up",
    "section": "",
    "text": "Overview\nIn this analysis our goal will be to train an ensemble of larger models with larger inputs. The challenge when training such models is generally GPU memory. Kaggle GPUs have 16280MiB of memory available, as at the time of writing. I like to try out my notebooks on my home PC, then upload them – but we still need them to run OK on Kaggle (especially if it’s a code competition, where this is required). Just because it runs OK at home doesn’t mean it’ll run OK on Kaggle.\nI’m using PaperSpace (I recently upgraded to a ‘Pro’ subscription but capacity for GPUs is still quite limited.\nIt’s really helpful to be able to quickly try a few models and image sizes and find out what will run successfully. To make this quick, we can just grab a small subset of the data for running short epochs – the memory use will still be the same, but it’ll be much faster.\nOne easy way to do this is to simply pick a category with few files in it. Here’s our options:\nFirst we’ll repeat the steps we used last time to access the data and ensure all the latest libraries are installed, and we’ll also grab the files we’ll need for the test set:\n::: {.cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ tags=‘[]’}\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n!pip install timm\nimport timm\n:::\n\n!pip install -q kaggle\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n\nfrom fastai.vision.all import *\nset_seed(42)\n\ntst_files = get_image_files(path/'test_images').sorted()\n\n\n# load dataset\ndf = pd.read_csv(path/'train.csv')\ndf.label.value_counts()\n\nnormal                      1764\nblast                       1738\nhispa                       1594\ndead_heart                  1442\ntungro                      1088\nbrown_spot                   965\ndowny_mildew                 620\nbacterial_leaf_blight        479\nbacterial_leaf_streak        380\nbacterial_panicle_blight     337\nName: label, dtype: int64\n\n\n\n\nMemory and gradient accumulation\nGradient accumulation refers to a very simple trick: rather than updating the model weights after every batch based on that batch’s gradients, instead keep accumulating (adding up) the gradients for a few batches, and them update the model weights with those accumulated gradients. In fastai, the parameter you pass to GradientAccumulation defines how many batches of gradients are accumulated. Since we’re adding up the gradients over accum batches, we therefore need to divide the batch size by that same number. The resulting training loop is nearly mathematically identical to using the original batch size, but the amount of memory used is the same as using a batch size accum times smaller!\nFor instance, here’s a basic example of a single epoch of a training loop without gradient accumulation:\nfor x,y in dl:\n    calc_loss(coeffs, x, y).backward()\n    coeffs.data.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\nHere’s the same thing, but with gradient accumulation added (assuming a target effective batch size of 64):\ncount = 0            # track count of items seen since last weight update\nfor x,y in dl:\n    count += len(x)  # update count based on this minibatch size\n    calc_loss(coeffs, x, y).backward()\n    if count>64:     # count is greater than accumulation target, so do weight update\n        coeffs.data.sub_(coeffs.grad * lr)\n        coeffs.grad.zero_()\n        count=0      # reset count\nThe full implementation in fastai is only a few lines of code – here’s the source code. To see the impact of gradient accumulation, consider this small model:\nLet’s use bacterial_panicle_blight since it’s the smallest:\n\n# create a susbset to include only the smallest disease dataset \n# we don't really care about this model, we just want to know how much memory it uses\ntrn_path = path/'train_images'/'bacterial_panicle_blight'\n\nNow we’ll set up a train function which is very similar to the steps we used for training in the last notebook. But there’s a few significant differences…\nThe first is that I’m using a finetune argument to pick whether we are going to run the fine_tune() method, or the fit_one_cycle() method – the latter is faster since it doesn’t do an initial fine-tuning of the head. When we fine tune in this function I also have it calculate and return the TTA predictions on the test set, since later on we’ll be ensembling the TTA results of a number of models. Note also that we no longer have seed=42 in the ImageDataLoaders line – that means we’ll have different training and validation sets each time we call this. That’s what we’ll want for ensembling, since it means that each model will use slightly different data.\nThe more important change is that I’ve added an accum argument to implement gradient accumulation. As you’ll see in the code below, this does two things:\n\nDivide the batch size by accum\nAdd the GradientAccumulation callback, passing in accum.\n\n\n# create a function to train a model which includes a gradient accumulation (accum) argument, when set to 1 no impact on batch size\n# note no seed set so different training & validation sets each time we call it\ndef train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):\n    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item,\n        batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)\n    cbs = GradientAccumulation(64) if accum else []\n    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16()\n    if finetune:\n        learn.fine_tune(epochs, 0.01)\n        return learn.tta(dl=dls.test_dl(tst_files))\n    else:\n        learn.unfreeze()\n        learn.fit_one_cycle(epochs, 0.01)\n\n\n# try out our new training function on a small model\ntrain('convnext_small_in22k', 128, epochs=1, accum=1, finetune=False)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      00:13\n    \n  \n\n\n\nLet’s create a function to find out how much memory it used, and also to then clear out the memory for the next run:\n\nimport gc\n# create a function that tells us how much memory model uses\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache() # clear out memory for next run\n\n\n!pip install pynvml\nreport_gpu()\n\nRequirement already satisfied: pynvml in /usr/local/lib/python3.9/dist-packages (11.4.1)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nGPU:0\nprocess      26004 uses     3768.625 MB GPU memory\n\n\nSo with accum=1 the GPU used around 3.8GB RAM. Let’s try accum=2:\n\n# try out our new training function on a model\ntrain('convnext_small_in22k', 128, epochs=1, accum=2, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      00:12\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     2750.625 MB GPU memory\n\n\nAs you see, the RAM usage has now gone down to 2.75GB. It’s not halved since there’s other overhead involved (for larger models this overhead is likely to be relatively lower).\nLet’s try 4:\n\n# try out our new training function on a model\ntrain('convnext_small_in22k', 128, epochs=1, accum=4, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      00:12\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     2216.625 MB GPU memory\n\n\nThe memory use is even lower!\n\n\nChecking memory use\nWe’ll now check the memory use for each of the architectures and sizes we’ll be training later, to ensure they all fit in 8GB RAM. For each of these, I tried accum=1 first, and then doubled it any time the resulting memory use was over 8GB. As it turns out, accum=32 covered most of what I needed. swin_large_patch4_window7_224 and vit_large_patch16_224 were too large.\nFirst, convnext_large:\n\n# number 1\ntrain('convnext_large_in22k', 224, epochs=1, accum=16, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      01:55\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     5168.625 MB GPU memory\n\n\n\n# number 2\ntrain('convnext_large_in22k', (320,240), epochs=1, accum=32, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      03:37\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     4996.625 MB GPU memory\n\n\nHere’s vit_large. Not able to run this one, even when setting accum to 64!\n\n# number 3\ntrain('vit_large_patch16_224', 224, epochs=1, accum=64, finetune=False)\nreport_gpu()\n\nThen finally our swinv2 and swin models:\n\n# number 4\ntrain('swinv2_large_window12_192_22k', 192, epochs=1, accum=32, finetune=False)\nreport_gpu()\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n      0.000000\n      01:03\n    \n  \n\n\n\nGPU:0\nprocess      26004 uses     7780.625 MB GPU memory\n\n\n\n#number 5\ntrain('swin_large_patch4_window7_224', 224, epochs=1, accum=64, finetune=False)\nreport_gpu()\n\n\n\nRunning the models\nIn my previous blog, we tried a bunch of different architectures and preprocessing approaches on small models, and picked a few which looked good. We can use a dict to list the preprocessing approaches we’ll use for each architecture of interest based on that analysis:\n\nres = 640,480\n\n\n# create a dictionary of model preprocessing\nmodels = {\n    'convnext_large_in22k': {\n        (Resize(res), (320,224)),\n    }, 'vit_large_patch16_224': {\n        (Resize(480, method='squish'), 224),\n        (Resize(res), 224),\n    }, 'swinv2_large_window12_192_22k': {\n        (Resize(480, method='squish'), 192),\n        (Resize(res), 192),\n    }, 'swin_large_patch4_window7_224': {\n        (Resize(res), 224),\n    }\n}\n\nWe’ll need to switch to using the full training set of course!\n\n# set training set\ntrn_path = path/'train_images'\n\nNow we’re ready to train all these models. Remember that each is using a different training and validation set, so the results aren’t directly comparable.\nWe’ll append each set of TTA predictions on the test set into a list called tta_res.\n\n# display for each model in our above dictionary\n# architecture, data augmentations, loss and error rates\ntta_res = []\n\nfor arch,details in models.items():\n    for item,size in details:\n        print('---',arch)\n        print(size)\n        print(item.name)\n        tta_res.append(train(arch, size, item=item, accum=32)) #, epochs=1))\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n\n\nconvnext_large_in22k.JPG\n\n\n\n\n\nvit_large_patch16_224.JPG\n\n\n\n\n\nvit_large_patch16_224_square.JPG\n\n\n\n\n\nswinv2_large_window12_192_22k_square.JPG\n\n\n\n\n\nswinv2_large_window12_192_22k.JPG\n\n\n\n\n\nswin_large_patch4_window7_224.JPG\n\n\nSince this has taken quite a while to run, let’s save the results, just in case something goes wrong!\n\n# pickle the results for future use\nsave_pickle('tta_res.pkl', tta_res)\n\n\n\nEnsembling\nAs you can see from the above, each of the individual models score well, but an ensemble (which simply refers to a model which is itself the result of combining a number of other models) can produce even better results. The simplest way to do ensembling is to take the average of the predictions of each model:\nLearner.tta returns predictions and targets for each rows. We just want the predictions:\n\ntta_prs = first(zip(*tta_res))\n\nOriginally I just used the above predictions, but later I realised in my experiments on smaller models that vit was a bit better than everything else, so I decided to give those double the weight in my ensemble. I did that by simply adding to the list a second time (we could also do this by using a weighted average):\n\ntta_prs += tta_prs[1:3]\n\n\n# calculate average predictions of our ensemble of models\navg_pr = torch.stack(tta_prs).mean(0)\navg_pr.shape\n\ntorch.Size([3469, 10])\nThat’s all that’s needed to create an ensemble! Finally, we copy the steps we used in the last notebook to create a submission file:\n\ndls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\nidxs = avg_pr.argmax(dim=1)\nvocab = np.array(dls.vocab)\nss = pd.read_csv(path/'sample_submission.csv')\nss['label'] = vocab[idxs]\nss.to_csv('subm.csv', index=False)\n\n\n\nSubmit to Kaggle\n\nif not iskaggle:\n    from kaggle import api\n    api.competition_submit_cli('subm.csv', 'part 3 v2', comp)\n\nThat’s it – at the time of creating this analysis, that got easily to the top of the leaderboard! Here are the four submissions Jeremy entered, each of which was better than the last, and each of which was ranked #1:\n\nEdit: Actually the one that got to the top of the leaderboard timed out when Jeremy ran it on Kaggle Notebooks, so he had to remove four of the runs from the ensemble. There’s only a small difference in accuracy however.\nGoing from bottom to top, here’s what each one was:\n\nconvnext_small trained for 12 epochs, with TTA\nconvnext_large trained the same way\nThe ensemble in this notebook, with vit models not over-weighted\nThe ensemble in this notebook, with vit models over-weighted.\n\n\n\nConclusion\n\nRuntimeError: CUDA error: out of memory\n\nWe all know how frustrating the above error is, and the inevitable lament - “if only I had a better GPU spec :(”\nThe key takeaway from this blog is to remain calm when faced with the above error, and resist the temptation to go and splash out on an expensive new GPU card! It is often possible to scale up and train large models using a technique called gradient accumulation (despite apparent GPU constraints). We can then use a further technique called ensembling which involvees averaging the results of models with different architectures, and varying performance, to achieve an overall performance which is better than any of the models on their own."
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html",
    "href": "posts/Financial Forecasting/Financial Forecasting.html",
    "title": "Financial Forecasting in Python",
    "section": "",
    "text": "In Financial Forecasting in Python, we will step into the role of CFO and learn how to advise a board of directors on key metrics while building a financial forecast, the basics of income statements and Balance Sheets, and cleaning messy financial data. During this blog we will examine real-life datasets from Netflix, Tesla, and Ford, using the pandas package. Following this blog we will be able to calculate financial metrics, work with assumptions and variances, and build our own forecast in Python."
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#income-statements",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#income-statements",
    "title": "Financial Forecasting in Python",
    "section": "1. Income statements",
    "text": "1. Income statements\n\n1.1 Tesla Motors Inc.\nIn this example we have chosen to download the latest Income Statemet from Tesla Motors Inc. as a csv file. Let’s have a look at our raw data:\n\n\n\ntesla_raw.JPG\n\n\nTesla has a financial year end of 31 December and we have the results for the financial years 2017 to 2021, as well as an additional column headed TTM which stands for trailing twelve months which is the most recent 12 months of data available. We will be using this column together with the historical information to produce a forecast for the 2022 financial year. .\nThere are some problems with the data. We need to ensure the data is in the desire format and eliminate any headers we don’t want to use. This could be done manually of course but this would require editing the file every time we have new data. Much better to make use of Python, in particular the pandas library.\n\n# get required packages\nimport pandas as pd\n\n\n# load in our financials\nincome_statement = pd.read_csv('Data/Income Statement_Annual_As Originally Reported.csv')\n\nLet’s focus our atttention on four key metrics - ‘Gross Profit’, ‘Total Revenue, ’Operating expenses’, and ‘Net Income’. To do this we’ll create a filtered income statement to only show these rows. The filtering code uses the following pattern.\ndataframe[dataframe.columnname.isin(list_of_categories)]\n\n# Choose some interesting metrics\ninteresting_metrics = ['Total Revenue', 'Operating Expenses', 'Gross Profit', 'Net Income']\n\n# Filter for rows containing these metrics\nfiltered_income_statement = income_statement[income_statement.metric.isin(interesting_metrics)]\n\n# See the result\nfiltered_income_statement\n\n\n\n\n\n  \n    \n      \n      metric\n      2017\n      2018\n      2019\n      2020\n      2021\n      TTM\n    \n  \n  \n    \n      0\n      Gross Profit\n      2222.0\n      4042.0\n      4069.0\n      6630.0\n      13606.0\n      19923.0\n    \n    \n      1\n      Total Revenue\n      11759.0\n      21461.0\n      24578.0\n      31536.0\n      53823.0\n      74863.0\n    \n    \n      5\n      Operating Expenses\n      -3855.0\n      -4295.0\n      -3989.0\n      -4636.0\n      -7110.0\n      -7413.0\n    \n    \n      19\n      Net Income\n      -2241.0\n      -1063.0\n      -775.0\n      862.0\n      5644.0\n      11223.0\n    \n  \n\n\n\n\n\n\n1.2 Forecasting revenue for Tesla\nLet’s now append a new column with 2022 Forecast data, which we will assign the header “Forecast”. For this exercise, we would like to set the filtered_income_statement to only show the row ‘Revenue’.\nRemember, the TTM column is the most recent 12-month value that we will use for the 2022 forecast. Thus far, we have the following information for 2022:\nTotal revenues for the 9 months to 30 September 2022 are 57,144 USD millions, up 58% on the 9 months to September 2021, so let’s ignore any seasonality and make a very crude estimate of revenue for 2022 for illustrative purposes of say 80,000 USD millions:\n\nrevenue_metric = ['Total Revenue']\n\n# Filter for rows containing the revenue metric\nfiltered_income_statement = income_statement[income_statement.metric.isin(revenue_metric)]\n\n# Get the number of columns in filtered_income_statement\nn_cols = len(filtered_income_statement.columns)\n\n# Insert a column in the correct position containing the column 'Forecast'\nfiltered_income_statement.insert(n_cols, 'Forecast', 80000) \n\nfiltered_income_statement\n\n\n\n\n\n  \n    \n      \n      metric\n      2017\n      2018\n      2019\n      2020\n      2021\n      TTM\n      Forecast\n    \n  \n  \n    \n      1\n      Total Revenue\n      11759.0\n      21461.0\n      24578.0\n      31536.0\n      53823.0\n      74863.0\n      80000\n    \n  \n\n\n\n\nExcellent, we successfully built a new table to include the 2022 forecast from a raw dataset."
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#balance-sheet-and-forecast-ratios",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#balance-sheet-and-forecast-ratios",
    "title": "Financial Forecasting in Python",
    "section": "2. Balance Sheet and forecast ratios",
    "text": "2. Balance Sheet and forecast ratios\n\n2.1 Calculating accounts receivable (debtors)\nWhen we sell something on credit, the credit portion is in the balance sheet under ‘Accounts Receivable’ or ‘Debtors’. For example, if credit sales are made in January with a 60-day payback period, they would be recorded in our ‘Debtors’ account in January, but only be paid (released) in March, and so on.\nIn this exercise, we will create the following lists:\n\nThe credit sales in the month credits, which in this exercise is 60% of the sale value.\nThe total accounts receivable debtors, to be calculated as the credits for the current month, plus the credits of the month before, minus the credits of two months before (as we assume the credits from 2 months ago or 60 days, will be repaid by then).\n\nWe have set an index for the variable month. The month value is set at 0.\n\n# Create the list for sales, and empty lists for debtors and credits\nmonth = 0\nsales = [500, 350, 700]\ndebtors = [] \ncredits = []\n\n# Create the statement to append the calculated figures to the debtors and credits lists\nfor mvalue in sales: \n    credits.append(mvalue * 0.6)\n    if month > 0:\n        debtors.append(credits[month] + credits[month-1])\n    else:\n        debtors.append(credits[month]) \n    month += 1\n# Print the result\nprint(\"The ‘Debtors’ are {}.\".format(debtors))\n\nThe ‘Debtors’ are [300.0, 510.0, 630.0].\n\n\n\n\n2.2 Bad debts\nWhen offering credit terms to customers, there is always a risk that the customer does not pay their debt. In the finance world, this is known as “bad debts”.\nAs we have already recorded sales, we need to record the loss of sales now, as we never received the payment.\nThis affects both the income statement and the balance sheet. In the income statement, we record a negative value in the sales for the month we write off the debt. In the balance sheet, we need to reduce our debtor’s asset.\nThe following variables have been defined for January: debtors_jan = 1500\nIn February, we received news that a customer has gone into liquidation. This customer currently owes 500 USD.\nWe expect to recover 70% of this amount; the rest has to be written off as bad debts.\n\ndebtors_jan = 1500\n\n# Calculate the bad debts for February\nbad_debts_feb = 500 * 0.3\n\n# Calculate the feb debtors amount\ndebtors_feb = (debtors_jan- bad_debts_feb)\n\n# Print the debtors for January and the bad debts and the debtors for February\nprint(\"The debtors are {} in January, {} in February. February's bad debts are {} USD.\".format(debtors_jan, debtors_feb, bad_debts_feb))\n\nThe debtors are 1500 in January, 1350.0 in February. February's bad debts are 150.0 USD.\n\n\nYou can see that our debtors amount is reduced by the amount of bad debts.\n\n\n2.3 Calculating accounts payable (creditors)\nNow we will look at a scenario where we are the ones being granted credit. This means that we can buy something, but only have to pay for this amount later.\nIn this exercise, T-Z needs to buy nuts and bolts to produce 1000 units in January and 1200 units in February. The cost of nuts and bolts per unit is 0.25 USD. The credit terms are 50% cash upfront and 50% in 30 days.\nTherefore, the creditors’ value, in this case, would be paid the month directly after. This means that the creditors’ value would only reflect the current month’s credit purchases.\n\n# Set the cost per unit\nunit_cost = 0.25\n\n# Create the list for production units and empty list for creditors\nproduction = [1000,1200]\ncreditors = []\n\n# Calculate the accounts payable for January and February\nfor mvalue in production: \n    creditors.append(mvalue * unit_cost * 0.5)\n    \n# Print the creditors balance for January and February\nprint(\"The creditors balance for January and February are {} and {} USD.\".format(creditors[0], creditors[1]))\n\nThe creditors balance for January and February are 125.0 and 150.0 USD.\n\n\nAs we can see, the Balance Sheet shows us what our real cash situation looks like, as just because we made a sale does not mean money in the bank, and incurring an expense also does not mean we have to pay it right away!\n\n\n2.4 Debtor days ratio\n\n\n\ndebtors_days_ratio.JPG\n\n\nThe first ratio we will look at is debtor days. This ratio looks at how many days it takes to receive our money from our debtors. It is usually calculated over a period of 1 financial year.\nThe following information is available to you:\n\nSales for the year: 12,500 USD\nEnding Debtors balance: 650\n\n\n# Create the variables\ndebtors_end = 650\nsales_tot = 12500\n\n# Calculate the debtor days variable\nddays_ratio = (debtors_end/sales_tot) * 365\n\n# Print the result\nprint(\"The debtor days ratio is {}.\".format(ddays_ratio))\n\nThe debtor days ratio is 18.98.\n\n\n\n\n2.5 Days payable outstanding\n\n\n\ncreditors_days_ratio.JPG\n\n\nWe will now have a look at our accounts payable, or creditors, and a ratio called the Days Payable Outstanding (DPO).\nThis ratio is an efficiency ratio that measures the average number of days a company takes to pay its suppliers.\nT-Z wants to know its days payable outstanding and has asked you to calculate it.\n\n# Get the variables\ncogs_tot = 4000\ncreditors_end = 650\n\n# Calculate the days payable outstanding\ndpo = (creditors_end/cogs_tot)*365\n\n# Print the days payable outstanding\nprint(\"The days payable outstanding is {}.\".format(dpo))\n\nThe days payable outstanding is 59.3125.\n\n\n\n\n2.6 Days in inventory\n\n\n\ndays_in_inventory.JPG\n\n\nIn this exercise, we will calculate the time it takes for a company to turn inventory into sales (days in inventory or DII ratio) based on the following information:\n\ncogs_total = 4000\nav_inv = 1900\nsales_tot = 10000\nob_assets = 2000\ncb_assets = 7000\n\n\n# Calculate the dii ratio \ndii_ratio = (av_inv/cogs_tot)*365\n\n# Print the result\nprint(\"The DII ratio is {}.\".format(dii_ratio))\n\nThe DII ratio is 173.375.\n\n\n\n\n2.7 Asset Turnover\n\n\n\nasset_turnover.JPG\n\n\nIn this exercise, we will calculate the efficiency of a company’s assets by seeing how the company uses its assets to generate sales (asset turnover ratio):\n\n# Calculate the Average Assets\nav_assets = (ob_assets + cb_assets)/2\n\n# Calculate the Asset Turnover Ratio\nat_ratio = sales_tot/av_assets\n\n# Print the Asset Turnover Ratio\nprint(\"The asset turnover ratio is {}.\".format(at_ratio))\n\nThe asset turnover ratio is 2.2222222222222223.\n\n\nLet’s test our understanding of Balance Sheet ratios:\n\n\n\nunderstanding_ratios.JPG"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#balance-sheet",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#balance-sheet",
    "title": "Financial Forecasting in Python",
    "section": "3. Balance Sheet",
    "text": "3. Balance Sheet\n\n3.1 Calculating Balance Sheet ratios for Ford\nNow we will look at a real life example, Ford Inc, a company producing motor vehicles. We will first upload a dataset: balance_sheet with the data for Ford Inc’s Balance Sheet as at 31 December 2017. The sales and cost of sales figures have been provided for 2017 within the Key_Figures_Memo dataset.\nWe are only interested in one line on the balance sheet, the Receivables (another name for Debtors), and therefore need to create a filter for this. In this exercise, we will use boolean indexing to filter our dataset for Receivables in the metric column. We will first specify our metric of interest ('Receivables'), and then check whether the column of interest has this value in each row. This will generate a boolean series of True and False values. With this series, we can then filter our existing dataset.\nOnce we have filtered our dataset, we can retrieve the receivables values from the most recent time period and calculate the debtor days ratio.\n\n# read in the Ford Balance Sheet data\nbalance_sheet = pd.read_csv('Data/F-Balance-Sheet.csv')\n\n\n# Create the filter metric for Receivables\nreceivables_metric = ['Receivables']\n\n# Create a boolean series with your metric\nreceivables_filter = balance_sheet.metric.isin(receivables_metric)\n\n# Use the series to filter the dataset\nfiltered_balance_sheet = balance_sheet[receivables_filter]\nfiltered_balance_sheet \n\n\n\n\n\n  \n    \n      \n      metric\n      2013-12\n      2014-12\n      2015-12\n      2016-12\n      2017-12\n    \n  \n  \n    \n      6\n      Receivables\n      87309.0\n      92819.0\n      101975.0\n      57368.0\n      62809.0\n    \n  \n\n\n\n\n\n# bring in values for Sales and Cost of Sales\nsales=156776\ncogs=131332 \n\n\n# From previous step\nreceivables_metric = ['Receivables']\nreceivables_filter = balance_sheet.metric.isin(receivables_metric)\nfiltered_balance_sheet = balance_sheet[receivables_filter]\n\n# Extract the zeroth value from the last time period (2017-12)\ndebtors_end = filtered_balance_sheet['2017-12'].iloc[0]\n\n# Calculate the debtor days ratio\nddays = (debtors_end/sales) * 365\n\n# Print the debtor days ratio\nprint(\"The debtor day ratio is {:.0f}. A higher debtors days ratio means it takes longer to collect cash from debtors.\".format(ddays))\n\nThe debtor day ratio is 146. A higher debtors days ratio means it takes longer to collect cash from debtors.\n\n\nNext, we will learn how to append this information into our forecast.\n\n\n3.2 Forecasting the Balance Sheet for Ford\nNow that we have the ratios for Ford, the management wishes to improve them for the 2018 financial year.\nThere is one ratio in particular that management would like to be improved – the debtor days ratio, which is currently sitting at 146 days, while better than Nissan (244 days), is still much higher than Toyota, the industry leader (31 days).\nManagement would like to bring the debtor days down to below 100 days.\n\n# set target debtor days\nddays = 99\n\nSales are expected to increase by 10%. Calculate these forecasted sales, f_sales from the existing sales :\n\n# Calculate the forecasted sales \nf_sales = sales * 1.10\nf_sales\n\n172453.6\n\n\nWe can rearrange the debtor days formula to calculate the closing balance of debtors needed to achieve 99 debtor days :\n\n# Solve for the forecasted debtors' ending balance\nf_debtors_end = f_sales * ddays/ 365\n\nprint(\"If sales rise by 10% and the debtor days decrease to {:.0f} then the forecasted closing balance for debtors will be {:.0f}.\".format(ddays, f_debtors_end))\n\nIf sales rise by 10% and the debtor days decrease to 99 then the forecasted closing balance for debtors will be 46775.\n\n\nLet’s now append a column to include the forecasted debtors:\n\n# Get the number of columns in the filtered balance sheet\nn_cols = len(filtered_balance_sheet.columns)\n\n# Append a Forecast column of the forecasted debtors' end balance\nfiltered_balance_sheet.insert(n_cols, 'Forecast', f_debtors_end)\n\n# See the result\nfiltered_balance_sheet\n\n\n\n\n\n  \n    \n      \n      metric\n      2013-12\n      2014-12\n      2015-12\n      2016-12\n      2017-12\n      Forecast\n    \n  \n  \n    \n      6\n      Receivables\n      87309.0\n      92819.0\n      101975.0\n      57368.0\n      62809.0\n      46775.086027"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#financial-periods-and-how-to-work-with-them",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#financial-periods-and-how-to-work-with-them",
    "title": "Financial Forecasting in Python",
    "section": "4. Financial periods and how to work with them",
    "text": "4. Financial periods and how to work with them\n\n4.1 Converting quarters into months\nA company has a challenge in separating data into months. It has received the following data:\n\nQuarter 1 = 700\nQuarter 2 = 650\n\nThe split across the months within each quarter is equal. Our goal is to separate this data into a list format containing the amounts per month for the first two quarters.\n\n# Create a list for quarters and initialize an empty list qrtlist\nquarters = [700, 650]\nqrtlist = []\n\n# Create a for loop to split the quarters into months and add to qrtlist\nfor qrt in quarters:\n month = round(qrt / 3, 2)\n qrtlist = qrtlist + [month, month, month]\n \n# Print the result\nprint(\"The values per month for the first two quarters are {}.\".format(qrtlist))\n\nThe values per month for the first two quarters are [233.33, 233.33, 233.33, 216.67, 216.67, 216.67].\n\n\n\n\n4.2 Merging months into quarters\nNow we saw what to do when we wanted to split quarters into months for more detailed monthly information. But what happens when the opposite is true and we wish to combine data into fewer columns? This is typical when dealing with historical data, when monthly details may not be necessary, or when we need a highly consolidated version of the data for a report.\nThe key here is to create an index, and then only add to the quarter total quarter in cycles of 3, or until the length of the list. We can do this with the following code:\nif index % 3 == 0 or index == len(months):\nThis code checks whether the index divided by three yields a remainder of 0, or if the index is at the end of the list months. Thus, in a loop it will execute the specified code every three months or when it reaches the end of the list.\nThe monthly sales are already provided in the code as months, containing the sales from the first two quarters, as well as the first month of Q3. Our task is to generate a new list called quarters that contains the quarterly totals from the first three months (which includes the partial total of Q3).\n\n# Create a months list, as well as an index, and set the quarter to 0\nmonths = [100, 100, 150, 250, 300, 10, 20]\nquarter = 0\nquarters = []\nindex = 1\n\n# Create for loop for quarter, print result, and increment the index\nfor sales in months:\n    quarter += sales\n    if index % 3 == 0 or index == len(months):\n        quarters.append(quarter)\n        quarter = 0\n    index = index + 1\n    \nprint(\"The quarter totals are Q1: {}, Q2: {}, Q3: {}\".format(quarters[0], quarters[1], quarters[2]))\n\nThe quarter totals are Q1: 350, Q2: 560, Q3: 20\n\n\nLet’s have a look at a library that can help us more when working with dates.\n\n\n4.3 The datetime library\nSales area A in Europe and Sales area B in Australia have different date formats.\n\nSale A: 4000 on 14/02/2018\nSale B: 3000 on 2 March 2018\n\nIf we want to consolidate or compare sales periods, we need to convert to the same date format. We can easily do this by using the datetime library and the datetime.strptime(date_string, format) method, using the following directives:\n\n\n\ndate_time.JPG\n\n\n\n# Import the datetime python library\nfrom datetime import datetime\n\n# Create a dt_object to convert the first date and print the month result\ndt_object1 = datetime.strptime('14/02/2018', '%d/%m/%Y')\nprint(dt_object1)\n\n# Create a dt_object to convert the second date and print the month result\ndt_object2 = datetime.strptime('2 March 2018', '%d %B %Y')\nprint(dt_object2)\n\n2018-02-14 00:00:00\n2018-03-02 00:00:00\n\n\n\n\n4.4 Converting date formats - explicit\nLet’s revisut one of the dates from the previous exercise.\n\nSale A: 4000 on 14/02/2018\n\nWe used the datetime library to identify the day d, month m, and year y which could help us to identify data from datasets with different date formats. However, what about a scenario where we want to convert date formats into a specific format?\nIn this exercise we will convert Sale A from the format 14/02/2018 to the same date format as Sale B (i.e. 14 February 2018).\nWe can do this easily with built-in Python functions. To split a string we can use the .split()method:\n\n\n\nsplit_().JPG\n\n\n\n# Set the variable for the datetime to convert\ndt = '14/02/2018'\n\n# Create the dictionary for the month values\nmm = {'01': 'January', '02': 'February', '03': 'March'}\n\n# Split the dt string into the different parts\nday, month, year = dt.split('/')\n\n# Print the concatenated date string\nprint(day + ' ' + mm['02'] + ' ' + year)\n\n14 February 2018"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#tips-and-tricks-when-working-with-datasets",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#tips-and-tricks-when-working-with-datasets",
    "title": "Financial Forecasting in Python",
    "section": "5. Tips and tricks when working with datasets",
    "text": "5. Tips and tricks when working with datasets\n\n\n\nchallenges.JPG\n\n\n\n5.1 Working with datasets - month totals\nIn this exercise, we will be exploring a dataset that has multiple sales in one month. We will create a script that will enable us to identify dates within the same month, and combine them into a new month total, and append this to the table.\nWe will be using the dataset df, which represents data from one of our sales areas. Print it out in the console to have a look at the data. As you can see, there were two sales in March. We will combine these sales into a single month total. We can iterate over the dataset using the .iteritems() method.\n\n\n\niteritems().JPG\n\n\nWe will also be using the .split() method.\n\n\n\nsplit_().JPG\n\n\n\n# create a DataFrame to include our sales data\ndf = pd.DataFrame(columns=['Description','14-Feb', '19-Mar', '22-Mar'])\n\ndf.loc[0] = ['Sales', 3000, 1200, 1500]\ndf\n\n\n\n\n\n  \n    \n      \n      Description\n      14-Feb\n      19-Mar\n      22-Mar\n    \n  \n  \n    \n      0\n      Sales\n      3000\n      1200\n      1500\n    \n  \n\n\n\n\n\n# Set the index to start at 0\nindex = 0\n\n# Create the dictionary for the months\ntt = {'Jan': 0, 'Feb': 0, 'Mar': 0}\n\n\n# Create a for loop that will iterate the date and amount values in the dataset\nfor date, amount in df.iteritems():\n    # Create the if statement to split the day and month, then add it to the new tt variable\n    if index > 0: \n        day, month = date.split('-')\n        tt[month] +=float(amount[0])\n    index += 1\n\nprint(tt)\n\n{'Jan': 0, 'Feb': 3000.0, 'Mar': 2700.0}\n\n\n/tmp/ipykernel_127/3317898835.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for date, amount in df.iteritems():\n\n\n\n\n5.2 Working with datasets - combining datasets\nIn this example, we will be working with two datasets, df1 and df2. You will notice that they contain different date formatting.\nMore specifically, df1 specifies the month by the name (e.g. 02-Feb-18), whereas df2 specifies the month numerically (e.g. 06/01/2018). Additionally, df1 uses a hyphen (-) as a separator, whereas df2 uses a forward slash (/) as a separator.\nWe will be combining these two datasets to form a consolidated forecast for the quarter. To do this, we will need to parse the different date formats of df1 and df2.\n\n# create a DataFrame to include our sales data\ndf1 = pd.DataFrame(columns=['02-Feb-18', '15-Mar-18'])\n\ndf1.loc[0] = [3000, 1200]\ndf1\n\n\n\n\n\n  \n    \n      \n      02-Feb-18\n      15-Mar-18\n    \n  \n  \n    \n      0\n      3000\n      1200\n    \n  \n\n\n\n\n\n# create an empty dictionary containing total sales for each month initialized to 0\ntotals = {'Jan': 0, 'Feb': 0, 'Mar': 0}\n\n# create a dictionary containing the months (Jan, Feb, Mar) and corresponding numbers\ncalendar = {'01': 'Jan', '02': 'Feb', '03': 'Mar'}\n\n\n# Create a for loop to iterate over the items in the first dataset df1\nfor date, amount in df1.iteritems():\n        day, month, year = date.split('-')\n        totals[month] +=float(amount[0]) \n\n/tmp/ipykernel_127/1296325721.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for date, amount in df1.iteritems():\n\n\n\n# create a DataFrame to include our sales data\ndf2 = pd.DataFrame(columns=['06/01/2018', '14/02/2018'])\n\ndf2.loc[0] = [1000, 1200]\ndf2\n\n\n\n\n\n  \n    \n      \n      06/01/2018\n      14/02/2018\n    \n  \n  \n    \n      0\n      1000\n      1200\n    \n  \n\n\n\n\n\n# Create a for loop to iterate over the items in the second dataset df2\n# This time month will yield a a numerical reference, so we will need to use our calendar dictionary to add the amount to our totals dictionary.\nfor date, amount in df2.iteritems():\n        day, month, year = date.split('/')\n        totals[calendar[month]] += float(amount[0])\n\nprint(totals)\n\n{'Jan': 1000.0, 'Feb': 4200.0, 'Mar': 1200.0}\n\n\n/tmp/ipykernel_127/4206160929.py:3: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for date, amount in df2.iteritems():\n\n\n\n\n5.3 Exporting data\n\n\n\nexport.JPG"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#assumptions-and-variances-in-forecasts",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#assumptions-and-variances-in-forecasts",
    "title": "Financial Forecasting in Python",
    "section": "6. Assumptions and variances in forecasts",
    "text": "6. Assumptions and variances in forecasts\n\n6.1 Building sensitive forecast models\n\n\n\nforecasting_considerations.JPG\n\n\n\n\n\nassumptions_2.JPG\n\n\n\n\n6.2 Weighted probability\nTxs Tools, a company selling hardware tools, is looking to expand out of their home market A into Market B. They have done some market research, and have received the following numeric probabilities:\n\n\n\nweighted_prob.JPG\n\n\nTxs Tools will only be motivated to expand if they can have reasonable assurance that they will achieve sales of 400 or more. To manage the different forecast sales probabilities, Txs Tools have asked us to calculate the weighted probability.\n\n# Create the combined list for sales and probability\nsales_probability = ['0|0.05', '200|0.10', '300|0.40', '500|0.2', '800|0.25'] \nweighted_probability = 0\n\n# Create a for loop to calculate the weighted probability\nfor pair in sales_probability:\n    parts = pair.split('|')\n    weighted_probability += float(parts[0]) * float(parts[1]) # float converts to a floating point \n\n# Print the weighted probability result\nprint(\"The weighted probability is {}.\".format(weighted_probability))\n\nThe weighted probability is 440.0.\n\n\nHave a look at the calculated weighted probability. We can see it reflects a weighted value between the highest and lowest sales figures. The weighted probability is a technique to manage the uncertainty in Txs Tools sales forecasting, and can give a more balanced view on expected sales numbers as opposed to just going for the lowest or highest number.\n\n\n6.3 Market sentiment\nTxs Tools has forecast sales of 500 in January, with an expected increase of 5% per month for the rest of the quarter.\nHowever, this is dependent on the market sentiment. Based on historical trends, the following information has been provided:\n\nIf the market sentiment drops below 0.6 then the sales will only be realized at an increase of 2% per month.\nIf market sentiment increases above 0.8. then sales are expected to increase by 7%.\n\n\n# Create the computevariance function\ndef computevariance(amount, sentiment):\n if (sentiment < 0.6):\n  res = amount + (amount * 0.02)\n elif (sentiment > 0.8):\n  res = amount + (amount * 0.07)\n else:\n  res = amount + (amount * 0.05)\n return res\n\n\n# Compute the variance for jan, feb and mar\njan = computevariance(500, 0.5)\nfeb = computevariance(500, 0.65)\nmar = computevariance(500, 0.85)\n\nprint(\"The forecast sales considering variance due to market sentiment is {} for Jan, {} for Feb, and {} for Mar.\".format(jan, feb, mar))\n\nThe forecast sales considering variance due to market sentiment is 510.0 for Jan, 525.0 for Feb, and 535.0 for Mar.\n\n\n\n\n6.4 Dependencies and sensitivity\n\n\n\ndep_sens.JPG\n\n\n\n\n6.5 Assigning dependencies for sales and COGS\nTxs Tools have built a monthly forecast for their gross profit. This will rely on dependencies for Sales and COGS.\nSet the dependencies for sales and cogs based on the information below:\n\nSales dependency sales_dep: The sale price is the net price after 1 USD commission. Commissions paid increase from 1 USD per unit to 2 USD per unit for every unit above 350 units sold.\nCost dependency cost_dep: When sales per unit increase above 500 units, an additional production line needs to be used, causing an increase in the cost per unit above 500 of 2 USD per unit.\n\nThe baseline sale price per unit (base_sales_price) is 15 USD and the baseline cost per unit (base_cost_price) is 7 USD.\n\n# instantiate the base sales price\nbase_sales_price = 15\n\n# instantiate the sales \nsales = 750\n\n\n# Set the Sales Dependency\nif sales >= 350:\n    sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\nelse:\n    sales_dep = sales * base_sales_price\n\n# Print the results\nprint(\"The sales dependency is {} USD.\".format(sales_dep))\n\nThe sales dependency is 10850 USD.\n\n\n\n# instantiate the bases cost price \nbase_cost_price = 7\n\n\n# Set the Cost Dependency\nif sales >= 500:\n    cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\nelse:\n    cost_dep = sales * base_cost_price\n    \n# Print the results\nprint(\"The cost dependency is {} USD.\".format(cost_dep))\n\nThe cost dependency is 5750 USD.\n\n\n\n\n6.6 Building a sensitivity analysis for gross profit\nxs Tools is now ready to use these dependencies in the gross profit forecast.\nThe following forecast unit sales have been provided:\nJul = 700 Aug = 350 Sep = 650\nThe dependencies for sales and cogs are based on the following:\n\nSales dependency sales_dep: The sale price is the net price after 1 USD commission. Commissions paid increase from 1 USD per unit to 2 USD per unit for every unit above 350 units sold.\nCost dependency cost_dep: When sales per unit increase above 500 units, an additional production line needs to be used, causing an increase in the cost per unit above 500 of 2 USD per unit.\n\nThe basic cost price base_cost_price = 7 and basic sales price base_sales_price = 15\n\n# Create the sales_usd list\nsales_usd = [700, 350, 650]\n\n\n# Create the if statement to calculate the forecast_gross_profit\nfor sales in sales_usd:\n    if sales > 350:\n        sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n    else:\n        sales_dep = sales * base_sales_price\n    if sales > 500:\n        cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n    else:\n        cost_dep = sales * base_cost_price\n    forecast_gross_profit = sales_dep - cost_dep\n\n    # Print the result\n    print(\"The gross profit forecast for a sale unit value of {} is {} USD.\".format(sales, forecast_gross_profit))\n\nThe gross profit forecast for a sale unit value of 700 is 4850 USD.\nThe gross profit forecast for a sale unit value of 350 is 2800 USD.\nThe gross profit forecast for a sale unit value of 650 is 4600 USD.\n\n\n\n\n6.7 Assigning dependencies for expenses\nTxs Tools wants to assign a dependency for its operating expenses, particularly admin salaries.\nThe conditions are as follows:\n\nAdmin expenses increase in July and August (Jul and Aug) as temporary workers need to be hired to cover the summer holiday.\nThe increase is based on the number of employees taking holidays during that time. For the current year, the value for August is emp_leave = 6 (6 employees expected to take leave).\nThe cost is 80 USD per temp employee hired.\n\n\n# instantiate the emp leave value for August\nemp_leave = 6\n\n\n# Set the admin dependency\nif emp_leave > 0:\n    admin_dep = emp_leave * 80\n\n# Print the results\nprint(\"The admin dependency for August is {} USD.\".format(admin_dep))\n\nThe admin dependency for August is 480 USD.\n\n\n\n\n6.8 Build a sensitivity analysis for the net profit\nTxs Tools has provided the following forecast admin cost in USD based on full-time employees:\nJul = 1500 Aug = 1500 Sep = 1500\nBuild the forecast net profit forecast_net_profit when emp_leave = [6, 6, 0] and the cost per temp employee is 80 USD.\n\n# instantiate our standing data\nadmin_usd = [1500, 1500, 1500]\nemp_leave = [6, 6, 0]\nforecast_gross_profit = [4850, 2800, 4600]\n\n\n# Create an index variable and initialize this index to 0\nindex = 0\n\n\n# Create the dependency by looping through the admin_usd list, using our index to access the correct month in our lists.\nfor admin in admin_usd:\n    temp = emp_leave[index]\n    if temp > 0:\n        admin_dep = temp * 80 + admin\n    else: \n         admin_dep = admin\n    forecast_net_profit = forecast_gross_profit[index] - admin_dep\n    print(forecast_net_profit)\n    index += 1\nprint(\"The forecast net profit is: {} USD.\".format(forecast_net_profit))\n\n2870\n820\n3100\nThe forecast net profit is: 3100 USD.\n\n\n\n\n6.9 Working with variances in the forecast\nIdentifying, quantifying, and investigating the difference between an old forecast and the new forecast is often referred to as Gap Analysis.\n\n\n\ngap_analysis.JPG\n\n\n\nBuilding an alternate forecast\n\n\n\nalternative_forecasts.JPG\n\n\nWe will now build an alternative forecast for Txs Tools. The new quarter forecast is based off actual data for Jul - Aug as well as adjusted forecast data for September. The data (units sold) is as follows:\n\nJul = 700\nAug = 220\nSep = 520\n\nThe dependencies calculations have already been completed from the previous exercise. The following information applies:\n\nbase_cost_price = 7\nbase_sales_price = 15\n\n\n# create a dependencies() function for sales and costs with the arguments base_cost_price,base_sales_price, and sales_usd\n# Pass the arguments into the function in this order.\n\ndef dependencies(base_cost_price, base_sales_price, sales_usd):\n    res = []\n    for sales in sales_usd:\n        if sales >= 350:\n            sales_dep = (350 * base_sales_price) + ((sales - 350) * (base_sales_price - 1))\n        else:\n            sales_dep = sales * base_sales_price\n        if sales >= 500:\n            cost_dep = (500 * base_cost_price) + ((sales - 500) * (base_cost_price + 2))\n        else:\n            cost_dep = sales * base_cost_price\n        res.append(sales_dep - cost_dep)\n    return res\n\n\n# Create scenario forecast1 for the original forecast\nforecast1 = dependencies(7, 15, [700, 350, 650])\n\n# Create scenario forecast2 for the alternative forecast. \n# Use the data provided above to calculate the alternative forecast\nforecast2 = dependencies(7, 15, [700, 220, 520])\n\n\nprint(\"The original forecast scenario is {}:\".format(forecast1))\nprint(\"The alternative forecast scenario is {}:\".format(forecast2))\n\nThe original forecast scenario is [4850, 2800, 4600]:\nThe alternative forecast scenario is [4850, 1760, 3950]:\n\n\n\n\n\n6.10 Building a gap analysis between forecasts\nTxs Tools now has two forecasts, the original forecast forecast1 and the adjusted forecast forecast2.\nThe dependencies have already been defined as def dependencies(base_cost_price, base_sales_price, sales_usd), where base_cost_price = 7 and base_sales_price = 15, with forecast2 based off the following adjusted sales unit values:\n\nJul = 700\nAug = 220\nSep = 520\n\nIn this exercise, we will look at how to use a for loop to cycle between two different lists, forecast1 and forecast2 and calculate the difference (“gap”) using an incremented index. It is possible to do this simultaneously as both lists have the same length.\n\n# Set the two results\nforecast1 = dependencies(7, 15, [700, 350, 650])\nforecast2 = dependencies(7, 15, [700, 220, 520])\n\n# Create an index and the gap analysis for the forecast\nindex = 0\nfor value in forecast2:\n    print(\"The gap between forecasts is {}\".format(value - forecast1[index]))\n    index += 1\n\nThe gap between forecasts is 0\nThe gap between forecasts is -1040\nThe gap between forecasts is -650\n\n\nYou can see how easy it is to use a for loop to compare results across different lists.\nNote that the gap between forecasts is driven purely by the difference in sales volume - base sales and cost prices are unchanged.\n\nIn July forecast2 sales are as per forecast 1 - so no gap.\nIn August forecast2 sales are 220 units against 350 - resulting in a gap of 130 units x profit per unit of 8 (15 - 7) which is 1040\nIn September forecast2 sales are 520 units against 650 - resulting in a gap of 130 units x profit per unit of 8 (15 - 7) which is 1040, but we also have a saving of 3 per unit (sales commission 1 and additional production line cost 2) which reduces the gap by 130 x 3 = 390 to 650.\n\n\n\n6.11 Setting dependencies for Netflix\nNetflix compiled a forecast up to the 2019 financial year netflix_f_is, and has based the sales figures in 2019 on the following dependency:\n\nNumber of active subscriptions, which are based on the success of Netflix original shows.\n\nFor 2019, the success of original shows (critical and commercial acclaim) are estimated at 78%. The total amount of subscribers per percentage point is 500, and set to the variable n_subscribers_per_pp (i.e there is a calculated correlation between show success and number of subscribers).\nIn this exercise, we will calculate how dependent sales are on the number of subscribers in the forecast, which we will use in the next exercise.\n\n# instantiate subscribers per % point\nn_subscribers_per_pp = 500\n\n# load in Netflix financials\nnetflix_f_is = pd.read_csv('Data/Netflix.csv')\nnetflix_f_is\n\n\n\n\n\n  \n    \n      \n      metric\n      2014_act\n      2015_act\n      2016_act\n      2017_fc\n      2018_fc\n      2019_fc\n    \n  \n  \n    \n      0\n      Sales\n      5505\n      6780\n      8831\n      11688\n      14979\n      17994\n    \n    \n      1\n      EBITDA\n      528\n      493\n      611\n      1088\n      1899\n      2943\n    \n    \n      2\n      Operating profit (EBIT)\n      403\n      306\n      380\n      837\n      1660\n      2702\n    \n    \n      3\n      Net income\n      267\n      123\n      187\n      559\n      1024\n      1721\n    \n  \n\n\n\n\n\n# Create a filter to select the sales row from the netflix_f_is dataset\nsales_metric = ['Sales']\n\n# Filter for rows containing the Sales metric\nfiltered_netflix_f_is = netflix_f_is[netflix_f_is.metric.isin(sales_metric)]\n\n# Extract the 2019 Sales forecast value\nforecast1 = netflix_f_is['2019_fc'].iloc[0]\n\n# Print the resulting forecast\nprint(\"The sales forecast is {}.\".format(forecast1))\n\nThe sales forecast is 17994.\n\n\n\n# Set the success percentage to 78%\npct_success = 0.78\n\n# Calculate the dependency for the subscriber base\nn_subscribers = n_subscribers_per_pp * pct_success\n\n# See the result\nprint(\"The dependency for the subscriber base is {}.\".format(n_subscribers))\n\nThe dependency for the subscriber base is 390.0.\n\n\n\n# Calculate the ratio between forecast sales and subscribers\nsales_subs_ratio = forecast1 / n_subscribers\n\n# See the result\nprint(\"The ratio between subscribers and sales is 1 subscriber equals ${:.2f}.\".format(sales_subs_ratio))\n\nThe ratio between subscribers and sales is 1 subscriber equals $46.14.\n\n\n\n\n6.12 Calculating an alternative forecast for Netflix\nThe original assumptions are as follows: the total amount of subscribers at a 78% success rate results in 39,000 subscribers. We used this to build the forecast numbers.\nHowever, the success rate for 2019 has been recalculated to have a probability of 65%, and the management has asked us to make an adjusted forecast based on this value.\nThe ratio between the subscribers and sales is 1 subscriber to 0.46 USD sales, set to variable sales_subs_ratio.\n\n# instantiate sales subs ratio\nsales_subs_ratio = 0.46\n\n\n# Set the proportion of successes to 65%\npct_success2 = 65\n\n# Calculate the number of subscribers\nn_subscribers2 = n_subscribers_per_pp * pct_success2 \n\n# Calculate the new forecast\nforecast2 = n_subscribers2  * sales_subs_ratio\nforecast2\n\n14950.0\n\n\n\n# Insert a column named AltForecast, containing forecast2\nfiltered_netflix_f_is.insert(len(filtered_netflix_f_is.columns), 'AltForecast', forecast2)\n\n# Insert a column named Gap, containing the difference\nfiltered_netflix_f_is.insert(len(filtered_netflix_f_is.columns), 'Gap', forecast1 - forecast2)\n\n# See the result\nfiltered_netflix_f_is\n\n\n\n\n\n  \n    \n      \n      metric\n      2014_act\n      2015_act\n      2016_act\n      2017_fc\n      2018_fc\n      2019_fc\n      AltForecast\n      Gap\n    \n  \n  \n    \n      0\n      Sales\n      5505\n      6780\n      8831\n      11688\n      14979\n      17994\n      14950.0\n      3044.0"
  },
  {
    "objectID": "posts/Financial Forecasting/Financial Forecasting.html#key-takeaways",
    "href": "posts/Financial Forecasting/Financial Forecasting.html#key-takeaways",
    "title": "Financial Forecasting in Python",
    "section": "Key takeaways",
    "text": "Key takeaways\nWe learned how to harnass Python, and in particular the pandas library to wrangle raw financial data, and extract relevant information to calculate key metrics.\nWe also learned how to handle date inconsistencies using the datetime library, parse dates using the split() method, and how to automate our work by writing functions and using for loops and .iteritems.\nAutomating the financial forecasting process allows fast iterations over different scenarios, saving time and reducing the scope of manual error."
  },
  {
    "objectID": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html",
    "href": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "This is my follow up to the second part of Lesson 7: Practical Deep Learning for Coders 2022 in which Jeremy shows how to build a Collaborative Filtering model from scratch, within Excel, and also using PyTorch, and explains latent factors and emdedding"
  },
  {
    "objectID": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html#recommendation-systems",
    "href": "posts/Collaborative Filtering/collaborative-filtering-deep-dive.html#recommendation-systems",
    "title": "Collaborative Filtering",
    "section": "Recommendation Systems",
    "text": "Recommendation Systems\nOne very common problem to solve is when you have a number of users and a number of products, and you want to recommend which products are most likely to be useful for which users. There are many variations of this: for example, recommending movies (such as on Netflix), figuring out what to highlight for a user on a home page, deciding what stories to show in a social media feed, and so forth. There is a general solution to this problem, called collaborative filtering, which works like this: look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend other products that those users have used or liked.\nFor example, on Netflix you may have watched lots of movies that are science fiction, full of action, and were made in the 1970s. Netflix may not know these particular properties of the films you have watched, but it will be able to see that other people that have watched the same movies that you watched also tended to watch other movies that are science fiction, full of action, and were made in the 1970s. In other words, to use this approach we don’t necessarily need to know anything about the movies, except who like to watch them.\nThere is actually a more general class of problems that this approach can solve, not necessarily involving users and products. Indeed, for collaborative filtering we more commonly refer to items, rather than products. Items could be links that people click on, diagnoses that are selected for patients, and so forth.\nThe key foundational idea is that of latent factors. In the Netflix example, we started with the assumption that you like old, action-packed sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to its movies table saying which movies are of these types. Still, there must be some underlying concept of sci-fi, action, and movie age, and these concepts must be relevant for at least some people’s movie watching decisions.\n\nThis is chapter 8 of the book Practical Deep Learning for Coders, provided courtesy of O’Reilly Media. The full book is available as Jupyter Notebooks. A free course that covers the book is available here.\n\nFor this chapter we are going to work on this movie recommendation problem. We’ll start by getting some data suitable for a collaborative filtering model.\n\nA First Look at the Data\n\n# load required packages and set seed for reproducibility\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\nset_seed(42)\n\nWe do not have access to Netflix’s entire dataset of movie watching history, but there is a great dataset that we can use, called MovieLens. This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If you’re interested, it would be a great learning project to try and replicate this approach on the full 25-million recommendation dataset, which you can get from their website.\nThe dataset is available through the usual fastai function:\n\n# download data\npath = untar_data(URLs.ML_100k)\n\nAccording to the README, the main table is in the file u.data. It is tab-separated and the columns are, respectively user, movie, rating, and timestamp. Since those names are not encoded, we need to indicate them when reading the file with Pandas. Here is a way to open this table and take a look:\n\n# load in table - specify colums names\nratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None, # tab(t) separated file, instead of a comma(c) separated file\n                      names=['user','movie','rating','timestamp']) # need to specify columns as not encoded\n\n# look at the first 5 rows\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nAlthough this has all the information we need, it is not a particularly helpful way for humans to look at this data. Here is the same data cross-tabulated into a human-friendly table:\n\n\n\nimage.png\n\n\nWe have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those are the places where a user has not reviewed the movie yet, presumably because they have not watched it. For each user, we would like to figure out which of those movies they might be most likely to enjoy.\nIf we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and +1, with positive numbers indicating stronger matches and negative numbers weaker ones, and the categories are science-fiction, action, and old movies, then we could represent the movie The Last Skywalker as:\n\n# embed features of the movie The Last Skywalker by creating vector of values between -1 and +1\n# science fiction 0.98, action 0.9, old movies -0.9\nlast_skywalker = np.array([0.98,0.9,-0.9])\n\nHere, for instance, we are scoring very science-fiction as 0.98, very action as 0.9, and very not old as -0.9. We could represent a user who likes modern sci-fi action movies as:\n\n# embed the features of a user based on their movie preferences by creating vector of values between -1 and +1\n# science fiction 0.9, action 0.8, old movies -0.6\nuser1 = np.array([0.9,0.8,-0.6])\n\nand we can now calculate the match between this combination:\n\n# calculate the dot product of the two vectors to see whether LastSkywalker is a good match for user 1\n(user1*last_skywalker).sum()\n\n2.1420000000000003\n\n\nWhen we multiply two vectors together and add up the results, this is known as the dot product. It is used a lot in machine learning, and forms the basis of matrix multiplication. We will be looking a lot more at matrix multiplication and dot products later.\n\njargon: dot product: The mathematical operation of multiplying the elements of two vectors together, and then summing up the result.\n\nOn the other hand, we might represent the movie Casablanca as:\n\n# embed features of the movie Casablanca by creating vector of values between -1 and +1\n# science fiction 0.98, action 0.9, old movies -0.9\ncasablanca = np.array([-0.99,-0.3,0.8])\n\nThe match between this combination is:\n\n# calculate the dot product of the two vectors to see whether Casabalance is a good match for user 1\n(user1*casablanca).sum()\n\n-1.611\n\n\nSince we don’t know what latent factors actually are, and we don’t know how to score them for each user and movie, we should learn them.\n\n\nCollaborative filtering - using Excel\nThe problem is we haven’t been given any information about the users, or the movies, and we might not even know what things about movies actually matter to users. But, not to worry, we can just use Stochastic Gradient Descent (SGD) to find them!\nThere is surprisingly little difference between specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general gradient descent approach.\n\nStep 1: randomly initialize some parameters\n\nThese parameters will be a set of latent factors for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes let’s use 5 for now. Because each user will have a set of these factors and each movie will have a set of these factors, we can show these randomly initialized values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle.\nSo, the initialized latent factors for movieId 27 are 0.71, 0.81, 0.74, 0.04, 0.04 and the latent factors for userID 14 are 0.19, 0.63, 0.31, 0.44, 0.51. We then multiply these together using the MMULT matrix multiplication function within Excel to obtain our initial predictions.\nWe don’t know what these factors are, but for example we can interpret that userID 14 doesn’t feel very strongly, with a value of 0.19 about movieID factor 1 which has a value of 0.71\nThis is what it looks like in Microsoft Excel:\n\n\n\nlatent_factors.JPG\n\n\n\nStep 2: Calculate our predictions using Matrix Multiplication\n\nAs we’ve discussed, we can do this by simply taking the dot product of each movie with each user. If, for instance, the first latent user factor represents how much the user likes action movies, and the first latent movie factor represents if the movie has a lot of action or not, the product of those will be particularly high if either the user likes action movies and the movie has a lot of action in it or the user doesn't like action movies and the movie doesn't have any action in it. On the other hand, if we have a mismatch (a user loves action movies but the movie isn’t an action film, or the user doesn’t like action movies and it is one), the product will be very low.\n\n\n\ninitialized.JPG\n\n\n\nStep 3: calculate our loss\n\nWe can use any loss function that we wish; let’s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction.\n\n\n\ninitialized.JPG\n\n\n\nStep 4: optimize using Stochastic Gradient Descent(SGD) - the Solver function in Excel approximates this\n\nThat’s all we need. With this in place, we can optimize our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimize the loss. At each step, the stochastic gradient descent optimizer will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie. It will then calculate the derivative of this value and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better.\n\n\n\noptimized.JPG\n\n\nThe above spreadsheet screenshot shows the updated predictions after applying Stohastic Gradient Descent using Excel’s inbuilt Solver function - note that the movie rating predictions are now much more in line with the actual ratings (with values betwen 0 and 5) and our loss function RMSE has reduced from 2.8 to 0.42.\n\n\nUsing PyTorch to do the same thing\nTo use the usual Learner.fit function we will need to get our data into a DataLoaders, so let’s focus on that now.\nWhen showing the data, we would rather see movie titles than their IDs. The table u.item contains the correspondence of IDs to titles:\n\n# load in movie titles table\nmovies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1', #\n                     usecols=(0,1), names=('movie','title'), header=None)\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movie\n      title\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n    \n    \n      1\n      2\n      GoldenEye (1995)\n    \n    \n      2\n      3\n      Four Rooms (1995)\n    \n    \n      3\n      4\n      Get Shorty (1995)\n    \n    \n      4\n      5\n      Copycat (1995)\n    \n  \n\n\n\n\nWe can merge this with our ratings table to get the user ratings by title:\n\n# merge ratings and movie tables\nratings = ratings.merge(movies)\nratings.head()\n\n\n\n\n\n  \n    \n      \n      user\n      movie\n      rating\n      timestamp\n      title\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n      Kolya (1996)\n    \n    \n      1\n      63\n      242\n      3\n      875747190\n      Kolya (1996)\n    \n    \n      2\n      226\n      242\n      5\n      883888671\n      Kolya (1996)\n    \n    \n      3\n      154\n      242\n      3\n      879138235\n      Kolya (1996)\n    \n    \n      4\n      306\n      242\n      5\n      876503793\n      Kolya (1996)\n    \n  \n\n\n\n\nWe can now build a DataLoaders object from this table. By default, it takes the first column for the user, the second column for the item (here our movies), and the third column for the ratings. We need to change the value of item_name in our case to use the titles instead of the IDs:\n\n# build a Collaborative Filtering DataLoaders from out ratings DataFrame\n# needs a user column and an item column - we have a user column called user so don't need to pass in\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64) # need to pass in item_name to get title\ndls.show_batch()\n\n\n\n  \n    \n      \n      user\n      title\n      rating\n    \n  \n  \n    \n      0\n      542\n      My Left Foot (1989)\n      4\n    \n    \n      1\n      422\n      Event Horizon (1997)\n      3\n    \n    \n      2\n      311\n      African Queen, The (1951)\n      4\n    \n    \n      3\n      595\n      Face/Off (1997)\n      4\n    \n    \n      4\n      617\n      Evil Dead II (1987)\n      1\n    \n    \n      5\n      158\n      Jurassic Park (1993)\n      5\n    \n    \n      6\n      836\n      Chasing Amy (1997)\n      3\n    \n    \n      7\n      474\n      Emma (1996)\n      3\n    \n    \n      8\n      466\n      Jackie Chan's First Strike (1996)\n      3\n    \n    \n      9\n      554\n      Scream (1996)\n      3\n    \n  \n\n\n\nTo represent collaborative filtering in PyTorch we can’t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices:\n\nn_users  = len(dls.classes['user']) # set number of users = number of rows of users\nn_movies = len(dls.classes['title']) # set number of movies = nuumber of rows of movies\nn_factors = 5 # set number of columns (latent factors) to whatever we want\n\n# create initial random weightings for user latent factors\n# user EMBEDDING matrix\nuser_factors = torch.randn(n_users, n_factors) # random tensors \n\n\n# create initial random weightings for movie latent factors\n# movie EMBEDDING matrix\nmovie_factors = torch.randn(n_movies, n_factors) # random tensors \n\nNote fast.ai has a built in formula for setting an appropriate number of latent factors\n\nuser_factors\n\ntensor([[-1.0827,  0.2138,  0.9310, -0.2739, -0.4359],\n        [-0.5195,  0.7613, -0.4365,  0.1365,  1.3300],\n        [-1.2804,  0.0705,  0.6489, -1.2110,  1.8266],\n        ...,\n        [ 0.8009, -0.4734, -0.8962, -0.7348, -0.0246],\n        [ 0.3354, -0.8262, -0.1541,  0.4699,  0.4873],\n        [ 2.4054, -0.2156, -1.4126, -0.2467,  1.0571]])\n\n\n\nmovie_factors\n\ntensor([[-0.3978,  0.4563,  1.2301,  0.3745,  0.9689],\n        [-1.1836, -0.5818, -0.5587, -0.4316,  0.2128],\n        [ 0.0420,  1.3201, -0.7999,  1.1123, -0.7585],\n        ...,\n        [ 2.4743,  1.3068,  0.4540,  0.6958,  0.5228],\n        [ 2.3970, -0.2559, -1.7196,  1.0440, -0.2662],\n        [ 0.2786, -0.6593,  0.5260, -0.3416, -1.3938]])\n\n\nTo calculate the result for a particular movie and user combination, we have to look up the index of the movie in our movie latent factor matrix and the index of the user in our user latent factor matrix; then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation our deep learning models know how to do. They know how to do matrix products, and activation functions.\nFortunately, it turns out that we can represent look up in an index as a matrix product. The trick is to replace our indices with one-hot-encoded vectors. Here is an example of what happens if we multiply a vector by a one-hot-encoded vector representing the index 3:\nTaking the dot product of a one hot coded vector and something, is the same as looking up the index in an array.\n\n# create a one-hot encoded vector of length n_users, with 2nd element set to 1 and everything else set to 0\none_hot_2 = one_hot(2, n_users).float()\n\n\n# matrix multiplication - users\n# .t transposes cols and rows to enable matrix multiplication\n# @ is the symbol for matrix multipy\nuser_factors.t() @ one_hot_2\n\ntensor([-1.2804,  0.0705,  0.6489, -1.2110,  1.8266])\n\n\nIt gives us the same vector as the one at index 2 in the user_factor matrix as shown previously.\n\n# create a one-hot encoded vector of length n_users, with 1st element set to 1 and everything else set to 0\none_hot_1 = one_hot(1, n_movies).float()\n\n\n# matrix multiplication - movie\n# .t transposes cols and rows to enable matrix multiplication\n# @ is the symbol for matrix multipy\nmovie_factors.t() @ one_hot_1\n\ntensor([-1.1836, -0.5818, -0.5587, -0.4316,  0.2128])\n\n\nIt gives us the same vector as the one at index 1 in the movie_factors matrix as shown previously.\n\n\nEmbedding layer\nIf we do that for a few indices at once, we will have a matrix of one-hot-encoded vectors, and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one-hot-encoded vector, or to search through it to find the occurrence of the number one — we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer that does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had done a matrix multiplication with a one-hot-encoded vector. This is called an embedding.\n\njargon: Embedding: Multiplying by a one-hot-encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. This is quite a fancy word for a very simple concept. The thing that you multiply the one-hot-encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix.\n\nIn computer vision, we have a very easy way to get all the information of a pixel through its RGB values: each pixel in a colored image is represented by three numbers. Those three numbers give us the redness, the greenness and the blueness, which is enough to get our model to work afterward (with values between 0 and 255).\nFor the problem at hand, we don’t have the same easy way to characterize a user or a movie. There are probably relations with genres: if a given user likes romance, they are likely to give higher scores to romance movies. Other factors might be whether the movie is more action-oriented versus heavy on dialogue, or the presence of a specific actor that a user might particularly like.\nHow do we determine numbers to characterize those? The answer is, we don’t. We will let our model learn them. By analyzing the existing relations between users and movies, our model can figure out itself the features that seem important or not. This is what embeddings are. We will attribute to each of our users and each of our movies a random vector of a certain length (here, n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rules of SGD (or another optimizer).\nAt the beginning, those numbers don’t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data about the relations between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance, and so on.\nWe are now in a position that we can create our whole model from scratch.\n\n\nCreating a Collaborative Filtering model in PyTorch from Scratch\nBefore we can write a model in PyTorch, we first need to learn the basics of object-oriented programming and Python. If you haven’t done any object-oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and getting some practice before moving on.\nThe key idea in object-oriented programming is the class. A model is a class. We have been using classes throughout this book, such as DataLoader, string, and Learner. Python also makes it easy for us to create new classes. Here is an example of a simple class:\n\n# example of a simple class\nclass Example:\n    def __init__(self, a): self.a = a # __init__ any method surrounded in double underscores like this is considered special\n    def say(self,x): return f'Hello {self.a}, {x}.'\n\nThe most important piece of this is the special method called __init__ (pronounced dunder init). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behavior associated with this method name. In the case of __init__, this is the method Python will call when your new object is created. So, this is where you can set up any state that needs to be initialized upon object creation.\nAny parameters included when the user constructs an instance of your class will be passed to the __init__ method as parameters. Note that the first parameter to any method defined inside a class is self, so you can use this to set and get any attributes that you will need:\n\nex = Example('Sylvain') # so self.a now equals Sylvain\nex.say('nice to meet you') # x is now 'nice to meet you - we can access the say function within the Example class using .say\n\n'Hello Sylvain, nice to meet you.'\n\n\nAlso note that creating a new PyTorch module requires inheriting from Module. Inheritance is an important object-oriented concept that we will not discuss in detail here—in short, it means that we can add additional behavior to an existing class. PyTorch already provides a Module class, which provides some basic foundations that we want to build on. So, we add the name of this superclass after the name of the class that we are defining, as shown in the following example.\nThe final thing that you need to know to create a new PyTorch module is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is the class defining our dot product model:\n\n# create a class to define our dot product module\nclass DotProduct(Module): # putting something in parentheses after a class name creates a SUPERclass\n    def __init__(self, n_users, n_movies, n_factors): # specify number of users, movies, and latent factors\n        self.user_factors = Embedding(n_users, n_factors) # create Embedding matrix for users - we will cover how to create Embedding Class later\n        self.movie_factors = Embedding(n_movies, n_factors) # create Embedding matrix for movies - we will cover how to create Embedding Class later\n    \n    # calculation of our model has to be defined in a function called forward\n    def forward(self, x):  # pass the object itself and thing calculating on - user and movie for a batch\n                           # each row will be one user and movie combination, columns will be users and movies\n        users = self.user_factors(x[:,0]) # grab first column i.e every row, and look it up using our user Embedding matrix\n        movies = self.movie_factors(x[:,1]) # grab second column i.e every row, and look it up using our movie Embedding matrix\n        return (users * movies).sum(dim=1) # calculate the dot product - # dim = 1 because we are summing across COLUMNS for each row # dim = 0 would sum across ROWS\n\nIf you haven’t seen object-oriented programming before, then don’t worry, you won’t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax.\nNote that the input of the model is a tensor of shape batch_size x 2, where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors:\n\n# inputs to the model are 64 rows x 2 columns - column 0 user IDs and column 1 movie IDs\nx,y = dls.one_batch()\nx.shape\n\ntorch.Size([64, 2])\n\n\nNow that we have defined our architecture, and created our parameter matrices, we need to create a Learner to optimize our model. In the past we have used special functions, such as cnn_learner, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain Learner class:\n\n# define our Dot Product model\nmodel = DotProduct(n_users, n_movies, 50)\n\n# we can pass our Dot Product class to our learner\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\nWe are now ready to fit our model:\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3) # 5 epochs, learning rate 5e^-3\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.385412\n      1.293633\n      00:04\n    \n    \n      1\n      1.061318\n      1.070560\n      00:04\n    \n    \n      2\n      0.968811\n      0.976037\n      00:04\n    \n    \n      3\n      0.862989\n      0.883624\n      00:04\n    \n    \n      4\n      0.797610\n      0.869864\n      00:04\n    \n  \n\n\n\n\n\nSqueezing our predictions using Sigmoid\nThe first thing we can do to make this model a little bit better is to force those predictions to be between 0 and 5. For this, we just need to use sigmoid_range. Sigmoid on its own squeezes values between 0 and 1 but if we multiply by 5 that wil ensure the values are between 0 and 5. One thing we discovered empirically is that it’s better to have the range go a little bit over 5, so we use (0, 5.5):\n\n# tweak our Dot Product Class to squeeze preds between 0 and 5\nclass DotProduct(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): # set range for predictions between 0 and 5 (with a little bit extra for comfort) \n        self.user_factors = Embedding(n_users, n_factors) # create Embedding matrix for users - we will cover how to create Embedding Class later\n        self.movie_factors = Embedding(n_movies, n_factors) # create Embedding matrix for movies - we will cover how to create Embedding Class later\n        self.y_range = y_range # range of predictions specified\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0]) # grab first column i.e every row, and look it up using our user Embedding matrix\n        movies = self.movie_factors(x[:,1]) # grab second column i.e every row, and look it up using our movie Embedding matrix\n        return sigmoid_range((users * movies).sum(dim=1), *self.y_range) # force predictions to be between 0 and 5 using sigmoid function\n\n\n# redefine our Dot Product model\nmodel = DotProduct(n_users, n_movies, 50)\n\n# pass in our Dot Product class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3) # 5 epochs, learning rate 5e^-3\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.991383\n      0.971459\n      00:04\n    \n    \n      1\n      0.862119\n      0.888047\n      00:04\n    \n    \n      2\n      0.677498\n      0.857523\n      00:04\n    \n    \n      3\n      0.464585\n      0.863056\n      00:04\n    \n    \n      4\n      0.384263\n      0.867252\n      00:05\n    \n  \n\n\n\nThis is negligibly better, but we cann improve on this.\n\n\nIntroducing Bias into our model\nOne obvious missing piece is that some users are just more positive or negative in their recommendations than others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say about a movie is, for instance, that it is very sci-fi, very action-oriented, and very not old, then you don’t really have any way to say whether most people like it.\nThat’s because at this point we only have weights; we do not have biases. If we have a single number for each user that we can add to our scores, and ditto for each movie, that will handle this missing piece very nicely. Let’s first look at this in Excel - we simply initialize an additional randomized bias factor to add to our existing latent factors and then optimize as before. This results in an improvement - our RMSE drops from 0.42 to 0.35 - see spreadsheet screenshot below:\n\n\n\nbias.JPG\n\n\nLet’s jump back to Python and adjust our model architecture there to introduce bias into our model:\n\n# create new Class to include bias \nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): # set range for predictions between 0 and 5 (with a little bit extra for comfort as sigmoid won't return as high as 1)\n        self.user_factors = Embedding(n_users, n_factors) # create Embedding matrix for users - we will cover how to create Embedding Class later\n        self.user_bias = Embedding(n_users, 1) # account for user BIAS (other factors outside of our latent factors)\n        self.movie_factors = Embedding(n_movies, n_factors) # create Embedding matrix for movies - we will cover how to create Embedding Class later\n        self.movie_bias = Embedding(n_movies, 1) # account for movie BIAS (other factors outside of our latent factors)\n        self.y_range = y_range # range of predictions specified\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0]) # grab first column i.e every row, and look it up using our user Embedding matrix\n        movies = self.movie_factors(x[:,1]) # grab second column i.e every row, and look it up using our movie Embedding matrix\n        res = (users * movies).sum(dim=1, keepdim=True) # calculate the dot product - # dim = 1 because we are summing across COLUMNS for each row # dim = 0 would sum across ROWS\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) # update dor product results for BIAS\n        return sigmoid_range(res, *self.y_range) # force predictions to be between 0 and 5 using sigmoid function\n\nLet’s try training this and see how it goes:\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50)\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3) # 5 epochs, learning rate 5e^-3\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.951611\n      0.925811\n      00:05\n    \n    \n      1\n      0.819404\n      0.855196\n      00:05\n    \n    \n      2\n      0.616164\n      0.856704\n      00:05\n    \n    \n      3\n      0.403988\n      0.885035\n      00:05\n    \n    \n      4\n      0.294023\n      0.891860\n      00:05\n    \n  \n\n\n\nUnlike in Excel, instead of being better, in PyTorch our validation loss has actually gone up (at least by the end of training)! Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As we’ve seen, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularization technique. One way to help avoid overfitting is an approach called weight decay.\n\n\nWeight Decay (L2 regularization)\nWeight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible.\nWhy would it prevent overfitting? The idea is that the larger the coefficients are, the sharper canyons we will have in the loss function. If we take the basic example of a parabola, y = a * (x**2), the larger a is, the more narrow the parabola is:\n\n# example illustrating imapct of using weight decay\nx = np.linspace(-2,2,100)\na_s = [1,2,5,10,50] \nys = [a * x**2 for a in a_s]\n_,ax = plt.subplots(figsize=(8,6))\nfor a,y in zip(a_s,ys): ax.plot(x,y, label=f'a={a}')\nax.set_ylim([0,5])\nax.legend();\n\n\n\n\nSo, letting our model learn high parameters might cause it to fit all the data points in the training set with an overcomplex function that has very sharp changes, which will lead to overfitting.\nLimiting our weights from growing too much is going to hinder the training of the model, but it will yield a state where it generalizes better. Going back to the theory briefly, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters):\nloss_with_wd = loss + wd * (parameters**2).sum()\nIn practice, though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing:\nparameters.grad += wd * 2 * parameters\nIn practice, since wd is a parameter that we choose, we can just make it twice as big, so we don’t even need the *2 in this equation. To use weight decay in fastai, just pass wd in your call to fit or fit_one_cycle:\nThe whole reason for calculating the loss is to then calculate the gradient of the loss, by taking the derivative. The derivative of parameters^2 is 2*parameters.\n\nWeight decay value 0.1\nA higher weight decay value forces the weights lower, reducing the capacity of our model to make good prediction, but reducing the risk of overfitting.\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50)\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3, wd=0.1) # 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.976209\n      0.929432\n      00:05\n    \n    \n      1\n      0.867723\n      0.859258\n      00:05\n    \n    \n      2\n      0.751625\n      0.823332\n      00:04\n    \n    \n      3\n      0.580325\n      0.811122\n      00:05\n    \n    \n      4\n      0.485529\n      0.811769\n      00:05\n    \n  \n\n\n\nThat’s much better! The key to regularization is to find the right balance of the magnitude of the weights of the coefficients - low enough so we don’t overfit, but high enough so that we can make useful predictions. We can’t reduce them too much (then we end up with underfitting) - but if the weights are increased too much then our model will start to overfit. If there are latent factors in our model that don’t have any influence on overall prediciton, it will just set the co-efficient for that latent factor to zero.\n\n\nWeight decay value 0.01\nA lower weight decay value keeps the weights higher, increasing the capacity of our model to make good predictions, but increasing the risk of overfitting.\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50) # set number of latent factors = 50\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3, wd=0.01) # 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.937280\n      0.919222\n      00:05\n    \n    \n      1\n      0.836111\n      0.858221\n      00:05\n    \n    \n      2\n      0.594563\n      0.858991\n      00:05\n    \n    \n      3\n      0.416554\n      0.887284\n      00:05\n    \n    \n      4\n      0.282974\n      0.894385\n      00:05\n    \n  \n\n\n\nAs we can see we start off with an improvement and then from epoch 2 performance gets worse, suggesting overfitting.\n\n\nWeight decay value 0.001\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50)\n\n# pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# fit (train) our model\nlearn.fit_one_cycle(5, 5e-3, wd=0.001) # 5 epochs, learning rate 5e^-3, try different wd values, start 0.1 then 0.01, 0.001 etc\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.922303\n      0.922695\n      00:05\n    \n    \n      1\n      0.856747\n      0.854244\n      00:05\n    \n    \n      2\n      0.600128\n      0.864396\n      00:05\n    \n    \n      3\n      0.404001\n      0.894145\n      00:05\n    \n    \n      4\n      0.283558\n      0.902557\n      00:04\n    \n  \n\n\n\nAgain, we start off with an improvement but then from epoch 2 performance gets worse, suggesting overfitting So, our original weight decay factor of 0.1 looks pretty optimal.\n\n\n\nCreating Our Own Embedding Module\nIf the following section proves to be difficult to follow then it would be a useful exercise to revisit the Linear model and neural net from scratch NoteBook.\nIn that Notebook we created functions to set initital weights, added layers, including bias, and created a further function to update the gradients i.e. perform gradient descent by calculating the layer gradients usind layer.grad * learning_rate. When using PyTorch a lot of this functionality is taken care of - PyTorch looks inside our Module and keeps track of anything that looks like a neural network parameter.\nSo far, we’ve used Embedding without thinking about how it really works. Let’s re-create DotProductBias without using this class. We’ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall that optimizers require that they can get all the parameters of a module from the module’s parameters method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:\n\n# create a simple module which only includes a tensor\nclass T(Module):\n    def __init__(self): self.a = torch.ones(3) # add a tensor as an attribute to our Module\n\nL(T().parameters()) # T() instantiates our Module, capital L in Fastcore returns a list of items\n\n(#0) []\n\n\nNote that the tensor is not included in parameters. To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the nn.Parameter class. This class doesn’t actually add any functionality (other than automatically calling requires_grad_ for us). It’s only used as a “marker” to show what to include in parameters:\n\n# create a simple module which only includes a tensor\nclass T(Module):\n    def __init__(self): self.a = nn.Parameter(torch.ones(3)) # for PyTorch to recognise the parameters, we need to include the nn.Parameter wrapper\n\nL(T().parameters()) # T() instantiates our Module, capital L in Fastcore returns a list of the parameters\n\n(#1) [Parameter containing:\ntensor([1., 1., 1.], requires_grad=True)]\n\n\nNow that we have included the tensor in an nn.Parameter wrapper, PyTorch can read the parameters and we can return these using Fastcore’s L.\nAll PyTorch modules use nn.Parameter for any trainable parameters, which is why we haven’t needed to explicitly use this wrapper up until now:\n\n# create a simple module which only includes a tensor\nclass T(Module):\n    def __init__(self): self.a = nn.Linear(1, 3, bias=False) # we can create our tensor as before but use nn.Linear which flags that parameters are included \n                                                             # no bias term, nn.Linear returns randomly initialized tensor values, size as defined, 1 x 3 \n\nt = T()\nL(t.parameters())  # T() instantiates our Module, capital L in Fastcore returns a list of the parameters\n\n(#1) [Parameter containing:\ntensor([[ 0.7645],\n        [ 0.8300],\n        [-0.2343]], requires_grad=True)]\n\n\nNow that we have included the tensor in an nn.Linear wrapper, PyTorch can read the parameters and we can return these using Fastcore’s L.\n\n# find out what the attribute a is\nt.a\n\nLinear(in_features=1, out_features=3, bias=False)\n\n\n\n# find out what type the attribute a is \ntype(t.a)\n\ntorch.nn.modules.linear.Linear\n\n\n\n# find out what type the attribute a is \nt.a.weight\n\nParameter containing:\ntensor([[ 0.7645],\n        [ 0.8300],\n        [-0.2343]], requires_grad=True)\n\n\nWe can create a tensor as a parameter, with random initialization, like so:\n\n# create params function - poss in size (in case below n_users x n_factors)\ndef create_params(size):\n    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) # creates a tensor of zeros of requested size, then Gaussian distribution with mean=0 and Std Dev = 0.01\n# normal_ modifies eplaces inline with the values specified in brackets \n\nLet’s use this to create DotProductBias again, but without Embedding i.e let’s create PyTorch’s Embedding Matrix from scratch:\n\n# create PyTorch's embedding matrix from scratch\nclass DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = create_params([n_users, n_factors]) # create our user Embedding matrix of normally randomized values of size n_users x n_factors\n        self.user_bias = create_params([n_users]) # build user bias into our model - vector of size n_users\n        self.movie_factors = create_params([n_movies, n_factors]) # create our movie Embedding matrix of normally randomized values of size n_users x n_factors\n        self.movie_bias = create_params([n_movies]) # build movie bias into our model - vector of size n_movies\n        self.y_range = y_range # range of predictions as set above, between 0 and 5.5\n        \n    def forward(self, x):\n        users = self.user_factors[x[:,0]] # user latent factors - note we can index into it\n        movies = self.movie_factors[x[:,1]] # movie latent factors - note we can index into it\n        res = (users*movies).sum(dim=1) # matrix multiplication\n        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] # add bias - note we ca \n        return sigmoid_range(res, *self.y_range) # force predictions to be between 0 and 5 using sigmoid function\n\nThen let’s train it again to check we get around the same results we saw in the previous section:\n\n# define our Dot Product Bias model\nmodel = DotProductBias(n_users, n_movies, 50) # latent factors set to 50\n\n# # pass in our Dot Product Bias class to our learner as before\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# train for 5 epochs, lr = 5e^-3, weight decay factor = 0.1\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.960358\n      0.956795\n      00:04\n    \n    \n      1\n      0.869042\n      0.874685\n      00:05\n    \n    \n      2\n      0.737840\n      0.839419\n      00:05\n    \n    \n      3\n      0.589841\n      0.823726\n      00:05\n    \n    \n      4\n      0.472334\n      0.824282\n      00:05\n    \n  \n\n\n\nNow, let’s take a look at what our model has learned.\n\n# what's inside movie bias?\nprint(model.movie_bias,len(model.movie_bias))\n\nParameter containing:\ntensor([-0.0010, -0.1098, -0.0022,  ..., -0.0443,  0.0685,  0.0255],\n       requires_grad=True) 1665\n\n\nMovie bias parameters that have been trained - 1,665 being the number of movies we have.\n\n# what is the shape of our movie bias vector?\nmodel.movie_bias.shape\n\ntorch.Size([1665])\n\n\n\n# what's inside movie factors?\nprint(model.movie_factors,len(model.movie_factors))\n\nParameter containing:\ntensor([[-0.0039, -0.0022,  0.0021,  ...,  0.0041, -0.0011,  0.0016],\n        [-0.1175, -0.1778, -0.0984,  ...,  0.0191,  0.0929,  0.0216],\n        [ 0.0109,  0.0653,  0.0031,  ..., -0.0156,  0.0204,  0.0313],\n        ...,\n        [-0.1234, -0.0363, -0.0474,  ..., -0.0825, -0.0893, -0.1314],\n        [ 0.0995,  0.1521,  0.0754,  ...,  0.0901,  0.1230,  0.1518],\n        [ 0.0164, -0.0041,  0.0183,  ..., -0.0054,  0.0122, -0.0150]],\n       requires_grad=True) 1665\n\n\n\n# what is the shape of our movie factors Embedding matrix?\nmodel.movie_factors.shape\n\ntorch.Size([1665, 50])\n\n\n1,665 movies, and 50 latent factors.\n\n# what's inside user factors?\nprint(model.user_factors,len(model.user_factors))\n\nParameter containing:\ntensor([[ 1.2866e-03,  7.8120e-04, -7.0611e-04,  ...,  8.2220e-06,\n         -3.2568e-03,  2.7836e-03],\n        [ 1.6745e-01,  9.3676e-02, -5.2638e-03,  ..., -2.9528e-02,\n         -1.1926e-01,  3.1058e-01],\n        [ 4.6036e-02, -4.4877e-03,  1.5233e-01,  ...,  9.4287e-02,\n          1.1350e-01,  1.4557e-01],\n        ...,\n        [ 6.7316e-02,  1.0262e-01,  2.9921e-01,  ...,  1.2235e-01,\n          4.4754e-02,  2.5394e-01],\n        [-8.0669e-03,  1.0943e-01,  2.0522e-01,  ...,  1.6869e-02,\n          1.7104e-01,  1.5911e-01],\n        [ 7.9618e-02,  2.9292e-01,  2.3172e-01,  ...,  1.1354e-01,\n          1.2088e-01,  9.0374e-02]], requires_grad=True) 944\n\n\nA bunch of user parameters (weights) that have been trained - 944 being the number of users we have.\n\n# what is the shape of our user factors Embedding matrix?\nmodel.user_factors.shape\n\ntorch.Size([944, 50])\n\n\n944 users, and 50 latent factors.\n\n\nInterpreting Embeddings and Biases\nOur model is already useful, in that it can provide us with movie recommendations for our users — but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector:\n\n# get movie_bias values\nmovie_bias = learn.model.movie_bias.squeeze() # \n\n# find out which movie id's have the lowest bias parameters\nidxs = movie_bias.argsort()[:5] # argsort sorts in ascending order by default - let's grab first 5 \n\n# look inside our DataLoaders to grab the names of those movies from the indexes\n[dls.classes['title'][i] for i in idxs]\n\n['Children of the Corn: The Gathering (1996)',\n 'Robocop 3 (1993)',\n 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n 'Amityville 3-D (1983)',\n 'Mortal Kombat: Annihilation (1997)']\n\n\nThink about what this means. What it’s saying is that for each of these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth), they still generally don’t like it. We could have simply sorted the movies directly by their average rating, but looking at the learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people tend not to like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias:\n\n# sort indexes by descending will give us movies with highest bias values\n# i.e movies that are popular even amongst users who don't normally like that kind of movie\nidxs = movie_bias.argsort(descending=True)[:5]\n[dls.classes['title'][i] for i in idxs]\n\n['Titanic (1997)',\n 'L.A. Confidential (1997)',\n 'Silence of the Lambs, The (1991)',\n 'Shawshank Redemption, The (1994)',\n 'Star Wars (1977)']\n\n\nSo, for instance, even if you don’t normally enjoy detective movies, you might enjoy LA Confidential!\nIt is not quite so easy to directly interpret the embedding matrices. There are just too many factors for a human to look at. But there is a technique that can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). If you are interested then we suggest you check out the fast.ai course Computational Linear Algebra for Coders. Here’s what our movies look like based on two of the strongest PCA components:\n\ng = ratings.groupby('title')['rating'].count()\ng\n\ntitle\n'Til There Was You (1997)                  9\n1-900 (1994)                               5\n101 Dalmatians (1996)                    109\n12 Angry Men (1957)                      125\n187 (1997)                                41\n                                        ... \nYoung Guns II (1990)                      44\nYoung Poisoner's Handbook, The (1995)     41\nZeus and Roxanne (1997)                    6\nunknown                                    9\nÁ köldum klaka (Cold Fever) (1994)         1\nName: rating, Length: 1664, dtype: int64\n\n\n\n# group movies by title and rating \ng = ratings.groupby('title')['rating'].count()\n\n# sort top movies by rating - top 1000\ntop_movies = g.sort_values(ascending=False).index.values[:1000]\n\n# get the indexes of the sorted top movies using: object to index (o2i)\ntop_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\n\n# \nmovie_w = learn.model.movie_factors[top_idxs].cpu().detach()\n\n# Compress our 50 latent factors into just 3 most important factors\nmovie_pca = movie_w.pca(3)\n\n# draw a chart of these features - Visualized Embeddings\nfac0,fac1,fac2 = movie_pca.t() # .t transposes the array\nidxs = list(range(50)) # restrict number of movies plotted to 50\n\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\nfor i, x, y in zip(top_movies[idxs], X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\nplt.show()\n\n\n\n\nWe can see here that the model seems to have discovered a concept of classic versus pop culture movies, or perhaps it is critically acclaimed that is represented here.\n\nj: No matter how many models I train, I never stop getting moved and surprised by how these randomly initialized bunches of numbers, trained with such simple mechanics, manage to discover things about my data all by themselves. It almost seems like cheating, that I can create code that does useful things without ever actually telling it how to do those things!\n\nWe defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. We’ll look at how to do that next.\n\n\nUsing fastai.collab\nWe can create and train a collaborative filtering model using the exact structure shown earlier by using fastai’s collab_learner. Let’s have a peek under the hood and see what is going on inside:\n\n# let's take a look at what's going on under the hood\ncollab_learner??\n\n\nSignature:\ncollab_learner(\n    dls,\n    n_factors=50,\n    use_nn=False,\n    emb_szs=None,\n    layers=None,\n    config=None,\n    y_range=None,\n    loss_func=None,\n    *,\n    opt_func=<function Adam at 0x7f6f614f4700>,\n    lr=0.001,\n    splitter: 'callable' = <function trainable_params at 0x7f6f63949870>,\n    cbs=None,\n    metrics=None,\n    path=None,\n    model_dir='models',\n    wd=None,\n    wd_bn_bias=False,\n    train_bn=True,\n    moms=(0.95, 0.85, 0.95),\n    default_cbs: 'bool' = True,\n)\nSource:   \n@delegates(Learner.__init__)\ndef collab_learner(dls, n_factors=50, use_nn=False, emb_szs=None, layers=None, config=None, y_range=None, loss_func=None, **kwargs):\n    \"Create a Learner for collaborative filtering on `dls`.\"\n    emb_szs = get_emb_sz(dls, ifnone(emb_szs, {}))\n    if loss_func is None: loss_func = MSELossFlat()\n    if config is None: config = tabular_config()\n    if y_range is not None: config['y_range'] = y_range\n    if layers is None: layers = [n_factors]\n    if use_nn: model = EmbeddingNN(emb_szs=emb_szs, layers=layers, **config)\n    else:      model = EmbeddingDotBias.from_classes(n_factors, dls.classes, y_range=y_range)\n    return Learner(dls, model, loss_func=loss_func, **kwargs)\nFile:      ~/mambaforge/lib/python3.10/site-packages/fastai/collab.py\nType:      function\n\n\n\n\n\n# let's take a look at what's going on under the hood\nEmbeddingDotBias??\n\n\nInit signature: EmbeddingDotBias(n_factors, n_users, n_items, y_range=None)\nSource:        \nclass EmbeddingDotBias(Module):\n    \"Base dot model for collaborative filtering.\"\n    def __init__(self, n_factors, n_users, n_items, y_range=None):\n        self.y_range = y_range\n        (self.u_weight, self.i_weight, self.u_bias, self.i_bias) = [Embedding(*o) for o in [\n            (n_users, n_factors), (n_items, n_factors), (n_users,1), (n_items,1)\n        ]]\n    def forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\n    @classmethod\n    def from_classes(cls, n_factors, classes, user=None, item=None, y_range=None):\n        \"Build a model with `n_factors` by inferring `n_users` and  `n_items` from `classes`\"\n        if user is None: user = list(classes.keys())[0]\n        if item is None: item = list(classes.keys())[1]\n        res = cls(n_factors, len(classes[user]), len(classes[item]), y_range=y_range)\n        res.classes,res.user,res.item = classes,user,item\n        return res\n    def _get_idx(self, arr, is_item=True):\n        \"Fetch item or user (based on `is_item`) for all in `arr`\"\n        assert hasattr(self, 'classes'), \"Build your model with `EmbeddingDotBias.from_classes` to use this functionality.\"\n        classes = self.classes[self.item] if is_item else self.classes[self.user]\n        c2i = {v:k for k,v in enumerate(classes)}\n        try: return tensor([c2i[o] for o in arr])\n        except KeyError as e:\n            message = f\"You're trying to access {'an item' if is_item else 'a user'} that isn't in the training data. If it was in your original data, it may have been split such that it's only in the validation set now.\"\n            raise modify_exception(e, message, replace=True)\n    def bias(self, arr, is_item=True):\n        \"Bias for item or user (based on `is_item`) for all in `arr`\"\n        idx = self._get_idx(arr, is_item)\n        layer = (self.i_bias if is_item else self.u_bias).eval().cpu()\n        return to_detach(layer(idx).squeeze(),gather=False)\n    def weight(self, arr, is_item=True):\n        \"Weight for item or user (based on `is_item`) for all in `arr`\"\n        idx = self._get_idx(arr, is_item)\n        layer = (self.i_weight if is_item else self.u_weight).eval().cpu()\n        return to_detach(layer(idx),gather=False)\nFile:           ~/mambaforge/lib/python3.10/site-packages/fastai/collab.py\nType:           PrePostInitMeta\nSubclasses:     \n\n\n\n\nOK, let’s now reproduce what we did from scratch earlier using the fast.ai functionality with just a few lines of code:\n\n# create a collaborative filtering model using fastai\nlearn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) # latebt factors =50, predictions between 0 and 5.5\n\n\n# train for 5 epochs, learning rate = 5e^-3, weight decay = 0.1\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.940161\n      0.954125\n      00:05\n    \n    \n      1\n      0.845409\n      0.871870\n      00:04\n    \n    \n      2\n      0.732785\n      0.837964\n      00:05\n    \n    \n      3\n      0.581802\n      0.822925\n      00:05\n    \n    \n      4\n      0.483456\n      0.823324\n      00:04\n    \n  \n\n\n\nThe names of the layers can be seen by printing the model:\n\n# let's look at the layers of our model\nlearn.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(944, 50)\n  (i_weight): Embedding(1665, 50)\n  (u_bias): Embedding(944, 1)\n  (i_bias): Embedding(1665, 1)\n)\n\n\nNote the slight difference in terminology. u = users, and i=items. So, we have the user Embedding layer (u_weight), and the movie Embedding layer (i_weight) and our bias layers.\nWe can use these to replicate any of the analyses we did in the previous section — for instance:\n\n# we can look at the movie bias and grab the weights\nmovie_bias = learn.model.i_bias.weight.squeeze()\n\n# get indexes of top 5 movies by bias factor\nidxs = movie_bias.argsort(descending=True)[:5]\n\n# get title of top 5 movies by bias factor\n[dls.classes['title'][i] for i in idxs]\n\n['L.A. Confidential (1997)',\n \"Schindler's List (1993)\",\n 'Titanic (1997)',\n 'Shawshank Redemption, The (1994)',\n 'Silence of the Lambs, The (1991)']\n\n\nWe get much the same results as before, that is LA Confidential is watched even by those that don’t normally watch that kind of movie.\nAnother interesting thing we can do with these learned embeddings is to look atdistance.\n\n\nEmbedding Distance\nOn a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: \\(\\sqrt{x^{2}+y^{2}}\\) (assuming that x and y are the distances between the coordinates on each axis). For a 50-dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances.\nIf there were two movies that were nearly identical, then their embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two movies’ embedding vectors can define that similarity. We can use this to find the most similar movie to Silence of the Lambs:\n\nmovie_factors = learn.model.i_weight.weight\n\n# convert Silence of the Lambs into its class ID using 'object to index' (o2i)\nidx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\n\n# calculate the 'distance' betweeen the Silence of the Lambs and other movie vectors\ndistances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) # Cosine Similarity normalizes the angle between the vectors\n\n# sort distances from closes\nidx = distances.argsort(descending=True)[1]\n\n# attach movie titles to the movie indexes\ndls.classes['title'][idx]\n\n\"One Flew Over the Cuckoo's Nest (1975)\"\n\n\nNow that we have succesfully trained a model, let’s see how to deal with the situation where we have no data for a user. How can we make recommendations to new users?\n\n\nBootstrapping a Collaborative Filtering Model\nThe biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\nBut even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of use your common sense. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste.\nBetter still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\nOne thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don’t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of best ever movies lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\nSuch a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\nIn a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It’s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.\nOur dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization (PMF). Another approach, which generally works similarly well given the same data, is deep learning.\n\n\nDeep Learning for Collaborative Filtering - from scratch\nTo turn our architecture into a deep learning model, the first step is to take the results of the embedding lookup and concatenate those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way. Since we’ll be concatenating the embeddings, rather than taking their dot product, the two embedding matrices can have different sizes (i.e., different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice:\n\n# use fast.ai recommended embedding sizes\nembs = get_emb_sz(dls)\nembs\n\n[(944, 74), (1665, 102)]\n\n\nSo the suggested number of latent factors for our 944 users is 74, and the suggested number of latent factors for our 1,665 movies is 102.\nLet’s implement this class:\n\n# build a Collaborative Filtering neural net from scratch\nclass CollabNN(Module):\n    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n        self.user_factors = Embedding(*user_sz)\n        self.item_factors = Embedding(*item_sz)\n        self.layers = nn.Sequential(\n            nn.Linear(user_sz[1]+item_sz[1], n_act), # \n            nn.ReLU(), # Rectified Linear Unit\n            nn.Linear(n_act, 1)) # Linear layer at the end to create a single output\n        self.y_range = y_range\n        \n    def forward(self, x):\n        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n        x = self.layers(torch.cat(embs, dim=1)) # concatenate user and item embeddings together\n        return sigmoid_range(x, *self.y_range)\n\nAnd use it to create a model:\n\n# instantiate our model \nmodel = CollabNN(*embs)\n\nCollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. self.layers is identical to the mini-neural net we created in the chapter for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass this through the mini-neural net. Finally, we apply sigmoid_range as we have in previous models.\nLet’s see if it trains:\n\nlearn = Learner(dls, model, loss_func=MSELossFlat())\n\n# train for 5 epochs, learning rate 5e^-3, weight decay = 0.01\nlearn.fit_one_cycle(5, 5e-3, wd=0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.960975\n      0.944082\n      00:06\n    \n    \n      1\n      0.899966\n      0.908818\n      00:06\n    \n    \n      2\n      0.877111\n      0.890931\n      00:06\n    \n    \n      3\n      0.791085\n      0.869468\n      00:06\n    \n    \n      4\n      0.771323\n      0.869940\n      00:06\n    \n  \n\n\n\nfastai provides this model in fastai.collab if you pass use_nn=True in your call to collab_learner (including calling get_emb_sz for you), and it lets you easily create more layers. For instance, here we’re creating two hidden layers, of size 100 and 50, respectively:\n\n\nDeep Learning for Collaborative Filtering - using fast.ai\n\n# create our Collab Filtering learner, define neural net layesr\nlearn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) # use_nn = True allows us to create a neural network, with 2 hidden layers\n\n# train for 5 epochs, learning rate 5e^-3, weight decay = 0.01\nlearn.fit_one_cycle(5, 5e-3, wd=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.957651\n      0.987930\n      00:07\n    \n    \n      1\n      0.894093\n      0.919895\n      00:07\n    \n    \n      2\n      0.907125\n      0.892506\n      00:06\n    \n    \n      3\n      0.863961\n      0.864401\n      00:06\n    \n    \n      4\n      0.766643\n      0.866521\n      00:06\n    \n  \n\n\n\nDeep learning models really come into play when we have a lot of metadata e.g. information about our users, where are they from, when did they sign up, what sex are they etc and for our movies e.g. when was it released, what genre is it etc. In our scenario here where we don’t have this information to hand, the deep learning model scores a bit worse than our dot product model, which is taking advantage of our understanding of the problem domain. In practice we often create a model which has a dot product component and a neural net component.\nlearn.model is an object of type EmbeddingNN. Let’s take a look at fastai’s code for this class:\n\n@delegates(TabularModel)\nclass EmbeddingNN(TabularModel):\n    def __init__(self, emb_szs, layers, **kwargs):\n        super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) # n_cont=0 means number of continuous variables is zero\n\nWow, that’s not a lot of code! This class inherits from TabularModel, which is where it gets all its functionality from. In __init__ it calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received.\n\nTabularModel??\n\n\nInit signature:\nTabularModel(\n    emb_szs: 'list',\n    n_cont: 'int',\n    out_sz: 'int',\n    layers: 'list',\n    ps: 'float | list' = None,\n    embed_p: 'float' = 0.0,\n    y_range=None,\n    use_bn: 'bool' = True,\n    bn_final: 'bool' = False,\n    bn_cont: 'bool' = True,\n    act_cls=ReLU(inplace=True),\n    lin_first: 'bool' = True,\n)\nSource:        \nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, \n        emb_szs:list, # Sequence of (num_embeddings, embedding_dim) for each categorical variable\n        n_cont:int, # Number of continuous variables\n        out_sz:int, # Number of outputs for final `LinBnDrop` layer\n        layers:list, # Sequence of ints used to specify the input and output size of each `LinBnDrop` layer\n        ps:float|list=None, # Sequence of dropout probabilities for `LinBnDrop`\n        embed_p:float=0., # Dropout probability for `Embedding` layer\n        y_range=None, # Low and high for `SigmoidRange` activation \n        use_bn:bool=True, # Use `BatchNorm1d` in `LinBnDrop` layers\n        bn_final:bool=False, # Use `BatchNorm1d` on final layer\n        bn_cont:bool=True, # Use `BatchNorm1d` on continuous variables\n        act_cls=nn.ReLU(inplace=True), # Activation type for `LinBnDrop` layers\n        lin_first:bool=True # Linear layer is first or last in `LinBnDrop` layers\n    ):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\nFile:           ~/mambaforge/lib/python3.10/site-packages/fastai/tabular/model.py\nType:           PrePostInitMeta\nSubclasses:     EmbeddingNN, EmbeddingNN\n\n\n\n\n\n\nkwargs and Delegates\n\nEmbeddingNN includes **kwargs as a parameter to __init__. In Python **kwargs in a parameter list means “put any additional keyword arguments into a dict called kwargs. And **kwargs in an argument list means”insert all key/value pairs in the kwargs dict as named arguments here”. This approach is used in many popular libraries, such as matplotlib, in which the main plot function simply has the signature plot(*args, **kwargs). The plot documentation says “The kwargs are Line2D properties” and then lists those properties.\n\n\nWe’re using **kwargs in EmbeddingNN to avoid having to write all the arguments to TabularModel a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesn’t know what parameters are available. Consequently things like tab completion of parameter names and pop-up lists of signatures won’t work.\n\n\nfastai resolves this by providing a special @delegates decorator, which automatically changes the signature of the class or function (EmbeddingNN in this case) to insert all of its keyword arguments into the signature.\n\nAlthough the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully constructing an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, date and time information, or any other information that may be relevant to the recommendation. That’s exactly what TabularModel does. In fact, we’ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So, we’d better spend some time learning about TabularModel, and how to use it to get great results! We’ll do that in the next chapter.\n\n\nNatural Language Processing (NLP)\nIt’s possible you may have heard about Embeddings before in the context of Natural Language Processing (NLP). We can turn words into integers using an embedding matrix. Let’s use the poem [Green Eggs and Ham] to illustrate:\n\n\n\nembedding_NLP.JPG\n\n\nFrom the spreadhseet screenshot above, we can see that each word that appears in the poem is given an index which can be arbitrary (in this case alphabetical) and then given 4 randomly initialized latent factors, and a bias factor. This allows the conversion from text to integers in the form of an Embedding matrix, which allows our neural net to interpret the text.\n\n\nKey takeaways\nThis blog has explored Collaborative Filtering and we have seen how to:\n\nbuild a Collaborative Filtering model from scratch\ncreate Embedding matrices from scratch\nreplicate the from-scratch model using PyTorch\nreplicate the from-scratch model using Fast.ai\n\nWe have also learned how to build a Collaborative Filtering Model using deep learning again, doing this from scratch, using PyTorch’s functionality, and also using the Fast.ai methodology. We saw how gradient descent can learn intrinsic factors or biases about items from a history of ratings, which can then give us information about the data, which can be used to provide e.g. tailored movie recommendations."
  },
  {
    "objectID": "posts/Kaggle/Kaggle.html",
    "href": "posts/Kaggle/Kaggle.html",
    "title": "Kaggle",
    "section": "",
    "text": "In order to download datasets from Kaggle when working outwith the Kaggle environment you will need to make use of a Kaggle API. You can get this by clicking on Account below your profile name, and then selecting Create New API Token\n\n\n\nKaggle_1.JPG\n\n\n\n\n\nKaggle_2.JPG\n\n\nInitially the file will be saved in your local environment - in a directory named .kaggle. We need to move this file into a .kaggle directory within your PaperSpace environment. I found this easiest to do by:\n\nOpen in JupyterLab\n\n\n\n\nJupyterLab.JPG\n\n\n\nLaunch Classic NoteBook\n\n\n\n\nClassic_NoteBook.JPG\n\n\n\nCreate a storage directory to store Kaggle API within PaperSpace - and add the Kaggle API json file from your local environment\n\n\n\n\nStorage.JPG\n\n\n\nMove the json file into a .kaggle directory within Paperspace. You can enter the following commands within the Terminal:\n\nmv kaggle.json ~/.kaggle\nThe .kaggle directory should already exist but if not you can create one by typing following command within the Terminal:\nmkdir ~/.kaggle\n\n\n\nTerminal.JPG\n\n\n\nTo prevent other users from using our API token, we can type the following command in the Terminal:\n\nchmod 600 /root/.kaggle/kaggle.json\nOK. That’s us now all set up and ready to access Kaggle datsets. If you are looking for a specific dataset, you can now type the following command within the Terminal:\nkaggle datasets list\n\n\n\ndatasets.JPG\n\n\nAs you can see, there’s plenty to keep us occupied!! If you wanted to download data from this API, just enter the command in the Terminal, following the format below :\n\nkaggle datasets download [ref]\n\nFor example if we wanted the (TOP 50)List if most expensive films dataset we would type:\nkaggle datasets download devrimtuner/top-50list-of-most-expensive-films"
  },
  {
    "objectID": "posts/Multi_Label_Classification/Multi_label_classification.html",
    "href": "posts/Multi_Label_Classification/Multi_label_classification.html",
    "title": "Multi-label classification",
    "section": "",
    "text": "Perhaps… but in some cases the opposite to be true, especially when training for quite a few epochs. By giving the model more signal about what is present in a picture, it may be able to use this information to find more interesting features that predict our target of interest. For instance, perhaps some of the features of disease change between varieties.\n\nSet up\n\n# install fastkaggle if not available\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\n\nFirst we’ll repeat the steps we used last time to access the data and ensure all the latest libraries are installed:\n::: {.cell _kg_hide-output=‘true’ tags=‘[]’}\n!pip install fastai\n\n\ncomp = 'paddy-disease-classification'\npath = setup_comp(comp, install='fastai \"timm>=0.6.2.dev0\"')\n\nfrom fastai.vision.all import *\nset_seed(42)\n\nfrom fastcore.parallel import *\ntrn_path = path/'train_images'\n:::\nHere’s the CSV that Kaggle provides, showing the variety of rice contained in each image – we’ll make image_id the index of our data frame so that we can look up images directly to grab their variety:\n\n# load in our training dataset - set index as image_id column\ndf = pd.read_csv(path/'train.csv', index_col='image_id')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      label\n      variety\n      age\n    \n    \n      image_id\n      \n      \n      \n    \n  \n  \n    \n      100330.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100365.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100382.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      100632.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n    \n      101918.jpg\n      bacterial_leaf_blight\n      ADT45\n      45\n    \n  \n\n\n\n\nPandas uses the loc attribute to look up rows by index. Here’s how we can get the variety of image 100330.jpg, for instance:\n\ndf.loc['100330.jpg', 'variety']\n\n'ADT45'\n\n\nOur DataBlock will be using get_image_files to get the list of training images, which returns Path objects. Therefore, to look up an item to get its variety, we’ll need to pass its name. Here’s a function which does just that:\n\n# create a function that looks up an item and gets its variety\ndef get_variety(p): return df.loc[p.name, 'variety']\n\nWe’re now ready to create our DataLoaders. To do this, we’ll use the DataBlock API, which is a flexible and convenient way to plug pieces of a data processing pipeline together:\n\n# Create our DataLoaders\ndls = DataBlock(\n    blocks=(ImageBlock,CategoryBlock,CategoryBlock), # these are inputs and outputs - specify which on line below\n    n_inp=1, # specify number of inputs included above - so first argument above is ImageBlock which is our inputs, and the 2 outputs are CategoryBlocks - disease and variety\n    get_items=get_image_files, # grab input images\n    get_y = [parent_label,get_variety], # grab labels - parent_label is disease, get_variety from function we defined earlier\n    splitter=RandomSplitter(0.2, seed=42), # split training set 80% validation 20%\n    item_tfms=Resize(192, method='squish'), # image augmentation\n    batch_tfms=aug_transforms(size=128, min_scale=0.75) # batch augmentation\n).dataloaders(trn_path)\n\nHere’s an explanation of each line:\nblocks=(ImageBlock,CategoryBlock,CategoryBlock),\nThe DataBlock will create 3 things from each file: an image (the contents of the file), and 2 categorical variables (the disease and the variety).\nn_inp=1,\nThere is 1 input (the image) – and therefore the other two variables (the two categories) are outputs.\nget_items=get_image_files,\nUse get_image_files to get a list of inputs.\nget_y = [parent_label,get_variety],\nTo create the two outputs for each file, call two functions: parent_label (from fastai) and get_variety (defined above).\nsplitter=RandomSplitter(0.2, seed=42),\nRandomly split the input into 80% train and 20% validation sets.\nitem_tfms=Resize(192, method='squish'),\nbatch_tfms=aug_transforms(size=128, min_scale=0.75)\nThese are the same item and batch transforms we’ve used in previous notebooks.\nLet’s take a look at part of a batch of this data:\n\ndls.show_batch(max_n=6)\n\n\n\n\nWe can see that fastai has created both the image input and two categorical outputs that we requested!\n\n\nReplicating the disease model\nNow we’ll replicate the same disease model we’ve made before, but have it work with this new data.\nThe key difference is that our metrics and loss will now receive three things instead of two: the model outputs (i.e. the metric and loss function inputs), and the two targets (disease and variety). Therefore, we need to define slight variations of our metric (error_rate) and loss function (cross_entropy) to pass on just the disease target:\n\n# modify our error function to accomodate two targets\ndef disease_err(inp,disease,variety): return error_rate(inp,disease)\n\n# modify our loss function to accomodate two targets\ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp,disease) # cross entropy function is what fastai picked for us when we just had a single outout category\n\n\n\nCross-Entropy\nNote that all of the loss functions in PyTorch have two versions. There is a class which you can instantiate passing in various tweaks, and there is also a version that is a function but everyone, including PyTorch official docs refers to this by F.\nLet’s take some time out to firm up on Cross-Entropy. To illustrate, let’s use a 5 class classification task where an image is classified as either a cat, dog, plane, fish or building.\nThe first step is:\n\nconvert raw outputs of our model (which at this stage are just numbers based on inital random weights applied) to probabilities using the SOFTMAX function.\n\nWe do this by first taking our raw outputs(z) and calculating e to the power of (z) for each prediction i. We then convert to probabilities by pro-rating the results between 0 and 1 - to give us our probabilities which sum to 1 - as illustrated below:\n\n\n\nSoftmax.JPG\n\n\nThe second step is:\n\ncalculate Cross-Entropy loss\n\nFor the purposes of this example, the rather terrifying looking equation below, can effectively be reduced to simply taking the log of output probabilities:\n\n\n\nX_Entropy.JPG\n\n\nThe mathematical image included above in my screenshotted spreadsheet are taken from Things that confused me about cross-entropy by Chris Said.\nWe’re now ready to create our learner. There’s just one wrinkle to be aware of. Now that our DataLoaders is returning multiple targets, fastai doesn’t know how many outputs our model will need. Therefore we have to pass n_out when we create our Learner – we need 10 outputs, one for each possible disease:\n\n!pip3 install --upgrade fastai\nfrom fastai.vision.all import vision_learner\n\n!pip install timm\nimport timm\n\n# replicate our disease model\narch = 'convnext_small_in22k'\nlearn = vision_learner(dls, arch, loss_func=disease_loss, metrics=disease_err, n_out=10).to_fp16() # note we now have to specify which loss_func to use and number of outputs n_out\nlr = 0.01\n\nWhen we train this model we should get similar results to what we’ve seen with similar models before:\n\n# train our model\nlearn.fine_tune(5, lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      time\n    \n  \n  \n    \n      0\n      1.234077\n      0.826925\n      0.270062\n      03:03\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      time\n    \n  \n  \n    \n      0\n      0.603252\n      0.421769\n      0.135031\n      05:13\n    \n    \n      1\n      0.470395\n      0.415115\n      0.125420\n      05:12\n    \n    \n      2\n      0.303026\n      0.212930\n      0.071120\n      05:13\n    \n    \n      3\n      0.179699\n      0.146253\n      0.042287\n      05:13\n    \n    \n      4\n      0.142097\n      0.138253\n      0.041326\n      05:13\n    \n  \n\n\n\n\n\nMulti-label classification\nIn order to predict both the probability of each disease, and of each variety, we’ll now need the model to output a tensor of length 20, since there are 10 possible diseases, and 10 possible varieties. We can do this by setting n_out=20:\n\n# set model outputs to 20 - 10 diseases and 10 varieties\nlearn = vision_learner(dls, arch, n_out=20).to_fp16()\n\nWe can define disease_loss just like we did previously, but with one important change: the input tensor is now length 20, not 10, so it doesn’t match the number of possible diseases. We can pick whatever part of the input we want to be used to predict disease. Let’s use the first 10 values:\n\n# we need to specify which part of inputs are for use in disease loss function \ndef disease_loss(inp,disease,variety): return F.cross_entropy(inp[:,:10],disease)\n\nThat means we can do the same thing for predicting variety, but use the last 10 values of the input, and set the target to variety instead of disease:\n\n# we need to specify which part of inputs are for use in variety loss function \ndef variety_loss(inp,disease,variety): return F.cross_entropy(inp[:,10:],variety)\n\nOur overall loss will then be the sum of these two losses:\n\n# overall loss - just add together loss functions for disease and variety\ndef combine_loss(inp,disease,variety): return disease_loss(inp,disease,variety)+variety_loss(inp,disease,variety)\n\nIt would be useful to view the error rate for each of the outputs too, so let’s do the same thing for out metrics:\n\n# function to include the error_rate for disease\ndef disease_err(inp,disease,variety): return error_rate(inp[:,:10],disease)\n\n# function to include the error_rate for disease\ndef variety_err(inp,disease,variety): return error_rate(inp[:,10:],variety)\n\n# combine disease error and variety error within variable err_metrics\nerr_metrics = (disease_err,variety_err)\n\nIt’s useful to see the loss for each of the outputs too, so we’ll add those as metrics:\n\n# combine error metrics and loss metrics within variable all_metrics \nall_metrics = err_metrics+(disease_loss,variety_loss)\n\nWe’re now ready to create and train our Learner:\n\n# pulling it all together into our Learner\nlearn = vision_learner(dls, arch, loss_func=combine_loss, metrics=all_metrics, n_out=20).to_fp16()\n\n\n# train the model\nlearn.fine_tune(5, lr)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      2.286528\n      1.215683\n      0.265257\n      0.113407\n      0.845696\n      0.369987\n      03:09\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      disease_err\n      variety_err\n      disease_loss\n      variety_loss\n      time\n    \n  \n  \n    \n      0\n      1.015679\n      0.607585\n      0.133109\n      0.062951\n      0.421834\n      0.185751\n      05:13\n    \n    \n      1\n      0.745607\n      0.412463\n      0.087938\n      0.043729\n      0.286902\n      0.125561\n      05:14\n    \n    \n      2\n      0.483214\n      0.263229\n      0.058626\n      0.025949\n      0.179259\n      0.083970\n      05:14\n    \n    \n      3\n      0.282286\n      0.204188\n      0.047093\n      0.017299\n      0.154198\n      0.049990\n      05:13\n    \n    \n      4\n      0.202148\n      0.174338\n      0.043248\n      0.013455\n      0.133468\n      0.040870\n      05:13\n    \n  \n\n\n\n\n\nKey takeaways\nSo, is this useful?\nWell… if we actually want a model that predicts multiple things, then yes, definitely! But as to whether it’s going to help us better predict rice disease, that is unknown. I haven’t come across any research that tackles this important question: when can a multi-target model improve the accuracy of the individual targets compared to a single target model? (That doesn’t mean it doesn’t exist of course – perhaps it does and I haven’t found it yet…)\nJeremy found that in previous projects there are cases where improvements to single targets can be made by using a multi-target model. It’ll be most useful when we’re having problems with overfitting and so try doing this with more epochs."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html",
    "href": "posts/Krakow_Football/Krakow_Football.html",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "",
    "text": "I arrived in Kraków a few months back and during my wanderings I noticed two football stadiums on either side of Błonia separated by just 700 metres. I didn’t know anything about either of these clubs but was told that the rivalry between them is intense. The so-called Holy War between Wisła Kraków and KS Cracovia has even featured in documentaries by Ross Kemp and Danny Dyer.\nI thought I would try to use Structured Query Language more commonly known as SQL to find out a bit more. To get the ball rolling I first needed to find a data source."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#accessing-sqlite-databases-using-python-and-pandas",
    "href": "posts/Krakow_Football/Krakow_Football.html#accessing-sqlite-databases-using-python-and-pandas",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Accessing SQLite Databases Using Python and Pandas",
    "text": "Accessing SQLite Databases Using Python and Pandas\nAfter downloading the database file which is in .sqlite format. my first question was:\n\nis it even possible to work with database data using Python within Jupyter Notebooks?\n\nThe good news is, yes it is possible :) After quite a bit of digging I found this article which gave me a platform to make a start on this project idea.\n\n# Install the required packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sqlite3\n\n\nThe sqlite3 module\nSQLite is a C library that provides a lightweight disk-based database that doesn’t require a separate server process and allows accessing the database using a nonstandard variant of the SQL query language. Some applications can use SQLite for internal data storage. It’s also possible to prototype an application using SQLite and then port the code to a larger database such as PostgreSQL or Oracle.\nThe sqlite3 module was written by Gerhard Häring and provides an SQL interface compliant with the DB-API 2.0 specification described by PEP 249, and requires SQLite 3.7.15 or newer.\n\n# Create a SQL connection to our downloaded SQLite database file\ncon = sqlite3.connect(\"Data/database.sqlite\")\n\nBasically you can run SQL queries just as you would within a database, and then convert the results to a pandas DataFrame for further analysis and presentation. Let’s have a look at each of the 7 tables in turn to establish what sort of information is included:"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-1---country",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-1---country",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 1 - Country",
    "text": "Table 1 - Country\n\n# Create a pandas DataFRame of the Country table\ncountry = pd.read_sql_query('''SELECT * \n                            FROM Country'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(country)\n\n       id         name\n0       1      Belgium\n1    1729      England\n2    4769       France\n3    7809      Germany\n4   10257        Italy\n5   13274  Netherlands\n6   15722       Poland\n7   17642     Portugal\n8   19694     Scotland\n9   21518        Spain\n10  24558  Switzerland\n\n\nSo, the database covers 11 countries and the country id for Poland is 15722."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-2---league",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-2---league",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 2 - League",
    "text": "Table 2 - League\n\n# Create a pandas DataFrame of the League table\nleagues = pd.read_sql_query('''SELECT * \n                            FROM League'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(leagues)\n\n       id  country_id                      name\n0       1           1    Belgium Jupiler League\n1    1729        1729    England Premier League\n2    4769        4769            France Ligue 1\n3    7809        7809     Germany 1. Bundesliga\n4   10257       10257             Italy Serie A\n5   13274       13274    Netherlands Eredivisie\n6   15722       15722        Poland Ekstraklasa\n7   17642       17642  Portugal Liga ZON Sagres\n8   19694       19694   Scotland Premier League\n9   21518       21518           Spain LIGA BBVA\n10  24558       24558  Switzerland Super League\n\n\nAnd, the id for the League name is the same as the country_id - for the Polish Ekstraklasa this is 15722."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-3---match",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-3---match",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 3 - Match",
    "text": "Table 3 - Match\n\n# Create a pandas DataFrame of the Match table\nmatches = pd.read_sql_query('''SELECT * \n                            FROM Match'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nmatches.head()\n\n\n\n\n\n  \n    \n      \n      id\n      country_id\n      league_id\n      season\n      stage\n      date\n      match_api_id\n      home_team_api_id\n      away_team_api_id\n      home_team_goal\n      ...\n      SJA\n      VCH\n      VCD\n      VCA\n      GBH\n      GBD\n      GBA\n      BSH\n      BSD\n      BSA\n    \n  \n  \n    \n      0\n      1\n      1\n      1\n      2008/2009\n      1\n      2008-08-17 00:00:00\n      492473\n      9987\n      9993\n      1\n      ...\n      4.00\n      1.65\n      3.40\n      4.50\n      1.78\n      3.25\n      4.00\n      1.73\n      3.40\n      4.20\n    \n    \n      1\n      2\n      1\n      1\n      2008/2009\n      1\n      2008-08-16 00:00:00\n      492474\n      10000\n      9994\n      0\n      ...\n      3.80\n      2.00\n      3.25\n      3.25\n      1.85\n      3.25\n      3.75\n      1.91\n      3.25\n      3.60\n    \n    \n      2\n      3\n      1\n      1\n      2008/2009\n      1\n      2008-08-16 00:00:00\n      492475\n      9984\n      8635\n      0\n      ...\n      2.50\n      2.35\n      3.25\n      2.65\n      2.50\n      3.20\n      2.50\n      2.30\n      3.20\n      2.75\n    \n    \n      3\n      4\n      1\n      1\n      2008/2009\n      1\n      2008-08-17 00:00:00\n      492476\n      9991\n      9998\n      5\n      ...\n      7.50\n      1.45\n      3.75\n      6.50\n      1.50\n      3.75\n      5.50\n      1.44\n      3.75\n      6.50\n    \n    \n      4\n      5\n      1\n      1\n      2008/2009\n      1\n      2008-08-16 00:00:00\n      492477\n      7947\n      9985\n      1\n      ...\n      1.73\n      4.50\n      3.40\n      1.65\n      4.50\n      3.50\n      1.65\n      4.75\n      3.30\n      1.67\n    \n  \n\n5 rows × 115 columns\n\n\n\nAs you can see we have a LOT of data here - 115 columns! Let’s focus on the season column for the moment. From the first 5 rows we can see that we have information starting from the 2008/2009 season. Let’s check how many seasons we have data for:\n\nmatches.season.unique()\n\narray(['2008/2009', '2009/2010', '2010/2011', '2011/2012', '2012/2013',\n       '2013/2014', '2014/2015', '2015/2016'], dtype=object)"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-4---player",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-4---player",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 4 - Player",
    "text": "Table 4 - Player\n\nplayers = pd.read_sql_query('''SELECT * \n                            FROM Player'''\n                            ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(players)\n\n          id  player_api_id          player_name  player_fifa_api_id  \\\n0          1         505942   Aaron Appindangoye              218353   \n1          2         155782      Aaron Cresswell              189615   \n2          3         162549          Aaron Doran              186170   \n3          4          30572        Aaron Galindo              140161   \n4          5          23780         Aaron Hughes               17725   \n...      ...            ...                  ...                 ...   \n11055  11071          26357       Zoumana Camara                2488   \n11056  11072         111182         Zsolt Laczko              164680   \n11057  11073          36491            Zsolt Low              111191   \n11058  11074          35506  Zurab Khizanishvili               47058   \n11059  11075          39902   Zvjezdan Misimovic              102359   \n\n                  birthday  height  weight  \n0      1992-02-29 00:00:00  182.88     187  \n1      1989-12-15 00:00:00  170.18     146  \n2      1991-05-13 00:00:00  170.18     163  \n3      1982-05-08 00:00:00  182.88     198  \n4      1979-11-08 00:00:00  182.88     154  \n...                    ...     ...     ...  \n11055  1979-04-03 00:00:00  182.88     168  \n11056  1986-12-18 00:00:00  182.88     176  \n11057  1979-04-29 00:00:00  180.34     154  \n11058  1981-10-06 00:00:00  185.42     172  \n11059  1982-06-05 00:00:00  180.34     176  \n\n[11060 rows x 7 columns]"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-5---player-attributes",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-5---player-attributes",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 5 - Player attributes",
    "text": "Table 5 - Player attributes\n\nplayer_attributes = pd.read_sql_query('''SELECT * \n                                      FROM Player_Attributes'''\n                                      ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nplayer_attributes.head()\n\n\n\n\n\n  \n    \n      \n      id\n      player_fifa_api_id\n      player_api_id\n      date\n      overall_rating\n      potential\n      preferred_foot\n      attacking_work_rate\n      defensive_work_rate\n      crossing\n      ...\n      vision\n      penalties\n      marking\n      standing_tackle\n      sliding_tackle\n      gk_diving\n      gk_handling\n      gk_kicking\n      gk_positioning\n      gk_reflexes\n    \n  \n  \n    \n      0\n      1\n      218353\n      505942\n      2016-02-18 00:00:00\n      67.0\n      71.0\n      right\n      medium\n      medium\n      49.0\n      ...\n      54.0\n      48.0\n      65.0\n      69.0\n      69.0\n      6.0\n      11.0\n      10.0\n      8.0\n      8.0\n    \n    \n      1\n      2\n      218353\n      505942\n      2015-11-19 00:00:00\n      67.0\n      71.0\n      right\n      medium\n      medium\n      49.0\n      ...\n      54.0\n      48.0\n      65.0\n      69.0\n      69.0\n      6.0\n      11.0\n      10.0\n      8.0\n      8.0\n    \n    \n      2\n      3\n      218353\n      505942\n      2015-09-21 00:00:00\n      62.0\n      66.0\n      right\n      medium\n      medium\n      49.0\n      ...\n      54.0\n      48.0\n      65.0\n      66.0\n      69.0\n      6.0\n      11.0\n      10.0\n      8.0\n      8.0\n    \n    \n      3\n      4\n      218353\n      505942\n      2015-03-20 00:00:00\n      61.0\n      65.0\n      right\n      medium\n      medium\n      48.0\n      ...\n      53.0\n      47.0\n      62.0\n      63.0\n      66.0\n      5.0\n      10.0\n      9.0\n      7.0\n      7.0\n    \n    \n      4\n      5\n      218353\n      505942\n      2007-02-22 00:00:00\n      61.0\n      65.0\n      right\n      medium\n      medium\n      48.0\n      ...\n      53.0\n      47.0\n      62.0\n      63.0\n      66.0\n      5.0\n      10.0\n      9.0\n      7.0\n      7.0\n    \n  \n\n5 rows × 42 columns"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-6---teams",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-6---teams",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 6 - Teams",
    "text": "Table 6 - Teams\n\n# Create a pandas DataFRame of the Team table\nteams = pd.read_sql_query('''SELECT * \n                          FROM Team'''\n                          ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nprint(teams)\n\n        id  team_api_id  team_fifa_api_id      team_long_name team_short_name\n0        1         9987             673.0            KRC Genk             GEN\n1        2         9993             675.0        Beerschot AC             BAC\n2        3        10000           15005.0    SV Zulte-Waregem             ZUL\n3        4         9994            2007.0    Sporting Lokeren             LOK\n4        5         9984            1750.0   KSV Cercle Brugge             CEB\n..     ...          ...               ...                 ...             ...\n294  49479        10190             898.0       FC St. Gallen             GAL\n295  49837        10191            1715.0             FC Thun             THU\n296  50201         9777             324.0         Servette FC             SER\n297  50204         7730            1862.0  FC Lausanne-Sports             LAU\n298  51606         7896               NaN              Lugano             LUG\n\n[299 rows x 5 columns]\n\n\nSo we can see that the Team table has information about 299 teams. It would be useful to see the breakdown of the number of teams from each country league. It’s not possibe to do this using the information contained in the Team table alone. We will need to join two tables together to obtain this information:\n\n# Join the Team table to the Match table using an INNER JOIN and create a pandas DataFrame\nteam_league = pd.read_sql_query('''SELECT DISTINCT t.team_api_id AS team_id, t.team_long_name AS team_name, l.name AS league \n                                FROM Team AS t \n                                JOIN Match AS m ON t.team_api_id = m.home_team_api_id \n                                JOIN League AS l ON l.country_id = m.country_id''',con)\n\n# Verify that result of SQL query is stored in the dataframe\nteam_league.head(10)\n\n\n\n\n\n  \n    \n      \n      team_id\n      team_name\n      league\n    \n  \n  \n    \n      0\n      1601\n      Ruch Chorzów\n      Poland Ekstraklasa\n    \n    \n      1\n      1773\n      Oud-Heverlee Leuven\n      Belgium Jupiler League\n    \n    \n      2\n      1957\n      Jagiellonia Białystok\n      Poland Ekstraklasa\n    \n    \n      3\n      2033\n      S.C. Olhanense\n      Portugal Liga ZON Sagres\n    \n    \n      4\n      2182\n      Lech Poznań\n      Poland Ekstraklasa\n    \n    \n      5\n      2183\n      P. Warszawa\n      Poland Ekstraklasa\n    \n    \n      6\n      2186\n      Cracovia\n      Poland Ekstraklasa\n    \n    \n      7\n      4049\n      Tubize\n      Belgium Jupiler League\n    \n    \n      8\n      4064\n      Feirense\n      Portugal Liga ZON Sagres\n    \n    \n      9\n      4087\n      Évian Thonon Gaillard FC\n      France Ligue 1\n    \n  \n\n\n\n\nGreat, now we have the required information to show the number of teams split across the 11 distinct leagues. Let’s use pandas to sort and group this data and then illustrate graphically using matplotlib:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"European Soccer Database \"\nsubtitle_string = \"Number of teams per country\"\n\nx = team_league.groupby(\"league\")[\"team_name\"].count().sort_values(ascending=True)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Number of teams per country')\n\n\n\n\n\n\nteam_league.groupby(\"league\")[\"team_name\"].count().sort_values(ascending=False)\n\nleague\nFrance Ligue 1              35\nEngland Premier League      34\nSpain LIGA BBVA             33\nItaly Serie A               32\nGermany 1. Bundesliga       30\nPortugal Liga ZON Sagres    29\nBelgium Jupiler League      25\nNetherlands Eredivisie      25\nPoland Ekstraklasa          24\nScotland Premier League     17\nSwitzerland Super League    15\nName: team_name, dtype: int64\n\n\nThere is quite a disparity between the various leagues. The French Ligue 1 has 20 teams (we have data for 35 teams because the dataset covers 8 seasons and accounts for promotions from the lower leagues), whereas the Swiss Super League only has 10 teams."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#table-7---team-attributes",
    "href": "posts/Krakow_Football/Krakow_Football.html#table-7---team-attributes",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Table 7 - Team attributes",
    "text": "Table 7 - Team attributes\n\n# Create a pandas DataFRame of the Team table\nteam_attributes= pd.read_sql_query('''SELECT * \n                                   FROM Team_Attributes'''\n                                   ,con)\n\n# Verify that result of SQL query is stored in the dataframe\nteam_attributes.head()\n\n\n\n\n\n  \n    \n      \n      id\n      team_fifa_api_id\n      team_api_id\n      date\n      buildUpPlaySpeed\n      buildUpPlaySpeedClass\n      buildUpPlayDribbling\n      buildUpPlayDribblingClass\n      buildUpPlayPassing\n      buildUpPlayPassingClass\n      ...\n      chanceCreationShooting\n      chanceCreationShootingClass\n      chanceCreationPositioningClass\n      defencePressure\n      defencePressureClass\n      defenceAggression\n      defenceAggressionClass\n      defenceTeamWidth\n      defenceTeamWidthClass\n      defenceDefenderLineClass\n    \n  \n  \n    \n      0\n      1\n      434\n      9930\n      2010-02-22 00:00:00\n      60\n      Balanced\n      NaN\n      Little\n      50\n      Mixed\n      ...\n      55\n      Normal\n      Organised\n      50\n      Medium\n      55\n      Press\n      45\n      Normal\n      Cover\n    \n    \n      1\n      2\n      434\n      9930\n      2014-09-19 00:00:00\n      52\n      Balanced\n      48.0\n      Normal\n      56\n      Mixed\n      ...\n      64\n      Normal\n      Organised\n      47\n      Medium\n      44\n      Press\n      54\n      Normal\n      Cover\n    \n    \n      2\n      3\n      434\n      9930\n      2015-09-10 00:00:00\n      47\n      Balanced\n      41.0\n      Normal\n      54\n      Mixed\n      ...\n      64\n      Normal\n      Organised\n      47\n      Medium\n      44\n      Press\n      54\n      Normal\n      Cover\n    \n    \n      3\n      4\n      77\n      8485\n      2010-02-22 00:00:00\n      70\n      Fast\n      NaN\n      Little\n      70\n      Long\n      ...\n      70\n      Lots\n      Organised\n      60\n      Medium\n      70\n      Double\n      70\n      Wide\n      Cover\n    \n    \n      4\n      5\n      77\n      8485\n      2011-02-22 00:00:00\n      47\n      Balanced\n      NaN\n      Little\n      52\n      Mixed\n      ...\n      52\n      Normal\n      Organised\n      47\n      Medium\n      47\n      Press\n      52\n      Normal\n      Cover\n    \n  \n\n5 rows × 25 columns"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#which-polish-teams-are-included-in-the-database",
    "href": "posts/Krakow_Football/Krakow_Football.html#which-polish-teams-are-included-in-the-database",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Which Polish teams are included in the database?",
    "text": "Which Polish teams are included in the database?\n\n\n\npoland.jpg\n\n\n\n# Subquery filtering list with IN\npoland_league = pd.read_sql_query ('''SELECT team_long_name, \n                                   team_short_name AS abbr, \n                                   team_api_id AS team_id \n                                   FROM Team WHERE team_api_id \n                                   IN (SELECT home_team_api_id FROM Match WHERE country_id = 15722)'''\n                                   ,con)\npoland_league\n\n\n\n\n\n  \n    \n      \n      team_long_name\n      abbr\n      team_id\n    \n  \n  \n    \n      0\n      Ruch Chorzów\n      CHO\n      1601\n    \n    \n      1\n      Jagiellonia Białystok\n      BIA\n      1957\n    \n    \n      2\n      Lech Poznań\n      POZ\n      2182\n    \n    \n      3\n      P. Warszawa\n      PWA\n      2183\n    \n    \n      4\n      Cracovia\n      CKR\n      2186\n    \n    \n      5\n      Górnik Łęczna\n      LEC\n      8019\n    \n    \n      6\n      Polonia Bytom\n      GOR\n      8020\n    \n    \n      7\n      Zagłębie Lubin\n      ZAG\n      8021\n    \n    \n      8\n      Pogoń Szczecin\n      POG\n      8023\n    \n    \n      9\n      Widzew Łódź\n      WID\n      8024\n    \n    \n      10\n      Śląsk Wrocław\n      SLA\n      8025\n    \n    \n      11\n      Zawisza Bydgoszcz\n      ZAW\n      8027\n    \n    \n      12\n      Piast Gliwice\n      PIG\n      8028\n    \n    \n      13\n      Lechia Gdańsk\n      LGD\n      8030\n    \n    \n      14\n      Polonia Bytom\n      POB\n      8031\n    \n    \n      15\n      Podbeskidzie Bielsko-Biała\n      POD\n      8033\n    \n    \n      16\n      Odra Wodzisław\n      ODR\n      8242\n    \n    \n      17\n      Widzew Łódź\n      LOD\n      8244\n    \n    \n      18\n      Korona Kielce\n      KKI\n      8245\n    \n    \n      19\n      Arka Gdynia\n      ARK\n      8322\n    \n    \n      20\n      GKS Bełchatów\n      BEL\n      8569\n    \n    \n      21\n      Legia Warszawa\n      LEG\n      8673\n    \n    \n      22\n      Wisła Kraków\n      WIS\n      10265\n    \n    \n      23\n      Termalica Bruk-Bet Nieciecza\n      TBN\n      177361\n    \n  \n\n\n\n\nOne or two familiar names there, Lech Poznań, Legia Warszawa. Having recently relocated to Kraków the focus for the rest of this project will be on the two Kraków teams:\n\nWisła Kraków\nKS Cracovia\n\nThe history of both clubs appears to be colourful. Cracovia received a 5 point deduction in 2020/2021 for alleged match-rigging dating back to 2003/2004, and this article on the history of Wisła Kraków is an entertaining read!"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#the-holy-war---święta-wojna",
    "href": "posts/Krakow_Football/Krakow_Football.html#the-holy-war---święta-wojna",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "The Holy War - Święta Wojna",
    "text": "The Holy War - Święta Wojna\nThe term “Holy War” refers to the intense rivalry between the two Kraków-based teams; Wisła Kraków and KS Cracovia. In 1906, the establishment of the two first Polish football clubs, Cracovia and Wisła, created a rivalry that now dates back more than 100 years. The term “Holy War” was first used to describe the rivalry of Kraków’s Jewish teams, Makkabi and Jutrzenka. A Jutrzenka defender, Ludwik Gintel, who later joined the Cracovia side referred to the derby match against Wisła as the “Holy War”. The phrase was incorporated into a song and has since been popular amongst both Wisła and Cracovia fans.\nThe first recorded Kraków Derby was contested on 20 September 1908, a 1–1 draw.\nWisła Kraków were formed in 1906 and play their football at the Stadion Miejski im. Henryka Reymana which has a capacity of 33,130.\nThey have 14 league titles in their trophy cabinet, the most recent win was 2010/2011.\n \nKS Cracovia also formed in 1906, are widely considered to be the oldest club in Kraków. They play their football at the Stadion Cracovii im. Józefa Piłsudskiego which has a capacity of 15,016.\nThey have 5 league titles in their trophy cabinet, however they are getting a bit dusty. The last time they won the title was 1948.\n \nSo, what happens when these clubs go toe to toe?! Let’s find out by looking at the results for the matches between the bitter rivals over the period of study:\n\n# Use multiple CASE WHEN THEN queries to obtain outcomes from all matches between Wisła Kraków and Cracovia\nderbies = pd.read_sql_query('''SELECT date, season, home_team_api_id, away_team_api_id, home_team_goal,away_team_goal,\n                                 CASE WHEN home_team_api_ID = 2186 AND home_team_goal > away_team_goal \n                                 THEN 'KS Cracovia win!'\n                                WHEN home_team_api_ID = 2186 AND home_team_goal < away_team_goal \n                                THEN 'Wisła Kraków win!'\n                                WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal\n                                THEN 'Wisła Kraków win!'\n                                 WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal\n                                THEN 'KS Cracovia win!'\n                                ELSE 'Draw' END AS outcome \n                                FROM Match\n                                WHERE home_team_api_id = 2186 AND away_team_api_id = 10265\n                                OR home_team_api_id = 10265 AND away_team_api_id = 2186'''                                \n                            ,con)\n\n\n# Create a piechart showing the derby outcomes\nderbies['outcome'].value_counts().plot(kind='pie',autopct='%.2f')\n\n<AxesSubplot: ylabel='outcome'>\n\n\n\n\n\n\nderbies\n\n\n\n\n\n  \n    \n      \n      date\n      season\n      home_team_api_id\n      away_team_api_id\n      home_team_goal\n      away_team_goal\n      outcome\n    \n  \n  \n    \n      0\n      2009-03-22 00:00:00\n      2008/2009\n      10265\n      2186\n      4\n      1\n      Wisła Kraków win!\n    \n    \n      1\n      2008-08-31 00:00:00\n      2008/2009\n      2186\n      10265\n      1\n      1\n      Draw\n    \n    \n      2\n      2009-11-22 00:00:00\n      2009/2010\n      10265\n      2186\n      0\n      1\n      Cracovia win!\n    \n    \n      3\n      2010-05-11 00:00:00\n      2009/2010\n      2186\n      10265\n      1\n      1\n      Draw\n    \n    \n      4\n      2010-11-05 00:00:00\n      2010/2011\n      2186\n      10265\n      0\n      1\n      Wisła Kraków win!\n    \n    \n      5\n      2011-05-15 00:00:00\n      2010/2011\n      10265\n      2186\n      1\n      0\n      Wisła Kraków win!\n    \n    \n      6\n      2011-11-06 00:00:00\n      2011/2012\n      2186\n      10265\n      1\n      0\n      Cracovia win!\n    \n    \n      7\n      2012-04-30 00:00:00\n      2011/2012\n      10265\n      2186\n      1\n      0\n      Wisła Kraków win!\n    \n    \n      8\n      2014-02-23 00:00:00\n      2013/2014\n      10265\n      2186\n      3\n      1\n      Wisła Kraków win!\n    \n    \n      9\n      2013-09-21 00:00:00\n      2013/2014\n      2186\n      10265\n      1\n      1\n      Draw\n    \n    \n      10\n      2014-09-28 00:00:00\n      2014/2015\n      2186\n      10265\n      1\n      0\n      Cracovia win!\n    \n    \n      11\n      2015-03-21 00:00:00\n      2014/2015\n      10265\n      2186\n      2\n      1\n      Wisła Kraków win!\n    \n    \n      12\n      2015-11-29 00:00:00\n      2015/2016\n      10265\n      2186\n      1\n      2\n      Cracovia win!\n    \n    \n      13\n      2015-07-24 00:00:00\n      2015/2016\n      2186\n      10265\n      1\n      1\n      Draw\n    \n  \n\n\n\n\nSo, Wisła Kraków have the bragging rights for the period of study. The clubs met 14 times (no derby in 2012/2013 due to KS Cracovia being relegated from the top league in 2011/2012) between 2008/2009 and 2015/2016 :\n\nWisła Kraków wins = 6\nKS Cracovia wins = 4\nDraws = 4\n\nBut of course it’s not all about the derby matches. Let’s now take a look at how the two Kraków rivals performed overall."
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#wisła-kraków",
    "href": "posts/Krakow_Football/Krakow_Football.html#wisła-kraków",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Wisła Kraków",
    "text": "Wisła Kraków\n\n\n2008/2009 to 2015/2016\n\nGooooooooooool !\n\n# Use a SQL CASE WHEN with SUM query to display a table summarizing goals scored and conceded for each season\n# Multiply goals conceded by -1 to allow bi-directional plotting\n\nwisla_krakow_goals = pd.read_sql_query('''SELECT season, SUM(CASE WHEN home_team_api_id = 10265 THEN home_team_goal END) AS home_goals_scored, \n                                       SUM(CASE WHEN away_team_api_id = 10265 THEN away_team_goal END) AS away_goals_scored,\n                                       SUM(CASE WHEN home_team_api_id = 10265 THEN away_team_goal END)*-1 AS home_goals_conceded, \n                                       SUM(CASE WHEN away_team_api_id = 10265 THEN home_team_goal END)*-1 AS away_goals_conceded \n                                       FROM Match \n                                       GROUP BY season'''\n                                       ,con)\n\n\n# Plot a bi-directional stacked bar plot showing home and away goals scored and conceded each season\nwisla_krakow_goals.plot.bar(x='season', stacked=True, title ='Wisła Kraków - goals for and against')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_goals\n\n\n\n\n\n  \n    \n      \n      season\n      home_goals_scored\n      away_goals_scored\n      home_goals_conceded\n      away_goals_conceded\n    \n  \n  \n    \n      0\n      2008/2009\n      31\n      22\n      -9\n      -12\n    \n    \n      1\n      2009/2010\n      18\n      30\n      -9\n      -11\n    \n    \n      2\n      2010/2011\n      27\n      17\n      -11\n      -18\n    \n    \n      3\n      2011/2012\n      14\n      15\n      -11\n      -15\n    \n    \n      4\n      2012/2013\n      15\n      13\n      -14\n      -21\n    \n    \n      5\n      2013/2014\n      23\n      15\n      -6\n      -24\n    \n    \n      6\n      2014/2015\n      20\n      27\n      -18\n      -21\n    \n    \n      7\n      2015/2016\n      20\n      25\n      -17\n      -18\n    \n  \n\n\n\n\nWe have data for all 8 seasons which confirms that they were not relegated from the top flight during the period of study. Generally speaking, for the period of study Wisła Kraków comfortably score more goals than they concede, the exception being season 2012/13 where they scored just 28 goals (only four more than relegated GKS Bełchatów) they finished 7th that year. We can represent the goal ‘difference’ graphically:\n\n\nGoal difference\n\n# Add a goal difference column to the wisla_krakow_goals DataFrame\nwisla_krakow_goals[\"goal_diff\"] = wisla_krakow_goals[\"home_goals_scored\"] + wisla_krakow_goals[\"away_goals_scored\"] + wisla_krakow_goals[\"home_goals_conceded\"] + wisla_krakow_goals[\"away_goals_conceded\"] \n\n# Select the season and goal difference columns and plot\nwisla_krakow_goal_diff = wisla_krakow_goals[[\"season\", \"goal_diff\"]]\nwisla_krakow_goal_diff.plot.bar(x='season', stacked=False, title ='Wisła Kraków - goal difference')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - goal difference'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_goal_diff\n\n\n\n\n\n  \n    \n      \n      season\n      goal_diff\n    \n  \n  \n    \n      0\n      2008/2009\n      32\n    \n    \n      1\n      2009/2010\n      28\n    \n    \n      2\n      2010/2011\n      15\n    \n    \n      3\n      2011/2012\n      3\n    \n    \n      4\n      2012/2013\n      -7\n    \n    \n      5\n      2013/2014\n      8\n    \n    \n      6\n      2014/2015\n      8\n    \n    \n      7\n      2015/2016\n      10\n    \n  \n\n\n\n\nFor the period of study Wisła Kraków scored more goals then they conceded, except for the 2012/2013 season.\n\n\nYou win some, you lose some\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won and lost\nwisla_krakow_wins_losses =  pd.read_sql_query('''SELECT season, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                              FROM Match \n                                              GROUP BY season'''\n                                              ,con)\n\n\n# Convert defeats to a negative value - multiply by -1 to enable bi-directional plotting\nwisla_krakow_wins_losses.home_defeats = wisla_krakow_wins_losses.home_defeats * -1\nwisla_krakow_wins_losses.away_defeats = wisla_krakow_wins_losses.away_defeats * -1\n\n# Plot a stacked bar plot showing home and away wins and defeats for each season\nwisla_krakow_wins_losses.plot.bar(x='season', stacked=True, title ='Wisła Kraków - games won and lost')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - games won and lost'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_wins_losses\n\n\n\n\n\n  \n    \n      \n      season\n      home_wins\n      away_wins\n      home_defeats\n      away_defeats\n    \n  \n  \n    \n      0\n      2008/2009\n      13\n      6\n      -1\n      -3\n    \n    \n      1\n      2009/2010\n      8\n      11\n      -4\n      -2\n    \n    \n      2\n      2010/2011\n      11\n      6\n      -2\n      -6\n    \n    \n      3\n      2011/2012\n      7\n      5\n      -6\n      -5\n    \n    \n      4\n      2012/2013\n      6\n      4\n      -6\n      -6\n    \n    \n      5\n      2013/2014\n      10\n      2\n      -3\n      -6\n    \n    \n      6\n      2014/2015\n      6\n      5\n      -4\n      -5\n    \n    \n      7\n      2015/2016\n      3\n      5\n      -4\n      -5\n    \n  \n\n\n\n\nFrom the above stacked bar chart we can see a downward trend. Wisła Kraków only won 8 matches in 2015/2016, finishing the season in 11th place.\n\n\nHome advantage?\n\n# Run an SQl query using COUNT CASE WHEN to extract % of home and away games won\nwisla_krakow_win_pct = pd.read_sql_query('''SELECT season, \n                                         ROUND(AVG(CASE WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal THEN 1 \n                                                   WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal THEN 0 END)*100,2) AS pct_home_wins, \n                                         ROUND(AVG(CASE WHEN away_team_api_id = 10265 AND away_team_goal > home_team_goal THEN 1 \n                                                   WHEN away_team_api_id = 10265 AND away_team_goal < home_team_goal THEN 0 END)*100,2) AS pct_away_wins \n                                         FROM Match \n                                         GROUP BY season'''\n                                         ,con)\n\n# Plot a bar plot showing % of home and away wins for each season\nwisla_krakow_win_pct.plot.bar(x='season', stacked=False, title ='Wisła Kraków - win percentages')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - win percentages'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_win_pct\n\n\n\n\n\n  \n    \n      \n      season\n      pct_home_wins\n      pct_away_wins\n    \n  \n  \n    \n      0\n      2008/2009\n      92.86\n      66.67\n    \n    \n      1\n      2009/2010\n      66.67\n      84.62\n    \n    \n      2\n      2010/2011\n      84.62\n      50.00\n    \n    \n      3\n      2011/2012\n      53.85\n      50.00\n    \n    \n      4\n      2012/2013\n      50.00\n      40.00\n    \n    \n      5\n      2013/2014\n      76.92\n      25.00\n    \n    \n      6\n      2014/2015\n      60.00\n      50.00\n    \n    \n      7\n      2015/2016\n      42.86\n      50.00\n    \n  \n\n\n\n\nIn the early part of the period of study, Wisła Kraków had a reputation of being a tough nut to crack at home. They won the league in 2008/2009, winning just under 93% of their home matches! They also had a high degreee of success on the road, winning two thirds of their away matches. They won the league again 2010/2011 with a lower percentage of wins.\n\n\nPoints make prizes\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won,lost, and drawn\nwisla_krakow_points =  pd.read_sql_query('''SELECT season, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal = away_team_goal THEN id END) AS home_draws, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal = home_team_goal THEN id END) AS away_draws, \n                                              COUNT(CASE WHEN home_team_api_id = 10265 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                              COUNT(CASE WHEN away_team_api_id = 10265 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                              FROM Match \n                                              GROUP BY season'''\n                                              ,con)\n\n# Calculate points - 3 for a win, 1 for a draw, 0 for a loss\nwisla_krakow_points[\"points\"] = wisla_krakow_points['home_wins'] * 3 +  wisla_krakow_points['away_wins'] * 3 + wisla_krakow_points['home_draws'] + wisla_krakow_points['away_draws']\nwisla_krakow_points = wisla_krakow_points[[\"season\",\"points\"]]\n\n# Plot a bar plot showing total points for each season\nwisla_krakow_points.plot.bar(x='season', stacked=False, title ='Wisła Kraków - total points')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - total points'}, xlabel='season'>\n\n\n\n\n\n\nwisla_krakow_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2008/2009\n      64\n    \n    \n      1\n      2009/2010\n      62\n    \n    \n      2\n      2010/2011\n      56\n    \n    \n      3\n      2011/2012\n      43\n    \n    \n      4\n      2012/2013\n      38\n    \n    \n      5\n      2013/2014\n      45\n    \n    \n      6\n      2014/2015\n      43\n    \n    \n      7\n      2015/2016\n      37\n    \n  \n\n\n\n\nWisła Kraków won the league in 2008/2009 with 64 points but 62 points the following year was only good enough for second place (behind a Robert Lewandowski inspired Lech Poznań). Only 56 points were needed to win the league in 2010/2011.\n\n\nFinal standings\nLet’s add an extra column for league position data:\n\n# Create an extra column and insert into our original points DataFrame\nwisla_krakow_points.insert(0,\"Position\",[1,2,1,7,7,5,5,11],True)\n\n\n# Plot a horizontal bar plot of league position\nwisla_krakow_points.plot.barh(x='season', y='Position', title ='Wisła Kraków - final league position')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - final league position'}, ylabel='season'>\n\n\n\n\n\n\nwisla_krakow_position = wisla_krakow_points[[\"season\",\"points\"]]\n\n\n\n\n\n  \n    \n      \n      Position\n      season\n      points\n    \n  \n  \n    \n      0\n      1\n      2008/2009\n      64\n    \n    \n      1\n      2\n      2009/2010\n      62\n    \n    \n      2\n      1\n      2010/2011\n      56\n    \n    \n      3\n      7\n      2011/2012\n      43\n    \n    \n      4\n      7\n      2012/2013\n      38\n    \n    \n      5\n      5\n      2013/2014\n      45\n    \n    \n      6\n      5\n      2014/2015\n      43\n    \n    \n      7\n      11\n      2015/2016\n      37\n    \n  \n\n\n\n\nThe general trend for Wisła Kraków is one of decline. They have struggled since the heights of 2008/2009 to 2010/2011 where they were placed 1, 2, 1, finishing 11th in 2015/2016.\n\n\n\n2016/2017 to 2021/2022\nThe European Soccer Database is now somewhat out of date, so I decided to gather data for seasons 2016/2017 to 2021/2022 and create dictionaries for conversion to DataFrames.\n\nGooooooooooool !\n\n# Create a dictionary for the most recent years\nwisla_recent = {'season': ['2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021','2021/2022'],\n        'goals_scored': [45, 41, 55, 37, 39, 37],\n        'goals_conceded': [46, 36, 48, 47,42, 54],\n        'games_won': [13, 12, 12, 10, 8, 7],\n        'games_lost': [12, 10, 12, 15, 13, 17],\n        'games_drawn': [5, 8, 6, 5, 9, 10],\n         'points':[44, 44, 42, 35, 33, 31],\n         'position':[5, 7, 9, 13, 13, 17]\n        } \n\n# Convert dictionary to a DataFrame\nwisla_recent = pd.DataFrame.from_dict(wisla_recent)  \n\n\n# Filter for columns to be plotted\nwisla_recent_goals = wisla_recent[['season','goals_scored','goals_conceded',]]\n\n# Plot a bar plot showing goals scored and conceded each season\nwisla_recent_goals.plot.bar(x='season', stacked=False, title ='Wisła Kraków - goals for and against')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\nwisla_recent_goals\n\n\n\n\n\n  \n    \n      \n      season\n      goals_scored\n      goals_conceded\n    \n  \n  \n    \n      0\n      2016/2017\n      45\n      46\n    \n    \n      1\n      2017/2018\n      41\n      36\n    \n    \n      2\n      2018/2019\n      55\n      48\n    \n    \n      3\n      2019/2020\n      37\n      47\n    \n    \n      4\n      2020/2021\n      39\n      42\n    \n    \n      5\n      2021/2022\n      37\n      54\n    \n  \n\n\n\n\nWisła Kraków leaked 54 goals in the 2021/2022 season which was enough to see them relegated to the 2nd tier of Polish football.\n\n\nWin, lose or draw\n\n# Filter for columns to be plotted\nwisla_recent_wins = wisla_recent[['season','games_won','games_drawn','games_lost']]\n\n# Plot a bar plot showing goals won, lost, and drawn each season\nwisla_recent_wins.plot.bar(x='season', stacked=False, title ='Wisła Kraków - win, draw, lose')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - win, draw, lose'}, xlabel='season'>\n\n\n\n\n\n\nwisla_recent_wins\n\n\n\n\n\n  \n    \n      \n      season\n      games_won\n      games_drawn\n      games_lost\n    \n  \n  \n    \n      0\n      2016/2017\n      13\n      5\n      12\n    \n    \n      1\n      2017/2018\n      12\n      8\n      10\n    \n    \n      2\n      2018/2019\n      12\n      6\n      12\n    \n    \n      3\n      2019/2020\n      10\n      5\n      15\n    \n    \n      4\n      2020/2021\n      8\n      9\n      13\n    \n    \n      5\n      2021/2022\n      7\n      10\n      17\n    \n  \n\n\n\n\nWisła Kraków lost 17 out of 34 games in the 2021/2022 season which contributed to their relegation to the 2nd tier of Polish football.\n\n\nPoints make prizes\n\n# Filter for columns to be plotted\nwisla_recent_points = wisla_recent[['season','points']]\n\n# Plot a bar plot showing total points for each season\nwisla_recent_points.plot.bar(x='season', stacked=False, title ='Wisła Kraków - total points')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - total points'}, xlabel='season'>\n\n\n\n\n\n\nwisla_recent_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2016/2017\n      44\n    \n    \n      1\n      2017/2018\n      44\n    \n    \n      2\n      2018/2019\n      42\n    \n    \n      3\n      2019/2020\n      35\n    \n    \n      4\n      2020/2021\n      33\n    \n    \n      5\n      2021/2022\n      31\n    \n  \n\n\n\n\nAs we can see. there was a steady decline in point totals from 2018/2019 onwards. Wisła Kraków accumulated just 31 points from their 34 matches in the 2021/2022 season which was not enough to retain their top flight status.\n\n\nFinal standings\n\n# Plot a horizontal bar plot of league position\nwisla_recent.plot.barh(x='season', y='position', title ='Wisła Kraków - final league position')\n\n<AxesSubplot: title={'center': 'Wisła Kraków - final league position'}, ylabel='season'>\n\n\n\n\n\n\nwisla_position = wisla_recent[[\"season\",\"position\"]]\nwisla_position \n\n\n\n\n\n  \n    \n      \n      season\n      position\n    \n  \n  \n    \n      0\n      2016/2017\n      5\n    \n    \n      1\n      2017/2018\n      7\n    \n    \n      2\n      2018/2019\n      9\n    \n    \n      3\n      2019/2020\n      13\n    \n    \n      4\n      2020/2021\n      13\n    \n    \n      5\n      2021/2022\n      17\n    \n  \n\n\n\n\nThe decline in points totals is mirrored by a declining league position. Wisła Kraków accumulated just 31 points from their 34 matches in the 2021/2022 season and were relegated.\n\n\n\n2022/2023\nThe second tier I Liga table for 2022/2023 at the time of writing is shown below. The season is on hold as the FIFA World Cup - Qatar 2022 progresses. Wisła Kraków are languishing mid table. A long way away from the dizzy heights of the Ekstraklasa championship winning side of 2010/2011.\n\n\n\nLiga_1_2022_2-23.PNG"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#ks-cracovia",
    "href": "posts/Krakow_Football/Krakow_Football.html#ks-cracovia",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "KS Cracovia",
    "text": "KS Cracovia\n\n\n2008/2009 to 2015/2016\n\nGooooooooooool !\n\n# Now that we already have the query structure in place, we can simply replace the home_team_api_id with that for Cracovia - 2186\ncracovia_goals = pd.read_sql_query('''SELECT season, \n                                   ROUND(SUM(CASE WHEN home_team_api_id = 2186 THEN home_team_goal END),2) AS home_goals_scored, \n                                   ROUND(SUM(CASE WHEN away_team_api_id = 2186 THEN away_team_goal END),2) AS away_goals_scored, \n                                   SUM(CASE WHEN home_team_api_id = 2186 THEN away_team_goal END)*-1 AS home_goals_conceded, \n                                   SUM(CASE WHEN away_team_api_id = 2186 THEN home_team_goal END)*-1 AS away_goals_conceded \n                                   FROM Match \n                                   GROUP BY season'''\n                                   ,con)\n\n# Plot a stacked bar plot showing home and away goals scored and conceded each season\ncracovia_goals.plot.bar(x='season', stacked=True, title ='KS Cracovia - goals for and against')\n\n<AxesSubplot: title={'center': 'KS Cracovia - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_goals \n\n\n\n\n\n  \n    \n      \n      season\n      home_goals_scored\n      away_goals_scored\n      home_goals_conceded\n      away_goals_conceded\n    \n  \n  \n    \n      0\n      2008/2009\n      12.0\n      12.0\n      -8.0\n      -32.0\n    \n    \n      1\n      2009/2010\n      18.0\n      7.0\n      -23.0\n      -16.0\n    \n    \n      2\n      2010/2011\n      30.0\n      7.0\n      -21.0\n      -26.0\n    \n    \n      3\n      2011/2012\n      10.0\n      10.0\n      -20.0\n      -21.0\n    \n    \n      4\n      2012/2013\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      2013/2014\n      19.0\n      18.0\n      -22.0\n      -21.0\n    \n    \n      6\n      2014/2015\n      21.0\n      14.0\n      -17.0\n      -24.0\n    \n    \n      7\n      2015/2016\n      34.0\n      23.0\n      -19.0\n      -23.0\n    \n  \n\n\n\n\nInterestingly, we have a Not a Number (NaN) reference for season 2012/2013. On follow up it transpires that KS Cracovia finished bottom of the Ekstraklasa in 2011/2012 (as we can see above they only scored 20 goals in total that season) and were relegated to the I liga, the second tier of Polish football. They made an immediate return and the 37 goals scored in 2013/2014 was enough to secure them a repectable 9th spot out of 16. They scored 57 goals in 2015/2016 (finishing 5th), a goal tally only bettered by champions Legia Warszawa.\n\n\nGoal difference\n\n# Add a goal difference column to the cracovia_goals DataFrame\ncracovia_goals[\"goal_diff\"] = cracovia_goals[\"home_goals_scored\"] + cracovia_goals[\"away_goals_scored\"] + cracovia_goals[\"home_goals_conceded\"] + cracovia_goals[\"away_goals_conceded\"] \n\n# Select the season and goal difference columns and plot\ncracovia_goal_diff = cracovia_goals[[\"season\", \"goal_diff\"]]\ncracovia_goal_diff.plot.bar(x='season', stacked=False, title ='KS Cracovia - goal difference')\n\n<AxesSubplot: title={'center': 'KS Cracovia - goal difference'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_goal_diff\n\n\n\n\n\n  \n    \n      \n      season\n      goal_diff\n    \n  \n  \n    \n      0\n      2008/2009\n      -16.0\n    \n    \n      1\n      2009/2010\n      -14.0\n    \n    \n      2\n      2010/2011\n      -10.0\n    \n    \n      3\n      2011/2012\n      -21.0\n    \n    \n      4\n      2012/2013\n      NaN\n    \n    \n      5\n      2013/2014\n      -6.0\n    \n    \n      6\n      2014/2015\n      -6.0\n    \n    \n      7\n      2015/2016\n      15.0\n    \n  \n\n\n\n\nAs noted before, we have no data for 2012/2013 as KS Cracovia were relegated in 2011/2012. A goal difference of -21 highlights the poor performance that year. For all the years of study (with the exception of 2015/2016 where they finished 5th) KS Cracovia conceded more goals than they scored.\n\n\nYou win some, you lose some\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won and lost\ncracovia_wins_losses =  pd.read_sql_query('''SELECT season, \n                                          COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                          COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                          COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                          COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                          FROM Match \n                                          GROUP BY season'''\n                                          ,con)\n\n\n# Convert defeats to a negative value - multiply by -1 to allow bi-directional plotting\ncracovia_wins_losses.home_defeats = cracovia_wins_losses.home_defeats * -1\ncracovia_wins_losses.away_defeats = cracovia_wins_losses.away_defeats * -1\n\n# Plot a stacked bar plot showing home and away wins and defeats each season\ncracovia_wins_losses.plot.bar(x='season', stacked=True, title ='KS Cracovia - games won and lost')\n\n<AxesSubplot: title={'center': 'KS Cracovia - games won and lost'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_wins_losses\n\n\n\n\n\n  \n    \n      \n      season\n      home_wins\n      away_wins\n      home_defeats\n      away_defeats\n    \n  \n  \n    \n      0\n      2008/2009\n      6\n      1\n      -3\n      -11\n    \n    \n      1\n      2009/2010\n      5\n      4\n      -7\n      -7\n    \n    \n      2\n      2010/2011\n      7\n      1\n      -6\n      -11\n    \n    \n      3\n      2011/2012\n      3\n      1\n      -8\n      -8\n    \n    \n      4\n      2012/2013\n      0\n      0\n      0\n      0\n    \n    \n      5\n      2013/2014\n      6\n      5\n      -7\n      -6\n    \n    \n      6\n      2014/2015\n      8\n      2\n      -4\n      -9\n    \n    \n      7\n      2015/2016\n      7\n      5\n      -4\n      -5\n    \n  \n\n\n\n\nFrom the above stacked bar chart we can see a downward trend from 2009/2010 culminating in KS Cracovia being relegated in 2011/2012, with only 4 wins all season. After their return in 2013/2014 they seem to have consolidated. During the period of study KS Cracovia generally lost more games than they won, with the exception of 2015/2016 where they finished the season in a very respectable 5th place.\n\n\nHome advantage?\n\n# Run an SQl query using AVG CASE WHEN to extract % of home and away games won\ncracovia_win_pct = pd.read_sql_query('''SELECT season, \n                                     ROUND(AVG(CASE WHEN home_team_api_id = 2186 AND home_team_goal > away_team_goal THEN 1 \n                                               WHEN home_team_api_id = 2186 AND home_team_goal < away_team_goal THEN 0 END)*100,2) AS pct_home_wins, \n                                     ROUND(AVG(CASE WHEN away_team_api_id = 2186 AND away_team_goal > home_team_goal THEN 1 \n                                               WHEN away_team_api_id = 2186 AND away_team_goal < home_team_goal THEN 0 END)*100,2) AS pct_away_wins \n                                     FROM Match \n                                     GROUP BY season'''\n                                     ,con)\n\n# Plot a bar plot showing % of home and away wins for each season\ncracovia_win_pct.plot.bar(x='season', stacked=False, title ='KS Cracovia - win percentages')\n\n<AxesSubplot: title={'center': 'KS Cracovia - win percentages'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_win_pct\n\n\n\n\n\n  \n    \n      \n      season\n      pct_home_wins\n      pct_away_wins\n    \n  \n  \n    \n      0\n      2008/2009\n      66.67\n      8.33\n    \n    \n      1\n      2009/2010\n      41.67\n      36.36\n    \n    \n      2\n      2010/2011\n      53.85\n      8.33\n    \n    \n      3\n      2011/2012\n      27.27\n      11.11\n    \n    \n      4\n      2012/2013\n      NaN\n      NaN\n    \n    \n      5\n      2013/2014\n      46.15\n      45.45\n    \n    \n      6\n      2014/2015\n      66.67\n      18.18\n    \n    \n      7\n      2015/2016\n      63.64\n      50.00\n    \n  \n\n\n\n\nIn the case of KS Cracovia, home advantage certainly seems to be a factor. For an in-depth study of home advantage in European Football Leagues see this article.\n\n\nPoints make prizes\n\n# Run an SQl query using COUNT CASE WHEN to extract home and away games won,lost, and drawn\ncracovia_points =  pd.read_sql_query('''SELECT season, \n                                              COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal > away_team_goal THEN id END) AS home_wins, \n                                              COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal > home_team_goal THEN id END) AS away_wins, \n                                              COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal = away_team_goal THEN id END) AS home_draws, \n                                              COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal = home_team_goal THEN id END) AS away_draws, \n                                              COUNT(CASE WHEN home_team_api_id = 2186 AND home_team_goal < away_team_goal THEN id END) AS home_defeats, \n                                              COUNT(CASE WHEN away_team_api_id = 2186 AND away_team_goal < home_team_goal THEN id END) AS away_defeats \n                                              FROM Match \n                                              GROUP BY season'''\n                                              ,con)\n\n# Calculate points total for each season - 3 for a win, 1 for a draw, 0 for a loss\ncracovia_points[\"points\"] = cracovia_points['home_wins'] * 3 +  cracovia_points['away_wins'] * 3 + cracovia_points['home_draws'] + cracovia_points['away_draws']\n\n# Filter for the seasom and points columns for plotting\ncracovia_points = cracovia_points[[\"season\",\"points\"]]\n\n# Plot a bar plot showing total points for each season\ncracovia_points.plot.bar(x='season', stacked=False, title ='KS Cracovia - total points')\n\n<AxesSubplot: title={'center': 'KS Cracovia - total points'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2008/2009\n      30\n    \n    \n      1\n      2009/2010\n      34\n    \n    \n      2\n      2010/2011\n      29\n    \n    \n      3\n      2011/2012\n      22\n    \n    \n      4\n      2012/2013\n      0\n    \n    \n      5\n      2013/2014\n      39\n    \n    \n      6\n      2014/2015\n      37\n    \n    \n      7\n      2015/2016\n      45\n    \n  \n\n\n\n\nCracovia were relegated in 2011/2012 with 22 points.\n\n\nFinal standings\n\n# Create an extra column and insert into our original points DataFrame\ncracovia_points.insert(0,\"Position\",[14,12,14,16,0,9,12,5],True)\n\n\n# Plot a horizontal bar plot of league position\ncracovia_points.plot.barh(x='season', y='Position', title ='KS Cracovia - final league position')\n\n<AxesSubplot: title={'center': 'KS Cracovia - final league position'}, ylabel='season'>\n\n\n\n\n\n\ncracovia_position = cracovia_points\ncracovia_position \n\n\n\n\n\n  \n    \n      \n      Position\n      season\n      points\n    \n  \n  \n    \n      0\n      14\n      2008/2009\n      30\n    \n    \n      1\n      12\n      2009/2010\n      34\n    \n    \n      2\n      14\n      2010/2011\n      29\n    \n    \n      3\n      16\n      2011/2012\n      22\n    \n    \n      4\n      0\n      2012/2013\n      0\n    \n    \n      5\n      9\n      2013/2014\n      39\n    \n    \n      6\n      12\n      2014/2015\n      37\n    \n    \n      7\n      5\n      2015/2016\n      45\n    \n  \n\n\n\n\nAs we can see the trend for KS Cracovia is better since they returned to the Ekstraklasa in 2013/2014. They finished a respectable 9th on their first season back in the top league, and finished in 5th place in 2015/2016.\n\n\n\n2016/2017 to 2021/2022\nThe European Soccer Database is now somewhat out of date, so I decided to gather data for seasons 2016/2017 to 2021/2022 and create dictionaries for conversion to DataFrames.\n\nGooooooooooool !\n\n# Create a dictionary for the most recent years\ncracovia_recent = {'season': ['2016/2017', '2017/2018', '2018/2019', '2019/2020', '2020/2021','2021/2022'],\n        'goals_scored': [38, 40, 39, 39,28,40],\n        'goals_conceded': [43, 40, 34, 29,32,42],\n        'games_won': [6, 10, 14, 14,8,12],\n        'games_lost': [11, 11, 10, 12,9,12],\n        'games_drawn': [13,9,6,4,13,10],\n        'points':[31,39,48,46,32,46],\n        'position':[13,10,4,5,14,9]\n        } \n\n# Convert dictionary to a DataFrame\ncracovia_recent = pd.DataFrame.from_dict(cracovia_recent)\n\n\n# Filter for columns to be plotted\ncracovia_recent_goals = cracovia_recent[['season','goals_scored','goals_conceded',]]\n\n# Plot a bar plot showing goals scored and conceded each season\ncracovia_recent_goals.plot.bar(x='season', stacked=False, title ='KS Cracovia - goals for and against')\n\n<AxesSubplot: title={'center': 'KS Cracovia - goals for and against'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_recent_goals\n\n\n\n\n\n  \n    \n      \n      season\n      goals_scored\n      goals_conceded\n    \n  \n  \n    \n      0\n      2016/2017\n      38\n      43\n    \n    \n      1\n      2017/2018\n      40\n      40\n    \n    \n      2\n      2018/2019\n      39\n      34\n    \n    \n      3\n      2019/2020\n      39\n      29\n    \n    \n      4\n      2020/2021\n      28\n      32\n    \n    \n      5\n      2021/2022\n      40\n      42\n    \n  \n\n\n\n\nIn the last couple of years KS Cracova have conceded more than they have scored, although they seem to have added extra fire power up front in 2021/2022 with 40 goals scored compared to only 28 in 2020/2021.\n\n\nWin, lose or draw\n\n# Filter for columns to be plotted\ncracovia_recent_wins = cracovia_recent[['season','games_won','games_drawn','games_lost']]\n\n# Plot a bar plot showing goals won, lost, and drawn each season\ncracovia_recent_wins.plot.bar(x='season', stacked=False, title ='KS Cracovia - win, draw, lose')\n\n<AxesSubplot: title={'center': 'KS Cracovia - win, draw, lose'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_recent_wins\n\n\n\n\n\n  \n    \n      \n      season\n      games_won\n      games_drawn\n      games_lost\n    \n  \n  \n    \n      0\n      2016/2017\n      6\n      13\n      11\n    \n    \n      1\n      2017/2018\n      10\n      9\n      11\n    \n    \n      2\n      2018/2019\n      14\n      6\n      10\n    \n    \n      3\n      2019/2020\n      14\n      4\n      12\n    \n    \n      4\n      2020/2021\n      8\n      13\n      9\n    \n    \n      5\n      2021/2022\n      12\n      10\n      12\n    \n  \n\n\n\n\nKS Cracovia performed very well in 2018/2019 and 2019/2020. Season 2021/2022 was a year of consolidation, with an almost even split of wins, draws, and losses.\n\n\nPoints make prizes\n\n# Filter for columns to be plotted\ncracovia_recent_points = cracovia_recent[['season','points']]\n\n\n# Plot a bar plot showing total points for each season\ncracovia_recent_points.plot.bar(x='season', stacked=False, title ='KS Cracovia - total points')\n\n<AxesSubplot: title={'center': 'KS Cracovia - total points'}, xlabel='season'>\n\n\n\n\n\n\ncracovia_recent_points\n\n\n\n\n\n  \n    \n      \n      season\n      points\n    \n  \n  \n    \n      0\n      2016/2017\n      31\n    \n    \n      1\n      2017/2018\n      39\n    \n    \n      2\n      2018/2019\n      48\n    \n    \n      3\n      2019/2020\n      46\n    \n    \n      4\n      2020/2021\n      32\n    \n    \n      5\n      2021/2022\n      46\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe points total of 32 for season 2020/2021 is net of a 5 point deduction imposed for corrupt activity backdated from the 2003/2004 season\nThe number of league games played increased to 34 for season 2021/2022 due to league reconstruction\n\n\n\n\n\nFinal standings\n\n# Plot a horizontal bar plot of league position\ncracovia_recent.plot.barh(x='season', y='position', title ='KS Cracovia - final league position')\n\n<AxesSubplot: title={'center': 'KS Cracovia - final league position'}, ylabel='season'>\n\n\n\n\n\n\ncracovia_position = cracovia_recent[[\"season\",\"position\"]]\ncracovia_position \n\n\n\n\n\n  \n    \n      \n      season\n      position\n    \n  \n  \n    \n      0\n      2016/2017\n      13\n    \n    \n      1\n      2017/2018\n      10\n    \n    \n      2\n      2018/2019\n      4\n    \n    \n      3\n      2019/2020\n      5\n    \n    \n      4\n      2020/2021\n      14\n    \n    \n      5\n      2021/2022\n      9\n    \n  \n\n\n\n\nMixed fortunes for KS Cracovia, an improvement from 2016/2017 and a 4th place finish in 2018/2019. Snce then things have tailed off somewhat.\n\n\n\n2022/2023\nThe Ekstraklasa league table for 2022/2023 at the time of writing is shown below. The season is on hold as the FIFA World Cup - Qatar 2022 progresses. KS Cracovia are sitting in 8th place, just four points behind Widzew and Pogon Szczecin in 3rd and 4th place respectively. Raków Częstochowa have a 9 point lead at the top of the table over nearest challengers Legia Warsaw.\n!"
  },
  {
    "objectID": "posts/Krakow_Football/Krakow_Football.html#key-takeaways",
    "href": "posts/Krakow_Football/Krakow_Football.html#key-takeaways",
    "title": "Wisła Kraków vs. KS Cracovia",
    "section": "Key takeaways",
    "text": "Key takeaways\nWhen I first had the idea for this project I did not know if it was even possible to run SQL queries using Python within JupyterLab. This was a very satisfying project as it opens up the possibility of further project ideas using data stored within a database. The combined power of SQL and Python is formidable, and allows data which is stored in different locations and different formats to be pulled together, cleaned, manipulated, filtered and presented graphically in a way that produces meaningful insights, and facilitates data-driven decision making."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html",
    "title": "Name that Genre",
    "section": "",
    "text": "Using a dataset comprised of songs of two music genres (Hip-Hop and Rock), we will train a classifier to distinguish between the two genres based only on track information derived from Echonest (now part of Spotify). We will first make use of pandas packages in Python for subsetting the data, aggregating information, and creating plots when exploring the data for obvious trends or factors we should be aware of when doing machine learning.\nNext, we will use the scikit-learn package to predict whether we can correctly classify a song’s genre based on features such as danceability, energy, acousticness, tempo, etc. We will go over implementations of common algorithms such as PCA, logistic regression, decision trees, and so forth.\n \n\n\n\nPreparing our dataset\nPairwise relationships between continuous variables\nSplitting our data\nNormalizing the feature data\nPrincipal Component Analysis on our scaled data\nFurther visualization of PCA\nProjecting on to our features\nTrain a decision tree to classify genre\nCompare our decision tree to a logistic regression\nBalance our data for greater performance\nDoes balancing our dataset improve model bias?\nUsing cross-validation to evaluate our models"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#preparing-our-dataset",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#preparing-our-dataset",
    "title": "Name that Genre",
    "section": "1. Preparing our dataset",
    "text": "1. Preparing our dataset\nOver the past few years, streaming services with huge catalogs have become the primary means through which most people listen to their favorite music. But at the same time, the sheer amount of music on offer can mean users might be a bit overwhelmed when trying to look for newer music that suits their tastes.\nFor this reason, streaming services have looked into means of categorizing music to allow for personalized recommendations. One method involves direct analysis of the raw audio information in a given song, scoring the raw data on a variety of metrics. Today, we’ll be examining data compiled by a research group known as The Echo Nest. Our goal is to look through this dataset and classify songs as being either ‘Hip-Hop’ or ‘Rock’ - all without listening to a single one ourselves. In doing so, we will learn how to clean our data, do some exploratory data visualization, and use feature reduction towards the goal of feeding our data through some simple machine learning algorithms, such as decision trees and logistic regression.\nTo begin with, let’s load the metadata about our tracks alongside the track metrics compiled by The Echo Nest. A song is about more than its title, artist, and number of listens. We have another dataset that has musical features of each track such as danceability and acousticness on a scale from -1 to 1. These exist in two different files, which are in different formats - CSV and JSON. While CSV is a popular file format for denoting tabular data, JSON (JavaScript Object Notation) is another common file format in which databases often return the results of a given query.\nLet’s start by creating two pandas DataFrames out of these files that we can merge so we have features and labels (often also referred to as X and y) for the classification later on.\n\nimport pandas as pd\nimport numpy as np\n\n# Read in the file with the track metadata \ntracks = pd.read_csv('Data/fma-rock-vs-hiphop.csv')\n\n# Read in JSON file with track metrics with the track acoustic metrics\nechonest_metrics = pd.read_json('Data/echonest-metrics.json',precise_float=True)\n\n# Merge the DataFrames on matching track_id values \n# Only retain genre_top columns of tracks (in addition to the key column that we are matching on\necho_tracks = echonest_metrics.merge(tracks[['genre_top','track_id']],on='track_id')\n\n# Inspect the resultant dataframe\necho_tracks.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4802 entries, 0 to 4801\nData columns (total 10 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   track_id          4802 non-null   int64  \n 1   acousticness      4802 non-null   float64\n 2   danceability      4802 non-null   float64\n 3   energy            4802 non-null   float64\n 4   instrumentalness  4802 non-null   float64\n 5   liveness          4802 non-null   float64\n 6   speechiness       4802 non-null   float64\n 7   tempo             4802 non-null   float64\n 8   valence           4802 non-null   float64\n 9   genre_top         4802 non-null   object \ndtypes: float64(8), int64(1), object(1)\nmemory usage: 412.7+ KB\n\n\n\n\n\n\n\n\nPandas documentation\n\n\n\nread_csv\nread_json\npd.merge\n\n\nSo our dependent variable or target is genre_top and our 8 independent variables or features are:\n- acousticness\n- danceability\n- energy\n- instrumentalness\n- liveness\n- speechiness\n- tempo\n- valence\nThe track_id is not really a feature and so we will drop that in due course."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#pairwise-relationships-between-continuous-variables",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#pairwise-relationships-between-continuous-variables",
    "title": "Name that Genre",
    "section": "2. Pairwise relationships between continuous variables",
    "text": "2. Pairwise relationships between continuous variables\nWe typically want to avoid using variables that have strong correlations with each other – hence avoiding feature redundancy – for a few reasons:\n\nTo keep the model simple and improve interpretability (with many features, we run the risk of overfitting).\nWhen our datasets are very large, using fewer features can drastically speed up our computation time.\n\nWe can visually inspect the correlation between the features by creating a correlation matrix using pandas.\n\ncorr_metrics = echo_tracks.corr()\ncorr_metrics.style.background_gradient()\n\n/tmp/ipykernel_116/408970472.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr_metrics = echo_tracks.corr()\n\n\n\n\n\n  \n    \n       \n      track_id\n      acousticness\n      danceability\n      energy\n      instrumentalness\n      liveness\n      speechiness\n      tempo\n      valence\n    \n  \n  \n    \n      track_id\n      1.000000\n      -0.372282\n      0.049454\n      0.140703\n      -0.275623\n      0.048231\n      -0.026995\n      -0.025392\n      0.010070\n    \n    \n      acousticness\n      -0.372282\n      1.000000\n      -0.028954\n      -0.281619\n      0.194780\n      -0.019991\n      0.072204\n      -0.026310\n      -0.013841\n    \n    \n      danceability\n      0.049454\n      -0.028954\n      1.000000\n      -0.242032\n      -0.255217\n      -0.106584\n      0.276206\n      -0.242089\n      0.473165\n    \n    \n      energy\n      0.140703\n      -0.281619\n      -0.242032\n      1.000000\n      0.028238\n      0.113331\n      -0.109983\n      0.195227\n      0.038603\n    \n    \n      instrumentalness\n      -0.275623\n      0.194780\n      -0.255217\n      0.028238\n      1.000000\n      -0.091022\n      -0.366762\n      0.022215\n      -0.219967\n    \n    \n      liveness\n      0.048231\n      -0.019991\n      -0.106584\n      0.113331\n      -0.091022\n      1.000000\n      0.041173\n      0.002732\n      -0.045093\n    \n    \n      speechiness\n      -0.026995\n      0.072204\n      0.276206\n      -0.109983\n      -0.366762\n      0.041173\n      1.000000\n      0.008241\n      0.149894\n    \n    \n      tempo\n      -0.025392\n      -0.026310\n      -0.242089\n      0.195227\n      0.022215\n      0.002732\n      0.008241\n      1.000000\n      0.052221\n    \n    \n      valence\n      0.010070\n      -0.013841\n      0.473165\n      0.038603\n      -0.219967\n      -0.045093\n      0.149894\n      0.052221\n      1.000000\n    \n  \n\n\n\n\n\n\n\n\n\nPandas documentation\n\n\n\nDataFrame.corr"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#splitting-our-data",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#splitting-our-data",
    "title": "Name that Genre",
    "section": "3. Splitting our data",
    "text": "3. Splitting our data\nAs mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn’t find any particularly strong correlations between our features, we can now split our data into an array containing our features, and another containing the labels - the genre of the track.\nOnce we have split the data into these arrays, we will perform some preprocessing steps to optimize our model development.\n\n# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Create features by storing all values of the echo_tracks DataFrame except for the \"genre_top\" and \"track_id\" columns.\nfeatures = echo_tracks.drop([\"genre_top\",\"track_id\"],axis=1).values\n\n# Create labels\nlabels = echo_tracks[\"genre_top\"].values\n\n# Split our data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=10)\n\n\n\n\n\n\n\nsci-kit learn documentation\n\n\n\ntrain_test_split\n\n\nLet’s take a look at the shape of our features:\n\ntrain_features.shape\n\n(3601, 8)\n\n\nSo we have 3,601 rows which represent the individual tracks, and 8 columns which are our features. Let’s have a look at a few of the values for our training features and associated labels:\n\n# Show the first 5 train_features\ntrain_features[:5]\n\narray([[9.48677459e-01, 1.37199242e-01, 2.47403596e-02, 3.06704890e-03,\n        1.20902996e-01, 4.94376494e-02, 2.11283000e+02, 3.81535017e-02],\n       [1.02727000e-05, 1.18115308e-01, 7.50281301e-01, 4.93739963e-01,\n        1.57670577e-01, 4.45210657e-02, 9.51060000e+01, 3.59604392e-01],\n       [3.12262570e-02, 6.28698289e-01, 8.31143855e-01, 1.11037480e-03,\n        1.82296940e-01, 6.79014582e-02, 9.70140000e+01, 6.53131979e-01],\n       [1.67591241e-02, 3.61955450e-01, 8.72379042e-01, 9.22775365e-01,\n        1.51773090e-01, 3.86009300e-02, 1.46546000e+02, 5.32632464e-01],\n       [7.52748738e-01, 4.53939425e-01, 4.90570217e-01, 9.66552918e-01,\n        8.74566097e-02, 2.58750898e-02, 1.50057000e+02, 3.09709828e-01]])\n\n\n\n# Show the first 5 train_lables\ntrain_labels[:5]\n\narray(['Rock', 'Rock', 'Hip-Hop', 'Rock', 'Rock'], dtype=object)\n\n\nLet’s check the mean and standard deviation of our training features dataset:\n\nnp.mean(train_features),np.std(train_features)\n\n(16.185663138852977, 43.42267453551983)\n\n\n\n\n\n\n\n\nOur training features have a large standard deviation (broad range of values). The danger here is that the larger value features will dominate our model and smaller, but potentially influential features may be disregarded.\n\n\n\nWe can address this through normalizing which in simple terms squeezes the values so that they are more centred around zero. The normalized data will have a mean of zero and a standard deviation of one."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#normalizing-the-feature-data",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#normalizing-the-feature-data",
    "title": "Name that Genre",
    "section": "4. Normalizing the feature data",
    "text": "4. Normalizing the feature data\nAs mentioned earlier, it can be particularly useful to simplify our models and use as few features as necessary to achieve the best result. Since we didn’t find any particular strong correlations between our features, we can instead use a common approach to reduce the number of features called principal component analysis (PCA). A comprehensive coverage of PCA can be found in this article by Matt Brems.\nIt is possible that the variance between genres can be explained by just a few features in the dataset. PCA rotates the data along the axis of highest variance, thus allowing us to determine the relative contribution of each feature of our data towards the variance between classes. However, since PCA uses the absolute variance of a feature to rotate the data, a feature with a broader range of values will overpower and bias the algorithm relative to the other features. To avoid this, we must first normalize our train and test features. There are a few methods to do this, but a common way is through standardization, such that all features have a mean = 0 and standard deviation = 1 (the resultant is a z-score).\n\n# Import the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Instantiate the Scaler without passign any agruments\nscaler = StandardScaler()\n\n# Use the fit_transform method to scale train_features and test_features\nscaled_train_features = scaler.fit_transform(train_features)\nscaled_test_features = scaler.transform(test_features)\n\n\n\n\n\n\n\nsci-kit learn documentation\n\n\n\nStandardScaler\n\n\n\n\n\n\n\n\nTo transform test features test it is important to use StandardScaler’s tranform method, after it has been fit to the training features (and potentially transformed them), rather than using fit_transform directly on the test features.\n\n\n\n\n\n\nNow that we have normalized our data, let’s take a look and see the impact on the feature values:\n\nscaled_train_features[:5]\n\narray([[ 1.24994743, -1.64927931, -2.45820448, -1.60032105, -0.44360329,\n        -0.37842314,  2.49377868, -1.55894527],\n       [-1.33744998, -1.75457157,  0.5134387 , -0.29343727, -0.19705986,\n        -0.41242335, -0.9269891 , -0.34245846],\n       [-1.25231142,  1.06248053,  0.8446325 , -1.60553256, -0.03192884,\n        -0.25073824, -0.87080909,  0.76835644],\n       [-1.29176911, -0.40922633,  1.01352202,  0.84927796, -0.23660521,\n        -0.45336354,  0.58763337,  0.31234255],\n       [ 0.71557091,  0.09827913, -0.55027614,  0.96587738, -0.6678766 ,\n        -0.54136801,  0.69101283, -0.53127759]])\n\n\nLet’s check the mean and standard deviation of our normalized data:\n\nnp.mean(scaled_train_features),np.std(scaled_train_features)\n\n(-6.010804800914205e-16, 1.0)\n\n\n\n\n\n\n\n\nThat’s much better. Our normalized feature values are all centred around zero (mean is basically zero, and standard deviation is one), thus reducing the risk of a particular feature dominating our model."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#principal-component-analysis-on-our-scaled-data",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#principal-component-analysis-on-our-scaled-data",
    "title": "Name that Genre",
    "section": "5. Principal Component Analysis on our scaled data",
    "text": "5. Principal Component Analysis on our scaled data\n\nNow that we have preprocessed our data, we are ready to use PCA to determine by how much we can reduce the dimensionality of our data. We can use scree-plots and cumulative explained ratio plots to find the number of components to use in further analyses.\nScree-plots display the number of components against the variance explained by each component, sorted in descending order of variance. Scree-plots help us get a better sense of which components explain a sufficient amount of variance in our data. When using scree plots, an ‘elbow’ (a steep drop from one data point to the next) in the plot is typically used to decide on an appropriate cutoff.\n\n# This is just to make plots appear in the notebook\n%matplotlib inline\n\n# Import our plotting module, and PCA class\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Instantiate our PCA class\npca = PCA()\n\n# Fit the model on our scaled_train_features\npca.fit(scaled_train_features)\n\n# Get the number of components\ncomponents = pca.n_components_\n\n# Retrieve the explained variance ratio\nexp_variance = pca.explained_variance_ratio_\n\n# plot the explained variance using a barplot\nfig, ax = plt.subplots()\nax.bar(range(components), exp_variance)\nax.set_xlabel('Principal Component #')\n\nText(0.5, 0, 'Principal Component #')\n\n\n\n\n\n\n\n\n\n\n\nsci-kit learn documentation\n\n\n\nPrincipal component analysis\n\n\n\n\n\n\n\n\n\nmatplotlib documentation\n\n\n\nbar plot"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#further-visualization-of-pca",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#further-visualization-of-pca",
    "title": "Name that Genre",
    "section": "6. Further visualization of PCA",
    "text": "6. Further visualization of PCA\nUnfortunately, there does not appear to be a clear elbow in this scree plot, which means it is not straightforward to find the number of intrinsic dimensions using this method.\nBut all is not lost! Instead, we can also look at the cumulative explained variance plot to determine how many features are required to explain, say, about 85% of the variance (cutoffs are somewhat arbitrary here, and usually decided upon by ‘rules of thumb’). Once we determine the appropriate number of components, we can perform PCA with that many components, ideally reducing the dimensionality of our data.\n\n# Import numpy\nimport numpy as np\n\n# Calculate the cumulative sums of our explained variance\ncum_exp_variance = np.cumsum(exp_variance)\n\n# Plot the cumulative explained variances and look for the no. of components at which we can account for >85% of our variance\nfig, ax = plt.subplots()\nax.plot(range(components), cum_exp_variance)\n\n# Draw a dashed line at 0.85\nax.axhline(y=0.85, linestyle='--')\n\n<matplotlib.lines.Line2D at 0x7f59f340c220>\n\n\n\n\n\n\n\n\n\n\n\nnumpy documentation\n\n\n\ncumsum()\n\n\n\n\n\n\n\n\nsklean documentation\n\n\n\nPCA"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#projecting-on-to-our-features",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#projecting-on-to-our-features",
    "title": "Name that Genre",
    "section": "7. Projecting on to our features",
    "text": "7. Projecting on to our features\nWe saw from the plot that 6 features (remember indexing starts at 0) can explain 85% of the variance! Therefore, we can use 6 components to perform PCA and reduce the dimensionality of our train and test features.\n\n# Perform PCA with the optimal no. of components from our cumulative explained variance plot\npca = PCA(n_components=6,random_state=10)\n\n# Fit and transform the scaled training features using pca\ntrain_pca = pca.fit_transform(scaled_train_features)\n\n# Fit and transform the scaled test features using pca\ntest_pca = pca.fit_transform(scaled_test_features)\n\n\ntrain_pca.shape\n\n(3601, 6)\n\n\n\n# Show the first 5 train_pca\ntrain_pca[:5]\n\narray([[-0.94683521, -0.98726024,  2.09691406,  1.82221026, -1.88745714,\n         2.63086297],\n       [-1.16474503,  0.81643176, -0.29364819, -1.2819193 , -0.85195312,\n        -0.22963785],\n       [ 1.52161734,  1.4246482 , -0.92071409, -1.25676649, -0.10178816,\n        -0.07969312],\n       [-0.97593229,  1.23672376, -1.10458784,  0.1487983 ,  0.02900411,\n        -0.0715869 ],\n       [-0.78621628, -1.10767364, -0.34725279,  0.89060792, -0.1259162 ,\n         0.3696838 ]])\n\n\n\n\n\n\n\n\nFollowing our Principal Component Analysis and dimensionality reduction, the train and test datasets now only include values for 6 features, reduced from our original 8."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#train-a-decision-tree-to-classify-genre",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#train-a-decision-tree-to-classify-genre",
    "title": "Name that Genre",
    "section": "8. Train a decision tree to classify genre",
    "text": "8. Train a decision tree to classify genre\nNow we can use the lower dimensional PCA projection of the data to classify songs into genres.\nHere, we will be using a simple algorithm known as a decision tree. Decision trees are rule-based classifiers that take in features and follow a ‘tree structure’ of binary decisions to ultimately classify a data point into one of two or more categories. In addition to being easy to both use and interpret, decision trees allow us to visualize the ‘logic flowchart’ that the model generates from the training data.\nHere is an example of a decision tree that demonstrates the process by which an input image (in this case, of a shape) might be classified based on the number of sides it has and whether it is rotated.\n\n\n# Import Decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate our DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=10)\n                              \n# Fit our DecisionTreeClassifier to the training data\ntree.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_tree = tree.predict(test_pca)\n\n# Show the first 10 labels\npred_labels_tree[:10]\n\narray(['Rock', 'Hip-Hop', 'Rock', 'Hip-Hop', 'Hip-Hop', 'Rock', 'Hip-Hop',\n       'Rock', 'Hip-Hop', 'Rock'], dtype=object)\n\n\n\n\n\n\n\n\nscikit-learn documentation\n\n\n\nDecisionTreeClassifier"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#compare-our-decision-tree-to-a-logistic-regression",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#compare-our-decision-tree-to-a-logistic-regression",
    "title": "Name that Genre",
    "section": "9. Compare our decision tree to a logistic regression",
    "text": "9. Compare our decision tree to a logistic regression\nAlthough our tree’s performance is decent, it’s a bad idea to immediately assume that it’s therefore the perfect tool for this job – there’s always the possibility of other models that will perform even better! It’s always a worthwhile idea to at least test a few other algorithms and find the one that’s best for our data.\nSometimes simplest is best, and so we will start by applying logistic regression. Logistic regression makes use of what’s called the logistic function to calculate the odds that a given data point belongs to a given class. Once we have both models, we can compare them on a few performance metrics, such as false positive and false negative rate (or how many points are inaccurately classified).\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate our LogisticRegression \nlogreg = LogisticRegression(random_state=10)\n\n# Fit our Logistic Regression model to the training data\nlogreg.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_logit = logreg.predict(test_pca)\n\n# Create the classification report for both models\nfrom sklearn.metrics import classification_report\n\nclass_rep_tree = classification_report(test_labels,pred_labels_tree)\nclass_rep_log = classification_report(test_labels,pred_labels_logit)\n\nprint(\"Decision Tree: \\n\", class_rep_tree)\nprint(\"Logistic Regression: \\n\", class_rep_log)\n\nDecision Tree: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.52      0.59      0.55       235\n        Rock       0.90      0.87      0.88       966\n\n    accuracy                           0.81      1201\n   macro avg       0.71      0.73      0.72      1201\nweighted avg       0.82      0.81      0.82      1201\n\nLogistic Regression: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.75      0.55      0.63       235\n        Rock       0.90      0.96      0.93       966\n\n    accuracy                           0.88      1201\n   macro avg       0.82      0.75      0.78      1201\nweighted avg       0.87      0.88      0.87      1201\n\n\n\n\n\n\n\n\n\nscikit-learn documentation\n\n\n\nLogisticRegression()\n\n\n\n\n\n\n\n\nscikit-learn documentation\n\n\n\nclassification_report()"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#balance-our-data-for-greater-performance",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#balance-our-data-for-greater-performance",
    "title": "Name that Genre",
    "section": "10. Balance our data for greater performance",
    "text": "10. Balance our data for greater performance\nBoth our models do well, with the Decision Tree scoring average precision of 82% and Logistic Regression scoring average precision of 87%. However, looking at our classification report, we can see that rock songs are fairly well classified, but hip-hop songs are disproportionately misclassified as rock songs.\nWhy might this be the case? Well, just by looking at the number of data points we have for each class (966 for rock, 235 for hip-hop) we can see that our data is imbalanced, potentially skewing our model’s ability to distinguish between classes. This also tells us that most of our model’s accuracy is driven by its ability to classify just rock songs, which is less than ideal.\nTo account for this, we can weight the value of a correct classification in each class inversely to the occurrence of data points for each class. Since a correct classification for “Rock” is not more important than a correct classification for “Hip-Hop” (and vice versa), we only need to account for differences in sample size of our data points when weighting our classes here, and not relative importance of each class.\n\n# Subset only the hip-hop tracks, and then only the rock tracks\nhop_only = echo_tracks.loc[echo_tracks['genre_top'] == 'Hip-Hop']\nrock_only = echo_tracks.loc[echo_tracks['genre_top'] == 'Rock']\n\n# sample the rocks songs to be the same number as there are hip-hop songs\nrock_only = rock_only.sample(hop_only.shape[0],random_state=10)\n\n# concatenate the dataframes rock_only and hop_only\nrock_hop_bal = pd.concat([rock_only,hop_only])\n\n# The features, labels, and pca projection are created for the balanced dataframe\nfeatures = rock_hop_bal.drop(['genre_top', 'track_id'], axis=1) \nlabels = rock_hop_bal['genre_top']\n\n# Redefine the train and test set with the pca_projection from the balanced data\ntrain_features, test_features, train_labels, test_labels = train_test_split(features,labels, random_state=10)\n\ntrain_pca = pca.fit_transform(scaler.fit_transform(train_features))\ntest_pca = pca.transform(scaler.transform(test_features))\n\n\n\n\n\n\n\npandas documentation\n\n\n\nDataFrame.loc[]\n\n\n\n\n\n\n\n\npandas documentation\n\n\n\nconcat()\n\n\n\n\n\n\n\n\npandas documentation\n\n\n\nDataFrame.sample()"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#does-balancing-our-dataset-improve-model-bias",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#does-balancing-our-dataset-improve-model-bias",
    "title": "Name that Genre",
    "section": "11. Does balancing our dataset improve model bias?",
    "text": "11. Does balancing our dataset improve model bias?\nWe’ve now balanced our dataset, but in doing so, we’ve removed a lot of data points that might have been crucial to training our models. Let’s test to see if balancing our data improves model bias towards the “Rock” classification while retaining overall classification performance. Note that we have already reduced the size of our dataset and will go forward without applying any dimensionality reduction. In practice, we would consider dimensionality reduction more rigorously when dealing with vastly large datasets and when computation times become prohibitively large.\n\n# Instantiate our DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=10)\n\n# Fit our DecisionTree Classifier model to the balanced training set\ntree.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_tree = tree.predict(test_pca)\n\n# Instantiate our LogisticRegression \nlogreg = LogisticRegression(random_state=10)\n\n# Fit our Logistic Regression model to the balanced training set\nlogreg.fit(train_pca, train_labels)\n\n# Find the predicted labels of the test set\npred_labels_logit = logreg.predict(test_pca)\n\n# Compare the models\nprint(\"Decision Tree: \\n\", classification_report(test_labels,pred_labels_tree))\nprint(\"Logistic Regression: \\n\", classification_report(test_labels,pred_labels_logit))\n\nDecision Tree: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.82      0.77      0.79       230\n        Rock       0.78      0.82      0.80       225\n\n    accuracy                           0.80       455\n   macro avg       0.80      0.80      0.80       455\nweighted avg       0.80      0.80      0.80       455\n\nLogistic Regression: \n               precision    recall  f1-score   support\n\n     Hip-Hop       0.84      0.80      0.82       230\n        Rock       0.81      0.85      0.83       225\n\n    accuracy                           0.82       455\n   macro avg       0.82      0.82      0.82       455\nweighted avg       0.83      0.82      0.82       455\n\n\n\nSuccess! Balancing our data has removed bias towards the more prevalent class - the precision scores for Hip-Hop and rock are now similar. To get a good sense of how well our models are actually performing, we can apply what’s called cross-validation (CV). This step allows us to compare models in a more rigorous fashion."
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#using-cross-validation-to-evaluate-our-models",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#using-cross-validation-to-evaluate-our-models",
    "title": "Name that Genre",
    "section": "12. Using cross-validation to evaluate our models",
    "text": "12. Using cross-validation to evaluate our models\nBefore we can perform cross-validation we will need to create pipelines to scale our data, perform PCA, and instantiate our model of choice - DecisionTreeClassifier or LogisticRegression.\nSince the way our data is split into train and test sets can impact model performance, CV attempts to split the data multiple ways and test the model on each of the splits. Although there are many different CV methods, all with their own advantages and disadvantages, we will use what’s known as K-fold CV here. K-fold first splits the data into K different, equally sized subsets. Then, it iteratively uses each subset as a test set while using the remainder of the data as train sets. Finally, we can then aggregate the results from each fold for a final model performance score.\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\n\ntree_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=5)), \n                      (\"tree\", DecisionTreeClassifier(random_state=10))])\n\nlogreg_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=5)), \n                        (\"logreg\", LogisticRegression(random_state=10))])\n\n# Set up our K-fold cross-validation with 10 folds\nkf = KFold(n_splits=10)\n\n# Train our models using KFold cv\ntree_score = cross_val_score(tree_pipe, features,labels,cv=kf)\nlogit_score = cross_val_score(logreg_pipe, features,labels,cv=kf)\n\n# Print the mean of each array of scores\nprint(\"Decision Tree:\", np.mean(tree_score), \"Logistic Regression:\", np.mean(logit_score))\n\nDecision Tree: 0.7417582417582418 Logistic Regression: 0.7835164835164835\n\n\nSo our final performance scores are:\n- Decision Tree 74.2%\n- Logistic Regression 78.4%\n\n\n\n\n\n\nsklearn documentation\n\n\n\nKFold()\n\n\n\n\n\n\n\n\nsklearn documentation\n\n\n\ncross_val_score()"
  },
  {
    "objectID": "posts/Song Genre Classifier/Song Genre Classifier.html#key-takeaways",
    "href": "posts/Song Genre Classifier/Song Genre Classifier.html#key-takeaways",
    "title": "Name that Genre",
    "section": "Key takeaways",
    "text": "Key takeaways\nIn this project we learned how to:\n\nload in two different file types, csv and json, using pandas pd.read\nmerge two datasets using pandas .merge\nexplore and visulaize the relationship between continuous variables using pandas .corr()\nsplit our data into a training and test set using scikit-learn train_test_split\nnormalize our data using StandardScaler from scikit-learn\nimprove model performance by reducing the dimensionality of our data via Principle Component Analysis\nvisualize feature importance using scree and cumulative explained variance plots\ntrain a Decison Tree and Logistic Regression model\nenhance model performance by balancing our data\nevaluate model performance using cross-validation"
  },
  {
    "objectID": "posts/DE_Zoomcamp_Week_2/DE_Zoomcamp_Week_2.html",
    "href": "posts/DE_Zoomcamp_Week_2/DE_Zoomcamp_Week_2.html",
    "title": "Data Engineering Zoomcamp - Week 2",
    "section": "",
    "text": "Just like a physical transport logistics system, it is important to have a smooth data logistics system. This process is also known as Workflow Orchestration. Workflow orchestration allows us to turn any code into a workflow that we can schedule, run and observe.\nCore features:\n\nremote execution\nscheduling\nretries\ncaching\nintegration with external systems (APIs, databases)\nad-hoc runs\nparametrization\nalert when something fails\n\n\n\n\nweek_2.JPG\n\n\n\n\n\n\n\ndata_lake.JPG\n\n\n\n\n\ndata_lake_origin.JPG\n\n\n\n\n\nlake_vs_warehouse.JPG\n\n\n \n\n\n\ndata_lake_cloud.JPG\n\n\n\n\n\n\n\n\ndata_engineering_map.JPG\n\n\n\n\n\nworkflow_orchestration.JPG\n\n\n\n\n\nPrefect is air traffic control for the modern data stack. Monitor, coordinate, and orchestrate dataflows between and across your applications. Build pipelines, deploy them anywhere, and configure them remotely. You might just love your workflows again.\nIn this session, we are going to take a look at a basic python script that pulls the yellow taxi data into a postgres db and then transforms that script to be orchestrated with Prefect.\nPrefect is the modern open source dataflow automation platform that will allow us to add observability and orchestration by utilizing python to write code as workflows to build,run and monitor pipelines at scale.\nFirst let’s clone the Prefect repo from the command line:\ngit clone https://github.com/discdiver/prefect-zoomcamp.git\nNext, create a python environment :\nconda create -n zoomcamp python=3.9   \nOnce created we need to activate it:\nconda activate zoomcamp\nTo deactivate an environment use:\nconda deactivate  \nNote from the terminal that we are no longer running in base but our newly created zoomcamp environment:\n\n\n\nzoomcamp_environ.JPG\n\n\nThen install all package dependencies with:\npip install -r requirements.txt\nOnce that’s done we can check that has installed successfully and which version we have from the command line:\nprefect version\n\n\n\nprefect_version.JPG\n\n\nI started Docker Desktop and executed these commands :\ndocker run -d\n-e POSTGRES_USER=“root”\n-e POSTGRES_PASSWORD=“root”\n-e POSTGRES_DB=“ny_taxi”\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data\n-p 5432:5432\npostgres:13\nThen, I executed the ingest_data.py file and ran with\npython ingest_data.py\n\n#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport argparse\nfrom time import time\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n\ndef ingest_data(user, password, host, port, db, table_name, url):\n\n# the backup files are gzipped, and it's important to keep the correct extension\n# for pandas to be able to open the file\nif url.endswith('.csv.gz'):\n    csv_name = 'yellow_tripdata_2021-01.csv.gz'\nelse:\n    csv_name = 'output.csv'\n\nos.system(f\"wget {url} -O {csv_name}\")\npostgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'\nengine = create_engine(postgres_url)\n\ndf_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n\ndf = next(df_iter)\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\ndf.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n\ndf.to_sql(name=table_name, con=engine, if_exists='append')\n\n\nwhile True: \n\n    try:\n        t_start = time()\n        \n        df = next(df_iter)\n\n        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\n        df.to_sql(name=table_name, con=engine, if_exists='append')\n\n        t_end = time()\n\n        print('inserted another chunk, took %.3f second' % (t_end - t_start))\n\n    except StopIteration:\n        print(\"Finished ingesting data into the postgres database\")\n        break\n\nif __name__ == '__main__':\nuser = \"root\"\npassword = \"root\"\nhost = \"localhost\"\nport = \"5432\"\ndb = \"ny_taxi\"\ntable_name = \"yellow_taxi_trips\"\ncsv_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\ningest_data(user, password, host, port, db, table_name, csv_url)\nI then opened up pgcli\npgcli -h localhost -p 5432 -u root -d ny_taxi\n\n\n\ndata_ingest.JPG\n\n\nSo we can see that the data ingested into the postgres db. This is great but we had to manually trigger this python script. Using a workflow orchestration tool will allow us to add a scheduler so that we won’t have to trigger this script manually anymore. Additionally, we’ll get all the functionality that comes with workflow orchestation such as visibility, and resilience to the dataflow with automatic retries or caching and more.\nLet’s transform this into a Prefect flow. A flow is the most basic Prefect object that is a container for workflow logic and allows you to interact and understand the state of the workflow. Flows are like functions, they take inputs, preform work, and return an output. We can start by using the @flow decoratorto a main_flow function.\n\nimport prefect with from prefect import flow, task\nmove everything that was in our 'if __name__ == '__main__' function to a new def main(): function (replace with a reference to main()\nadd @flow(name=\"Ingest Flow\") above a new def main() function\nremove the while True part of our original script\n\nI started Docker Desktop and executed these commands:\ndocker run -d\n-e POSTGRES_USER=“root”\n-e POSTGRES_PASSWORD=“root”\n-e POSTGRES_DB=“ny_taxi”\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data\n-p 5432:5432\npostgres:13\nThen start the Prefect Orion orchestration engine using:\nprefect orion start\nOpen another terminal window and run the following command:\nprefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n\n\n#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport argparse\nfrom time import time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom prefect import flow, task                 # Added\n\ndef ingest_data(user, password, host, port, db, table_name, url):\n\n# the backup files are gzipped, and it's important to keep the correct extension\n# for pandas to be able to open the file\nif url.endswith('.csv.gz'):\n    csv_name = 'yellow_tripdata_2021-01.csv.gz'\nelse:\n    csv_name = 'output.csv'\n\nos.system(f\"wget {url} -O {csv_name}\")\npostgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'\nengine = create_engine(postgres_url)\n\ndf_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n\ndf = next(df_iter)\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\ndf.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n\ndf.to_sql(name=table_name, con=engine, if_exists='append')\n\n\n@flow(name=\"Ingest Flow\")                       # Added\ndef main_flow():\nuser = \"root\"\npassword = \"root\"\nhost = \"localhost\"\nport = \"5432\"\ndb = \"ny_taxi\"\ntable_name = \"yellow_taxi_trips\"\ncsv_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\ningest_data(user, password, host, port, db, table_name, csv_url)\n\n\nif __name__ == '__main__':\nmain_flow()                                      # everything that was here moved into the new def main(): function \n\n\n\nflow_run1.JPG\n\n\nWe successfully completed a flow run.\nFlows contain tasks so let’s transform ingest_data into a task by adding the @task decorator. Tasks are not required for flows but tasks are special because they receive metadata about upstream dependencies and the state of those dependencies before the function is run, which gives you the opportunity to have a task wait on the completion of another task before executing.\nWe can simplify this script and transform it into an extract and transform before we load the data into the postgres db. We start by breaking apart the large ingest_data function into multiple functions so that we can get more visibility into the tasks that are running or potentially causing failures.\nLet’s create a new task called extract data that will take the url for the csv and the task will actually return the results. Since this is pulling data from external my system (something we may not control) we want to add automatic retries and also add a caching so that if this task has already been run, it will not need to run again.\nimport from prefect.tasks import task_input_hash\nIf we look at the data in PotsgreSQL we can see that on row 4, there is a passenger count of 0. So let’s do a transformation step to cleanse the data before we load the data to postgres. We can create a new task called transform_data for this.\nLastly, let’s actually simplify the original ingest_data() function and rename this to load_data()\n#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport argparse\nfrom time import time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom prefect import flow, task \nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\n\n@task(log_prints=True, tags=[\"extract\"], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\ndef extract_data(url: str):\n# the backup files are gzipped, and it's important to keep the correct extension\n# for pandas to be able to open the file\nif url.endswith('.csv.gz'):\n    csv_name = 'yellow_tripdata_2021-01.csv.gz'\nelse:\n    csv_name = 'output.csv'\n\nos.system(f\"wget {url} -O {csv_name}\")\n\ndf_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n\ndf = next(df_iter)\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\nreturn df\n\n@task(log_prints=True)\ndef transform_data(df):\n    print(f\"pre: missing passenger count: {df['passenger_count'].isin([0]).sum()}\")\n    df = df[df['passenger_count'] != 0]\n    print(f\"post: missing passenger count: {df['passenger_count'].isin([0]).sum()}\")\n    return df\n\n\n\n@task(log_prints=True, retries=3)\ndef load_data(user, password, host, port, db, table_name, df):\n  postgres_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'\n  engine = create_engine(postgres_url)\n  df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n  df.to_sql(name=table_name, con=engine, if_exists='append')\n\n\n\n@flow(name=\"Ingest Flow\")\ndef main_flow():\n    user = \"root\"\n    password = \"root\"\n    host = \"localhost\"\n    port = \"5432\"\n    db = \"ny_taxi\"\n    table_name = \"yellow_taxi_trips\"\n    csv_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\n    raw_data=extract_data(csv_url)\n    data = transform_data(raw_data)\n    load_data(user, password, host, port, db, table_name, data)\n\nif __name__ == '__main__':\n    main_flow()\nThere’s a lot more we can add by sprinkling in Prefect to our flow. We could parameterize the flow to take a table name so that we could change the table name loaded each time the flow was run.\nFlows can also contain other flows - and so we can create a sub-flow :\n#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport argparse\nfrom time import time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom prefect import flow, task \nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\n\n@task(log_prints=True, tags=[\"extract\"], cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\ndef extract_data(url: str):\n# the backup files are gzipped, and it's important to keep the correct extension\n# for pandas to be able to open the file\nif url.endswith('.csv.gz'):\n    csv_name = 'yellow_tripdata_2021-01.csv.gz'\nelse:\n    csv_name = 'output.csv'\n\nos.system(f\"wget {url} -O {csv_name}\")\n\ndf_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n\ndf = next(df_iter)\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\nreturn df\n\n@task(log_prints=True)\ndef transform_data(df):\n    print(f\"pre: missing passenger count: {df['passenger_count'].isin([0]).sum()}\")\n    df = df[df['passenger_count'] != 0]\n    print(f\"post: missing passenger count: {df['passenger_count'].isin([0]).sum()}\")\n    return df\n\n@task(log_prints=True, retries=3)\ndef load_data(user, password, host, port, db, table_name, df):\n  postgres_url = f\"postgresql://{user}:{password}@{host}:{port}/{db}\"\n  engine = create_engine(postgres_url)\n  df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n  df.to_sql(name=table_name, con=engine, if_exists='append')\n\n  @flow(name=\"Subflow\", log_prints=True)\n  def log_subflow(table_name:str):\n      print(\"Logging Subflow for: {table_name}\")\n\n@flow(name=\"Ingest Flow\")\ndef main_flow(table_name: str):\n    user = \"root\"\n    password = \"root\"\n    host = \"localhost\"\n    port = \"5432\"\n    db = \"ny_taxi\"\n    csv_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n    log_subflow = (table_name)\n    raw_data = extract_data(csv_url)\n    data = transform_data(raw_data)\n    load_data(user, password, host, port, db, table_name, data)\n\nif __name__ == '__main__':\n    main_flow(\"yellow_taxi_trips\")\nThat has run successfully:\n\n\n\nsub_flow.JPG\n\n\nLet’s now open the open source UI to visualise our flow runs :\nprefect orion start\n\nThis should default but if you are having problems or just want to make sure you set the prefect config to point to the api URL:\nprefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n\n\n\nprefect_api_url.JPG\n\n\nThis is especially important if you are going to host the Url somewhere else and need to change the url for the api that your flows are communicating with.\nOpening up the localhost we can see the Prefect UI, which gives us a nice dashboard to see all of our flow run history.\n\n\n\nprefect_UI.JPG\n\n\nWe can then drill down into the runs to obtain more details :\n\n\n\noptimistic_flounder.JPG\n\n\nA quick navigation lets us dive into the logs of that flow run, navigate around. You’ll notice over on the side we have Deployments, Work Queues, Blocks, Notifications, and Task Run Concurrency.\nBlocks are a primitive within Prefect that enable the storage of configuration and provide an interface with interacting with external systems. There are several different types of blocks you can build, and you can even create your own. Block names are immutable so they can be reused across multiple flows. Blocks can also build upon blocks or be installed as part of Integration collection which is prebuilt tasks and blocks that are pip installable. For example, a lot of users use the SqlAlchemy.\n\n\n\nprefect_blocks.JPG\n\n\nLet’s add the sqlalchemy connector :\n\n\n\nsqlalchemy_connector.PNG\n\n\nAs you can see we have now created the block and can now update our ingestion script to include the necessary code to make the connection work :\n\n\n\nsqlalchemy_connector_block.PNG\n\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport argparse\nfrom time import time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom prefect import flow, task\nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\nfrom prefect_sqlalchemy import SqlAlchemyConnector\n\n@task(log_prints=True, retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\ndef extract_data(csv_url):\n# the backup files are gzipped, and it's important to keep the correct extension\n# for pandas to be able to open the file\nif csv_url.endswith('.csv.gz'):\n    csv_name = 'yellow_tripdata_2021-01.csv.gz'\nelse:\n    csv_name = 'output.csv'\n\nos.system(f\"wget {csv_url} -O {csv_name}\")\n\ndf_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n\ndf = next(df_iter)\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\nreturn df \n\n@task(log_prints=True)\ndef transform_data(df):\n    print(f\"pre: missing passenger count: {df['passenger_count'].isin([0]).sum()}\")\n    df = df[df['passenger_count'] != 0]\n    print(f\"post: missing passenger count: {df['passenger_count'].isin([0]).sum()}\")\n    return df \n\n@task(log_prints=True, retries=3)\ndef ingest_data(table_name, df):\n    connection_block = SqlAlchemyConnector.load(\"postgres-connector\")\n\nwith connection_block.get_connection(begin=False) as engine:    \n    df.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n    df.to_sql(name=table_name, con=engine, if_exists='append')\n\n@flow(name=\"Subflow\", log_prints=True)\ndef log_subflow(table_name: str):\n    print(f\"Logging Subflow for: {table_name}\")\n\n@flow(name=\"Ingest Flow\")\ndef main_flow(table_name: str):\n    csv_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n    log_subflow(table_name)\n    raw_data = extract_data(csv_url)\n    data = transform_data(raw_data)\n    ingest_data(table_name, data)\n\nif __name__ == '__main__':\n    main_flow(\"yellow_taxi_trips\")\nAnd run our flow:\n\n\n\ningest_data_flow.PNG\n\n\n\n\n\nOK, so next up we are going to create an ETL flow using Prefect to grab a csv file from the web, clean it up, write it out locally and finally upload the data to a data lake within Google Cloud Storage(GCS).\n\n\nFirst of all let’s register our GCP blocks from the command line:\nprefect block register -m prefect_gcp\n\n\n\nprefect_gcp_block.png\n\n\nAnd now let’s create a GCS Bucket block from the Prefect GUI :\n+\nand add credentials :\n\n\n\ngcs_bucket_plus_credentials.PNG\n\n\nNot that I created a service account previously within GCP and generated a key which I have saved on my machine in JSON format. I then copied and pasted the contents of the file (a dictionary) into the Service Account Info field.\n\n\n\ngcp_blocks.PNG\n\n\nWe can now grab the block code and incorporate this into our workflow:\nfrom pathlib import Path\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect_gcp.cloud_storage import GcsBucket # this code provided on creation of our block\nfrom random import randint\n\n\n@task(retries=3)\ndef fetch(dataset_url: str) -> pd.DataFrame:\n\"\"\"Read taxi data from web into pandas DataFrame\"\"\"\n# if randint(0, 1) > 0:\n#     raise Exception\n\n    df = pd.read_csv(dataset_url)\n    return df\n\n\n@task(log_prints=True)\ndef clean(df: pd.DataFrame) -> pd.DataFrame:\n\"\"\"Fix dtype issues\"\"\"\n    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"])\n    print(df.head(2))\n    print(f\"columns: {df.dtypes}\")\n    print(f\"rows: {len(df)}\")\n    return df\n\n\n@task()\ndef write_local(df: pd.DataFrame, color: str, dataset_file: str) -> Path:\n\"\"\"Write DataFrame out locally as parquet file\"\"\"\n    path = Path(f\"data/{color}/{dataset_file}.parquet\")\n    df.to_parquet(path, compression=\"gzip\")\n    return path\n\n\n@task()\ndef write_gcs(path: Path) -> None:\n\"\"\"Upload local parquet file to GCS\"\"\"\n    gcs_block = GcsBucket.load(\"de-zoomcamp\") # the name we gave our block on creation\n    gcs_block.upload_from_path(from_path=path, to_path=path)\n    return\n\n\n@flow()\ndef etl_web_to_gcs() -> None:\n\"\"\"The main ETL function\"\"\"\n    color = \"yellow\"\n    year = 2021\n    month = 1\n    dataset_file = f\"{color}_tripdata_{year}-{month:02}\"\n    dataset_url = f\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{color}/{dataset_file}.csv.gz\"\n\n    df = fetch(dataset_url)\n    df_clean = clean(df)\n    path = write_local(df_clean, color, dataset_file)\n    write_gcs(path)\n\n\nif __name__ == \"__main__\":\n    etl_web_to_gcs()\nNote the above flow has been hardcoded in places, but we will be refining / parametrizing this later.\nFirst let’s create directories to store our data locally :\nmkdir data\nmkdir yellow\nand then run the script :\n\n\n\netl_web_to_gcs.PNG\n\n\nWe can visualise the run on Prefect:\n\n\n\nprefect_etl_web_to_gcs.PNG\n\n\nAnd confirm that the data has been successfully uploaded to our data lake bucket on GCS :\n\n\n\ndata_lake_bucket_yellow_taxi.PNG\n\n\n\n\n\n\nNow that we have successfully uploaded our data to a data lake within Google Cloud Storage let’s now move it to Big Query which is a data warehouse. We can do this by creating a flow which extracts our data from Google Cloud Storage, creates a DataFrame, does some very basic cleaning (fill missing passenger count with zero) and then writes our dataframe to Big Query.\nBefore we go ahead and create our flow let’s first head over to GCP and configure Big Query. First let’s add our data which we previously uploaded to our data bucket:\n\n\n\nbig_query_add_data.PNG\n\n\n\n\n\nyellow_parquet.PNG\n\n\n\n\n\ncreate_table.PNG\n\n\n\n\n\ntable.PNG\n\n\nLet’s delete the data from the table prior to creating a flow which will automate this process for us:\n\n\n\ndelete_data_biq_query.PNG\n\n\nLet’s now create our flow. Note that we can make use of our Prefect blocks created previously, namely our GcsBucket, and GcpCredentials.\nfrom pathlib import Path\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect_gcp.cloud_storage import GcsBucket\nfrom prefect_gcp import GcpCredentials\n\n\n@task(retries=3)\ndef extract_from_gcs(color: str, year: int, month: int) -> Path:\n\"\"\"Download trip data from GCS\"\"\"\n    gcs_path = f\"data/{color}/{color}_tripdata_{year}-{month:02}.parquet\"\n    gcs_block = GcsBucket.load(\"de-zoomcamp\") # this is the name we gave our Prefect block on creation\n    gcs_block.get_directory(from_path=gcs_path, local_path=f\"../data/\") # .. saves the file into a directory up a level\n    return Path(f\"../data/{gcs_path}\")\n\n\n@task()\ndef transform(path: Path) -> pd.DataFrame:\n\"\"\"Data cleaning example\"\"\"\n    df = pd.read_parquet(path)\n    print(f\"pre: missing passenger count: {df['passenger_count'].isna().sum()}\")\n    df[\"passenger_count\"].fillna(0, inplace=True)\n    print(f\"post: missing passenger count: {df['passenger_count'].isna().sum()}\")\n    return df\n\n\n@task()\ndef write_bq(df: pd.DataFrame) -> None:\n\"\"\"Write DataFrame to BiqQuery\"\"\"\n\n    gcp_credentials_block = GcpCredentials.load(\"de-gcp-creds\") # \n\n    df.to_gbq(\n    destination_table=\"de_zoomcamp.ny_taxi_rides\", # this is the name of our table created in Big Query\n    project_id=\"taxi-rides-ny-137\", # this is our project ID on Google Cloud Platform\n    credentials=gcp_credentials_block.get_credentials_from_service_account(),\n    chunksize=500_000,\n    if_exists=\"append\",\n)\n\n\n@flow()\ndef etl_gcs_to_bq():\n\"\"\"Main ETL flow to load data into Big Query\"\"\"\n    color = \"yellow\"\n    year = 2021\n    month = 1\n\n    path = extract_from_gcs(color, year, month)\n    df = transform(path)\n    write_bq(df)\n\n\nif __name__ == \"__main__\":\n    etl_gcs_to_bq()\n\n\n\netl_gcs_to_BQ.PNG\n\n\nOur flow appears to have run successfully - let’s check Big Query:\n\n\n\ngcs_to_BQ_confirm.PNG\n\n\nYes, all 1,369,765 rows of our data are now available for query within BigQuery.\n\n\n\nLet’s now build upon the existing flow and blocks that we configured previously and learn how to add Parameterization to our flows and create deployments. This removes the inflexibility of hard coded flows, by allowing our flow to take parameters, to be defined at run time. So to start, let’s allow our flow to take parameters of year, month, and color:\nfrom pathlib import Path\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect_gcp.cloud_storage import GcsBucket\nfrom random import randint\nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\n\n\n@task(retries=3, cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1)) # make use of cache so not having to re-read dataset\ndef fetch(dataset_url: str) -> pd.DataFrame:\n\"\"\"Read taxi data from web into pandas DataFrame\"\"\"\n# if randint(0, 1) > 0:\n#     raise Exception\n\n    df = pd.read_csv(dataset_url)\n    return df\n\n\n@task(log_prints=True)\ndef clean(df: pd.DataFrame) -> pd.DataFrame:\n\"\"\"Fix dtype issues\"\"\"\n    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"])\n    print(df.head(2))\n    print(f\"columns: {df.dtypes}\")\n    print(f\"rows: {len(df)}\")\n    return df\n\n\n@task()\ndef write_local(df: pd.DataFrame, color: str, dataset_file: str) -> Path:\n\"\"\"Write DataFrame out locally as parquet file\"\"\"\n    path = Path(f\"data/{color}/{dataset_file}.parquet\")\n    df.to_parquet(path, compression=\"gzip\")\n    return path\n\n\n@task()\ndef write_gcs(path: Path) -> None:\n\"\"\"Upload local parquet file to GCS\"\"\"\n    gcs_block = GcsBucket.load(\"de-zoomcamp\")  # the name we gave our block on creation\n    gcs_block.upload_from_path(from_path=path, to_path=path)\n    return\n\n\n@flow()\ndef etl_web_to_gcs(year: int, month: int, color: str) -> None:\n\"\"\"The main ETL function\"\"\"\n    dataset_file = f\"{color}_tripdata_{year}-{month:02}\"\n    dataset_url = f\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{color}/{dataset_file}.csv.gz\"\n\n    df = fetch(dataset_url)\n    df_clean = clean(df)\n    path = write_local(df_clean, color, dataset_file)\n    write_gcs(path)\n\n\n@flow()\ndef etl_parent_flow(\n    months: list[int] = [1, 2], year: int = 2021, color: str = \"yellow\" # pass in months as a list\n):\nfor month in months:\n    etl_web_to_gcs(year, month, color)\n\n\nif __name__ == \"__main__\":\n    color = \"yellow\"\n    months = [1, 2, 3]\n    year = 2021\n    etl_parent_flow(months, year, color)\nOnce again create the required directories to save locally and run the flow:\n\n\n\nparametrize1.PNG\n\n\n\n\n\nparametrize2.PNG\n\n\n\n\n\nparametrize3.PNG\n\n\nExcellent. Our data has been picked up, cleansed and flow runs commpleted - we can see the logs from our Orion terminal :\n\n\n\nprefect_parametrize.PNG\n\n\nAnd we can see our parent run, and the three subflow runs :\n\n\n\nprefect_parent_subflows.PNG\n\n\nDeployment using Prefect\nThis is all very good and works well but we had to execute the flows from the terminal manually. Let’s look at how we can use Prefect to configure and deploy runs from the API. A deployment in Prefect is a server-side concept that encapsulates a flow, allowing it to be scheduled and triggered via the API.\n\n\n\ndeployments.PNG\n\n\nA flow can have multiple deployments and you can think of it as the container of metadata needed for the flow to be scheduled. This might be what type of infrastructure the flow will run on, or where the flow code is stored, maybe it’s scheduled or has certain parameters.\nThere are two ways to create a deployment:\n\nusing the CLI command\nwith python.\n\nWe will see how to set up the deployment with Python in the next section. So for now we are going to create one using the CLI.\nInside our terminal we can type :\nprefect deployment build ./parameterized_flow.py:etl_parent_flow -n \"Parameterized ETL\"\nThe file name is ./parametrized_flow.py and :etl_parent_flow specifies the entry point of our flow and the -n refers to the name that we want to give our deployment.\n\n\n\ndeployment_yaml.PNG\n\n\nNow we can see it created a yaml file with all our details. This is the metadata. The default file did not include any parameters but I have amended the file to include :\n{ \"color\": \"yellow\", \"months\" :[1, 2, 3], \"year\": 2021}\n\n###\n### A complete description of a Prefect Deployment for flow 'etl-parent-flow'\n###\nname: Parameterized ETL\ndescription: null\nversion: 13fb131389eead1a3008d65d3170ceb0\n# The work queue that will handle this deployment's runs\nwork_queue_name: default\nwork_pool_name: null\ntags: []\nparameters: { \"color\": \"yellow\", \"months\" :[1, 2, 3], \"year\": 2021} # default file did not include any parameters\nschedule: null\nis_schedule_active: null\ninfra_overrides: {}\ninfrastructure:\n  type: process\n  env: {}\n  labels: {}\n  name: null\n  command: null\n  stream_output: true\n  working_dir: null\n  block_type_slug: process\n  _block_type_slug: process\n\n###\n### DO NOT EDIT BELOW THIS LINE\n###\nflow_name: etl-parent-flow\nmanifest_path: null\nstorage: null\npath: /home/stephen137/data-engineering-zoomcamp/prefect-zoomcamp/flows/03_deployments\nentrypoint: parameterized_flow.py:etl_parent_flow\nparameter_openapi_schema:\n  title: Parameters\n  type: object\n  properties:\nmonths:\n  title: months\n  default:\n  - 1\n  - 2\n  position: 0\n  type: array\n  items:\n    type: integer\nyear:\n  title: year\n  default: 2021\n  position: 1\n  type: integer\ncolor:\n  title: color\n  default: yellow\n  position: 2\n  type: string\n  required: null\n  definitions: null\ntimestamp: '2023-03-14T09:33:56.112462+00:00'\nNow we need to apply the deployment by running the yaml file:\nprefect deployment apply etl_parent_flow-deployment.yaml\nThis sends all the metadata over to the Prefect API.\n\n\n\ndeployment_apply.PNG\n\n\nFrom the Prefect UI we can see the deployment, trigger a flow run, change parameters etc. Let’s go ahead and trigger a quick run :\n\n\n\nprefect_deployment.PNG\n\n\n\n\n\nprefect_scheduled_run.PNG\n\n\nSo, our run is scheduled but is queued up. If we go now to the Prefect Orion UI Work Queues we can see we have 1 late run.\n\n\n\nwork_queue_late_run.PNG\n\n\nWe need to have an agent living in our execution environment (local) to trigger it. When we deployed from the terminal the following message was generated :\nTo execute flow runs from this deployment, start an agent that pulls work from the 'default' work queue:\n$ prefect agent start -q 'default'\nSo let’s now go ahead and start our agent:\n\n\n\nagent_1.PNG\n\n\n\n\n\nagent_2.PNG\n\n\n\n\n\nagent_3.PNG\n\n\nOur run has completed successfully and this is confirmed within the Prefect Orion UI:\n\n\n\ndeployment_complete.PNG\n\n\nSo this completed successfully however it is good practice to make use of Notifications. We can customise these from the UI or build these into our flows using Prefect blocks.\n\n\n\nnotifications.PNG\n\n\n\n\n\nLet’s now have a look at scheduling our flows and running our flows in containers in Docker. First let’s look at scheduling. We have a deployment that we ran in the previous section - we have various scheduling options which we can configure via the Prefect UI :\n\n\n\nprefect_scheduler.PNG\n\n\nThe Interval scheduler is fairly self-explanatory, for example every five minutes. Another option is something called Cron which allows us to configure period runs on a given schedule based on the following syntax :\n\n\n\ncrontab.PNG\n\n\nFor example, using our Prefect deployment, we can schedule for every minute on day 3 of the month:\n\n\n\nprefect_cron.PNG\n\n\nThe RRule (recurring rule) feature is more complex and is not currently available from within the Prefect UI, but we can configure these from the command line at the deployment stage :\nprefect deployment build ./parameterized_flow.py:etl_parent_flow -n \"ETL2\" --cron \"0 0 * * *\" -a\n\n\n\nschedule_ETL2.PNG\n\n\nWe can see this has been scheduled within the Prefect UI to run every day at 12:00am UTC :\n\n\n\nprefect_schedule_ETL2.PNG\n\n\nWe can find out more using :\nprefect deployment build --help\n \nSo far we have had our flow code sitting locally on our machine, but if we want to make things a little more production ready and enable other people to access our flow code, we could put our code on GitHub, BitBucket, GitLab, AWS S3, MS Azure - any of those version control systems.\nLet’s now look at how to store our flow in a Docker image on DockerHub and then when we run a docker container our code will be right there - we will be baking it into the image.\nThe first thing we need to do is create a Dockerfile :\nFROM prefecthq/prefect:2.7.7-python3.9\n\nCOPY docker-requirements.txt .\n\nRUN pip install -r docker-requirements.txt --trusted-host pypi.python.org --no-cache-dir\n\nCOPY flows /opt/prefect/flows  \nRUN mkdir -p /opt/prefect/data/yellow\nAnd we also need to create a requirements file :\npandas==1.5.2\nprefect-gcp[cloud_storage]==0.2.4\nprotobuf==4.21.11\npyarrow==10.0.1\npandas-gbq==0.18.1\nLet’s now build our docker image from the command line :\ndocker image build -t stephen137/prefect:zoomcamp .\n\n\n\ndocker_schedule.PNG\n\n\nNow we want to push the image to dockerhub :\ndocker image push stephen137/prefect:zoomcamp\n\n\n\ndocker_schedule_image_push.PNG\n\n\nThat was successful and we can see our image within the dockerhub UI :\n\n\n\ndockerhub.PNG\n\n\nIt is possible to create a docker block using code - but let’s for now just use:\npip install prefect-docker\nand then register the block :\nprefect block register -m prefect_docker\n\n\n\ndocker_block.PNG\n\n\nAnd then we can do some basic block config (image name being most important - as previously specified at docker build stage) within the Prefect UI :\n\n\n\nprefect_docker_container_block.PNG\n\n\nAnd the block code that we can add to our flow is :\n\n\n\ndocker_container_block_code.PNG\n\n\nLet’s now create our deployment from a python file (we did this previously from the command line) and include the block code above :\nfrom prefect.deployments import Deployment\nfrom parameterized_flow import etl_parent_flow\nfrom prefect.infrastructure.docker import DockerContainer\n\ndocker_block = DockerContainer.load(\"de-zoomcamp\") # as specified on creation of block\n\ndocker_dep = Deployment.build_from_flow(\n    flow=etl_parent_flow,\n    name=\"docker-flow\",\n    infrastructure=docker_block,\n)\n\n\nif __name__ == \"__main__\":\n    docker_dep.apply()\n\n\n\ndocker_deploy.PNG\n\n\n\n\n\ndocker_flow_deployment.PNG\n\n\nBefore we run this, let’s just take a look at something :\nprefect profile ls\n\n\n\nprefect_profile.PNG\n\n\nSo we are using default at the moment, but let’s configure to the Prefect API url :\nprefect config set PREFECT_API_URL=\"http://127.0.0.1:4200/api\"\nThis will ensure our docker container is able to interface with the Prefect Orion server.\nOK, so we are almost ready to start our flow. Let’s fire up an agent :\nprefect agent start -q default\n\n\n\nprefect_agent.PNG\n\n\nAnd as confirmed from the Orion UI :\n\n\n\norion_agent_start.PNG\n\n\nAnd let’s now run our flow from the command line, and override the default parameter :\nprefect deployment run etl-parent-flow/docker-flow -p \"months=[1,2]\"\n\n\n\ndocker_prefect_flow.PNG\n\n\n\n\n\ndocker_flow_crash.PNG\n\n\nAccording to the course FAQs this error occurs because we are running Prefect locally on our machine at localhost:4200, when we run docker without specifying their network, Docker calls the localhost:4200 inside the container but not the localhost:4200 on our machine.\nOne suggested solution is to set the network Mode to bridge although that didn’t work for me.\n\n\n\ndocker_container_bridge.PNG\n\n\nHopefully I can find a solution to this as I progress with the course, but let’s push on.\n\n\n\nFurther resource in support of the workflow orchestrations we have covered is included in the Prefect docs which cover the core concepts for flows, tasks and task runners. It also includes getting started tutorials as well as a recipes which are common, extensible examples for setting up Prefect in your execution environment with ready-made ingredients such as Dockerfiles, Terraform files, and GitHub Actions.\nPrefect also offers a hosted cloud solution, instead of having to host the Orion cloud UI yourself, and the cloud version includes some additional features such as Automations and workspaces which can be used collaboratively.\nFor technical discussions you can visit the Prefect Discourse and the Prefect Github. For blogs and guides it is well worth checking out Anna Geller’s Github.\n\n\n\nWe covered a lot this week! We started off by creating a simple python script that ingested a csv file into a database (postgreSQL).\nWe then showed how to streamline our data pipeline by leveraging the Prefect workflow orchestration tool. We created an ETL flow which:\n\ngrabbed a csv file from the web (1.4 million rows)\ncarried out some preprocessing, converted to a DataFrame\nuploaded to a data lake [Google Cloud Storage]\nmoved it to a data warehouse [BigQuery]\n\nIn week 3 we will be digging deeper into Biq Query, and taking a look at partitioning, clustering and best practice. We also learn about Machine Learning techniques using Big Query."
  },
  {
    "objectID": "posts/PySpark/Spark.html",
    "href": "posts/PySpark/Spark.html",
    "title": "What is Spark, anyway?",
    "section": "",
    "text": "I’ve noticed that Apache Spark is cited as a requirement on many data science job specs. My natural curiosity led me to the Introduction to PySpark course available through DataCamp which is a superb coding portal, offering slides, interactive lessons, projects, courses and career tracks. DataCamp was one of my first ports of call when I decided to upskill and make the transition from chartered accountancy to data science.\nToday we will learn how to use Spark from Python! Spark is a tool for doing parallel computation with large datasets and it integrates well with Python. PySpark is the Python package that makes the magic happen. We’ll use this package to work with data about flights from Portland and Seattle. We’ll learn to wrangle this data and build a whole machine learning pipeline to predict whether or not flights will be delayed. Get ready to put some Spark in your Python code and dive into the world of high-performance machine learning!\n\n\nIn this section, we’ll learn how Spark manages data and how to read and write tables from Python.\n\n\nSpark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\nHowever, with greater computing power comes greater complexity.\nDeciding whether or not Spark is the best solution for your problem takes some experience, but you can consider questions like:\n- Is my data too big to work with on a single machine?\n- Can my calculations be easily parallelized?\n\n\n\nThe first step in using Spark is connecting to a cluster.\nIn practice, the cluster will be hosted on a remote machine that’s connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called workers. The master sends the workers data and calculations to run, and they send their results back to the master.\nWhen you’re just getting started with Spark it’s simpler to just run a cluster locally. Creating the connection is as simple as creating an instance of the SparkContext class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you’re connecting to.\nAn object holding all these attributes can be created with the SparkConf() constructor. Take a look at the documentation for all the details!\n\n\n\n\n\n\nHow do you connect to a Spark cluster from PySpark?\n\n\n\n\n\nCreate an instance of the SparkContext class.\n\n\n\n\n\n\nYou’ll probably notice that code takes longer to run than you might expect. This is because Spark is some serious software. It takes more time to start up than you might be used to. You may also find that running simpler computations might take longer than expected. That’s because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions!\nPyspark can be installed from the command line using:\n\nmamba install pyspark\n\nFor detailed installation guidance consult the Pyspark documentation.\n\n# import SparkContext class\nfrom pyspark import SparkContext as sc\nfrom pyspark.sql import SparkSession\n\n\n# Create SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\nprint('PySpark Version :'+spark.version)\nprint('PySpark Version :'+spark.sparkContext.version)\n\n\n\n\nSpark’s core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you’ll be using the Spark DataFrame abstraction built on top of RDDs.\nThe Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\nWhen you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it’s up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\nTo start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.\n\n\n\n\n\n\nWhat is the advantage of Spark DataFrames over RDDs?\n\n\n\n\n\nOperations using DataFrames are automatically optimized.\n\n\n\n\n\n\nWe’ve already created a SparkSession called spark, but what if you’re not sure there already is one?\n\n\n\n\n\n\nCreating multiple SparkSessions and SparkContexts can cause issues.\n\n\n\nIt’s best practice to use the SparkSession.builder.getOrCreate() method. This returns an existing SparkSession if there’s already one in the environment, or creates a new one if necessary!\n\n\n\n# Import SparkSession from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create my_spark\nmy_spark = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(my_spark)\n\n\n\n\nmy_spark.PNG\n\n\n\n\n\nOnce we’ve created a SparkSession, we can start poking around to see what data is in our cluster! Our SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\nOne of the most useful is the .listTables() method, which returns the names of all the tables in our cluster as a list.\n\n# Print the tables in the catalog\nprint(spark.catalog.listTables())\n\n[Table(name=‘flights’, database=None, description=None, tableType=‘TEMPORARY’, isTemporary=True)]\n\n\n\nOne of the advantages of the DataFrame interface is that you can run SQL queries on the tables in your Spark cluster.\nAs we saw above, one of the tables in our cluster is the flights table. This table contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015. Running a query on this table is as easy as using the .sql() method on your SparkSession. This method takes a string containing the query and returns a DataFrame with the results!\nIf you look closely, you’ll notice that the table flights is only mentioned in the query, not as an argument to any of the methods. This is because there isn’t a local object in our environment that holds that data, so it wouldn’t make sense to pass the table as an argument.\n\n# get the first 10 rows of the flights table using SQL query\nquery = \"FROM flights SELECT * LIMIT 10\"\n\n# assign SQL query \nflights10 = spark.sql(query)\n\n# Show the results\nflights10.show()\n\n\n\n\nQuery-ious.PNG\n\n\n\n\n\nSuppose we’ve run a query on our huge dataset and aggregated it down to something a little more manageable.\nSometimes it makes sense to then take that table and work with it locally using a tool like pandas. Spark DataFrames make that easy with the .toPandas() method. Calling this method on a Spark DataFrame returns the corresponding pandas DataFrame. It’s as simple as that!\nThis time the query counts the number of flights to each airport from SEA and PDX.\n\n# Count the number of flights to each airport from SEA and PDX using a SQL query\nquery = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n\n# assign the query \nflight_counts = spark.sql(query)\n\n# Convert the results to a pandas DataFrame\npd_counts = flight_counts.toPandas()\n\n# Print the head of pd_counts\nprint(pd_counts.head())\n\n\n\n\npandafy.PNG\n\n\n\n\n\nIn the last section, we saw how to move data from Spark to pandas. However, maybe we want to go in the other direction, and put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well.\nThe .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\nThe output of this method is stored locally, not in the SparkSession catalog. This means that we can use all the Spark DataFrame methods on it, but we can’t access the data in other contexts.\nFor example, a SQL query (using the .sql() method) that references our DataFrame will throw an error. To access the data in this way, we have to save it as a temporary table. We can do this using the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you’d like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\nThere is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. We’ll use this method to avoid running into problems with duplicate tables.\nCheck out the diagram below to see all the different ways our Spark data structures interact with each other:\n\n\n# Create a pandas Dataframe of random numbers\npd_temp = pd.DataFrame(np.random.random(10))\n\n# Create a Spark DataFrame from the pandas DataFrame\nspark_temp = spark.createDataFrame(pd_temp)\n\n# Examine the list of tables in our Spark cluster and verify that the new DataFrame is not present\nprint(spark.catalog.listTables())\n\n# Register the newly created Spark DataFrame as a temporary table - name it \"temp\"\nspark_temp.createOrReplaceTempView(\"temp\")\n\n# Examine the list of tables in the catalog again\nprint(spark.catalog.listTables())\n\n\n\n\nput_some_spark.PNG\n\n\n\n\n\nNow we know how to put data into Spark via pandas, but you’re probably wondering why deal with pandas at all? Wouldn’t it be easier to just read a text file straight into Spark? Of course it would! Luckily, our SparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these we can create a DataFrame from a .csv file just like with regular pandas DataFrames!\nThe variable file_path is a string with the path to the file airports.csv. This file contains information about different airports all over the world.\n\n# Set the path\nfile_path = \"/usr/local/share/datasets/airports.csv\"\n\n# Read in the airports data creating a Spark DataFrame\nairports = spark.read.csv(file_path, header=True)\n\n# Show the data\nairports.show()\n\n\n\n\ndropping middle man.PNG\n\n\n\n\n\n\nIn this section, we’ll learn about the pyspark.sql module, which provides optimized data queries to your Spark session.\n\n\nIn this section, we’ll learn how to use the methods defined by Spark’s DataFrame class to perform common data operations.\nLet’s look at performing column-wise operations. In Spark we can do this using the .withColumn() method, which takes two arguments. First, a string with the name of our new column, and second the new column itself. The new column must be an object of class Column. Creating one of these is as easy as extracting a column from our DataFrame using df.colName.\n\n\n\n\n\n\nUpdating a Spark DataFrame is somewhat different than working in pandas because the Spark DataFrame is immutable. This means that it can’t be changed, and so columns can’t be updated in place. Thus, all these methods return a new DataFrame.\n\n\n\n\n\n\n\n\n\n\n\n\nTo overwrite the original DataFrame we must reassign the returned DataFrame using the method like so:\n\n\n\ndf = df.withColumn(\"newCol\", df.oldCol + 1)\n\n\nThe above code creates a DataFrame with the same columns as df plus a new column, newCol, where every entry is equal to the corresponding entry from oldCol, plus one. To overwrite an existing column, just pass the name of the column as the first argument!\n\n# Use the spark.table() method to create a DataFrame containing the values of the flights table\nflights = spark.table(\"flights\")\n\n# Add a new column \"duration_hrs\" - the column air_time includes minutes\nflights = flights.withColumn(\"duration_hrs\", flights.air_time/60)\n\n# Show the head\nflights.show()\n\n\n\n\nflights.PNG\n\n\n\n\n\nA SQL query returns a table derived from one or more tables contained in a database. Every SQL query is made up of commands that tell the database what you want to do with the data. The two commands that every query has to contain are SELECT and FROM.\nThe SELECT command is followed by the columns we want in the resulting table.\nThe FROM command is followed by the name of the table that contains those columns. The minimal SQL query is:\nSELECT * FROM my_table;\nThe * selects all columns, so this returns the entire table named my_table.\nSimilar to .withColumn(), we can do column-wise computations within a SELECT statement. For example,\nSELECT origin, dest, air_time / 60 FROM flights;\nreturns a table with the origin, destination, and duration in hours for each flight.\n\n\n\nAnother commonly used command is *WHERE. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where our condition is true. For example, if we had a table of students and grades we could do:\nSELECT * FROM students\nWHERE grade = 'A';\nto select all the columns and the rows containing information about students who got As.\n\n\n\nAnother common database task is aggregation. That is, reducing our data by breaking it into chunks and summarizing each chunk. This is done in SQL using the GROUP BY command. This command breaks our data into groups and applies a function from our SELECT statement to each group.\nFor example, if we wanted to COUNT the number of flights from each of two origin destinations, we could use the query\nSELECT COUNT(*) FROM flights\nGROUP BY origin;\nGROUP BY origin tells SQL that we want the output to have a row for each unique value of the origin column. The SELECT statement selects the values we want to populate each of the columns. Here, we want to COUNT() every row in each of the groups.\nIt’s possible to GROUP BY more than one column. When we do this, the resulting table has a row for every combination of the unique values in each column. The following query counts the number of flights from SEA and PDX to every destination airport:\nSELECT origin, dest, COUNT(*) FROM flights\nGROUP BY origin, dest;\nThe output will have a row for every combination of the values in origin and dest (i.e. a row listing each origin and destination that a flight flew to). There will also be a column with the COUNT() of all the rows in each group.\n\n\n\nLet’s now talk about the analogous operations using Spark DataFrames.\nLet’s take a look at the .filter() method. As you might suspect, this is the Spark counterpart of SQL’s WHERE clause. The .filter() method takes either an expression that would follow the WHERE clause of a SQL expression as a string, or a Spark Column of boolean (True/False) values.\nFor example, the following two expressions will produce the same output:\nflights.filter(\"air_time > 120\").show()\nflights.filter(flights.air_time > 120).show()\nNotice that in the first case, we pass a string to .filter(). In SQL, we would write this filtering task as SELECT * FROM flights WHERE air_time > 120. Spark’s .filter() can accept any expression that could go in the WHERE clause of a SQL query (in this case, “air_time > 120”), as long as it is passed as a string. Notice that in this case, we do not reference the name of the table in the string – as we wouldn’t in the SQL request.\nIn the second case, we actually pass a column of boolean values to .filter(). Remember that flights.air_time > 120 returns a column of boolean values that has True in place of those records in flights.air_time that are over 120, and False otherwise.\n\n# Filter all flights that flew over 1000 miles by passing a string\nlong_flights1 = flights.filter(\"distance > 1000\").show()\n\n# Filter flights that flew over 1000 miles by passing a column of boolean values\nlong_flights2 = flights.filter(flights.distance > 1000).show()\n\nBoth methods return the same output:\n\n\n\nFiltering using Spark.PNG\n\n\n\n\n\nThe Spark variant of SQL’s SELECT is the .select() method. This method takes multiple arguments - one for each column we want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). When we pass a column object, we can perform operations like addition or subtraction on the column to change the data contained in it, much like inside .withColumn().\nThe difference between .select() and .withColumn() methods is that .select() returns only the columns you specify, while .withColumn() returns ALL the columns of the DataFrame in addition to the one we defined.\n\n\n\n\n\n\nIt’s often a good idea to drop columns you don’t need at the beginning of an operation so that you’re not dragging around extra data as you’re wrangling\n\n\n\nIn this case, we would use .select() and not .withColumn().\n\n\n\n# Select the columns \"tailnum\", \"origin\", and \"dest\" from flights by passing the column names as strings\nselected1 = flights.select(\"tailnum\", \"origin\", \"dest\")\n\n#Select the columns \"origin\", \"dest\", and \"carrier\" using the df.colName syntax and then filter the result  \ntemp = flights.select(flights.origin, flights.dest, flights.carrier)\n\n# Define first filter\nfilterA = flights.origin == \"SEA\"\n\n# Define second filter\nfilterB = flights.dest == \"PDX\"\n\n# Filter the data, first by filterA then by filterB and show result\nselected2 = temp.filter(filterA).filter(filterB).show()\n\n\n\n\nselecting using spark.PNG\n\n\nSimilar to SQL, we can also use the .select() method to perform column-wise operations. When we’re selecting a column using the df.colName notation, we can perform any column operation and the .select() method will return the transformed column. For example,\nflights.select(flights.air_time/60)\nreturns a column of flight durations in hours instead of minutes. We can also use the .alias() method to rename a column you’re selecting. So if you wanted to .select() the column duration_hrs (which isn’t in oour DataFrame) we could do\nflights.select((flights.air_time/60).alias(\"duration_hrs\"))\nThe equivalent Spark DataFrame method .selectExpr() takes SQL expressions as a string:\nflights.selectExpr(\"air_time/60 as duration_hrs\")\nwith the SQL as keyword being equivalent to the .alias() method. To select multiple columns, we can pass multiple strings.\n\n# Calculate average speed and use the .alias() method to name the column\navg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n\n# Select the required columns\nspeed1 = flights.select(\"origin\", \"dest\", \"tailnum\", avg_speed).show()\n\n# Create the same table using an SQL expression\nspeed2 = flights.selectExpr(\"origin\", \"dest\", \"tailnum\", \"distance/(air_time/60) as avg_speed\").show()\n\nBoth methods return the same output:\n\n\n\nselecting using spark 2.PNG\n\n\n\n\n\nAll of the common aggregation methods, like .min(), .max(), and .count() are GroupedData methods. These are created by calling the .groupBy() DataFrame method. All we have to do to use these functions is call that method on your DataFrame. For example, to find the minimum value of a column, col, in a DataFrame, df, we could do\ndf.groupBy().min(\"col\").show()\nThis creates a GroupedData object (so we can use the .min() method), then finds the minimum value in col, and returns it as a DataFrame.\n\n# Find the length of the shortest flight that left from PDX in terms of distance\nflights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n\n\n\n\nmin_distance.PNG\n\n\n\n# Find the longest flight from SEA in terms of air time\nflights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()\n\n\n\n\nmax_air_time.PNG\n\n\nTo get us familiar with more of the built in aggregation methods, here’s a few more exercises involving the flights table!\n\n# Get the average air time of Delta Airlines flights (where the carrier column has the value \"DL\") that left SEA - using the .avg() method\nflights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n\n# get the total number of hours all planes in this dataset spent in the air - using the .sum() method \nflights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n\n\n\n\naggregate_spark_2.PNG\n\n\n\n\n\nPart of what makes aggregating so powerful is the addition of groups. PySpark has a whole class devoted to grouped data frames: pyspark.sql.GroupedData. We’ve learned how to create a grouped DataFrame by calling the .groupBy() method on a DataFrame with no arguments.\nNow we’ll see that when we pass the name of one or more columns in our DataFrame to the .groupBy() method, the aggregation methods behave like when we use a GROUP BY statement in a SQL query!\n\n# Create a DataFrame that is grouped by the column tailnum\nby_plane = flights.groupBy(\"tailnum\")\n\n# se the .count() method with no arguments to count the number of flights each plane made.\nby_plane.count().show()\n\n\n\n\ntail_number.PNG\n\n\n\n# Create a DataFrame that is grouped by the column origin\nby_origin = flights.groupBy(\"origin\")\n\n# Find the .avg() of the air_time column to find average duration of flights from PDX and SEA.\nby_origin.avg(\"air_time\").show()\n\n\n\n\nby_origin.PNG\n\n\nIn addition to the GroupedData methods we’ve already seen, there is also the .agg() method. This method lets us pass an aggregate column expression that uses any of the aggregate functions from the pyspark.sql.functions submodule.\nThis submodule contains many useful functions for computing things like standard deviations. All the aggregation functions in this submodule take the name of a column in a GroupedData table.\n\n# Import the submodule pyspark.sql.functions\nimport pyspark.sql.functions as F\n\n# Create a GroupedData table, grouped by month and dests. Refer to the two columns by passing both strings as separate arguments.\nby_month_dest = flights.groupBy(\"month\", \"dest\")\n\n# Get the average dep_delay in each month for each destination, by using the .avg() method \nby_month_dest.avg(\"dep_delay\").show()\n\n\n\n\nave_dep_delay.PNG\n\n\n\n# Find the standard deviation of dep_delay by using the .agg() method with the function F.stddev()\nby_month_dest.agg(F.stddev(\"dep_delay\")).show()\n\n\n\n\nstd_dev_dep_delay.PNG\n\n\n\n\n\nAnother very common data operation is the join. Joins are a whole topic unto themselves, so in this blog we’ll just look at simple joins. If you’d like to learn more about joins, you can take a look here. A join will combine two different tables along a column that they share. This column is called the key. Examples of keys here include the tailnum and carrier columns from the flights table.\nFor example, suppose that we want to know more than just the tail number of the plane that flew a flight. This information isn’t in the flights table because the same plane flies many different flights over the course of two years, so including this information in every row would result in a lot of duplication. To avoid this, we’d have a second table that has only one row for each plane and whose columns list all the information about the plane, including its tail number. You could call this table planes.\nWhen we join the flights table to this table of airplane information, we’re adding all the columns from the planes table to the flights table. To fill these columns with information, we’ll look at the tail number from the flights table and find the matching one in the planes table, and then use that row to fill out all the new columns.\nNow we’ll have a much bigger table than before, but now every row has all information about the plane that flew that flight!\nIn PySpark, joins are performed using the DataFrame method .join(). This method takes three arguments\n\nthe second DataFrame that you want to join with the first one.\non - the name of the key column(s) as a string. The names of the key column(s) must be the same in each table.\nhow - specifies the kind of join to perform.\n\n\n# Examine the airports DataFrame\nairports.show()\n\n\n\n\nairports.PNG\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe faa column refers to the alphanumeric code identifying United States airports. The dest column of the flights DataFrame has this same information and will therefore be our key but it is in a different format (the typical airport ‘LAX’ format).\nPrior to joining we must therefore rename the faa column.\n\n\n\n# Rename the faa column to enable joining\nairports = airports.withColumnRenamed(\"faa\", \"dest\")\n\n# Join the flights to the airports DataFrame \n# First argument should be the other DatafRame\n# Second argument (on) should be the key\n# Thirs argument (how) should be 'leftover'\nflights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n\n# Examine the new DataFrame\nflights_with_airports.show()\n\n\n\n\nflights_with_airports.PNG\n\n\n\n\n\n\nPySpark has built-in, cutting-edge machine learning routines, along with utilities to create full machine learning pipelines. We’ll learn about them in this section.\nAt the core of the pyspark.ml module are the Transformer and Estimator classes. Almost every other class in the module behaves similarly to these two basic classes.\nTransformer classes have a .transform() method that takes a DataFrame and returns a new DataFrame; usually the original one with a new column appended. For example, you might use the class Bucketizer to create discrete bins from a continuous feature or the class PCA to reduce the dimensionality of your dataset using principal component analysis.\nEstimator classes all implement a .fit() method. These methods also take a DataFrame, but instead of returning another DataFrame they return a model object. This can be something like a StringIndexerModel for including categorical data* saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression.\n\n\nIn the next two sections we’ll be working towards building a model that predicts whether or not a flight will be delayed based on the flights data we’ve been working with. This model will also include information about the plane that flew the route, so the first step is to join the two tables: flights and planes!\n\n# Rename year column of planes to avoid duplicate column names\nplanes = planes.withColumnRenamed(\"year\",\"plane_year\")\n\n# Join the planes table to the flights table on tailnum\nmodel_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")\n\n\n\n\n\n\n\n\n\n\nWhat kind of data does Spark need for modeling?\n\n\n\n\n\nDoubles\n\n\n\nBefore we start modeling, it’s important to know that Spark only handles numeric data. That means all of the columns in our DataFrame must be either integers or decimals (called ‘doubles’ in Spark). When we imported our data, we let Spark guess what kind of information each column held. Unfortunately, Spark doesn’t always guess right and you can see that some of the columns in our DataFrame are strings containing numbers as opposed to actual numeric values.\n\n\n\n\n\n\nTo find out the datatype of each of the columns of a dataset use .dtypes\n\n\n\n\n\n\n\nmodel_data.dtypes\n\n\n\n\ndata_types_before.PNG\n\n\nAs we can see all columns are of type ‘string’.\nTo remedy this, we can use the .cast() method in combination with the .withColumn() method. It’s important to note that .cast() works on columns, while .withColumn() works on DataFrames.\nThe only argument we need to pass to .cast() is the kind of value we want to create, in string form. For example, to create integers, we’ll pass the argument “integer” and for decimal numbers you’ll use “double”.\nWe can put this call to .cast() inside a call to .withColumn() to overwrite the already existing column, just like we did in the previous section.\n\n\n\nNow we’ll use the .cast() method to convert all the appropriate columns from our DataFrame model_data to integers! To convert the type of a column using the .cast() method, we can write code like this:\ndataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\")\n\n# Cast the columns to integers\nmodel_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\nmodel_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n\nmodel_data.dtypes\n\n\n\n\ndata_types_after.PNG\n\n\nWe can see that the our arr_delay, air_time, month, and plane_year columns are now of the required type - ‘int’.\n\n\n\nNow we have converted the column plane_year to an integer. This column holds the year each plane was manufactured. However, our model will use the planes’ age, which is slightly different from the year it was made!\n\n# Create the column plane_age by subtracting the year of manufacture from the year of the flight\nmodel_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n\n\n\n\nConsider that we’re modeling a yes or no question: is the flight late? However, our data contains the arrival delay in minutes for each flight. Thus, we’ll need to create a boolean column which indicates whether the flight was late or not!\n\n# Create an is_late column \nmodel_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n\n# Convert our is_late column to an integer\nmodel_data = model_data.withColumn(\"label\",model_data.is_late.cast(\"integer\"))   \n\n# Remove missing values\nmodel_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")\n\n\n\n\n\n\n\nWarning\n\n\n\n“label” is the default name for the response variable in Spark’s machine learning routines\n\n\nGreat. Now we’ve defined the label column - the column that we’re going to use as the outcome in our model.\n\n\n\nAs we know, Spark requires numeric data for modeling. So far this hasn’t been an issue; even boolean columns can easily be converted to integers without any trouble. But we’ll also be using the airline and the plane’s destination as features in our model. These are coded as strings and there isn’t any obvious way to convert them to a numeric data type.\nFortunately, PySpark has functions for handling this built into the pyspark.ml.features submodule. WE can create what are called ‘one-hot vectors’ to represent the carrier and the destination of each flight. A one-hot vector is a way of representing a categorical feature where every observation has a vector in which all elements are zero except for at most one element, which has a value of one (1).\nEach element in the vector corresponds to a level of the feature, so it’s possible to tell what the right level is by seeing which element of the vector is equal to one (1).\n\nThe first step to encoding our categorical feature is to create a StringIndexer. Members of this class are Estimators that take a DataFrame with a column of strings and map each unique string to a number. Then, the Estimator returns a Transformer that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column.\nThe second step is to encode this numeric column as a one-hot vector using a OneHotEncoder. This works exactly the same way as the StringIndexer by creating an Estimator and then a Transformer.\n\nThe end result is a column that encodes our categorical feature as a vector that’s suitable for machine learning routines! This may seem complicated, but don’t worry! All we have to remember is that we need to create a StringIndexer and a OneHotEncoder, and the Pipeline will take care of the rest.\n\n\n\n\n\n\nWhy do you have to encode a categorical feature as a one-hot vector?\n\n\n\n\n\nSpark can only model numeric features.\n\n\n\n\n\n\nLet’s now create a StringIndexer and a OneHotEncoder to code the carrier column. To do this, we’ll call the class constructors with the arguments inputCol and outputCol.\n\nthe inputCol is the name of the column you want to index or encode;\nthe outputCol is the name of the new column that the Transformer should create\n\n\n# Create a StringIndexer for the carrier column\ncarr_indexer = StringIndexer(inputCol=\"carrier\",outputCol=\"carrier_index\")\n\n# Create a OneHotEncoder \ncarr_encoder = OneHotEncoder(inputCol=\"carrier_index\",outputCol=\"carrier_fact\")\n\nNow the carrier information is in the correct format for inclusion in our model. Let’s do the same thing for the dest column:\n\n# Create a StringIndexer for the dest column\ndest_indexer = StringIndexer(inputCol=\"dest\",outputCol=\"dest_index\")\n\n# Create a OneHotEncoder\ndest_encoder = OneHotEncoder(inputCol=\"dest_index\",outputCol=\"dest_fact\")\n\n\n\n\nThe last step in the Pipeline is to combine all of the columns containing our features into a single column. This has to be done before modeling can take place because every Spark modeling routine expects the data to be in this form. We can do this by storing each of the values from a column as an entry in a vector. Then, from the model’s point of view, every observation is a vector that contains all of the information about it and a label that tells the modeler what value that observation corresponds to.\nBecause of this, the pyspark.ml.feature submodule contains a class called VectorAssembler. This Transformer takes all of the columns you specify and combines them into a new vector column.\n\n# Create a VectorAssembler with or inputCols as a list, and outputCol name \"features\"\nvec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")\n\n\n\n\nPipeline is a class in the pyspark.ml module that combines all the Estimators and Transformers that you’ve already created. This lets you reuse the same modeling process over and over again by wrapping it up in one simple object.\n\n# Import Pipeline\nfrom pyspark.ml import Pipeline\n\n# Call the pipeline with keyword argument \"stages\"\nflights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n\n\n\n\n\n\n\nImportant\n\n\n\nstages should be a list holding all the stages you want your data to go through in the pipeline.\n\n\nExcellent, we now have a fully reproducible machine learning pipeline!\n\n\n\nAfter we’ve cleaned our data and knocked it into shape for modeling, one of the most important steps is to split the data into a test set and a train set. After that, we don’t touch our test data until we think we have a good model! As we’re building models and forming hypotheses, we can test them on our training data to get an idea of their performance.\nOnce we’ve got our favorite model, we can see how well it predicts the new data in our test set. This never-before-seen data will give us a much more realistic idea of our model’s performance in the real world when we’re trying to predict or classify new data.\nIn Spark it’s important to make sure we split the data after all the transformations. This is because operations like StringIndexer don’t always produce the same index even when given the same list of strings.\n\n\n\n\n\n\nWhy is it important to use a test set in model evaluation?\n\n\n\n\n\nBy evaluating your model with a test set you can get a good idea of performance on new data.\n\n\n\n\n\n\nWe’re finally ready to pass our data through the Pipeline we created!\n\n# Fit and transform the model_data\npiped_data = flights_pipe.fit(model_data).transform(model_data)\n\n\n\n\nNow that we’ve done all your manipulations, the last step before modeling is to split the data!\n\n# Split the data into a training set (60%) and test set (40%)\ntraining, test = piped_data.randomSplit([.6, .4])\n\n\n\n\n\n\n\nImportant\n\n\n\nThe train:test split is implemented by passing a list containing the desired split in decimal format\n\n\nNow we are ready to start fitting the model.\n\n\n\n\n\nIn this last section, we’ll apply what we’ve learned to create a model that predicts which flights will be delayed.\n\n\nThe model we’ll be fitting in this section is called a logistic regression. This model is very similar to a *linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event.\nTo use this as a classification algorithm, all we have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, we classify that observation as a ‘yes’ (in this case, the flight being late), if it’s below, we classify it as a ‘no’!\nWe’ll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that’s not estimated from the data, but rather is supplied by the user to maximize performance.\n\n\n\n\n# Import LogisticRegression\nfrom pyspark.ml.classification import LogisticRegression\n\n# Instantiate a LogisticRegression Estimator\nlr = LogisticRegression()\n\n\n\n\nIn the next few exercises we’ll be tuning our logistic regression model using a procedure called k-fold cross validation. This is a method of estimating the model’s performance on unseen data (like our test DataFrame). It works by splitting the training data into a few different partitions. The exact number is up to you, but we’ll be using PySpark’s default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data.\nWe’ll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare all the different models so we can choose the best one!\n\n\n\n\n\n\nWhat does cross validation allow you to estimate?\n\n\n\n\n\nThe model’s error on held out data.\n\n\n\n\n\n\nThe first thing we need when doing cross validation for model selection is a way to compare different models. Luckily, the pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Our model is a binary classification model, so we’ll be using the BinaryClassificationEvaluator from the pyspark.ml.evaluation module.\nThis evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. We’ll learn more about this later!\n\n# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n\n\n\n\nNext, we need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that (maybe you’re starting to notice a pattern here; PySpark has a submodule for just about everything!).\nWE’ll need to use the .addGrid() and .build() methods to create a grid that we can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that we created earlier) and a list of values that we want to try. The .build() method takes no arguments, it just returns the grid that we’ll use later.\n\n# Import the tuning submodule\nimport pyspark.ml.tuning as tune\n\n# Create the parameter grid\ngrid = tune.ParamGridBuilder()\n\n# Add the learning rate hyperparameters \ngrid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n\n\n\n\n\n\n\nThe lr. regParam parameters are set by using np.arange which creates a list of numbers (from, increment, to) which give us the learning rate values to try.\n\n\n\n\n\n\n\n# Add the elasticNetParam hyperparameter\ngrid = grid.addGrid(lr.elasticNetParam, [0, 1])\n\n\n\n\n\n\n\nElastic net regularization uses a weighted combination of LASSO (L1) and Ridge (L2). The specified value relates to the weighting applied to L1 regularization, so a value of 0 would be pure L2, and a value of 1 would be pure Ridge. Any values in between result in a blend of the two.\n\n\n\n\n\n\n\n# Build the grid\ngrid = grid.build()\n\n\n\n\nThe submodule pyspark.ml.tuning also has a class called CrossValidator for performing cross validation. This Estimator takes the modeler we want to fit, the grid of hyperparameters we created, and the evaluator we want to use to compare our models.\nWe’ll create the CrossValidator by passing it the logistic regression Estimator lr, the parameter grid, and the evaluator we created earlier.\n\n# Create the CrossValidator\ncv = tune.CrossValidator(estimator=lr,\n               estimatorParamMaps=grid,\n               evaluator=evaluator\n               )\n\n\n\n\nUnfortunately, cross validation is a very computationally intensive procedure. To do this locally you would use the code:\n# Fit cross validation models\nmodels = cv.fit(training)\n\n# Extract the best model\nbest_lr = models.bestModel\nRemember, the training data is called training and we’re using lr to fit a logistic regression model. Cross validation selected the parameter values regParam=0 and elasticNetParam=0 as being the best. These are the default values, so we don’t need to do anything else with lr before fitting the model.\n\n# Extract the best model \nbest_lr = lr.fit(training)\n\n# Print best_lr to verify it's an object of the LogisticRegressionModel class\nprint(best_lr)\n\nLogisticRegressionModel: uid=LogisticRegression_2bd11bb498b4, numClasses=2, numFeatures=83\n\n\n\nWe’ll be using a common metric for binary classification algorithms call the AUC, or area under the curve. In this case, the curve is the ROC, or receiver operating curve. For our purposes, the closer the AUC is to one (1), the better the model is!\n\n\n\n\n\n\nIf you’ve created a perfect binary classification model, what would the AUC be?\n\n\n\n\n\n1\n\n\n\n\n\n\nIt’s finally time to test our model on the test data we set aside earlier. We can use the same evaluator we made to fit the model.\n\n# Use the model to generate predictions on the test set\ntest_results = best_lr.transform(test)\n\n# Evaluate the predictions to compute the AUC\nprint(evaluator.evaluate(test_results))\n\n0.7123313100891033\n\n\n\n\nl learned how to use Spark from Python to wrangle data about flights from Portland and Seattle and using this data, build a machine learning pipeline to predict whether or not flights will be delayed."
  },
  {
    "objectID": "posts/Random_Forests/Random_Forests.html",
    "href": "posts/Random_Forests/Random_Forests.html",
    "title": "Random Forests",
    "section": "",
    "text": "This is my follow up to the first part of Lesson 6: Practical Deep Learning for Coders 2022 in which Jeremy introduces Decision Trees and Random Forests.\nFor tabular data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it’s more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines. In this notebook, we’re going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition!\nWe’ll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:\n\n# import required package\nfrom fastai.imports import *\n\n# optimize display settings\nnp.set_printoptions(linewidth=130)\n\nNow let’s create DataFrames from the CSV files and carry out some preprocessing:\n\n# grab our data from Kaggle\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)   \n\ntitanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\n\n# read in our training and test datasets\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\n\n# let's see what is the most common value for each colums\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\nOne difference with Random Forests however is that we don’t generally have to create dummy variables like we do for non-numeric columns in linear models and neural networks. Instead, we can just convert those fields to categorical variables, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.\n\ndf.dtypes\n\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\n\n\n# create a function to carry out some preprocessing\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0) # replace Fare Na with 0\n    df.fillna(modes, inplace=True)\n    df['LogFare'] = np.log1p(df['Fare']) # take log of fares and add 1 - normalization\n    df['Embarked'] = pd.Categorical(df.Embarked) # convert embaked column to categorical\n    df['Sex'] = pd.Categorical(df.Sex) # convert sex column to categorical\n\n\n# apply our pre-processign function to our training set\nproc_data(df)\n\n# apply our pre-processign function to our test set\nproc_data(tst_df)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      LogFare\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      B96 B98\n      S\n      2.110213\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n      4.280593\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      B96 B98\n      S\n      2.188856\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n      3.990834\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      B96 B98\n      S\n      2.202765\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      B96 B98\n      S\n      2.639057\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n      3.433987\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      24.0\n      1\n      2\n      W./C. 6607\n      23.4500\n      B96 B98\n      S\n      3.196630\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n      3.433987\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      B96 B98\n      Q\n      2.169054\n    \n  \n\n891 rows × 13 columns\n\n\n\nWe’ll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider pclass a categorical variable. That’s because it’s ordered (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we’ll see, only care about order, not about absolute value.\n\n# set our categorical variables\ncats=[\"Sex\",\"Embarked\"]\n\n# set our continuous variables\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\n\n# set our dependent(target/y) variable\ndep=\"Survived\"\n\nEven although we’ve made the cats columns categorical, they are still shown by Pandas as their original values:\n\n# take a look at first 5 rows of sex column\ndf.Sex.head()\n\n0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']\n\n\nHowever behind the scenes they’re now stored as integers, with indices that are looked up in the Categories list shown in the output above. We can view the stored values by looking in the pandas **cat.codes** attribute:\n\n# take a look at indexes applied to values in first 5 rows of sex column\ndf.Sex.cat.codes.head()\n\n0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8\n\n\n\n\nBefore we create a Random Forest or Gradient Boosting Machine, we’ll first need to learn how to create a decision tree, from which both of these models are built. And to create a decision tree, we’ll first need to create a binary split, since that’s what a decision tree is built from.\nA binary split is where all rows are placed into one of two groups, based on whether they’re above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold 0.5 and the column Sex (since the values in the column are 0 for female and 1 for male). We can use a plot to see how that would split up our data – we’ll use the Seaborn library, which is a layer on top of matplotlib that makes some useful charts easier to create, and more aesthetically pleasing by default:\n\n\n\n# import required package for plotting\nimport seaborn as sns\n\n\n# create side by side histograms \nfig,axs = plt.subplots(1,2, figsize=(11,5))\n\n# survival rate histogram by sex - axs[0] so this is first plot\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\n\n# countplot by sex - axs[1] so this is second plot\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");\n\n\n\n\n\ndf['Sex'].value_counts()\n\nmale      577\nfemale    314\nName: Sex, dtype: int64\n\n\n\ndf['Survived'].sum()\n\n342\n\n\nHere we see that (on the left) if we split the data into males and females, we’d have groups that have very different survival rates: >70% for females, and <20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of 891) in each group.\nWe could create a very simple “model” which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:\n\n# import required packages\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\n\n# set seee for reproducibility\nrandom.seed(42)\n\n# create training & validation sets\ntrn_df,val_df = train_test_split(df, test_size=0.25)\n\n# replace categorical variables with their integer codes\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n\n\ntrn_df\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n      LogFare\n    \n  \n  \n    \n      298\n      299\n      1\n      1\n      Saalfeld, Mr. Adolphe\n      1\n      24.00\n      0\n      0\n      19988\n      30.5000\n      C106\n      2\n      3.449988\n    \n    \n      884\n      885\n      0\n      3\n      Sutehall, Mr. Henry Jr\n      1\n      25.00\n      0\n      0\n      SOTON/OQ 392076\n      7.0500\n      B96 B98\n      2\n      2.085672\n    \n    \n      247\n      248\n      1\n      2\n      Hamalainen, Mrs. William (Anna)\n      0\n      24.00\n      0\n      2\n      250649\n      14.5000\n      B96 B98\n      2\n      2.740840\n    \n    \n      478\n      479\n      0\n      3\n      Karlsson, Mr. Nils August\n      1\n      22.00\n      0\n      0\n      350060\n      7.5208\n      B96 B98\n      2\n      2.142510\n    \n    \n      305\n      306\n      1\n      1\n      Allison, Master. Hudson Trevor\n      1\n      0.92\n      1\n      2\n      113781\n      151.5500\n      C22 C26\n      2\n      5.027492\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      106\n      107\n      1\n      3\n      Salkjelsvik, Miss. Anna Kristine\n      0\n      21.00\n      0\n      0\n      343120\n      7.6500\n      B96 B98\n      2\n      2.157559\n    \n    \n      270\n      271\n      0\n      1\n      Cairns, Mr. Alexander\n      1\n      24.00\n      0\n      0\n      113798\n      31.0000\n      B96 B98\n      2\n      3.465736\n    \n    \n      860\n      861\n      0\n      3\n      Hansen, Mr. Claus Peter\n      1\n      41.00\n      2\n      0\n      350026\n      14.1083\n      B96 B98\n      2\n      2.715244\n    \n    \n      435\n      436\n      1\n      1\n      Carter, Miss. Lucile Polk\n      0\n      14.00\n      1\n      2\n      113760\n      120.0000\n      B96 B98\n      2\n      4.795791\n    \n    \n      102\n      103\n      0\n      1\n      White, Mr. Richard Frasar\n      1\n      21.00\n      0\n      1\n      35281\n      77.2875\n      D26\n      2\n      4.360388\n    \n  \n\n668 rows × 13 columns\n\n\n\nNow we can create our independent variables (the x variables) and dependent (the y variable):\n\n# create a function \ndef xs_y(df):\n    xs = df[cats+conts].copy() # independent variables are catoegorical and continuous\n    return xs,df[dep] if dep in df else None # return independent variables, dependent variable\n\n\n# apply function to training & validation sets\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n\n\n# check last 5 rows of our training set\ntrn_xs.tail()\n\n\n\n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      106\n      0\n      2\n      21.0\n      0\n      0\n      2.157559\n      3\n    \n    \n      270\n      1\n      2\n      24.0\n      0\n      0\n      3.465736\n      1\n    \n    \n      860\n      1\n      2\n      41.0\n      2\n      0\n      2.715244\n      3\n    \n    \n      435\n      0\n      2\n      14.0\n      1\n      2\n      4.795791\n      1\n    \n    \n      102\n      1\n      2\n      21.0\n      0\n      1\n      4.360388\n      1\n    \n  \n\n\n\n\n\n# check last 5 rows of our validation set\nval_xs.tail()\n\n\n\n\n\n  \n    \n      \n      Sex\n      Embarked\n      Age\n      SibSp\n      Parch\n      LogFare\n      Pclass\n    \n  \n  \n    \n      880\n      0\n      2\n      25.0\n      0\n      1\n      3.295837\n      2\n    \n    \n      425\n      1\n      2\n      24.0\n      0\n      0\n      2.110213\n      3\n    \n    \n      101\n      1\n      2\n      24.0\n      0\n      0\n      2.185579\n      3\n    \n    \n      199\n      0\n      2\n      24.0\n      0\n      0\n      2.639057\n      2\n    \n    \n      424\n      1\n      2\n      18.0\n      1\n      1\n      3.054591\n      3\n    \n  \n\n\n\n\nHere’s the predictions for our extremely simple model, where female is coded as 0:\n\n# set predictions for survival for validation set as Sex = female\npreds = val_xs.Sex==0\n\nWe’ll use mean absolute error to measure how good this model is:\n\n# import required package for our metric and calculate\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n\n0.21524663677130046\n\n\n\n\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work – here’s an example of how we could look at LogFare using a boxenplot and kernel density estimate(KDE) plot:\n\n# create subset of data to include only logfare\ndf_fare = trn_df[trn_df.LogFare>0]\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\n\n# create a boxenplot of logfare v survived \nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\n\n# create a kernel density estimate plot\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n\n\n\n\nThe boxenplot (above left) shows quantiles of LogFare for each group - didn’t survive Survived==0, and did survive, Survived==1. It shows that the average LogFare for passengers that didn’t survive is around 2.5, and for those that did it’s around 3.2. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\nLet’s create a simple model based on this observation:\n\n# set prediction for survival for validation set as > 2.7 logfare\npreds = val_xs.LogFare>2.7\n\n…and test it out:\n\nmean_absolute_error(val_y, preds) # binary split based on sex 0.21524663677130046\n\n0.336322869955157\n\n\nThis is quite a bit less accurate than our model that used Sex as the single binary split.\nIdeally, we’d like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We’ll create a score function to do this. Instead of returning the mean absolute error, we’ll calculate a measure of impurity – that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it’s higher, then it means the rows are more different to each other. We’ll then multiply this by the number of rows, since a bigger group has more impact than a smaller group:\n\n# create a function to calculate IMPURITY score\ndef _side_score(side, y):\n    tot = side.sum()\n    if tot<=1: return 0\n    return y[side].std()*tot\n\nNow we’ve got that written, we can calculate the score for a split by adding up the scores for the “left hand side” (lhs) and “right hand side” (rhs):\n\n# create a fucntion to calculate score for a split\ndef score(col, y, split):\n    lhs = col<=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)\n\nFor instance, here’s the impurity score for the split on Sex:\n\n# apply our score function to a split based on Sex\nscore(trn_xs[\"Sex\"], trn_y, 0.5)\n\n0.40787530982063946\n\n\nand for LogFare:\n\n# apply our score function to a split based on LogFare\nscore(trn_xs[\"LogFare\"], trn_y, 2.7) # score based on split by sex 0.40787530982063946\n\n0.47180873952099694\n\n\nA higher score means the values within the split are more different to eacch other i.e. impure, so as we’d expect from our earlier tests, Sex appears to be a better split as it has a lower impurity score. To make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle if you click “Copy and Edit” in the top right to open the notebook editor):\n\n# create interactve tool to play around with splits\nfrom ipywidgets import interact\n\n# create function that shows score for chosen splits\ndef iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\n\n# set variables (nm) to play around with as our continuous variables\n# set initial split point\ninteract(nm=conts, split=15.5)(iscore);\n\n\n\n\nTry selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\nWe can do the same thing for the categorical variables:\n\n# set variables (nm) to play around with as our cotegorical variables\n# set initial split point\ninteract(nm=cats, split=2)(iscore);\n\n\n\n\nThat works well enough, but it’s rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for age we’d first need to make a list of all the possible split points (i.e all the unique values of that field) :\n\n# obtain all unique age values\nnm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq\n\narray([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])\n\n\n…and find which index of those values is where score() is the lowest:\n\nscores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)]) # use list comprehension rather than for loop\nunq[scores.argmin()] # grab lowest score \n\n6.0\n\n\nBased on this, it looks like, for instance, that for the Age column, 6 is the optimal cutoff according to our training set. We can write a little function that implements this idea:\n\n# create function that pulls this idea together\ndef min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx] # return value that gives lowest score, and that score\n\n\n# find age value that gives lowest impurity score\nmin_col(trn_df, \"Age\")\n\n(6.0, 0.478316717508991)\n\n\nLet’s try all the columns:\n\n# combine categorical and continuous as previously defined\ncols = cats+conts\n\n# return col name: and then result from function i.e (split value that gives lowest score, and that score)\n{o:min_col(trn_df, o) for o in cols}\n\n{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}\n\n\nAccording to this, Sex<=0 is the best split we can use.\nWe’ve just re-invented the OneR classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it’s so simple and surprisingly effective, it makes for a great baseline – that is, a starting point that you can use to compare your more sophisticated models to.\nWe found earlier that out OneR rule had an error of around 0.215, so we’ll keep that in mind as we try out more sophisticated approaches.\n\n\n\n\nHow can we improve our OneR classifier, which predicts survival based only on Sex?\nHow about we take each of our two groups, female and male, and create one more binary split for each of them. That is: find the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section’s steps, once for males, and once for females.\nFirst, we’ll remove Sex from the list of possible splits (since we’ve already used it, and there’s only one possible split for that binary column), and create our two groups:\n\n# remove Sex column from our previous defined cols\ncols.remove(\"Sex\")\n\n# create ouur 2 groups males and females\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]\n\nNow let’s find the single best binary split for males:\n\n# return col name: and then result from function i.e (split value that gives lowest score, and that score)\n{o:min_col(males, o) for o in cols}\n\n{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}\n\n\n…and for females:\n\n# return col name: and then result from function i.e (split value that gives lowest score, and that score)\n{o:min_col(females, o) for o in cols}\n\n{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}\n\n\nWe can see that the next best binary split for males is Age<=6 and for females is Pclass<=2.\nBy adding these rules, we have created a decision tree, where our model will first check whether Sex is female or male, and depending on the result will then check either the above Age or Pclass rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we’ve now created.\nRather than writing that code manually, we can use DecisionTreeClassifier, from sklearn, which does exactly that for us:\n\n# import decision tree classifier and graphical \nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\n\n\n# fit a decision tree to our training data\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);\n\n\n# create a function that draws decision tree \ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\n\n# draw decision teee based on \ndraw_tree(m, trn_xs, size=10)\n\n\n\n\n\nThe first split looks at Sex\n\n\nless than or equal to 0.5 True effectively means 0 i.e female (229) which sets us off down the LEFT hand side of the tree\nless than or equal to 0.5 False effectively means 1 i.e. male (439) which sets us off down the RIGHT hand side of the tree\n\n\nThe second split\n\n\nfor females is based on below (120) and above (109) Pclass 2; and\nfor males is based on below (21) and above (418) age 6\n\nWe can see that our training set of 668 rows (415 survivors, 253 not survived) has been split exactly as we did!\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (“samples”) match that set of rules, and shows how many perish or survive (“values”). There’s also something called gini. That’s another measure of impurity, and it’s very similar to the score() function we created earlier.\nGini is defined as follows:\n\n# derive the gini calculation\ndef gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2 # probability that if you pick two rows from a group that you get same survived result each time\n\nWhat this calculates is the probability that, if you pick two rows from a group, you’ll get the same Survived result each time. If the group is all the same, the probability is 0.0, and 1.0 if they’re all different.\n\n# apply our function to split by sex\ngini(df.Sex=='female'), gini(df.Sex=='male')\n\n(0.3828350034484158, 0.3064437162277842)\n\n\nLet’s see how this model compares to our OneR version:\n\nmean_absolute_error(val_y, m.predict(val_xs)) # oneR score 0.21524663677130046\n\n0.2242152466367713\n\n\nIt’s actually marginally worse. Since this is such a small dataset (we’ve only got around 200 rows in our validation set) this small difference isn’t really meaningful. Perhaps we’ll see better results if we create a bigger tree:\n\nm = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=12)\n\n\n\n\nLet’s check how many leaf nodes and data points we have:\n\nm.get_n_leaves(), len(trn_xs)\n\n(11, 668)\n\n\n\n\n\nSo we have 11 leaf nodes, and 668 data points. This seems reasonable, no suggestion of overfitting.\n\nHere’s some intuition for an overfitting decision tree with more leaf nodes than data items. Consider the game Twenty Questions. In that game, the chooser secretly imagines an object (like, “our television set”), and the guesser gets to pose 20 yes or no questions to try to guess what the object is (like “Is it bigger than a breadbox?”). The guesser is not trying to predict a numerical value, but just to identify a particular object out of the set of all imaginable objects. When your decision tree has more leaves than there are possible objects in your domain, then it is essentially a well-trained guesser. It has learned the sequence of questions needed to identify a particular data item in the training set, and it is “predicting” only by describing that item’s value. This is a way of memorizing the training set—i.e., of overfitting.\n\n\nmean_absolute_error(val_y, m.predict(val_xs)) # oneR score 0.21524663677130046\n\n0.18385650224215247\n\n\nIt looks like this is an improvement, although again it’s a bit hard to tell with small datasets like this. Let’s try submitting it to Kaggle:\n\n# create a Kaggle submission csv file\ntst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(m.predict(tst_xs), 'tree')\n\nWhen I submitted this I got a score of 0.76555, which isn’t as good as our linear models or most of our neural nets, but it’s pretty close to those results.\nHopefully you can now see why we didn’t really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here’s how the first few items of Embarked are labeled:\n\ndf.Embarked.head()\n\n0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']\n\n\n…resulting in these integer codes:\n\ndf.Embarked.cat.codes.head()\n\n0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8\n\n\nSo let’s say we wanted to split into “C” in one group, vs “Q” or “S” in the other group. Then we just have to split on codes <=0 (since C is mapped to category 0). Note that if we wanted to split into “Q” in one group, we’d need to use two binary splits, first to separate “C” from “Q” and “S”, and then a second split to separate “Q” from “S”. For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\nAs a rough guide, consider using dummy variables for <4 levels, and numeric codes for >=4 levels.\nBuilding a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalizes (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees).\nSo how do we get the best of both worlds?\n\n\n\nIn 1994 Berkeley professor Leo Breiman, one year after his retirement, published a small technical report called “Bagging Predictors”, which turned out to be one of the most influential ideas in modern machine learning. The report began:\n\nBagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions… The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.\n\nHere is the procedure that Breiman is proposing:\n\nRandomly choose a subset of the rows of your data (i.e., “bootstrap replicates of your learning set”).\nTrain a model using this subset.\nSave that model, and then return to step 1 a few times.\nThis will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model’s predictions.\n\nThis procedure is known as “bagging.” It is based on a deep and important insight: although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors, therefore, is: zero! So if we take the average of all of the models’ predictions, then we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. This is an extraordinary result—it means that we can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of the data, and averaging its predictions.\nIn 2001 Leo Breiman went on to demonstrate that this approach to building models, when applied to decision tree building algorithms, was particularly powerful. He went even further than just randomly choosing rows for each model’s training, but also randomly selected from a subset of columns when choosing each split in each decision tree. He called this method the random forest. Today it is, perhaps, the most widely used and practically important machine learning method.\nIn essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. Bagging is a particular approach to “ensembling,” or combining the results of multiple models together. To see how it works in practice, let’s get started on creating our own random forest!\nOne of the most important properties of random forests is that they aren’t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train—the more trees you have, the more accurate the model will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4 both tend to work well, although sklearn’s defaults work well too.\nThe sklearn docs show an example of the effects of different max_features choices, with increasing numbers of trees. In the plot, the blue plot line uses the fewest features and the green line uses the most (it uses all the features). As you can see below, the models with the lowest error result from using a subset of features but with a larger number of trees.\n\n\n\nsklearn_features.png\n\n\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here’s how we can create a tree on a random subset of the data:\n\n# create a function that generates a bunch of uncorrelated decision trees\ndef get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier\n    (min_samples_leaf=5)\n    .fit(trn_xs.iloc[idxs], \n         trn_y.iloc[idxs])\n\nNow we can create as many trees as we want:\n\ntrees = [get_tree() for t in range(100)]\n\nOur prediction will then be the average of these trees’ predictions:\n\nall_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\n\nmean_absolute_error(val_y, avg_probs)\n\n0.2272645739910314\n\n\nThis is nearly identical to what sklearn’s RandomForestClassifier does. The main extra piece in a “real” random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here’s how we repeat the above process with a random forest:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))\n\n0.18834080717488788\n\n\nWe can submit that to Kaggle too:\n\n# create a random forest Kaggle submission csv file\nsubm(rf.predict(tst_xs), 'rf')\n\nThis actually scored slightly worse 0.76315 than the original decision tree classifier.\nOne particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using feature_importances_:\n\npd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');\n\n\n\n\nWe can see that Sex is by far the most important predictor, with LogFare a distant second, and Age and Pclass behind that. In datasets with many columns, it is recommended to create a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn’t really need to take the log() of Fare, since random forests only care about order, and log() doesn’t change the order – we only did it to make our graphs earlier easier to read).\nThe way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1.\n\n\n\nSo what can we take away from all this?\nI think the first thing I’d note from this is that, clearly, more complex models aren’t always better. Our OneR model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn’t an improvement on the single decision tree at all.\nSo we should always be careful to benchmark simple models, as see if they’re good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there’s no need to guess – it’s so easy to try a few different models, there’s no reason not to give the simpler ones a go too!\nAnother thing I think we can take away is that random forests aren’t actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren’t sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html",
    "href": "posts/Excelosaurus/Excelosaurus.html",
    "title": "Excelosaurus",
    "section": "",
    "text": "This is my follow up to Lesson 3: Practical Deep Learning for Coders 2022 in which Jeremy built a linear regression model, and neural net from scratch (from the Titanic dataset) using the ‘dinosaur’ named ‘Excelosaurus’!"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#data-source",
    "href": "posts/Excelosaurus/Excelosaurus.html#data-source",
    "title": "Excelosaurus",
    "section": "Data source",
    "text": "Data source\nI managed to ‘reptilicate’ Jeremy’s example by creating my own dataset in Excel from my son’s favourite dino book, which is written in Polish. He is bilingual so thankfully he was able to help me with the translation :)"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#independent-variables-features",
    "href": "posts/Excelosaurus/Excelosaurus.html#independent-variables-features",
    "title": "Excelosaurus",
    "section": "Independent variables (features)",
    "text": "Independent variables (features)\nThe book provided some very basic information about each dinosaur:\n- Name \n- Period (Cretaceous, Jurassic, Triassic)\n- Discovered (South America, North America, Europe, Asia etc. - 8 unique locations)\n- Min_Len (metres)\n- Max_Len (metres)\n- Min_Height (metres)\n- Max_Height (metres)\n- Min_Weight (000 kg)\n- Max_Weight (000 kg)\nThese are our independent variables (or features).\n\nCategorical features\nOur model needs data in numerical format however as we can see from above, we have values for Period and Discovered which are clearly not numerical. How do we represent these in numerical format?\nWell, the trick is to create a dummy variable which creates n-1 columns where n is the number of categories:\nfor our Period variable ( n(3) - 1 ) = 2 columns created \n\nfor our Discovered variable ( n(8) - 1) = 7 columns created\nWe then populate these columns with a numerical value of 1 to indicate that our dino falls into that category, or 0 to signal that it does not.\n\n\nFeature engineering\nDue to the nature and limited number of features provided by the book I performed some very basic feature engineering to try to help our model with its predictions.\n\nCreating new features\nI created some new features from the existing info:\n- Ave_Len\n- Ave_Height\n- Ave_Weight\n- Ave_Weight_Length\n\n\nNormalization\nAnother thing I noticed was that the values for the dino dimensions are very large in comparison to our newly created dummy variables, and risk dominating our model, relegating the categorical features, which may actually hold some predictive insight! To counter this I used a simple normalization technique to squeeze the values for the dino dimensions between 0 and 1, by dividing each of the original values by the maximum value for that column. This results in a maximum value of 1 for the largest original value (anything divided by itself = 1) with the other values spread out proporionately betweeen 0 and 1."
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#dependent-target-variables-labels",
    "href": "posts/Excelosaurus/Excelosaurus.html#dependent-target-variables-labels",
    "title": "Excelosaurus",
    "section": "Dependent | Target variables (labels)",
    "text": "Dependent | Target variables (labels)\nThe book also classified the dinos as either:\n- roslinozernosc (veggie) ; or \n- miesozernosc (meat-eater)\nThese are our dependent or target variables (or labels).\n\n\n\nFeatures.PNG"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#linear-model",
    "href": "posts/Excelosaurus/Excelosaurus.html#linear-model",
    "title": "Excelosaurus",
    "section": "Linear model",
    "text": "Linear model\nOur task here can be represented on a basic level by a linear model\ny = wx + b\nwhere we try to map the independent variables x_1, x_2, …, x_n (our dino features) to our target variable y (1 = meat eater, 0 = veggie).\nWe already have our values for x and y, and because our dummy variables already cover the entire dataset e.g. there’s a column for “Cretaceous”, “Jurassic”, and “Triassic”, and every dino in the dataset was around during exactly one of these; therefore, we don’t need a separate intercep(or bias) term b to cover rows that aren’t otherwise part of a column.\nThat just leaves us to compute the values for w - the weights applied to each of our features!\n\nWeights | Parameters\nThe first step is to initilaise random weights | parameter for each of our chosen input features. To do this in excel:\n= RAND()-0.5\nSubtracting -0.5 is an arbitrary technique, but it is useful as it tightens up the random values around either side of zero.\n\n\n\nParameters.PNG\n\n\nNow that we have our weightings, we can populate our Prediction column, which takes our feature values and multiplies them by our randomly generated weightings.\nWe can use the SUMPRODUCT function within excel to performs this quickly and efficiently across all rows of our dataset in one fell swoop!\n=SUMPRODUCT(Table2[@[Ave_Weight_Length]:[Ones]],$AD\\(10:\\)AQ$10)\n\n\nCalculating our loss\nIt is important from the outset to specify which loss function is to be used. There are a number of different loss functions and it is often unclear which should be used for the particular task at hand. This article provides a useful guide.\n\nMean squared error\n\n\n\nMSE.jpg\n\n\nI chose mean squared error (as used in Jeremy’s Titanic dataset walkthough) as my loss function. Our task, predicting whether a dino is a meat eater [1] or veggie [0] is a binary classification problem, similar to the Kaggle Titanic competition, where the task was to predict survived or not-survived.\nFor each dino we simply square the difference between our prediction value (BB10) and the target value (Table2(@Target). The squared error for one dino prediction is calculated in excel as follows:\n=(BB10-Table2[@Target])^2\nOur total loss or MSE is then simply the average across all n (60) dinos:\n=AVERAGE(BD10:BD69)\n\n\nCross-entropy loss\nHaving done some digging on loss functions, there is the suggestion that Cross-entropy is the default loss function to use for binary classification problems. The mathematical formula is:\n\n\n\ncross_entropy.png\n\n\nIt turns out Jeremy posted a very useful video a few years back on how to calculate this using Excel and there is a useful summary provided here by Aman Arora."
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#a-neural-network",
    "href": "posts/Excelosaurus/Excelosaurus.html#a-neural-network",
    "title": "Excelosaurus",
    "section": "A neural network",
    "text": "A neural network\nWe can add another linear layer to our exisitng linear model, although this in itself would not constitute a neural network. What we can do however, is add rectified linear activation functions or ReLU for short.\nIn simple terms this function clips the values produced by our linear models so that they all lie between 0 and 1. This can easily be achieved in Excel using an if function:\nif AS<0,0,AS\nOur predictions are then updated to include the outputs from both linear models and the two ReLUs, and our loss also updated accordingly with reference to the original unchanged target values - meat-eater [1] veggie [0]."
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#using-matrix-multiplication",
    "href": "posts/Excelosaurus/Excelosaurus.html#using-matrix-multiplication",
    "title": "Excelosaurus",
    "section": "Using Matrix Multiplication",
    "text": "Using Matrix Multiplication\nMatrix Multiplication is one of the most fundamental computational tasks and, one of the core mathematical operations in today’s neural networks. Not the sort of thing you might expect to be found in Excel perhaps? Well it turns out that there is function called MMULT built into excel.\nAll we need to do is transpose our randomized weights and our features, so that the input features are now the rows, and the weights values are incuded in column format.\n\n\n\nMatrix Mult.PNG\n\n\nAn example from one of the cells from our linear model prediction is included below:\n=MMULT(Table2456[[Ave_Weight_Length]:[Const]],AE20:AF33)\nThe first part of the formula:\nTable2456[[Ave_Weight_Length]:[Const]]\nis the range over which we are computing matrix multiplicaiton - i.e our features data\nThe second part of the formula:\nAE20:AF33\nis our parameters/weightings\nFinally, it’s time to now update our initial randomized parameters using Gradient Descent, which we can do in excel!"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#gradient-descent",
    "href": "posts/Excelosaurus/Excelosaurus.html#gradient-descent",
    "title": "Excelosaurus",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nGradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\nIt might be that you are familiar with Solver function within Excel, as part of an optimization strategy? Well this is basically Gradient Descent to give it another name.\nWe can use this Solver function to optimise our loss function.\nThe steps required within excel are as follows:\n\nclick on the Data tab then select the Solver function (as highlighted below) Note, you may have to add this function to your dashboard. See this guide for details on how to do this.\nSet Objective: enter the cell reference for your total loss\nTo: select Min (we are seeking to minimise our total loss)\nBy Changing Variable cells: select the range of our parameters (we want to update our original randomized values)\nMake Unconstrianed Variables Non-Negative: ensure this is UNticked\nSelect a Solving Method: select GRG Nonlinear from the dropdown\n\nFinally, hit Solve and wait a few moments. You should find that your parameters have been updated to optimal values, and your total loss has reduced.\n\n\n\nSolver_1.PNG\n\n\n\n\n\nSolver_2.PNG"
  },
  {
    "objectID": "posts/Excelosaurus/Excelosaurus.html#final-thoughts",
    "href": "posts/Excelosaurus/Excelosaurus.html#final-thoughts",
    "title": "Excelosaurus",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe main takeaway from this project, apart from hanging out with my son, and finding out about cool dinos, is that I have successfully managed to:\n\ncreate my own dataset from scratch\nclean the data and perform some feature engineering\ncarry out Matrix Multiplication within Excel using the MMULT function\n\n\n\ncarry out Gradient Descent within excel using the Solver function\n\nThis exercise really consolidated my understanding of the architecture of linear and neural models, and hopefully, some of the techniques included in this NoteBook prove to be helpful to other data science newcomers like me. Looking forward to Lesson 4!"
  },
  {
    "objectID": "posts/Introducing/Introducing.html",
    "href": "posts/Introducing/Introducing.html",
    "title": "Introducing….",
    "section": "",
    "text": "Christian Wittmann ! We’ve never actually met but I think I might owe you a drink! If you are ever in Krakow look me up :)\nAs a data science noob (I’m working my way through the fast.ai course under the guidance of Jeremy Howard, starting a blog seemed like a great way to integrate into the community and also to document the learning journey ahead.\nI did look at fastpages, but a new contender Quarto caught my eye.\nI follow the fast.ai course using Jupyter Notebooks installed on my Linux box, and the go to Quarto tutorial is perfectly fine for getting nicely rendered Jupyter Notebooks, but how to get these into a blog?\nAgain, the documentation provided by Quarto is fine, but you won’t see Jupyter in the Quick Start setup.\nAfter multiple, and ultimately abortive attempts to deploy via GitHub (I created and deleted SO many repositories) I did manage to get something up and running using Netlify, but that just adds another step in the chain.\nIf only there was a way to work on Notebooks within Jupyter and blog my findings, all under the same roof! Sounds too good to be true.\nWell the good news is that it IS possible thanks to this post which has made this blog post (my first ever) possible!\nThe key takeaway for me from reading Christian’s blog was these two steps, which although referred to in the Quarto docs are not very clear.\nTwo final activities are needed to finalize the setup of your repo:\n\nCreate a new branch called gh-pages. To do this, go to your branches (for me that is https://github.com/Stephen137/Stephen137.github.io/branches), and create the new branch by clicking the “New branch”- button in the top right.\nSet the new branch as the branch for GitHub Pages. In your repo, navigate to Settings -> Pages. (In my repo that takes me to https://github.com/Stephen137/Stephen137.github.io/settings/pages.) Change main to gh-pages.\n\nI have run into soooo many obstacles over the first few weeks of my transition into data science, but I have also learned so much in a short space of time. For any fellow newcomers, my advice is to be tenacious. Do your research, but when (not if) you hit a wall, don’t be afraid to reach out. The fast.ai forums is a great place to hang out.The community is really so collaborative.\nNow that I have my own blogging platform, I’ll try to retrace my steps and post some useful insights.\nHave a great weekend all :)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html",
    "title": "Excelosaurus meets Python",
    "section": "",
    "text": "This is my follow up to Lesson 5: Practical Deep Learning for Coders 2022 in which Jeremy builds a linear regresson model and neural net from scratch using Python."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#aim-of-the-project",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#aim-of-the-project",
    "title": "Excelosaurus meets Python",
    "section": "Aim of the project",
    "text": "Aim of the project\nThe aim of this project is :\n\nto build a linear and neural network model from scratch within Python that predicts whether a dino is a meat-eater or a vegetarian"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#data-source",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#data-source",
    "title": "Excelosaurus meets Python",
    "section": "Data source",
    "text": "Data source\nI created my own dataset in Excel from my son’s favourite dino book."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#set-path",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#set-path",
    "title": "Excelosaurus meets Python",
    "section": "Set path",
    "text": "Set path\n\nfrom pathlib import Path"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#install-required-packages",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#install-required-packages",
    "title": "Excelosaurus meets Python",
    "section": "Install required packages",
    "text": "Install required packages\nWe’ll be using NumPy and Pytorch for array calculations in this notebook, and pandas for working with tabular data, so we’ll import them and set them to display using a bit more space than they default to.\n\nimport os\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#loading-the-data",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#loading-the-data",
    "title": "Excelosaurus meets Python",
    "section": "Loading the data",
    "text": "Loading the data\nThe starting point for this project is the csv file that I created as part of my Lesson 3 project to build a linear model and neural network from scratch within Excel. See my earlier post titled Excelosaurus.\n\n\n\nFeatures.PNG\n\n\nThe data is in the form of a table, as a Comma Separated Values (CSV) file. We can open it using the pandas library, which will create a DataFrame.\n\n# Set the path and load in the spreadsheet\nfrom pathlib import Path\ndinos = pd.read_csv('Data/Dinos.csv')"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#exploratory-data-analysis",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#exploratory-data-analysis",
    "title": "Excelosaurus meets Python",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nWe can automate some of the exploratory data analysis by writing a function:\n\ndef initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))\n\n\ninitial_eda(dinos)\n\nDimensions : 60 rows, 10 columns\nTotal NA Values : 0 \n                           Column Name  Data Type      #Distinct  NA Values\n                                  Name     object           60          0\n                                Period     object            3          0\n                            Discovered     object            8          0\n                               Min_Len    float64           21          0\n                               Max_Len    float64           22          0\n                            Min_Height    float64           15          0\n                            Max_Height    float64           15          0\n                            Min_Weight    float64           29          0\n                            Max_weight    float64           27          0\n                                  Meat      int64            2          0\n\n\nAs we can see there are no missing values, amd the dataset includes:\n\n60 rows, each representing a unique dinosaur, and\n10 columns, representing 9 features and the target variable.\n\nNote that the Period and Discovered columns are of data type object. We will deal with these later in the section on Categorical Features. The other columnns are numeric (float64 or int64).\nStraight off the bat, we can safely drop the *Name” column as the dino name is clearly not an indicator of whether it is a meat eater or a plant eater:\n\ndinos = dinos.drop(['Name'], axis=1);\n\n\ndinos['Meat'].value_counts()\n\n1    36\n0    24\nName: Meat, dtype: int64\n\n\n\nThere are two possible prediction outputs so this is a binary classification problem. It’s a small dataset but reasonably balanced; 36 meat-eating dinos [indexed 1] and 24 veggies [indexed 0]."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#numeric-features",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#numeric-features",
    "title": "Excelosaurus meets Python",
    "section": "Numeric features",
    "text": "Numeric features\nHere’s how we get a quick summary of all the numeric columns in the dataset:\n\nimport numpy as np\n\ndinos.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      Min_Len\n      Max_Len\n      Min_Height\n      Max_Height\n      Min_Weight\n      Max_weight\n      Meat\n    \n  \n  \n    \n      count\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n      60.000000\n    \n    \n      mean\n      8.480000\n      9.015000\n      2.788333\n      3.075000\n      6.666750\n      7.333992\n      0.600000\n    \n    \n      std\n      7.152356\n      7.757661\n      2.548518\n      2.754326\n      13.310511\n      14.709908\n      0.494032\n    \n    \n      min\n      0.300000\n      0.400000\n      0.000000\n      0.000000\n      0.000500\n      0.000500\n      0.000000\n    \n    \n      25%\n      3.750000\n      4.500000\n      1.000000\n      1.000000\n      0.175000\n      0.175000\n      0.000000\n    \n    \n      50%\n      6.500000\n      7.000000\n      2.000000\n      2.750000\n      2.500000\n      3.000000\n      1.000000\n    \n    \n      75%\n      10.250000\n      11.000000\n      4.000000\n      4.000000\n      7.000000\n      8.000000\n      1.000000\n    \n    \n      max\n      35.000000\n      40.000000\n      14.000000\n      14.000000\n      70.000000\n      80.000000\n      1.000000\n    \n  \n\n\n\n\n\nWe need to watch out for dominant features as these can cause problems for our model, because once that column is multiplied by a coefficient later, the few rows with really big values will strongly influence the result.\nMin_Weight and Max_Weight contain mainly values of around 0.0005 to 8, but there are a few much bigger ones.\n\nLet’s illustrate this visually using a histogram :"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#histogram-of-max_weight",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#histogram-of-max_weight",
    "title": "Excelosaurus meets Python",
    "section": "Histogram of Max_weight",
    "text": "Histogram of Max_weight\n\ndinos ['Max_weight'].hist();"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#feature-engineering",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#feature-engineering",
    "title": "Excelosaurus meets Python",
    "section": "Feature engineering",
    "text": "Feature engineering\nAs you can see the distribution is long-tailed which linear models don’t like. To fix this, the most common approach is to take the logarithm, which squishes the big numbers and makes the distribution more reasonable:\n\n# Note that the log of zero will return NaN (not a number) so the workaround is to include +1\ndinos['Max_weight'] = np.log(dinos['Max_weight']+1)\n\nThe histogram now shows a more even distribution of values without the long tail:\n\ndinos ['Max_weight'].hist()\n\n<AxesSubplot: >\n\n\n\n\n\nLet’s do the same for Min_Weight :\n\ndinos['Min_Weight'] = np.log(dinos['Min_Weight']+1)\ndinos ['Min_Weight'].hist()\n\n<AxesSubplot: >"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#categorical-features",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#categorical-features",
    "title": "Excelosaurus meets Python",
    "section": "Categorical features",
    "text": "Categorical features\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\ndinos.describe(include=[object])\n\n\n\n\n\n  \n    \n      \n      Period\n      Discovered\n    \n  \n  \n    \n      count\n      60\n      60\n    \n    \n      unique\n      3\n      8\n    \n    \n      top\n      Cretaceous\n      North America\n    \n    \n      freq\n      41\n      36\n    \n  \n\n\n\n\n\ndinos.tail(5)\n\n\n\n\n\n  \n    \n      \n      Period\n      Discovered\n      Min_Len\n      Max_Len\n      Min_Height\n      Max_Height\n      Min_Weight\n      Max_weight\n      Meat\n    \n  \n  \n    \n      55\n      Cretaceous\n      Africa\n      12.0\n      12.0\n      1.5\n      1.5\n      2.197225\n      2.197225\n      1\n    \n    \n      56\n      Cretaceous\n      North America\n      11.0\n      11.0\n      1.5\n      1.5\n      2.302585\n      2.302585\n      1\n    \n    \n      57\n      Jurassic\n      Europe\n      0.3\n      0.4\n      0.2\n      0.2\n      0.001000\n      0.001000\n      1\n    \n    \n      58\n      Triassic\n      Europe\n      5.0\n      6.0\n      2.0\n      3.0\n      0.470004\n      0.470004\n      1\n    \n    \n      59\n      Triassic\n      Europe\n      4.0\n      5.0\n      3.0\n      3.0\n      2.302585\n      2.302585\n      0\n    \n  \n\n\n\n\nAs you can see from the above our data includes the following categorical data -\n\nthe period when the dinos lived (Cretaceous, Jurassic, Triassic)\nthe place of discovery (Africa, North America, Europe etc.)\n\nClearly we can’t multiply strings like Jurassic or Africa by coefficients, so we need to replace those with numbers.\nWe do that by creating new columns containing dummy variables. A dummy variable is a column that contains a 1 where a particular column contains a particular value, or a 0 otherwise. For instance, we could create a dummy variable for Period = ‘Cretaceous’, which would be a new column containing 1 for rows where Period is ‘Cretaceous’, and 0 for rows where it isn’t, and apply the same logic for the ‘Discovered’ values.\nPandas can create these automatically using get_dummies, which also remove the original columns. We’ll create dummy variables for Period and Discovered\n\ndinos = pd.get_dummies(dinos, columns=[\"Period\",\"Discovered\"])\ndinos.columns\n\nIndex(['Min_Len', 'Max_Len', 'Min_Height', 'Max_Height', 'Min_Weight', 'Max_weight', 'Meat', 'Period_Cretaceous', 'Period_Jurassic',\n       'Period_Triassic', 'Discovered_Africa', 'Discovered_Antartica', 'Discovered_Asia', 'Discovered_Australia', 'Discovered_Europe',\n       'Discovered_North America', 'Discovered_South America', 'Discovered_UK'],\n      dtype='object')\n\n\nWe can see that columns have been added to the end – one for each of the possible values for Period and Discovered, and that the original Period and Discovered columns have been removed.\nHere’s what the first few rows of those newly added columns look like:\n\nadded_cols = ['Period_Cretaceous', 'Period_Jurassic',\n       'Period_Triassic', 'Discovered_Africa', 'Discovered_Antartica', 'Discovered_Asia', 'Discovered_Australia', 'Discovered_Europe',\n       'Discovered_North America', 'Discovered_South America', 'Discovered_UK']\ndinos[added_cols].head()\n\n\n\n\n\n  \n    \n      \n      Period_Cretaceous\n      Period_Jurassic\n      Period_Triassic\n      Discovered_Africa\n      Discovered_Antartica\n      Discovered_Asia\n      Discovered_Australia\n      Discovered_Europe\n      Discovered_North America\n      Discovered_South America\n      Discovered_UK\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      3\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      4\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n  \n\n\n\n\nLet’s review our dataset now following the changes prior to building a model:\n\ninitial_eda(dinos)\n\nDimensions : 60 rows, 18 columns\nTotal NA Values : 0 \n                           Column Name  Data Type      #Distinct  NA Values\n                               Min_Len    float64           21          0\n                               Max_Len    float64           22          0\n                            Min_Height    float64           15          0\n                            Max_Height    float64           15          0\n                            Min_Weight    float64           29          0\n                            Max_weight    float64           27          0\n                                  Meat      int64            2          0\n                     Period_Cretaceous      uint8            2          0\n                       Period_Jurassic      uint8            2          0\n                       Period_Triassic      uint8            2          0\n                     Discovered_Africa      uint8            2          0\n                  Discovered_Antartica      uint8            2          0\n                       Discovered_Asia      uint8            2          0\n                  Discovered_Australia      uint8            2          0\n                     Discovered_Europe      uint8            2          0\n              Discovered_North America      uint8            2          0\n              Discovered_South America      uint8            2          0\n                         Discovered_UK      uint8            2          0"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#create-our-independentpredictors-and-dependenttarget-variables",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#create-our-independentpredictors-and-dependenttarget-variables",
    "title": "Excelosaurus meets Python",
    "section": "Create our independent(predictors) and dependent(target) variables",
    "text": "Create our independent(predictors) and dependent(target) variables\n\nThey both need to be PyTorch tensors. Our dependent variable is Meat :\n\n\nfrom torch import tensor\n\nt_dep = tensor(dinos.Meat)\n\n\nOur independent variables are all the continuous variables of interest plus all the dummy variables we created earlier:\n\n\nindep_cols = [\"Min_Len\",\"Max_Len\",\"Min_Height\",\"Max_Height\",\"Min_Weight\",\"Max_weight\"] + added_cols\n\nt_indep = tensor(dinos[indep_cols].values, dtype=torch.float)\nt_indep\n\ntensor([[   35.0000,    40.0000,    11.0000,    11.0000,     4.2627,     4.3944,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     0.0000,     1.0000,     0.0000],\n        [   25.0000,    30.0000,    10.0000,    12.0000,     3.9318,     3.9318,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   30.0000,    30.0000,     5.0000,     6.0000,     2.4849,     2.5649,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   20.0000,    22.0000,     5.0000,     6.0000,     3.0445,     3.1781,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   25.0000,    25.0000,    14.0000,    14.0000,     3.9318,     3.9318,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [    7.0000,     7.0000,     2.5000,     2.5000,     1.3863,     1.3863,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   12.0000,    12.0000,     4.0000,     4.0000,     1.6094,     1.6094,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        ...,\n        [    5.0000,     5.0000,     2.0000,     2.0000,     0.4055,     0.4055,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   20.0000,    20.0000,     4.0000,     4.0000,     3.7136,     3.9318,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [   12.0000,    12.0000,     1.5000,     1.5000,     2.1972,     2.1972,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     0.0000,     0.0000,     0.0000],\n        [   11.0000,    11.0000,     1.5000,     1.5000,     2.3026,     2.3026,     1.0000,  ...,     0.0000,     0.0000,     0.0000,\n             0.0000,     1.0000,     0.0000,     0.0000],\n        [    0.3000,     0.4000,     0.2000,     0.2000,     0.0010,     0.0010,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             1.0000,     0.0000,     0.0000,     0.0000],\n        [    5.0000,     6.0000,     2.0000,     3.0000,     0.4700,     0.4700,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             1.0000,     0.0000,     0.0000,     0.0000],\n        [    4.0000,     5.0000,     3.0000,     3.0000,     2.3026,     2.3026,     0.0000,  ...,     0.0000,     0.0000,     0.0000,\n             1.0000,     0.0000,     0.0000,     0.0000]])\n\n\n\nHere’s the number of rows and columns we have for our independent variables:\n\n\nt_indep.shape\n\ntorch.Size([60, 17])\n\n\n\nSo 60 rows or examples, and 17 columns, or features."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#setting-up-a-linear-model",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#setting-up-a-linear-model",
    "title": "Excelosaurus meets Python",
    "section": "Setting up a linear model",
    "text": "Setting up a linear model\nNow that we’ve got a matrix of independent variables and a dependent variable vector, we can work on calculating our predictions and our loss. In this section, we’re going to manually do a single step of calculating predictions and loss for every row of our data.\nOur first model will be a simple linear model. We’ll need a coefficient for each column in t_indep. We’ll pick random numbers in the range (-0.5,0.5), and set our manual seed so that my explanations in the prose in this notebook will be consistent with what you see when you run it.\n\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff)-0.5\ncoeffs\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625,  0.1722,  0.2324,\n        -0.3575, -0.0010, -0.1833])\n\n\nOur predictions will be calculated by multiplying each row by the coefficients, and adding them up. One interesting point here is that we don’t need a separate constant term (also known as a “bias” or “intercept” term), or a column of all 1s to give the same effect has having a constant term. That’s because our dummy variables already cover the entire dataset – e.g. there’s a column for “Cretaceous”, “Jurassic”, and “Triassic”, and every dino in the dataset was around during exactly one of these; therefore, we don’t need a separate intercept term to cover rows that aren’t otherwise part of a column.\nHere’s what the multiplication looks like:\n\nt_indep*coeffs\n\ntensor([[   -16.2015,      5.5432,      2.6499,     -2.4877,     -1.1221,     -1.3830,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0010,     -0.0000],\n        [   -11.5725,      4.1574,      2.4090,     -2.7138,     -1.0350,     -1.2374,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [   -13.8870,      4.1574,      1.2045,     -1.3569,     -0.6541,     -0.8072,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -9.2580,      3.0487,      1.2045,     -1.3569,     -0.8015,     -1.0002,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [   -11.5725,      3.4645,      3.3726,     -3.1662,     -1.0350,     -1.2374,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -3.2403,      0.9701,      0.6023,     -0.5654,     -0.3649,     -0.4363,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -5.5548,      1.6629,      0.9636,     -0.9046,     -0.4237,     -0.5065,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        ...,\n        [    -2.3145,      0.6929,      0.4818,     -0.4523,     -0.1067,     -0.1276,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -9.2580,      2.7716,      0.9636,     -0.9046,     -0.9776,     -1.2374,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -5.5548,      1.6629,      0.3614,     -0.3392,     -0.5784,     -0.6915,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000],\n        [    -5.0919,      1.5244,      0.3614,     -0.3392,     -0.6062,     -0.7247,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.1389,      0.0554,      0.0482,     -0.0452,     -0.0003,     -0.0003,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -2.3145,      0.8315,      0.4818,     -0.6785,     -0.1237,     -0.1479,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -1.8516,      0.6929,      0.7227,     -0.6785,     -0.6062,     -0.7247,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000]])\n\n\nWe can see we’ve got a problem here. The sums of each row will be dominated by the first three columns, which represent average length, height, and weight, since these values are bigger on average than all the others.\nLet’s make all the columns contain numbers from -1 to 1, by dividing each column by its max():\n\nvals,indices = t_indep.max(dim=0)\nt_indep = t_indep / vals\n\nAs we see, that removes the problem of one column dominating all the others:\n\nt_indep*coeffs\n\ntensor([[    -0.4629,      0.1386,      0.1893,     -0.1777,     -0.2632,     -0.3147,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0010,     -0.0000],\n        [    -0.3306,      0.1039,      0.1721,     -0.1938,     -0.2428,     -0.2816,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.3968,      0.1039,      0.0860,     -0.0969,     -0.1535,     -0.1837,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.2645,      0.0762,      0.0860,     -0.0969,     -0.1880,     -0.2276,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.3306,      0.0866,      0.2409,     -0.2262,     -0.2428,     -0.2816,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.0926,      0.0243,      0.0430,     -0.0404,     -0.0856,     -0.0993,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.1587,      0.0416,      0.0688,     -0.0646,     -0.0994,     -0.1153,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        ...,\n        [    -0.0661,      0.0173,      0.0344,     -0.0323,     -0.0250,     -0.0290,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.2645,      0.0693,      0.0688,     -0.0646,     -0.2293,     -0.2816,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.1587,      0.0416,      0.0258,     -0.0242,     -0.1357,     -0.1574,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.0000,     -0.0000,     -0.0000],\n        [    -0.1455,      0.0381,      0.0258,     -0.0242,     -0.1422,     -0.1649,      0.4876,  ...,      0.0000,      0.0000,\n              0.0000,      0.0000,     -0.3575,     -0.0000,     -0.0000],\n        [    -0.0040,      0.0014,      0.0034,     -0.0032,     -0.0001,     -0.0001,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -0.0661,      0.0208,      0.0344,     -0.0485,     -0.0290,     -0.0337,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000],\n        [    -0.0529,      0.0173,      0.0516,     -0.0485,     -0.1422,     -0.1649,      0.0000,  ...,      0.0000,      0.0000,\n              0.0000,      0.2324,     -0.0000,     -0.0000,     -0.0000]])\n\n\nNote this line of code in particular:\nt_indep = t_indep / vals\nThat is dividing a [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics) by a [vector](https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics). The trick here is that we’re taking advantage of a technique in Numpy and PyTorch (and many other languages, going all the way back to APL) called broadcasting. In short, this acts as if there’s a separate copy of the vector for every row of the matrix, so it divides each row of the matrix by the vector. In practice, it doesn’t actually make any copies, and does the whole thing in a highly optimized way, taking full advantage of modern CPUs (or, indeed, GPUs, if we’re using them). Broadcasting is one of the most important techniques for making your code concise, maintainable, and fast, so it’s well worth studying and practicing.\nWe can now create predictions from our linear model, by adding up the rows of the product:\n\npreds = (t_indep*coeffs).sum(axis=1)\n\nLet’s take a look at the first few:\n\npreds[:10]\n\ntensor([-0.4040, -0.8168, -0.5108, -0.6587, -0.6236, -0.1205, -0.1975, -0.1085, -0.1350, -0.1344])\n\n\nOf course, these predictions aren’t going to be any use, since our coefficients are random – they’re just a starting point for our gradient descent process.\nTo do gradient descent, we need a loss function. Taking the average error of the rows (i.e. the absolute value of the difference between the prediction and the dependent) is generally a reasonable approach:\n\nloss = torch.abs(preds-t_dep).mean()\nloss\n\ntensor(0.6421)\n\n\n\nNow that we’ve tested out a way of calculating predictions, and loss, let’s pop them into functions to make life easier:\n\n\ndef calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1)\ndef calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean()"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#doing-a-gradient-descent-step",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#doing-a-gradient-descent-step",
    "title": "Excelosaurus meets Python",
    "section": "Doing a gradient descent step",
    "text": "Doing a gradient descent step\nIn this section, we’re going to do a single “epoch” of gradient descent manually. The only thing we’re going to automate is calculating gradients, because let’s face it that’s pretty tedious and entirely pointless to do by hand! To get PyTorch to calculate gradients, we’ll need to call requires_grad_() on our coeffs (if you’re not sure why, review the previous notebook, How does a neural net really work?, before continuing):\n\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625,  0.1722,  0.2324,\n        -0.3575, -0.0010, -0.1833], requires_grad=True)\n\n\nNow when we calculate our loss, PyTorch will keep track of all the steps, so we’ll be able to get the gradients afterwards:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.6421, grad_fn=<MeanBackward0>)\n\n\nUse backward() to ask PyTorch to calculate gradients now:\n\nloss.backward()\n\nLet’s see what they look like:\n\ncoeffs.grad\n\ntensor([-0.2290, -0.2129, -0.1825, -0.2030, -0.2670, -0.2669, -0.6167, -0.2000, -0.0500, -0.0667, -0.0167, -0.0833, -0.0167, -0.0500,\n        -0.5667, -0.0500, -0.0167])\n\n\nNote that each time we call backward, the gradients are actually added to whatever is in the .grad attribute. Let’s try running the above steps again:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\ntensor([-0.4579, -0.4258, -0.3650, -0.4060, -0.5340, -0.5337, -1.2333, -0.4000, -0.1000, -0.1333, -0.0333, -0.1667, -0.0333, -0.1000,\n        -1.1333, -0.1000, -0.0333])\n\n\nAs you see, our .grad values have doubled. That’s because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero.\nWe can now do one gradient descent step, and check that our loss decreases:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)\n    coeffs.grad.zero_()\n    print(calc_loss(coeffs, t_indep, t_dep))\n\ntensor(0.4833)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#training-the-linear-model",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#training-the-linear-model",
    "title": "Excelosaurus meets Python",
    "section": "Training the linear model",
    "text": "Training the linear model\nBefore we begin training our model, we’ll need to ensure that we hold out a validation set for calculating our metrics (for details on this, see “Getting started with NLP for absolute beginners”.\nlet’s use RandomSplitter to get indices that will split our data into training and validation sets:\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=137)(dinos)\n\nNow we can apply those indicies to our independent and dependent variables:\n\ntrn_indep,val_indep = t_indep[trn_split],t_indep[val_split]\ntrn_dep,val_dep = t_dep[trn_split],t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(48, 12)\n\n\nSo our 60 examples have been split 80:20 between training set and validation set\nWe’ll create functions for the three things we did manually above:\n\nupdating coeffs\ndoing one full gradient descent step, and\ninitilising coeffs to random numbers:\n\n\n# updating coeffs\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n\n# doing one full gradient descent step\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n\n# initilising coeffs to random numbers\ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\nWe can now use these functions to train our model:\n\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(137)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\nLet’s try it. Our loss will print at the end of every step, so we hope we’ll see it going down:\n\ncoeffs = train_model(7, lr=0.9)\n\n0.935; 0.722; 0.595; 0.535; 0.497; 0.463; 0.450; \n\n\nIt does. Let’s take a look at the coefficients for each column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()\n\n{'Min_Len': tensor(0.2610),\n 'Max_Len': tensor(0.1809),\n 'Min_Height': tensor(0.2024),\n 'Max_Height': tensor(0.3190),\n 'Min_Weight': tensor(-0.4248),\n 'Max_weight': tensor(-0.2794),\n 'Period_Cretaceous': tensor(0.5788),\n 'Period_Jurassic': tensor(0.4300),\n 'Period_Triassic': tensor(0.6414),\n 'Discovered_Africa': tensor(0.4464),\n 'Discovered_Antartica': tensor(0.3232),\n 'Discovered_Asia': tensor(0.2413),\n 'Discovered_Australia': tensor(-0.0671),\n 'Discovered_Europe': tensor(0.1147),\n 'Discovered_North America': tensor(0.2534),\n 'Discovered_South America': tensor(-0.2927),\n 'Discovered_UK': tensor(0.5903)}"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#measuring-accuracy",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#measuring-accuracy",
    "title": "Excelosaurus meets Python",
    "section": "Measuring accuracy",
    "text": "Measuring accuracy\nAn alternative metric to absolute error (which is our loss function) is accuracy – the proportion of rows where we correctly predict meat-eater. Let’s see how accurate we were on the validation set. First, calculate the predictions:\n\npreds = calc_preds(coeffs, val_indep)\n\nWe’ll assume that any dinosaur with a score of over 0.5 is predicted to be a meat-eater. So that means we’re correct for each row where preds>0.5 is the same as the dependent variable:\n\nresults = val_dep.bool()==(preds>0.5)\nresults[:10]\n\ntensor([ True, False, False, False, False,  True, False,  True,  True, False])\n\n\nLet’s see what our average accuracy is:\n\nresults.float().mean()\n\ntensor(0.4167)\n\n\nThat’s not a great start, worse than a 50:50 guess. We’ll create a function so we can calcuate the accuracy easy for other models we train.\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nacc(coeffs)\n\ntensor(0.4167)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#using-sigmoid",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#using-sigmoid",
    "title": "Excelosaurus meets Python",
    "section": "Using sigmoid",
    "text": "Using sigmoid\n\n! pip install sympy\n\nLooking at our predictions, all of our predictions of the probability of meat-eater are between 0 and 1 so there is no benefit from using a sigmoid function.\n\npreds[:]\n\ntensor([0.7410, 0.8814, 0.8523, 0.8898, 0.8832, 0.8720, 0.7918, 0.9176, 0.5626, 0.6464, 0.6439, 0.7686])\n\n\nThe sigmoid function, has a minimum at zero and maximum at one, and is defined as follows:\nHowever, let’s proceed in any event for illustrative purposes:\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\nPyTorch already defines that function for us, so we can modify calc_preds to use it:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\nLet’s train a new model now, using this updated function to calculate predictions:\n\ncoeffs = train_model(lr=100)\n\n0.515; 0.365; 0.357; 0.329; 0.340; 0.343; 0.277; 0.256; 0.299; 0.268; 0.319; 0.261; 0.221; 0.204; 0.195; 0.205; 0.353; 0.316; 0.234; 0.315; 0.300; 0.257; 0.196; 0.299; 0.284; 0.257; 0.248; 0.188; 0.230; 0.271; \n\n\nThe loss has improved by a lot. Let’s check the accuracy:\n\nacc(coeffs)\n\ntensor(0.4167)\n\n\nAs expected, that hasn’t improved. Here’s the coefficients of our trained model:\n\nshow_coeffs()\n\n{'Min_Len': tensor(1.1502),\n 'Max_Len': tensor(0.5529),\n 'Min_Height': tensor(-3.6500),\n 'Max_Height': tensor(-1.4523),\n 'Min_Weight': tensor(-8.8942),\n 'Max_weight': tensor(-7.9555),\n 'Period_Cretaceous': tensor(10.2919),\n 'Period_Jurassic': tensor(8.6934),\n 'Period_Triassic': tensor(7.9045),\n 'Discovered_Africa': tensor(9.1483),\n 'Discovered_Antartica': tensor(1.6134),\n 'Discovered_Asia': tensor(7.2239),\n 'Discovered_Australia': tensor(5.5307),\n 'Discovered_Europe': tensor(-2.3057),\n 'Discovered_North America': tensor(-1.3442),\n 'Discovered_South America': tensor(3.7572),\n 'Discovered_UK': tensor(3.2253)}\n\n\nThese coefficients seem reasonable – in general, heavier dinos were less agile and therefore more likely to be veggie."
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#using-matrix-multiplication",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#using-matrix-multiplication",
    "title": "Excelosaurus meets Python",
    "section": "Using Matrix Multiplication",
    "text": "Using Matrix Multiplication\nWe can make things quite a bit neater…\nTake a look at the inner-most calculation we’re doing to get the predictions:\n\n(val_indep*coeffs).sum(axis=1)\n\ntensor([ 6.7534,  3.3826,  2.4559,  8.3519, 14.9058, 17.1442,  2.3888,  8.3883,  6.4539, -0.3698, -0.0583,  4.3533])\n\n\nMultiplying elements together and then adding across rows is identical to doing a matrix-vector product! Python uses the @ operator to indicate matrix products, and is supported by PyTorch tensors. Therefore, we can replicate the above calculate more simply like so:\n\nimport time\nfrom datetime import timedelta\nstart_time = time.monotonic()\nend_time = time.monotonic()\nprint(timedelta(seconds=end_time - start_time))\n\n(val_indep*coeffs).sum(axis=1)\nval_indep@coeffs\n\n0:00:00.000017\n\n\ntensor([ 6.7534,  3.3826,  2.4559,  8.3519, 14.9058, 17.1442,  2.3888,  8.3883,  6.4539, -0.3698, -0.0583,  4.3533])\n\n\nIt also turns out that this is much faster, because matrix products in PyTorch are very highly optimised.\nLet’s use this to replace how calc_preds works:\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs)\n\nIn order to do matrix-matrix products (which we’ll need in the next section), we need to turn coeffs into a column vector (i.e. a matrix with a single column), which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column:\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\n\nWe’ll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position:\n\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\n\nWe can now train our model as before and confirm we get identical outputs…:\n\ncoeffs = train_model(lr=100)\n\n0.494; 0.365; 0.360; 0.358; 0.357; 0.357; 0.357; 0.356; 0.356; 0.356; 0.356; 0.356; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; \n\n\n…and identical accuracy:\n\nacc(coeffs)\n\ntensor(0.4167)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#a-neural-network",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#a-neural-network",
    "title": "Excelosaurus meets Python",
    "section": "A neural network",
    "text": "A neural network\nWe’ve now got what we need to implement our neural network.\nFirst, we’ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs. We can choose whatever n_hidden we like – a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We’ll divide these coefficients by n_hidden so that when we sum them up in the next layer we’ll end up with similar magnitude numbers to what we started with.\nThen our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added.\n\ndef init_coeffs(n_hidden=20):\n    layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    layer2 = torch.rand(n_hidden, 1)-0.3\n    const = torch.rand(1)[0]\n    return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_()\n\nNow we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that’s our non-linearity), and the second is passed to torch.sigmoid as before.\n\nimport torch.nn.functional as F\n\n\ndef calc_preds(coeffs, indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nFinally, now that we have more than one set of coefficients, we need to add a loop to update each one:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nThat’s it – we’re now ready to train our model!\n\ncoeffs = train_model(lr=1.4)\n\n0.456; 0.446; 0.437; 0.428; 0.421; 0.414; 0.408; 0.403; 0.398; 0.394; 0.391; 0.388; 0.385; 0.383; 0.381; 0.379; 0.377; 0.375; 0.374; 0.373; 0.372; 0.371; 0.370; 0.369; 0.368; 0.367; 0.366; 0.366; 0.365; 0.364; \n\n\n\ncoeffs = train_model(lr=20)\n\n0.456; 0.373; 0.363; 0.360; 0.358; 0.357; 0.357; 0.356; 0.356; 0.356; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; 0.355; \n\n\nIt’s looking good – our loss is lower than before. Let’s see if that translates to a better result on the validation set:\n\nacc(coeffs)\n\ntensor(0.4167)\n\n\nIn this case our neural net isn’t showing better results than the linear model. That’s not surprising; this dataset is very small and very simple, and isn’t the kind of thing we’d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like!"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#deep-learning",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#deep-learning",
    "title": "Excelosaurus meets Python",
    "section": "Deep learning",
    "text": "Deep learning\nThe neural net in the previous section only uses one hidden layer, so it doesn’t count as “deep” learning. But we can use the exact same technique to make our neural net deep, by adding more matrix multiplications.\nFirst, we’ll need to create additional coefficients for each layer:\n\ndef init_coeffs():\n    hiddens = [10, 10]  # <-- set this to the size of each hidden layer you want\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)]\n    consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)]\n    for l in layers+consts: l.requires_grad_()\n    return layers,consts\n\nYou’ll notice here that there’s a lot of messy constants to get the random numbers in just the right ranges. When you train the model in a moment, you’ll see that the tiniest changes to these initialisations can cause our model to fail to train at all! This is a key reason that deep learning failed to make much progress in the early days – it’s very finicky to get a good starting point for our coefficients. Nowadays, we have ways to deal with that, which we’ll learn about in other notebooks.\nOur deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately:\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        if i!=n-1: res = F.relu(res)\n    return torch.sigmoid(res)\n\nWe also need a minor update to update_coeffs since we’ve got layers and consts separated now:\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers+consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\nLet’s train our model…\n\ncoeffs = train_model(lr=4)\n\n0.376; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; 0.354; \n\n\n…and check its accuracy:\n\nacc(coeffs)\n\ntensor(0.4167)"
  },
  {
    "objectID": "posts/Excel_meets_Python/Excel_meets_Python.html#final-thoughts",
    "href": "posts/Excel_meets_Python/Excel_meets_Python.html#final-thoughts",
    "title": "Excelosaurus meets Python",
    "section": "Final thoughts",
    "text": "Final thoughts\nThe main takeaway from this project, apart from hanging out with my son, and finding out about cool dinos, is that I have successfully managed to:\n\nclean the data using Python and carry out some Exploratory Data Analysis (EDA)\nuse Broadcasting to carry out matrix multiplicaiton\n\n\n\ncreate a real deep learning model from scratch using Python and train it\n\nThe “real” deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you’ll recognise the basic steps are the same.\nThe biggest differences in practical models to what we have above are:\n\nHow initialisation and normalisation is done to ensure the model trains correctly every time\nRegularization (to avoid over-fitting)\nModifying the neural net itself to take advantage of knowledge of the problem domain\nDoing gradient descent steps on smaller batches, rather than the whole dataset.\n\nHopefully, some of the techniques included in this NoteBook prove to be helpful to other data science newcomers like me. Looking forward to Lesson 6!"
  },
  {
    "objectID": "posts/Convolutional_Neural_Network/CNN.html",
    "href": "posts/Convolutional_Neural_Network/CNN.html",
    "title": "Convolutional Neural Networks (CNNs)",
    "section": "",
    "text": "This is my follow up to the second half of Lesson 8: Practical Deep Learning for Coders 2022 in which Jeremy demonstrates the inner workings of a Convolutional Neural Network (CNN) using Excel. In the video, which draws heavily on Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD : Chapter 13, Jeremy walks through the CNN architecture for a handwritten digit 7 in CSV format taken from the MNIST dataset. To consolidate my understanding I replicated the same process for a sample handwritten digit 3.\n\n\nI downloaded the MNIST dataset in csv format from here. Each row of the CSV file has 785 columns, the first column specifies which of the ten handwritten digits is represented (between 0 and 9). The remaining 784 columns contain values which lie on the scale 0 (white) to 255 (black). Why 0 to 255 you may ask? Well, it has to do with computer memory. The number 255 is written as 11111111 in binary form which is 8 bits. In our case we have a grayscale (or 1 channel image) so each pixel takes up 8 bits. Note that a colour image is most commonly represented by 3 channels (Reg, Green, Blue) in which case each pixel requires 24 bits.\nTo see the digit visually we first have to re-arrange the 784 values into a 28 x 28 square, and then use conditional formatting to match the scale - darker for large values, lighter for low values:\n\n\n\n3.JPG\n\n\nOne of the most powerful tools that machine learning practitioners have at their disposal is feature engineering. A feature is a transformation of the data which is designed to make it easier to model. In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the number 3 is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels?\nIt turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a convolution. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model.\n\n\n\nThe underlying concept of CNNs has not changed although there have been architecture modifications. First, let’s look at a traditional CNN - how they generally used to be constructed. A convolution applies a kernel across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of the image below:\n\n\n\nkernel.JPG\n\n\nWe can think of a convolution as a sliding window of little mini dot products of these 3 x 3 matrices or kernels. Note that they don’t have to be of size 3 x 3. We randomly initalize these kernels and then use Stochastic Gradient Descent(SGD) to optimize these parameters. We can repeat the same idea as we add layers.\nAfter the application of the first filter, we now have two channels:\n\nChannel 1: which picks out horizontal edges\n\n\n\n\nhorizontal_detector.JPG\n\n\nNote that we clip the resulting value to zero by taking the maximum of 0 and the mini dot product, and our grid sizes have reduced from 28 x 28 to 26 x 26.\n\nChannnel 2: which picks out vertical edges\n\n\n\n\nvertical_detector.JPG\n\n\nIn the second layer we have 2 kernels applied to each channel:\n\n\n\nconv2.JPG\n\n\nNote that our grid sizes have reduced further to 24 x 24 from our original 28 x 28.\nAt this stage, instead of applying further kernels, we would instead use MaxPool which just takes the maximum value over say 2 x 2 grid areas, with no overlap:\n\n\n\nmaxpool.JPG\n\n\nIn the spreadsheet screenshot above, note that the maximum value of the 2 x 2 grid on the left is 3.54, which is the value returned in the MaxPool layer on the right. Note also that our grid size is now just 12 x 12 compared with our original size of 28 x 28.\nThe final step would be to apply a dense layer which is just randomized weightings applied as SUMPRODUCT in excel over the Maxpool layer outputs, to give a final activation value for conversion to a probability using Softmax:\n\n\n\ndense.JPG\n\n\n\n\n\nAs intimated earlier, the architecture of modern CNNs is generally a slight variant of that illustrated above. In the above examples our kernels applied mini dot products across our initial image grid, with no overlap. As a result, it takes a lot of steps (and therfore layers) to reduce our grid into the number of activations that we are interested in - for our handwritten digits problem we would perhaps be interested in 10 activations, the probability for each of the 10 digits, or maybe just 1 - the probability of it being a particular digit.\n\n\n\n\n\n\nstride.png\n\n\nIt’s all about getting there faster!\nModern approaches tend to apply something called stride 2 convolutions. This works by skipping over a column and row when sliding the kernel over our input grid. This effectively reduces the grid feature size by a factor of 4 (2 rows x 2 columns) each convolution, resulting in fewer steps to get down to the required number of activations.\n\n\n\n\n\n\ndrop_out.png\n\n\nThis isn’t our model giving up on the image classification problem! Drop out refers to the removal of different random bits of our image from each batch. Why would we want to do this? It sounds somewhat counter-intuitive - surely we want to give our model the best possible chance of classifying our image? Well, yes and no.\nEssentially there is, as with all models, an inherent compromise between a model that generalizes well to new images and getting good training results, with the risk of overfitting. You can think of dropout as a kind of data augmentation, except we are applying the corruption or augmentation to our activations, rather than our original input image. These dropout layers are really helpful for avoiding overfitting. Setting a higher drop out rate will mean that our model generalizes well to new images, but perform less well on the training set.\nHere is an example of how it works:\n\n\n\ndropout.JPG\n\n\nWe generate a grid of random numbers to match the size of our 24 x 24 input layer, and then set our drop our rate (a value between o and 1) to create a mask to be applied to our image. In this case we have chosen a drop out rate of 0.4 - which basically removes 40% of the pixels - thus corrupting our image, forcing our model to interpret the underlying structure of the image, thus reducing the risk of overfitting.\n\n\n\nNowadays there is no single dense layer matrix multiply at the end as illustrated previously. In addition, once we get down to say a 7 x 7 grid after stride convolutions, instead of doing a Maxpool generally we carry out an Average Pooling.\nSay, for example we have a bear detector image classifier - the model will basically be asking “Is there a bear in this part of the image?” for each of the say 49 remaining pixels in our final 7 x 7 activation. This works well for a single image that fills the whole grid, but if it is a small image in the corner, or a multi-image image then it might not be classified correctly (maybe only 1 out of the 49 pixels has a bear in it). So we might be better choosing Maxpool in this case.\nThe key takeaway is that it is very important that we undestand the architecture of our model, especially the final layer, to take account of the specific task at hand and the nature of the data included in our model.\nFast.ai in fact goes for a blend, and concatenates both MaxPool and AvgPool.\n\n\n\nThis blog does not by any means attempt to understate the complexity of CNNs, but hopefully by simplifying the concept it might help provide a satisfactory overview, and after working through an example of your own, you will have the confidence to dig deeper.\nAn excellent and comprehensive coverage of CNNs is included in Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD : Chapter 13."
  },
  {
    "objectID": "posts/Cat_or_Dog/Cat_or_dog.html",
    "href": "posts/Cat_or_Dog/Cat_or_dog.html",
    "title": "Cat or Dog?",
    "section": "",
    "text": "This is my follow up to Lesson 1: Practical Deep Learning for Coders 2022 taught by Jeremy Howard, co-founder, along with Dr. Rachel Thomas, of fast.ai. This is my first attempt at building a classifier model."
  },
  {
    "objectID": "posts/Cat_or_Dog/Cat_or_dog.html#introducing-dúi",
    "href": "posts/Cat_or_Dog/Cat_or_dog.html#introducing-dúi",
    "title": "Cat or Dog?",
    "section": "Introducing Dúi",
    "text": "Introducing Dúi\nA puppy named Dúi went viral on Reddit a couple of years back, after some people pointed out that he looks like a mix of a dog and a cat. What do you think?\n\nLet’s put a Machine Learning model to the test and see what it predicts.\nBelieve it or not, the following few lines of code represent a complete system for creating and training a state-of-the-art model for recognizing cats versus dogs. So, let’s train it now! To do so, just press Shift-Enter on your keyboard, or press the Play button on the toolbar. Then wait a few minutes while the following things happen:\n\nA dataset called the Oxford-IIIT Pet Dataset that contains 7,349 images of cats and dogs from 37 different breeds will be downloaded from the fast.ai datasets collection to the GPU server you are using, and will then be extracted.\nA pretrained model that has already been trained on 1.3 million images, using a competition-winning model will be downloaded from the internet.\nThe pretrained model will be fine-tuned using the latest advances in transfer learning, to create a model that is specially customized for recognizing dogs and cats.\n\nThe first two steps only need to be run once on your GPU server. If you run the cell again, it will use the dataset and model that have already been downloaded, rather than downloading them again. Let’s take a look at the contents of the cell, and the results:\n::: {.cell _cell_guid=‘936cf7ef-02f7-4a8c-bcfe-f028b63b6b4c’ _kg_hide-output=‘true’ _uuid=‘803d9fc5-fed7-464c-b6b6-5efdd0af1961’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\n!pip install -Uqq fastbook\nfrom fastbook import *\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00<?]\n    \n    \n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n  \n\n\n    \n      \n      33.70% [31/92 01:30<02:57 0.7032]\n    \n    \n\n:::\nSo, how do we know if this model is any good? In the last column of the table you can see the error rate, which is the proportion of images that were incorrectly identified. The error rate serves as our metric—our measure of model quality, chosen to be intuitive and comprehensible. As you can see, the model is nearly perfect, even though the training time was only a few minutes (not including the one-time downloading of the dataset and the pretrained model). There are a lot of sources of small random variation involved in training models, however we generally see an error rate of well less than 0.02. In this example, the error rate is approx 0.008, which equates to 99.2% accuracy.\nFinally, let’s check that this model actually works by uploading a picture of Dúi for our model to classify:\n::: {.cell _cell_guid=‘392c10cc-9782-4c29-b131-6f42289856f8’ _uuid=‘eac26ec1-f23a-41ba-96e4-0eb504324959’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\nimport ipywidgets as widgets\nuploader = widgets.FileUpload()\nuploader\n:::\n::: {.cell _cell_guid=‘efa0ad57-03a5-43c1-abf1-ff8eb9d0600a’ _uuid=‘08497abe-f829-476a-ba24-bc5079922994’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\nimg = PILImage.create(uploader.data[0])\ndisplay(img)\n:::\nNow let’s see what the model predicts:\n::: {.cell _cell_guid=‘f98c4e24-adf7-4272-a6e9-42236bd686df’ _uuid=‘a1e1b32c-476e-4fbc-8feb-7a902f5e58e3’ jupyter=‘{“outputs_hidden”:false}’ tags=‘[]’}\nis_cat,_,probs = learn.predict(img)\nprint(f\"Is Dúi a cat?: {is_cat}.\")\nprint(f\"Probability Dúi is a cat: {probs[1].item():.6f}\")\n:::"
  },
  {
    "objectID": "posts/Cat_or_Dog/Cat_or_dog.html#conclusion",
    "href": "posts/Cat_or_Dog/Cat_or_dog.html#conclusion",
    "title": "Cat or Dog?",
    "section": "Conclusion",
    "text": "Conclusion\nSo, it seems that the confusion out there was justified. Although our model recorded an error rate of just 0.008119 (which means that over 99.1% of images were correctly classified), the model predicts with almost certainty (93.6% probability) that Dúi is in fact a cat (and not a dog)!"
  },
  {
    "objectID": "posts/Tableau/Tableau.html",
    "href": "posts/Tableau/Tableau.html",
    "title": "Tableau",
    "section": "",
    "text": "Tableau is a widely used business intelligence (BI) and analytics software trusted by companies like Amazon, Experian, and Unilever to explore, visualize, and securely share data in the form of Workbooks and Dashboards. With its user-friendly drag-and-drop functionality it can be used by everyone to quickly clean, analyze, and visualize your team’s data. We’ll learn how to navigate Tableau’s interface and connect and present data using easy-to-understand visualizations. By the end of this training, we’ll have the skills we need to confidently explore Tableau and build impactful data dashboards. Let’s dive in."
  },
  {
    "objectID": "posts/Tableau/Tableau.html#getting-started-with-tableau",
    "href": "posts/Tableau/Tableau.html#getting-started-with-tableau",
    "title": "Tableau",
    "section": "1. Getting started with Tableau",
    "text": "1. Getting started with Tableau\nWe will get an understanding of Tableau’s fundamental concepts and features: how to connect to data sources, use Tableau’s drag-and-drop interface, and create compelling visualizations. We will explore an Airbnb dataset for the city of Amsterdam.\n\n1.1 Introduction\n\n\n\ntableau.JPG\n\n\n\n\n\ntableau_versions.JPG\n\n\n\n\n1.2 Connecting to data\n\n1.2.1 Loading workbooks\n\n\n\ngame_revenue.JPG\n\n\n\n\n\n\n\n\nIn which year did mobile games overtake console games in terms of revenue?\n\n\n\n\n\n2015\n\n\n\n\n\n1.2.2 Loading data\nIn this exercise we will start by loading the new_york.csv dataset which we will use throughout this section.\n\n\n\nnew_york.JPG\n\n\nThe dashboard gives us an overview of our data - we can see there are 12 fields or columns and we have different data types indicated at the top of each column:\n# numerical\nAbc text\nglobe geographical\nNote that continuous data is green and categorical data is blue.\nThe displayed column names can be changed although their remote field name will remain unchanged.\nWe can see that there are 17,614 rows or observations.\n\n\n\n1.3 Navigating Tableau\n\n1.3.1 Dimensions and measures\n\n\n\ndimensions_measures.JPG\n\n\nNow that we have connected our data sources let’s now look at the Worksheets interface, where we will create our visualizations:\n\n\n\nnew_york_worksheet.JPG\n\n\nDimensions positioned at the top of the data pane contain qualitative values such as names or dates - in our example Id Neighborhood Room Type.\nMeasures positioned below our dimensions contain numerical, quantitative values that can be measured and aggregated - in our example Days Occupied in 2019 Minimum Nights Price and so on.\nNote that we have Reviews Per Month included here. This data is not qualitative, and should be converted to quantitative as shown below:\n\n\n\nconvert_to_measure.JPG\n\n\n\n\n\n1.4 A tour of the interface\n\n1.4.1 New York neighbourhood prices\nLet’s visualize our data. Start by dragging the Neighborhood field to the Rows shelf :\n\n\n\nnew_york_neighb_prices_1.JPG\n\n\nDrag the Price field to the Text Marks card :\n\n\n\nnew_york_neighb_prices_2.JPG\n\n\nGetting the ‘SUM’ of prices does not make much sense.\nHowever on SUM(Price) at the bottom of the Marks field, click on the down arrow that appears and instead of SUM select Average as the measure :\n\n\n\nnew_york_neighb_prices_3.JPG\n\n\n\n\n\n\n\n\nWhat is the average price of a listing in the Brooklyn Heights neighbourhood?\n\n\n\n\n\n367\n\n\n\n\n\n1.4.2 Segmenting by room type\nPreviously we found the average price of rooms in each neighbourhood. Now, we want to know the average number of days listings were occupied in 2019 segmented first by neighbourhood, but also by room type.\nFirst we need to replace Price from the Marks field with Days Occupied In 2019. We can do this by dragging Price outwith the Marks pane (until we see a red cross) and then dropping it, and then dragging in Days Occupied In 2019 :\n\n\n\nnyc_segment_1.JPG\n\n\nThen get the AVG instead of the SUM :\n\n\n\nnyc_segment_2.JPG\n\n\nAnd then segment by room type. We can do this by adding Room Type to the Rows shelf :\n\n\n\nnyc_segment_3.JPG\n\n\n\n\n\n\n\n\nHow many days on average, were private rooms occupied in Bayside?\n\n\n\n\n\n116.5\n\n\n\n\n\n\n1.5 How to create visualizations in Tableau\n\n1.5.1 Creating our first visualization\nIn the previous section we created a table to find out the number of days listings were occupied in 2019 per neighbourhood and rooom type. However a table is tedious to look at, so we would prefer to visualize the results instead.\nHowever, there are 100+ neighbourhoods in New York, which can be challenging to visualize, so let’s use a prefiltered workbook focusing on a few popular areas for illustrative purposes:\n\n\n\nNYC_filter_1.JPG\n\n\nClick Show Me in the top right and select the stacked barchart option. Note to show numerical values on the face of the bars, click on the T on the dashboard as highlighted below:\n\n\n\nNYC_filter_2.JPG\n\n\n\n\n\n\n\n\nHow many days on average, were private rooms occupied in the Bronxdale neighbourhood?\n\n\n\n\n\n106.3\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that Tableau shows the visualizations that can be used, but it is up to us to customize based on the question being asked and the type of data.\n\n\n\n\n1.5.2 Bringing it all together\nTo close this section, we will build a visualization on our own from scratch. We will compare occupancy and average prices for each neighbourhood and room type combinations. This would be excellent information to have to hand when deciding whether an investment may be worthwhile.\nReplace the average Days Occupied In 2019 with Price on the bar chart by:\n\ndragging Price on top of AVG([Days Occupied In 2019)] in the Rows section\nchanging the aggregation to Average\n\n\n\n\nall_together.JPG\n\n\n\n\n\n\n\n\nWhat is the highest average price for shared rooms?\n\n\n\n\n\n1,250\n\n\n\nThis value is suspicious - how can a shared room be that expensive and cost more than six times the price of an entire home? This outlier does not necessarily represent an error within our data, and there may a perfectly legitimate reason for the anomaly, however it must be investigated further."
  },
  {
    "objectID": "posts/Tableau/Tableau.html#building-and-customizing-visualizations",
    "href": "posts/Tableau/Tableau.html#building-and-customizing-visualizations",
    "title": "Tableau",
    "section": "2. Building and Customizing Visualizations",
    "text": "2. Building and Customizing Visualizations\nLet’s take it up a level and review the core concepts required for analyzing and exploring data in Tableau. We’ll learn how to slice and dice data with filters, create new columns using our own calculated fields, and aggregate dimensions and measures in a view. We will be working with education, social and infrastructure data.\n\n2.1 Filtering and sorting\n\n\n\nfiltering.JPG\n\n\n\n2.1.1 The order of filtering\nThe order of filtering is important when dealing with large datasets. The extract and data source filters are applied prior to loading our data into Tableau but we will focus on the Dimension and Measure filters.\n\n\n\nfilter_types.JPG\n\n\n\n\n\n2.2 Sorting and filtering through selection\n\n\n\ndimension_filters.JPG\n\n\n\n\n\nmeasure_filters.JPG\n\n\n\n\n\nsorting.JPG\n\n\n\n2.2.1 Sorting and excluding multiple fields\nKeep only values for 2017 and sort Country alphabetically :\n\n\n\nsort_1.jpg\n\n\nExclude United Arab Emirates from the list, sort by Cell Phones per 100 people in descending order, and keep only the first 10 countries that are listed:\n\n\n\nsort_3.JPG\n\n\n\n\n\n\n\n\nWhich country had the highest number of cell phones per 100 people in 2017?\n\n\n\n\n\nMaldives\n\n\n\n\n\n2.2.2 Comparing G7 countries\nThe Group of Seven (G7) is an international organization consisting of Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States. In this exercise we will filter data to look only at countries that are part of the G7 with Broadband Subscribers per 100 people greater than 30.\nNavigate to the G7 worksheet :\n\n\n\nG7_1.JPG\n\n\nFilter Country to show the countries that make up the G7 :\n\n\n\nG7_2.JPG\n\n\nFilter Broadband Subscribers per 100 ppl to show where the value is at least 30 :\n\n\n\nG7_3.JPG\n\n\n\n\n\n\n\n\nWhich G7 countries have consistently had at least a value of 30 for Broadband Subscribers per 100 ppl from 2009 onwards?\n\n\n\n\n\nCanada, France, Germany\n\n\n\n\n\n\n2.3 Filtering through the filter shelf\nNotice that often our data includes nulls. This is misleading because it makes it look like the values are zero, but this might not be the case.\n\n2.3.1 Filtering for null values\nNavigate to the Cell vs Broadband worksheet:\n\n\n\ncell_vs_bband_1.JPG\n\n\nWe want to know for which countries we lack values for the measures Cell Phones per 100 People and Broadband Subscribers per 100 ppl\nAdd Cell Phones per 100 People to the filters card. Select Next and navigate to the Special option in the pop-u. Select Null Values :\n\n\n\ncell_vs_bband_2.JPG\n\n\nRepeat the above instruction with the Broadband Subscribers per 100 ppl field :\n\n\n\ncell_vs_bband_3.JPG\n\n\n\n\n\n\n\n\nWhich country has null values for both Cell Phones per 100 People and ’Broadband Subscribers per 100 ppl` in 2010?\n\n\n\n\n\nMarshall Islands\n\n\n\nFiltering for null values is a useful feature when cleaning and exploring our data, by identifying missing data for follow up.\n\n\n2.3.2 Top filters on Tableau\nNow we want to filter countries on their average Cell Phones per 100 People across years 2006-2015. The sum aggregation is set as the default for measures so we’ll have to look out for this.\nNavigate to the Top Filters worksheet :\n\n\n\ntop_filter_1.JPG\n\n\nAdd a filter on Country. Use Tableau’s top filter option on the bottom two countries based on the Cell Phones per 100 People average:\n\n\n\ntop_filter_2.JPG\n\n\n\n\n\n\n\n\nWhich country has the second lowest average Cell Phones per 100 People ?\n\n\n\n\n\nCuba.\n\n\n\nAs you may have noticed, there are many other aggregation options like median, count, minimum and variance. Let’s have a closer look at these now.\n\n\n\n2.4 Aggregation\n\n\n\naggregation.JPG\n\n\n\n2.4.1 Aggregating measures and dimensions\nAggregating can be defined as gathering and summarizing data points for analytics. Aggregating measures is more common, but also some dimensions can be aggregated, depending on your use case.\n\n\n\nagg_measures.JPG\n\n\n\n\n\nagg_dimensions.JPG\n\n\n\n\n\n2.5 Scatter plots and aggregations\n\n2.5.1 CO2 Emmisions and GDP in Sub Regions\nIn this exercise we will create a scatter plot comparing GDP per Capita and CO2 Emissions per Person. The data points in the scatter plot should represent Sub Regions.\nOpen the workbook 2_5_co2emissions_and_gdp_in_sub_regions.twbx and ensure you are in the CO2 and GDP worksheet :\n\n\n\nCO2_1.JPG\n\n\nCreate a scatter plot with average GDP per Capita on the x-axis and the average CO2 Emissions per Person on the y-axis :\n\n\n\nCO2_2.JPG\n\n\nColour the points based on Sub Regions :\n\n\n\nCO2_3.JPG\n\n\n\n\n\n\n\n\nBased on our scatter plot the values for Eastern Asia and Caribbean sub regions are very different. True or False ?\n\n\n\n\n\nFalse, the values are very similar, the data points overlap on the scatterplot.\n\n\n\n\n\n2.5.2 Counting on GDP per capita\nThe aggregation options Count and Count (Distinct) are useful for analyzing dimensions and whether they have reached certain dimensions.\nOpen the workbook 2_6_counting_on_gdp_per_capita.twbx and navigate to the GDP per Capita :\n\n\n\nGDP_1.JPG\n\n\nCreate a chart with GDP per Capita (Grouped) as rows and the distinct count of GDP per Capita (Grouped) as columns :\n\n\n\nGDP_2.JPG\n\n\nColour the bars based on Sub Region :\n\n\n\nGDP_3.JPG\n\n\n\n\n\n\n\n\nWhich sub-region has been in all six GDP per Capita (Grouped) categories ?\n\n\n\n\n\nWestern Asia.\n\n\n\nWestern Asia has been in all six categories since 1960 showing that it either had significant growth or a decline in GDP. We would have to see how Western Asia’s GDP has shifted over the years.\n\n\n2.5.3 Standard deviation of life expectancy\nStandard deviation is a useful aggregation for analyzing how much data varies. In this exercise we will use standard deviation to see how much life expectancy has varied across the years for different countries.\nLoad the workbook 2_7_std_dev_of_life_expectancy.twbx and navigate to the Life Expectancy worksheet :\n\n\n\nlife_expectancy.JPG\n\n\nCreate a bar chart with the bars representing each country’s standard deviation of life expectancy :\n\n\n\nlife_expectancy_2.JPG\n\n\nConsider only the years from 1980 to 2000 :\n\n\n\nlife_expectancy_3.JPG\n\n\n\n\n\n\n\n\nWhich country has the highest standard deviation on the Life Expectancy measure ?\n\n\n\n\n\nLebanon.\n\n\n\nLebanon has the highest standard deviation between the years 1980 to 2000. We would have to analyze further if this relatively large variation was due to increased or decreased quality of life.\n\n\n\n2.6 Calculated fields\n\n\n\ncalculated_fields.JPG\n\n\n\n\n\nimage.png\n\n\n\n\n\nfunctions.JPG\n\n\nThere’s no need to memorize all of the functions because of Tableau’s handy built-in documentation :\n\n\n\nfunction_search.JPG\n\n\n\n\n2.7 Creating calculated fields\n\n2.7.1 Calculated field for rounding\nOpen the workbook 2_8_calculated_field_for_rounding.twbx and navigate to the Rounding worksheet :\n\n\n\nrounding.JPG\n\n\nCreate a new calculated field called Rounded Women 25-34 - use the built-in documentation and look up ROUND() and round the new column to whole numbers (0 decimal points) :\n\n\n\nrounding_2.JPG\n\n\nReplace Women 25-34 in the Marks field with our newly created field :\n\n\n\nrounding_3.JPG\n\n\nRemove any existing filters and create new filters for Year = 1976 and >=10 for our newly created field :\n\n\n\nrounding_4.JPG\n\n\n\n\n\n\n\n\nHow many countries in 1976 had women in the 25-34 age range spending, on average, ten years or more in school ?\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7.2 Ratio between genders\nAnother useful calculated field to make is a ratio. Ratios are ana excellent way to compare two values. In our case, we can compare the mean years of education between men and women using a ratio. The closer the ratio is to one, the more equal levels of education are between women and men.\nLoad the workbook 2_9_ratio_between_genders.twbx and navigate to the Ratio worksheet :\n\n\n\nratio_1.JPG\n\n\nCreate a calculated field called Men:Women (25-34) which calculates the men:women ratio for years spent in school in the 25-34 age group. Add this new field as a column to the table by dragging it to the Text card and change the aggregation from sum to average :\n\n\n\nratio_2.JPG\n\n\n\n\n\n\n\n\nWhat is Russia’s Men:Women (25-34) ratio for years spent in school?\n\n\n\n\n\n0.9821.\n\n\n\nRussia has a ratio of ~ 0.98 which means that Russian men have an average of 0.98 years of schooling for every year of schooling that Russian women receive.\nLet’s now create a field for the average across women and women in the age group 25-34.\nNavigate to the Average worksheet:\n\n\n\naverage.JPG\n\n\nCreate a new field called 25-34 that sums the values for men and women in the 25-34 age group and divides them by 2 to get the average:\n\n\n\naverage_2.JPG\n\n\nAdd the new field to the table. Make sure to change the aggregation from SUM to AVG :\n\n\n\naverage_3.JPG\n\n\n\n\n\n\n\n\nWhat is the highest 25-34 average for a country?\n\n\n\n\n\n13.40\n\n\n\nRemember, this isn’t a weighted average. To make this calculation more accurate, we could add the country’s population data for each gender and age group. Then we could alter the calculated field’s formula to have weighted proportions."
  },
  {
    "objectID": "posts/Tableau/Tableau.html#digging-deeper",
    "href": "posts/Tableau/Tableau.html#digging-deeper",
    "title": "Tableau",
    "section": "3. Digging Deeper",
    "text": "3. Digging Deeper\nLet’s dive deeper into analytics by learning how to visualize geographic data and plot data onto a map visualization. We’ll learn how to work with dates in Tableau and explore how the data changes with time. We will also learn how to add reference, trend, and forecasting lines to our views. We will do all of this exploring health statistics worldwide.\n\n3.1 Mapping our data\n\n\n\ngeog_types.JPG\n\n\nNote that Tableau does not geocode territories, as these are customized region/country groupings that are used differently by people and organizations.\n\n\n\nfilled_map.JPG\n\n\n\n\n\nsymbol_maps.JPG\n\n\n\n\n3.2 Creating a symbol map\nOpen the workbook 3_1_your_first_symbol_map.twbx and navigate to the Symbol Map worksheet. Initialize a symbol map with a circle in each country:\n\n\n\nsymbol_map_1.JPG\n\n\nAdjust the sizes of the circles based on the country’s number of Stomach Cancer cases :\n\n\n\nsymbol_map_2.JPG\n\n\nIncrease the size of the circles to make it easier to read the map :\n\n\n\nsymbol_map_3.JPG\n\n\n\n\n\n\n\n\nHow many people were diagnosed with stomach cancer in the country with the highest stomach cancer cases?\n\n\n\n\n\n7,224,000\n\n\n\nDuring the period 7,224,000 people were diagnosed with stomach cancer in China. Using automatically generated latitude and longitude, Tableau lets you quickly generate powerful spatial visualizations.\nLet’s add some colour to this map. We are asked to add some information about the countries’ population growth. To make the map clearer, we decide to add the coastline.\nColour the circles based on the average Population Growth :\n\n\n\nsymbol_map_4.JPG\n\n\nAdd a black border around the circles :\n\n\n\nsymbol_map_5.JPG\n\n\nRemove the halo :\n\n\n\nsymbol_map_6.JPG\n\n\nAdd a Map layer responsible for showing the Coastline :\n\n\n\nsymbol_map_7.JPG\n\n\n\n\n\n\n\n\nWas the population growth of the United Kingdom positive or negative?\n\n\n\n\n\nPositive. The colour of the circle is blue which represents positive growth. The orange circles represent negative growth.\n\n\n\nLets’ combine everything we’ve learned so far. In this final exercise we will create a symbol map that will make it easy to look at the countries’ population and population growth rates. With this information the World Health Organization can decide on which countries they need to focus on preventing health risks due to overpopulation.\nNavigate to the Population worksheet and create a symbol map with varying circle sizes based on Population :\n\n\n\npopulation.JPG\n\n\nColour the circles based on average Population Growth :\n\n\n\npopulation_2.JPG\n\n\nAdd a black border around the circles, remove the halo, and filter year to include 2008 data :\n\n\n\npopulation_3.JPG\n\n\n\n\n\n\n\n\nWhat was the average population growth of the country with the highest population in 2008?\n\n\n\n\n\n0.56 - China.\n\n\n\nChina had a growth rate of 0.56 in 2008. It’s positive but not as high as other countries on the map. India’s growth rate for example is 1.48. India might surpass China in terms of population during the following years. The World Health Organization should make sure that they think about India’s high population and rapid growth.\n\n\n3.3 Working with dates\n\n\n\ndate_data.JPG\n\n\n\n3.3.1 Date hierarchies\n\n\n\ndate_hierarchy.JPG\n\n\n\n\n\n3.4 Visualizing dates\n\n3.4.1 Our data by year\nOur task is to analyze natality in Poland over the past 40 years. The World Health Organization is interested in how the number of births has evolved.\nOpen workbook 3_4_your_data_by_year.twbx and navigate to the Births worksheet. Create a linechart with the date (in years) as columns and the total number of births as rows :\n\n\n\nbirths.JPG\n\n\nFilter the data on Country to only include Poland :\n\n\n\nbirths_2.JPG\n\n\n\n\n\n\n\n\nWhat was the total number of births in Poland in 2017 ?\n\n\n\n\n\n401,946.\n\n\n\nThe number of births in Poland declined between 1983 and 2003, increased in the years to 2009, and then dipped and resurfaced, centred around 2009 levels in the period 2009 to 2017. But what if we are interested in the births at a more granular level? Let’s analyze the data on a monthly level next.\n\n\n3.4.2 Our data by month\nThe World Health Organization has asked us to focus specifically on the last five years available data and look at the monthly number of births.\nOpen the workbook 3_5_your_data_by_month.twbx and navigate to the Births worksheet. Display the date as continuous month values :\n\n\n\nbirths_3.JPG\n\n\nFilter the data on the Years 2013-2017 :\n\n\n\nbirths_4.JPG\n\n\n\n\n\n\n\n\nIn what month and year were the most babies born in Poland during the 2013-2017 period ?\n\n\n\n\n\nJuly 2017.\n\n\n\nIn July 2017, a total number of 35,993 babies were born in Poland. Note how easy Tableau makes changing the granularity level of the date variable.\n\n\n3.4.3 Birth seasonality\nLet’s combine everything we have learned so far in our final exercise. We will create a line chart that will help the World Health Organization plan how many resources should be sent to Chile to ensure newborn care at birth throughout the year.\nLoad the workbook 3_6_birth_seasonality.twbx and navigate to the Birth Seasonality worksheet. Create a line chart with the date (in years) as columns and the total number of births as rows:\n\n\n\nbirths_5.JPG\n\n\nFilter the data on Country to include only Chile :\n\n\n\nbirths_6.JPG\n\n\nDisplay the date as continuous month values:\n\n\n\nbirths_7.JPG\n\n\nFilter the data on the Years 1995-1999 :\n\n\n\nbirths_8.JPG\n\n\n\n\n\n\n\n\nDuring which month were the fewest babies born in Chile during 1995-1999 ?\n\n\n\n\n\nFebruary 1999.\n\n\n\nIt looks like there is some seasonality in the natality data. During the 1995-1999 period, fewer babies were born each February, although note that this is most likely because February is the shortest month.\n\n\n\n3.5 Reference lines, trend lines, and forecasting\n\n3.5.1 Reference lines\n\n\n\nreference_line.JPG\n\n\nOur task is to analyze the number of births in Denmark over the last six years. We have been asked to add a reference line to our graph indicating the minimum monthly births over the period.\nOpen up the workbook 3_7_reference_lines.twbx and navigate to the Reference, Trends & Forecasting worksheet. Create a line chart with the date (continuous month values) as columns and the total number of births as rows:\n\n\n\ntotal_births.JPG\n\n\nFilter the data on Country to only include Denmark, and Date to only include 2012-2017 :\n\n\n\nbirths_denmark.JPG\n\n\nAdd a reference line indicating the minimum number of births in the chart :\n\n\n\ndenmark_refline_min.png\n\n\n\n\n\n\n\n\nWhat was the minimum number of monthly births in Denmark during the 2012-2017 period ?\n\n\n\n\n\n4,225.\n\n\n\nIn February 2015, 4,225 babies were born in Denmark. With the reference line chart, we can easily see how the number of births in a certain month compares to the minimum of that period.\n\n\n3.5.2 Trend lines\n\n\n\ntrend_lines.JPG\n\n\nNext, we are asked to investigate the trend in the number of female births during the same period. We can easily spot this by adding a trend line. Load the workbook 3_9_trend_lines.twbx and filter to show only female births :\n\n\n\ndenmark_female_births.JPG\n\n\nAdd a linear trend line for the number of births over time :\n\n\n\ndenmark_female_trend.JPG\n\n\n\n\n\n\n\n\nIs there an upward or downward trend in the number of female births in Denmark during the 2012-2017 period ?\n\n\n\n\n\nUpward.\n\n\n\nOverall, the number of female births in Denmark has been slowly increasing over the last couple of years. This is much easier to spot with the trend line added to our visualization.\n\n\n3.5.3 Forecasting\n\n\n\nforecasting.JPG\n\n\nForecasting is a valuable technique that can help us anticipate and make informed decisions for the future. Let’s do some forecasting. This time we are tasked with forecasting the number of female births in Denmark for the next year.\nLoad the workbook 3_9_forecasting.twbx and remove the trend line we created before. Add a forecast for the number of female births during 2018 :\n\n\n\ndenmark_forecast.JPG\n\n\n\n\n\n\n\n\nHow many female births do we expect in Denmark in August 2018 ?\n\n\n\n\n\n2,890.\n\n\n\nThe forecast tells us that we can expect 2,890 new baby girls in Denmark in August 2018. Forecasting future events can put us in a position to be proactive instead of reactive.\n\n\n3.5.4 Bringing it all together\nLet’s combine what we’ve learned so far. Let’s create a line chart which that will make it easy to answer the following question:\n\nIs the forecasted number of births in Japan during December 2019 higher than the average over the last five years?\n\nLoad the workbook 3_10_natality_forecast and navigate to the Natality Forecast worksheet. Create a line chart with the date (continuous month values) as columns and the total number of births as rows :\n\n\n\njapan_births.JPG\n\n\nFilter the data on Country to only include Japan and on Date to only include years 2014-2018 :\n\n\n\njapan_births_filter.JPG\n\n\nAdd a forecast for the number of births during 2019 :\n\n\n\njapan_forecast.JPG\n\n\nAdd a reference line indicating the average number of births in the chart:\n\n\n\njapan_refline.JPG\n\n\n\n\n\n\n\n\nIs the forecasted number of births in Japan During December 2019 higher than the average over the last five years?\n\n\n\n\n\nNo.\n\n\n\nThe number of births in December 2019 is forecast to be lower so the World Health Organization might need to send over fewer resources during that period."
  },
  {
    "objectID": "posts/Tableau/Tableau.html#presenting-our-data",
    "href": "posts/Tableau/Tableau.html#presenting-our-data",
    "title": "Tableau",
    "section": "4. Presenting Our Data",
    "text": "4. Presenting Our Data\nOur data is full of interesting stories and insights still waiting to be told. Learn best practices for formatting and presenting visualisations to tell data-driven stories. Using a new dataset on video game sales we’ll be building our first dashboard!\n\n4.1 Make our data visually appealing\n\n\n\nformatting.JPG\n\n\n\n\n\nformat_levels.JPG\n\n\n\n\n4.2 Applying visual best practices\n\n4.2.1 Create a dual-axis graph\nThe video games dataset contains information on video game sales from 1990 until 2010. Our task is to investigate the dataset and uncover insights about the gaming industry. Our first job is to investigate global and European sales of Atari over time - we can do so by creating a dual-axis graph.\nOpen WorkBook 4_1_dual_axis.twbx and rename Sheet 1 tab to EU vs Global Sales - drag Release Year to columns, and Global Sales to rows :\n\n\n\ngames.JPG\n\n\nDrag EU sales to the right of the graph until a dotted line appears. Synchronize and hide the right axis :\n\n\n\ngames_2.JPG\n\n\nChange Global Sales from a line to a bar chart using the Marks card. This will make it more evident Global Sales is the sum of all regions :\n\n\n\ngames_global_bar.JPG\n\n\nClean up the graph and add a filter:\n\ncentre the title of the graph\nrename the y-axis to Video Game Sales (millions of units)\nfilter Publisher for Atari\n\n\n\n\ngames_atari.JPG\n\n\n\n\n\n\n\n\nHow many video games (million units) from Atari were sold in the release year 2010 ?\n\n\n\n\n\n1.44\n\n\n\nThe visualization shows Atari game sales went downhill fast after a second successful stint in the ear;y 2000s, eventually resulting in their bankruptcy in 2013.\n\n\n4.2.2 Expanding a dual-axis graph\nBreaking video gaame sales down by Genre can reveal many insights. We have been asked to investigate Nintendo's Puzzle video game sales in North America.\nDuplicate the EU vs Global Sales worksheet and rename it All Regions - Sales. Next, drag Measure Values on top of EU Sales in the rows section. By doing this, Tableau automatically adds all Measures (highlighted in green) to the graph :\n\n\n\ngames_measures.JPG\n\n\nNext, remove the following from Measure Values (below the Marks cars) that are not needed:\n\nVideo Games - Sales.csv (count). This is a count of rows of the database generated by Tableau. In this case, it does not add any info\nGlobal Sales The measure is already visualized on the bar chart on the left axis\n\n\n\n\ngames_measures_removed.JPG\n\n\nExpand the tooltip to allow us to hover over the bar :\n\nAdd EU Sales, Global Sales, NA Sales, and Other Sales to Tooltips and edit to our liking:\n\n\n\n\ngames_tooltips.JPG\n\n\nInclude a Genre filter for Puzzle and change Publisher filter to Nintendo :\n\n\n\ngames_nintendo.JPG\n\n\n\n\n\n\n\n\nWhat is the amount of Puzzle genre video games sold (million units) by Nintendo in North America that were released in 1989 ?\n\n\n\n\n\n26.34\n\n\n\nThe massive spike in the puzzle genre in 1989, is thanks to the success of Tetris. It accounts for almost 90% of sales from that release year!\n\n\n4.2.3 Formatting our visualization\nTime to play around with colours. Create a new worksheet and rename it Global Sales Breakdown. Create a bar chart of Global Sales by Release Year, centre the title of the graph, and change its font size to 16 :\n\n\n\nglobal_sales.JPG\n\n\nAdd different Dimensions to the colours pane to see what happens:\n\nadd Name - ignore the warning and press Add all members :\n\n\n\n\nglobal_sales_colour_name.JPG\n\n\n\nreplace Name with Release Year by dragging it on top :\n\n\n\n\nglobal_sales_colour_release_year.JPG\n\n\n\nreplace Release Year with Genre by dragging it on top :\n\n\n\n\nglobal_sales_colour_genre.JPG\n\n\n\n\n\n\n\n\nOut of the 3 dimensions (name, release year, genre) we dragged to colour, which one was the most useful ?\n\n\n\n\n\nGenre.\n\n\n\nGenre is the only option that adds extra information to the graph and is thus the most useful.\n\n\n\n4.3 Dashboards and stories\n\n\n\ndashboard.JPG\n\n\n\n\n\ndashboards.JPG\n\n\n\n\n\nstories.JPG\n\n\n\n\n\nstory_1.JPG\n\n\n\n\n\nstory_2.JPG\n\n\n\n4.3.1 Worksheet vs. dashboard vs. story\n\n\n\nworksheet_dashboard_story.JPG\n\n\n\n\n\n4.4 Creating dashboards and stories\nWe have been asked to investigate how Playstation video game sales developed over time, which platform was the most popular, and which were its most popular games and genres. Let’s create a dashboard to answer these questions.\n\n4.4.1 Building a dashboard\nLoad the WorkBook 4_4_first_dashboard.twbx and navigate to the empty dashboard Playstation Overview 1994 - 2010.\nMake Platform analysis over time visible on the dashboard, make the legend floating, and add the other three sheets so they appear in a 2 x 2 grid clockwise, like so:\n[Platform Analysis over Time] [Sales by Platform]\n[Top Video Games] [Sales by Genre]\n\n\n\nplaystation_dashboard.JPG\n\n\n\n\n\n\n\n\nWhat is the genre of GTA San Andreas, the top selling Playstation game? Is it equal to the most popular Playstation genre?\n\n\n\n\n\nAction. Yes.\n\n\n\n\n\n4.4.2 Filters and dashboards\nThe dashboard is a great start but what if we are only interested in games from a specific publisher (like “Sony Entertainment”) for a particular game? Let’s make this possible by adding a Publisher filter and enabling the Treemap to function as a genre filter.\nDisplay the dashboard title, centre it and change the font size to 20 :\n\n\n\nplaystation_dashboard_title.JPG\n\n\nAdd a Publisher filter, click on any graph and click on Analysis in the toolbar, navigate to Filters in the dropdown menu and select Publisher :\n\n\n\nplaystation_dashboard_filter.JPG\n\n\nChange the filter style to Single Value (dropdown) and drag the filter below the title without making it floating :\n\n\n\nplaystation_dashboard_filter_edit.JPG\n\n\nEnable the option to use the Treemap as a filter. Note that the graph on the TOP RIGHT changes depending on how you select your filter(s) e.g. select the genre by clicking on Racing on the Treemap chart in the bottom right, and select Sony Computer Entertainment from the dropdown Publisher filter under the title :\n\n\n\nplaystation_dashboard_racing.JPG\n\n\n\n\n\n\n\n\nHow many video game units (millions) did Sony Computer Entertainment sell for the PS2 in the Racing genre?\n\n\n\n\n\n35.41.\n\n\n\nSopny Computer Entertainment sold 35.41 million units thanks to its success with the Gran Turismo series.\n\n\n4.4.3 Creating and navigating a story\nLoad the WorkBook 4_6_first_story and create a new story named Competitor Analysis 1994 - 2010. Centre the story’s title :\n\n\n\nstory.JPG\n\n\nAdd the Playstation, Nintendo, and Xbox dashboards to the story. Rename the captions accordingly :\n\n\n\nstory_ps_nintendo_xbox.JPG\n\n\n\n\n\n\n\n\nThe best selling Shooter on both the Playstation(PS) and the Xbox was Call of Duty:Black Ops. On which platform did it have the highest sales? Playstation or Xbox ?\n\n\n\n\n\n\n\n\n\nshooter_playstation.JPG\n\n\n\n\n\nshooter_xbox.JPG\n\n\nThe Playstation version of Call of Duty: Black Ops shifted 39.33 million units. The Xbox version sold 62.13 million.\n\n\n\n\n\n\nWhat was the name of the best-selling Playstation 2 (PS2) game published by Electronic Arts in the Simulation genre ?\n\n\n\n\n\n\n\n\n\nps2_simulation_ea.JPG\n\n\nThe best selling Simulation genre game on the Playstation 2 (PS2) published by Electronic Arts was The Sims with sales of 2.77 million units globally."
  },
  {
    "objectID": "posts/Tableau/Tableau.html#key-takeaways-and-acknowledgements",
    "href": "posts/Tableau/Tableau.html#key-takeaways-and-acknowledgements",
    "title": "Tableau",
    "section": "Key Takeaways and Acknowledgements",
    "text": "Key Takeaways and Acknowledgements\nThanks to Maarten Van den Broeck, Lis Sulmont, Sara Billen, and Carl Rosseel for introducing me to Tableau.\n\nSection 1\n\nload data and workbooks and how to navigate the Tableau interface\nbuild visualizations using stacked bar charts\n\n\n\nSection 2\n\nsliced, diced, and ordered data using sorting and filtering, and aggregation\ncreated new columns from existing info (calculated fields)\n\n\n\nSection 3\n\nmapped geographical data\nworked with dates\nenhanced visualisations using reference lines, trend lines, and forecasting\n\n\n\nSection 4\n\nhow to improve and format our visualizations\nconvey findings with Dashboards and Stories"
  },
  {
    "objectID": "posts/World Cup 2022/World_Cup_2022.html",
    "href": "posts/World Cup 2022/World_Cup_2022.html",
    "title": "FIFA World Cup - Qatar 2022",
    "section": "",
    "text": "At the time of writing the 2022 World Cup is already underway, with 32 teams battling it out in Qatar for the famous golden globe. I’ve lost touch a bit in recent years with football, and thought it would be interesting to use Python to get back up to speed.\nThe main aim of this project is to try and uncover some insights about the 32 teams - who has played it safe, and gone for experience, who has been bold and decided to give youth a chance to flourish. I’m also interested in where the players play their football at club level, and hope to quantify their geographical spread.\n\n\nThe data was sourced from Sporting News website.\n\n\n\nLet’s dive in!\n\n\n\nfootbal dive.jpg\n\n\n\n## Import the required packages\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib as plt\n\npd.options.display.float_format = '{:.2f}'.format\n\n\n# Load in our data\nworld_cup = pd.read_csv('Data/World_Cup_2022.csv')\n\nWe can automate some of the exploratory data analysis by writing a function:\n\ndef initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))  \n\n\ninitial_eda(world_cup)\n\nDimensions : 830 rows, 11 columns\nTotal NA Values : 0 \n                           Column Name  Data Type      #Distinct  NA Values\n                                  Name     object          830          0\n                              Position     object            4          0\n                                   Age    float64           29          0\n                               Country     object           32          0\n                              Ctry_cap     object           32          0\n                                  Caps      int64          132          0\n                                 Group     object            8          0\n                                  Club     object          446          0\n                             Club_ctry     object           43          0\n                         Club_ctry_cap     object           43          0\n                             Home_Away     object            2          0\n\n\nLooks like there is no missing data within the dataset. However, there are 8 groups of 4, so 32 teams competing and the maximum permitted squad size is 26, which would be 832 players. We only have 830 observations. Let’s investigate this by grouping the squad numbers by country:\n\nworld_cup.groupby(\"Country\")[\"Name\"].count().sort_values(ascending=False)\n\nCountry\nArgentina       26\nAustralia       26\nUruguay         26\nUSA             26\nTunisia         26\nSwitzerland     26\nSpain           26\nSouth Korea     26\nSerbia          26\nSenegal         26\nSaudi Arabia    26\nQatar           26\nPortugal        26\nPoland          26\nNetherlands     26\nMorocco         26\nMexico          26\nJapan           26\nGhana           26\nGermany         26\nEngland         26\nEcuador         26\nDenmark         26\nCroatia         26\nCosta Rica      26\nCanada          26\nCameroon        26\nBrazil          26\nBelgium         26\nWales           26\nIran            25\nFrance          25\nName: Name, dtype: int64\n\n\nIran and France only have 25 players in their squad. On follow up this was confirmed to be correct, so we haven’t lost any players! Let’s move on and take a closer look at our data by looking at the first few rows:\n\n# View the first 5 rows\nworld_cup.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      0\n      Saad Al-Sheeb\n      GK\n      32.00\n      Qatar\n      Doha\n      80\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      1\n      Meshaal Barsham\n      GK\n      24.00\n      Qatar\n      Doha\n      15\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      2\n      Yousuf Hassan\n      GK\n      26.00\n      Qatar\n      Doha\n      9\n      A\n      Al-Gharafa\n      Qatar\n      Doha\n      Home\n    \n    \n      3\n      Pedro Miguel\n      DEF\n      32.00\n      Qatar\n      Doha\n      78\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      4\n      Musaab Khidir\n      DEF\n      29.00\n      Qatar\n      Doha\n      29\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n  \n\n\n\n\n….and the last few rows:\n\n# View the last 5 rows\nworld_cup.tail()\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      825\n      Kamaldeen Sulemana\n      MID\n      20.00\n      Ghana\n      Accra\n      11\n      H\n      Stade Rennes\n      France\n      Paris\n      Away\n    \n    \n      826\n      Antoine Semenyo\n      FWD\n      22.00\n      Ghana\n      Accra\n      1\n      H\n      Bristol City\n      England\n      London\n      Away\n    \n    \n      827\n      Andre Ayew\n      FWD\n      32.00\n      Ghana\n      Accra\n      107\n      H\n      Al Sadd\n      Qatar\n      Doha\n      Away\n    \n    \n      828\n      Jordan Ayew\n      FWD\n      31.00\n      Ghana\n      Accra\n      82\n      H\n      Crystal Palace\n      England\n      London\n      Away\n    \n    \n      829\n      Inaki Williams\n      FWD\n      28.00\n      Ghana\n      Accra\n      1\n      H\n      Athletic Club\n      Spain\n      Madrid\n      Away\n    \n  \n\n\n\n\nOK, so we can see that we have some basic information about each player:\n- Name\n- Position (GK = Goalkeeper DEF = Defender MID = Midfielder FWD = Forward)\n- Age\n- Country they represent\n- Ctry_cap - capital of country they represent\n- Caps (Number of matches played for their country)\n- Group (32 teams divided into 8 groups A-H of 4)\n- Club (the team that pays the player's wages!)\n- Club_ctry (the location of the player's domestic team)\n- Club_ctry_cap (capital of their club country)\n- Home_Away - where they play their club football\n\n\n\n\nimport numpy as np\n\nworld_cup.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      Age\n      Caps\n    \n  \n  \n    \n      count\n      830.00\n      830.00\n    \n    \n      mean\n      26.80\n      33.87\n    \n    \n      std\n      4.59\n      33.81\n    \n    \n      min\n      0.00\n      0.00\n    \n    \n      25%\n      24.00\n      8.00\n    \n    \n      50%\n      27.00\n      23.00\n    \n    \n      75%\n      30.00\n      47.00\n    \n    \n      max\n      45.00\n      191.00\n    \n  \n\n\n\n\n\n\n\n\n\n\nIt turned out of course that Alan was wrong - Manchester United went on to win the English Premier league that season. There is no master recipe for success it seems at these tournaments. Some managers like to lean on the old guard, some like to throw the gauntlet down and give the kids a chance. Let’s take a look at the age profile of the players using a histogram:\n\nworld_cup ['Age'].hist();\n\n\n\n\nSomething doesn’t look right here, some ages between 0 and 5! Let’s look into this. We can use .loc to access a group of rows and columns by name or .iloc to access by index:\n\nworld_cup.loc[world_cup['Age'] <5 ]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      34\n      Diego Palacios\n      DEF\n      2.00\n      Ecuador\n      Quito\n      11\n      A\n      LAFC\n      USA\n      Washington, DC\n      Away\n    \n    \n      520\n      Ahmed Reda Tagnaouti\n      GK\n      3.00\n      Morocco\n      Rabat\n      3\n      F\n      Wydad Casablanca\n      Morocco\n      Rabat\n      Home\n    \n    \n      642\n      Fabian Rieder\n      MID\n      0.00\n      Switzerland\n      Berne\n      0\n      G\n      Young Boys\n      Switzerland\n      Berne\n      Home\n    \n  \n\n\n\n\nOn follow up, Diego Palacios Fabian Redier is 23, Ahmed Reda Tagnaouti is 26, and Fabian Rieder is 20 years old. We can correct these errors using .iat. The values we wish to update are located at rows 34, 520 and 642 of column 2 - watch out, indexing starts at 0 in Python!\n\n# Update Diego Palacios age\nworld_cup.iat[34,2]=23\n\n# Update Ahmed Reda Tagnaouti age\nworld_cup.iat[520,2]=26\n\n# Update Fabian Rieder age\nworld_cup.iat[642,2]=20\n\nLet’s check that’s worked:\n\nworld_cup.loc[34:34]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      34\n      Diego Palacios\n      DEF\n      23.00\n      Ecuador\n      Quito\n      11\n      A\n      LAFC\n      USA\n      Washington, DC\n      Away\n    \n  \n\n\n\n\n\nworld_cup.loc[520:520]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      520\n      Ahmed Reda Tagnaouti\n      GK\n      26.00\n      Morocco\n      Rabat\n      3\n      F\n      Wydad Casablanca\n      Morocco\n      Rabat\n      Home\n    \n  \n\n\n\n\n\nworld_cup.loc[642:642]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      642\n      Fabian Rieder\n      MID\n      20.00\n      Switzerland\n      Berne\n      0\n      G\n      Young Boys\n      Switzerland\n      Berne\n      Home\n    \n  \n\n\n\n\nGreat, the ages have been successfully updated. Let’s now take a look at the age profile of each of the 32 squads:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Average age of squad\"\n\nx = world_cup.groupby('Country')['Age'].mean().sort_values()\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Average age of squad')\n\n\n\n\n\n\nave_age = world_cup.groupby(\"Country\")[\"Age\"].mean()\nave_age.sort_values(ascending=False)\n\nCountry\nIran           29.08\nMexico         28.58\nTunisia        27.92\nBrazil         27.86\nBelgium        27.77\nSouth Korea    27.77\nUruguay        27.73\nJapan          27.69\nArgentina      27.69\nSaudi Arabia   27.38\nCroatia        27.31\nAustralia      27.23\nCosta Rica     27.15\nPoland         27.00\nCanada         26.92\nDenmark        26.92\nQatar          26.92\nPortugal       26.77\nSerbia         26.77\nSwitzerland    26.73\nGermany        26.69\nNetherlands    26.58\nEngland        26.35\nWales          26.23\nMorocco        26.19\nCameroon       26.19\nFrance         26.12\nSenegal        26.04\nEcuador        25.54\nSpain          25.31\nUSA            25.00\nGhana          24.73\nName: Age, dtype: float64\n\n\nSo Ghana and Ecaudor have the youngest squads (average age 24.73) whilst Iran has the oldest, with an average age of 29.08.\n\n\n\n\n\n\nThe players below will be looking at the board and hoping for a large number. Players are looking after themselves more and more, extending their playing careers, but realistically, for the players below, this is possibly their last opportunity to appear in a World Cup. So make sure to see catch them while you can!\n\nmature = world_cup[world_cup['Age'] > 33]\nmature_sorted = mature.sort_values(by=\"Age\",ascending = False)\nmature_sorted.head(10)\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      143\n      Ramin Rezaeian\n      DEF\n      45.00\n      Iran\n      Tehran\n      2\n      B\n      Sepahan\n      Iran\n      Tehran\n      Home\n    \n    \n      261\n      Alfredo Talavera\n      GK\n      40.00\n      Mexico\n      Mexico City\n      40\n      C\n      FC Juarez\n      Mexico\n      Mexico City\n      Home\n    \n    \n      440\n      Eiji Kawashima\n      GK\n      39.00\n      Japan\n      Tokyo\n      95\n      E\n      Strasbourg\n      France\n      Paris\n      Away\n    \n    \n      781\n      Pepe\n      DEF\n      39.00\n      Portugal\n      Lisbon\n      128\n      H\n      Porto\n      Portugal\n      Lisbon\n      Home\n    \n    \n      608\n      Atiba Hutchinson\n      MID\n      39.00\n      Canada\n      Ottawa\n      98\n      F\n      Besiktas\n      Turkey\n      Istanbul\n      Away\n    \n    \n      682\n      Dani Alves\n      DEF\n      39.00\n      Brazil\n      Brasilia\n      125\n      G\n      Pumas UNAM\n      Mexico\n      Mexico City\n      Away\n    \n    \n      679\n      Thiago Silva\n      DEF\n      38.00\n      Brazil\n      Brasilia\n      108\n      G\n      Chelsea\n      England\n      London\n      Away\n    \n    \n      78\n      Remko Pasveer\n      GK\n      38.00\n      Netherlands\n      Amsterdam\n      2\n      A\n      Ajax\n      Netherlands\n      Amsterdam\n      Home\n    \n    \n      340\n      Aymen Mathlouthi\n      GK\n      38.00\n      Tunisia\n      Tunis\n      73\n      D\n      Etoile du Sahel\n      Tunisia\n      Tunis\n      Home\n    \n    \n      365\n      Steve Mandanda\n      GK\n      37.00\n      France\n      Paris\n      34\n      D\n      Rennes\n      France\n      Paris\n      Home\n    \n  \n\n\n\n\n\n\n\n\nnew = world_cup[world_cup['Age'] < 20]\nnew_sorted = new.sort_values(by=\"Age\",ascending = True)\nnew_sorted.head(10)\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      435\n      Youssoufa Moukoko\n      FWD\n      17.00\n      Germany\n      Berlin\n      1\n      E\n      Borussia Dortmund\n      Germany\n      Berlin\n      Home\n    \n    \n      820\n      Fatawu Issahaku\n      MID\n      18.00\n      Ghana\n      Accra\n      11\n      H\n      Sporting\n      Portugal\n      Lisbon\n      Away\n    \n    \n      577\n      Zeno Debast\n      DEF\n      18.00\n      Belgium\n      Brussels\n      3\n      F\n      Anderlecht\n      Belgium\n      Brussels\n      Home\n    \n    \n      531\n      Bilal El Khannouss\n      MID\n      18.00\n      Morocco\n      Rabat\n      0\n      F\n      Racing Genk\n      Belgium\n      Brussels\n      Away\n    \n    \n      505\n      Jewison Bennette\n      MID\n      18.00\n      Costa Rica\n      San Jose\n      7\n      E\n      Sunderland\n      England\n      London\n      Away\n    \n    \n      412\n      Garang Kuol\n      FWD\n      18.00\n      Australia\n      Canberra\n      1\n      D\n      Central Coast Mariners\n      Australia\n      Canberra\n      Home\n    \n    \n      478\n      Gavi\n      MID\n      18.00\n      Spain\n      Madrid\n      13\n      E\n      Barcelona\n      Spain\n      Madrid\n      Home\n    \n    \n      784\n      Antonio Silva\n      DEF\n      19.00\n      Portugal\n      Lisbon\n      0\n      H\n      Benfica\n      Portugal\n      Lisbon\n      Home\n    \n    \n      648\n      Simon Ngapandouetnbu\n      GK\n      19.00\n      Cameroon\n      Yaounde\n      0\n      G\n      Marseille\n      France\n      Paris\n      Away\n    \n    \n      504\n      Brandon Aguilera\n      MID\n      19.00\n      Costa Rica\n      San Jose\n      4\n      E\n      Guanacasteca\n      Costa Rica\n      San Jose\n      Home\n    \n  \n\n\n\n\nSo it looks like the youngest player at the tournament is Youssoufa Moukoko of Germany at just 17.\n\n\n\n\n\n\nexperience.jpg\n\n\nAlthough in some sports physical caps may not now always be given (whether at all or for each appearance) the term cap for an international or other appearance has been retained as an indicator of the number of occasions on which a sportsperson has represented a team in a particular sport. Thus, a “cap” is awarded for each game played and so a player who has played x games for the team is said to have been capped x times or have won x caps.\nLet’s first look at the distribution of the number of caps received going into this tournament, using a histogram:\n\nworld_cup ['Caps'].hist();\n\n\n\n\nAs we can see the majority of players are relatively inexperienced, with less than around 40 appearances, although there are some very experienced players, with over 150 caps. Let’s have a look at who they are:\n\nworld_cup.loc[world_cup['Caps'] > 150]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      20\n      Hassan Al-Haydos\n      MID\n      31.00\n      Qatar\n      Doha\n      160\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n    \n    \n      232\n      Lionel Messi\n      FWD\n      35.00\n      Argentina\n      Buenos Aires\n      165\n      C\n      PSG\n      France\n      Paris\n      Away\n    \n    \n      272\n      Andres Guardado\n      MID\n      36.00\n      Mexico\n      Mexico City\n      180\n      C\n      Real Betis\n      Spain\n      Madrid\n      Away\n    \n    \n      506\n      Celso Borges\n      MID\n      34.00\n      Costa Rica\n      San Jose\n      154\n      E\n      Alajuelense\n      Costa Rica\n      San Jose\n      Home\n    \n    \n      558\n      Luka Modric\n      MID\n      37.00\n      Croatia\n      Zagreb\n      155\n      F\n      Real Madrid\n      Spain\n      Madrid\n      Away\n    \n    \n      733\n      Diego Godin\n      DEF\n      36.00\n      Uruguay\n      Montevideo\n      159\n      G\n      Velez Sarsfield\n      Argentina\n      Buenos Aires\n      Away\n    \n    \n      798\n      Cristiano Ronaldo\n      FWD\n      37.00\n      Portugal\n      Lisbon\n      191\n      H\n      Manchester United\n      England\n      London\n      Away\n    \n  \n\n\n\n\nCristiano Ronaldo is the most capped player at the tournament with 191. The 200 mark is in sight, although at 37 maybe it’s time to make way for some new blood?\n\n\n\n\n\n\nThe minimum number of caps shown is 0 which means there are players at this tournament who have yet to play for their country - the stage has been set! Let’s find out who they are:\n\nworld_cup.loc[world_cup['Caps'] == 0]\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      12\n      Jassim Jabir\n      MID\n      20.00\n      Qatar\n      Doha\n      0\n      A\n      Al-Arabi\n      Qatar\n      Doha\n      Home\n    \n    \n      33\n      William Pacho\n      DEF\n      21.00\n      Ecuador\n      Quito\n      0\n      A\n      Royal Antwerp\n      Belgium\n      Brussels\n      Away\n    \n    \n      50\n      Kevin Rodriguez\n      FWD\n      22.00\n      Ecuador\n      Quito\n      0\n      A\n      Imbabura SC\n      Ecuador\n      Quito\n      Home\n    \n    \n      61\n      Moussa Ndiaye\n      DEF\n      20.00\n      Senegal\n      Dakar\n      0\n      A\n      Anderlecht\n      Belgium\n      Brussels\n      Away\n    \n    \n      70\n      Pathe Ciss\n      MID\n      28.00\n      Senegal\n      Dakar\n      0\n      A\n      Rayo Vallecano\n      Spain\n      Madrid\n      Away\n    \n    \n      79\n      Andries Noppert\n      GK\n      28.00\n      Netherlands\n      Amsterdam\n      0\n      A\n      Heerenveen\n      Netherlands\n      Amsterdam\n      Home\n    \n    \n      83\n      Jeremie Frimpong\n      DEF\n      21.00\n      Netherlands\n      Amsterdam\n      0\n      A\n      Bayer Leverkusen\n      Germany\n      Berlin\n      Away\n    \n    \n      92\n      Xavi Simons\n      MID\n      19.00\n      Netherlands\n      Amsterdam\n      0\n      A\n      PSV Eindhoven\n      Netherlands\n      Amsterdam\n      Home\n    \n    \n      234\n      Nawaf Al-Aqidi\n      GK\n      22.00\n      Saudi Arabia\n      Riyadh\n      0\n      C\n      Al-Nassr FC\n      Saudi Arabia\n      Riyadh\n      Home\n    \n    \n      235\n      Mohamed Al-Yami\n      GK\n      25.00\n      Saudi Arabia\n      Riyadh\n      0\n      C\n      Al-Ahli Saudi FC\n      Saudi Arabia\n      Riyadh\n      Home\n    \n    \n      366\n      Axel Disasi\n      DEF\n      24.00\n      France\n      Paris\n      0\n      D\n      Monaco\n      France\n      Paris\n      Home\n    \n    \n      474\n      Alejandro Balde\n      DEF\n      19.00\n      Spain\n      Madrid\n      0\n      E\n      Barcelona\n      Spain\n      Madrid\n      Home\n    \n    \n      531\n      Bilal El Khannouss\n      MID\n      18.00\n      Morocco\n      Rabat\n      0\n      F\n      Racing Genk\n      Belgium\n      Brussels\n      Away\n    \n    \n      597\n      James Pantemis\n      GK\n      25.00\n      Canada\n      Ottawa\n      0\n      F\n      CF Montreal\n      Canada\n      Ottawa\n      Home\n    \n    \n      624\n      Philipp Kohn\n      GK\n      24.00\n      Switzerland\n      Berne\n      0\n      G\n      RB Salzburg\n      Austria\n      Vienna\n      Away\n    \n    \n      642\n      Fabian Rieder\n      MID\n      20.00\n      Switzerland\n      Berne\n      0\n      G\n      Young Boys\n      Switzerland\n      Berne\n      Home\n    \n    \n      648\n      Simon Ngapandouetnbu\n      GK\n      19.00\n      Cameroon\n      Yaounde\n      0\n      G\n      Marseille\n      France\n      Paris\n      Away\n    \n    \n      779\n      Jose Sa\n      GK\n      29.00\n      Portugal\n      Lisbon\n      0\n      H\n      Wolves\n      England\n      London\n      Away\n    \n    \n      784\n      Antonio Silva\n      DEF\n      19.00\n      Portugal\n      Lisbon\n      0\n      H\n      Benfica\n      Portugal\n      Lisbon\n      Home\n    \n    \n      802\n      Goncalo Ramos\n      FWD\n      21.00\n      Portugal\n      Lisbon\n      0\n      H\n      Benfica\n      Portugal\n      Lisbon\n      Home\n    \n    \n      806\n      Ibrahim Danlad\n      GK\n      19.00\n      Ghana\n      Accra\n      0\n      H\n      Asante Kotoko\n      Ghana\n      Accra\n      Home\n    \n    \n      823\n      Salis Abdul Samed\n      MID\n      22.00\n      Ghana\n      Accra\n      0\n      H\n      Lens\n      France\n      Paris\n      Away\n    \n    \n      824\n      Kamal Sowah\n      MID\n      22.00\n      Ghana\n      Accra\n      0\n      H\n      Club Brugge\n      Belgium\n      Brussels\n      Away\n    \n  \n\n\n\n\nAs expected, these players are generally quite young (although Portugal’s Jose Sa is 29 - better late than never) or goalkeepers, where the first choice tends to be difficult to oust! Keep an eye out for these names - they might be the stars of the future.\nLet’s take a look at the average number of caps for each squad:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Average number of caps per squad\"\n\nx = world_cup.groupby('Country')['Caps'].mean().sort_values()\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Average number of caps per squad')\n\n\n\n\n\n\ncaps = world_cup.groupby(\"Country\")[\"Caps\"].mean()\ncaps.sort_values(ascending=False)\n\nCountry\nQatar          53.46\nBelgium        52.19\nMexico         51.12\nUruguay        45.85\nCosta Rica     43.42\nPortugal       40.42\nSwitzerland    38.58\nWales          37.96\nTunisia        37.88\nCroatia        37.65\nIran           36.72\nDenmark        36.65\nBrazil         36.31\nSouth Korea    35.46\nGermany        35.19\nJapan          35.15\nArgentina      34.12\nPoland         33.88\nFrance         31.80\nEngland        31.54\nCanada         31.46\nSerbia         29.77\nSaudi Arabia   28.50\nSpain          28.42\nNetherlands    26.23\nCameroon       24.73\nUSA            24.69\nEcuador        23.73\nAustralia      22.42\nSenegal        21.62\nMorocco        20.04\nGhana          17.00\nName: Caps, dtype: float64\n\n\nThe host nation Quatar have the most experienced squad with an average of 53.46 international apperances per player. The least experienced squad is Ghana, with an average of 17.\nTo caveat this, it is worth noting that qualification for the World Cup is segregated by region, and there can be a wide disparity between the number of qualifying matches played. This can result in some nations playing a large number of matches, without necessarily playing in a major tournament, which is perhaps a better indicator of experience.\n\n\n\nHere’s how we get a quick summary of all the non-numeric columns in the dataset:\n\nworld_cup.describe(include=[object])\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Country\n      Ctry_cap\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      count\n      830\n      830\n      830\n      830\n      830\n      830\n      830\n      830\n      830\n    \n    \n      unique\n      830\n      4\n      32\n      32\n      8\n      446\n      43\n      43\n      2\n    \n    \n      top\n      Saad Al-Sheeb\n      DEF\n      Qatar\n      Doha\n      G\n      Al-Sadd\n      England\n      London\n      Away\n    \n    \n      freq\n      1\n      274\n      26\n      26\n      130\n      13\n      159\n      159\n      551\n    \n  \n\n\n\n\n\ncols = ['Position', 'Group', 'Home_Away']\nworld_cup[cols] = world_cup[cols].astype('category')\n\n\nworld_cup[['Name','Country','Ctry_cap','Club','Club_ctry','Club_ctry_cap']] = world_cup[['Name','Country','Ctry_cap','Club','Club_ctry','Club_ctry_cap']].astype(str)\n\n\n\n\nFirst of all let’s look at where these 830 players play their club football:\n\nimport matplotlib.pyplot as plt\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Number of players playing in....\"\n\nx = world_cup.groupby('Club_ctry')['Club'].count().sort_values(ascending=True).tail(10)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=16)\n\nText(0.5, 1.0, 'Number of players playing in....')\n\n\n\n\n\n\nworld_cup.groupby(\"Club_ctry\")[\"Club\"].count().sort_values(ascending=False).head(10)\n\nClub_ctry\nEngland         159\nSpain            86\nGermany          80\nItaly            67\nFrance           58\nSaudi Arabia     34\nQatar            33\nUSA              27\nBelgium          25\nMexico           23\nName: Club, dtype: int64\n\n\nSo out of 830 players represented at the World Cup, 159 play their football in England. Let’s take a look at the top 10 clubs with the most players playing at this tournament:\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Number of players playing for..\"\n\nx = world_cup.groupby(\"Club\")[\"Club_ctry\"].count().sort_values(ascending=True).tail(10)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=16)\n\nText(0.5, 1.0, 'Number of players playing for..')\n\n\n\n\n\nAl-Sadd, wo play in the Qatar Stars league have 13 players (all playing for Qatar) at the tournament, closely followed by Barcelona with 12, Munich and Manchester City, with 11 and Manchester United having 10. Saudi Arabia’s squad all play within Saudi Arabia, 7 of them for Al_Hilal.\n\n\n\n\n\n\nSadly, my country didn’t make it, but there are some players who play domestically in Scotland who will be in Qatar representing their country. Let’s have a look and see who they are:\n\nscotland = world_cup[world_cup['Club_ctry'] == 'Scotland']\nscotland\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      159\n      Cameron Carter-Vickers\n      DEF\n      24.00\n      USA\n      Washington, DC\n      10\n      B\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      200\n      Dylan Levitt\n      MID\n      21.00\n      Wales\n      Cardiff\n      13\n      B\n      Dundee United\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      392\n      Aziz Behich\n      DEF\n      31.00\n      Australia\n      Canberra\n      53\n      D\n      Dundee United\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      394\n      Nathaniel Atkinson\n      DEF\n      23.00\n      Australia\n      Canberra\n      5\n      D\n      Hearts\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      397\n      Kye Rowles\n      DEF\n      24.00\n      Australia\n      Canberra\n      3\n      D\n      Hearts\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      400\n      Aaron Mooy\n      MID\n      32.00\n      Australia\n      Canberra\n      53\n      D\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      403\n      Keanu Baccus\n      MID\n      29.00\n      Australia\n      Canberra\n      53\n      D\n      St Mirren\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      404\n      Cameron Devlin\n      MID\n      24.00\n      Australia\n      Canberra\n      1\n      D\n      Hearts\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      464\n      Daizen Maeda\n      FWD\n      25.00\n      Japan\n      Tokyo\n      8\n      E\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      548\n      Borna Barisic\n      DEF\n      29.00\n      Croatia\n      Zagreb\n      28\n      F\n      Rangers\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      554\n      Josip Juranovic\n      DEF\n      27.00\n      Croatia\n      Zagreb\n      21\n      F\n      Celtic\n      Scotland\n      Edinburgh\n      Away\n    \n    \n      614\n      David Wotherspoon\n      MID\n      32.00\n      Canada\n      Ottawa\n      10\n      F\n      St. Johnstone\n      Scotland\n      Edinburgh\n      Away\n    \n  \n\n\n\n\nInterestingly, out of the 12 players who play their club football in Scotland, 6 are Australian. The shared language is probably a contributory factor, certainly not the search for warmer weather.\n\n\n\n\n\n\nHaving recently relocated here, at least I now have a team to follow! Let’s have a look at the players who play their club football in Poland:\n\npoland_club = world_cup[world_cup['Club_ctry'] == 'Poland']\npoland_club\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      295\n      Artur Jedrzejczyk\n      DEF\n      34.00\n      Poland\n      Warsaw\n      40\n      C\n      Legia Warsaw\n      Poland\n      Warsaw\n      Home\n    \n    \n      297\n      Michal Skoras\n      MID\n      22.00\n      Poland\n      Warsaw\n      1\n      C\n      Lech Poznan\n      Poland\n      Warsaw\n      Home\n    \n    \n      301\n      Kamil Grosicki\n      MID\n      34.00\n      Poland\n      Warsaw\n      87\n      C\n      Pogon Szczecin\n      Poland\n      Warsaw\n      Home\n    \n    \n      706\n      Filip Mladenovic\n      DEF\n      31.00\n      Serbia\n      Belgrade\n      20\n      G\n      Legia Warsaw\n      Poland\n      Warsaw\n      Away\n    \n  \n\n\n\n\nOnly 4 players, 3 of which are Polish. The lone soldier is Filip Mladenovic of Serbia who plays his club football with Legia Warsaw. Let’s have a look at the Poland squad in general:\n\npoland = world_cup[world_cup['Country'] == 'Poland']\npoland\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n    \n  \n  \n    \n      285\n      Wojciech Szczesny\n      GK\n      32.00\n      Poland\n      Warsaw\n      66\n      C\n      Juventus\n      Italy\n      Rome\n      Away\n    \n    \n      286\n      Lukasz Skorupski\n      GK\n      31.00\n      Poland\n      Warsaw\n      8\n      C\n      Bologna\n      Italy\n      Rome\n      Away\n    \n    \n      287\n      Kamil Grabara\n      GK\n      23.00\n      Poland\n      Warsaw\n      1\n      C\n      Copenhagen\n      Denmark\n      Copenhagen\n      Away\n    \n    \n      288\n      Jan Bednarek\n      DEF\n      26.00\n      Poland\n      Warsaw\n      45\n      C\n      Aston Villa\n      England\n      London\n      Away\n    \n    \n      289\n      Kamil Glik\n      DEF\n      34.00\n      Poland\n      Warsaw\n      99\n      C\n      Benevento\n      Italy\n      Rome\n      Away\n    \n    \n      290\n      Matty Cash\n      DEF\n      25.00\n      Poland\n      Warsaw\n      7\n      C\n      Aston Villa\n      England\n      London\n      Away\n    \n    \n      291\n      Jakub Kiwior\n      DEF\n      22.00\n      Poland\n      Warsaw\n      3\n      C\n      Spezia\n      Italy\n      Rome\n      Away\n    \n    \n      292\n      Robert Gumny\n      DEF\n      24.00\n      Poland\n      Warsaw\n      5\n      C\n      FC Augsburg\n      Germany\n      Berlin\n      Away\n    \n    \n      293\n      Bartosz Bereszynski\n      DEF\n      30.00\n      Poland\n      Warsaw\n      46\n      C\n      Sampdoria\n      Italy\n      Rome\n      Away\n    \n    \n      294\n      Mateusz Wieteska\n      DEF\n      25.00\n      Poland\n      Warsaw\n      2\n      C\n      Clermont\n      France\n      Paris\n      Away\n    \n    \n      295\n      Artur Jedrzejczyk\n      DEF\n      34.00\n      Poland\n      Warsaw\n      40\n      C\n      Legia Warsaw\n      Poland\n      Warsaw\n      Home\n    \n    \n      296\n      Nicola Zalewski\n      MID\n      20.00\n      Poland\n      Warsaw\n      7\n      C\n      Roma\n      Italy\n      Rome\n      Away\n    \n    \n      297\n      Michal Skoras\n      MID\n      22.00\n      Poland\n      Warsaw\n      1\n      C\n      Lech Poznan\n      Poland\n      Warsaw\n      Home\n    \n    \n      298\n      Grzegorz Krychowiak\n      MID\n      32.00\n      Poland\n      Warsaw\n      94\n      C\n      Al Shabab\n      Saudi Arabia\n      Riyadh\n      Away\n    \n    \n      299\n      Piotr Zielinski\n      MID\n      28.00\n      Poland\n      Warsaw\n      74\n      C\n      Napoli\n      Italy\n      Rome\n      Away\n    \n    \n      300\n      Krystian Bielik\n      MID\n      24.00\n      Poland\n      Warsaw\n      5\n      C\n      Birmingham\n      England\n      London\n      Away\n    \n    \n      301\n      Kamil Grosicki\n      MID\n      34.00\n      Poland\n      Warsaw\n      87\n      C\n      Pogon Szczecin\n      Poland\n      Warsaw\n      Home\n    \n    \n      302\n      Przemyslaw Frankowski\n      MID\n      27.00\n      Poland\n      Warsaw\n      26\n      C\n      Lens\n      France\n      Paris\n      Away\n    \n    \n      303\n      Sebastian Szymanski\n      MID\n      23.00\n      Poland\n      Warsaw\n      18\n      C\n      Feyenoord\n      Netherlands\n      Amsterdam\n      Away\n    \n    \n      304\n      Damian Szymanski\n      MID\n      27.00\n      Poland\n      Warsaw\n      9\n      C\n      AEK Athens\n      Greece\n      Athens\n      Away\n    \n    \n      305\n      Szymon Zurkowski\n      MID\n      25.00\n      Poland\n      Warsaw\n      7\n      C\n      Fiorentina\n      Italy\n      Rome\n      Away\n    \n    \n      306\n      Jakub Kaminski\n      MID\n      20.00\n      Poland\n      Warsaw\n      4\n      C\n      Wolfsburg\n      Germany\n      Berlin\n      Away\n    \n    \n      307\n      Krzysztof Piatek\n      FWD\n      27.00\n      Poland\n      Warsaw\n      11\n      C\n      Salernitana\n      Italy\n      Rome\n      Away\n    \n    \n      308\n      Karol Swiderski\n      FWD\n      25.00\n      Poland\n      Warsaw\n      18\n      C\n      Charlotte\n      USA\n      Washington, DC\n      Away\n    \n    \n      309\n      Arkadiusz Milik\n      FWD\n      28.00\n      Poland\n      Warsaw\n      64\n      C\n      Juventus\n      Italy\n      Rome\n      Away\n    \n    \n      310\n      Robert Lewandowski\n      FWD\n      34.00\n      Poland\n      Warsaw\n      134\n      C\n      Barcelona\n      Spain\n      Madrid\n      Away\n    \n  \n\n\n\n\n\n\n\n\n\n\nhome and away.jpg\n\n\nThese days players travel far and wide to ply their trade. I wondered what the impact of that might be on the tightness of a squad, and thought it would be interesting to take a closer look at where players play domestically.\nLet’s take a look at the distribution of where the players play their club football:\n\nworld_cup['Home_Away'].value_counts()\n\nAway    551\nHome    279\nName: Home_Away, dtype: int64\n\n\nMost players play their club football outside of their home nation. Let’s illustrate that graphically with a bar plot:\n\nhome_away_plot = world_cup.groupby(['Home_Away', 'Country']).size().sort_values(ascending=False).reset_index().pivot(columns='Home_Away', index='Country', values=0)\nhome_away_plot.plot(kind='barh', stacked=True)\n\n<AxesSubplot: ylabel='Country'>\n\n\n\n\n\nThat’s quite insightful and re-emphasises that most players do play their club football outside of their home nation, exemplified at the extreme by Senegal, where the entire squad are based outside of Senegal. At the other extreme, all of the Qatar and Saudi Arabia squads are based at home. England are the next ‘tightest’ squad. Will this contribute to a successful tournament? All will be revealed over the next few weeks!\n\n\n\n\n\n\nhuddle.jpg\n\n\nWe have already established that the total distance for Saudi Arabia and Qatar is zero (their squads all play club football locally), but let’s try to establish just how far flung the other squads are, by calculating the distance for each player from their nation’s capital to the capital of the country where they play their club football.\nApologies for this next section which is not very Pythonic! In hindsight this was probably a tad ambitious for me as someone just starting out, and I ran into all sorts of obstacles, but I got there in the end, and thankfully before the tournament ended!\n\n\nWe can obtain the co-ordinates of the capital cities of the countries where the players play their club football using the Geopy library in Python. There is a very useful guide available here:\n\n## Install required package\n!pip install geopy\n\n# import required module\nfrom geopy.geocoders import Nominatim\n\nRequirement already satisfied: geopy in /home/stephen137/mambaforge/lib/python3.10/site-packages (2.2.0)\nRequirement already satisfied: geographiclib<2,>=1.49 in /home/stephen137/mambaforge/lib/python3.10/site-packages (from geopy) (1.52)\n\n\n\ncountries = ('England', 'Spain', 'Germany', 'Italy', 'France', 'Saudi Arabia', 'Qatar', 'United States', 'Belgium', 'Mexico', 'Turkey', 'Netherlands', 'Portugal', 'Costa Rica', 'South Korea', 'Greece', 'Scotland', 'Japan',             \n'Canada', 'Switzerland', 'Iran',  'Denmark',  'Tunisia',  'Australia',  'Croatia' , 'Brazil',  'Argentina',  'Wales', 'Poland',  'Morocco',  'Ecuador', 'Austria', 'Serbia', 'Uruguay',  'Kuwait', 'Russia',  'Ghana', 'Egypt',                                    \n'Cyprus', 'China', 'Cameroon', 'Colombia',  'United Arab Emirates', 'Senegal') \n\nFirst, we can create a list of capital cities for the above countries, and then create a function to loop through this list, and extract the longitude and latitude for each of the cities:\n\n# List of capitals\ncapitals = ['London', 'Madrid', 'Berlin', 'Rome', 'Paris', 'Riyadh', 'Doha', 'Washington, DC', 'Brussels', 'Mexico City', 'Ankara', 'Amsterdam', 'Lisbon', 'San Jose', 'Seoul', 'Athens', 'Edinburgh', 'Tokyo', \n'Ottawa', 'Berne', 'Tehran', 'Copenhagen', 'Tunis', 'Canberra', 'Zagreb', 'Brasilia', 'Buenos Aires', 'Cardiff', 'Warsaw', 'Rabat', 'Quito', 'Vienna', 'Belgrade', 'Montevideo', 'Kuwait City', 'Moscow', 'Accra', 'Cairo',\n'Nicosia', 'Beijing', 'Yaounde', 'Bogota','Abu Dhabi','Dakar']\n\ngeolocator = Nominatim(user_agent=\"GetLoc\")\n\n# loop through list of capitals and return their co-ordinates\nfor capital in capitals:\n    location = geolocator.geocode(capital)\n    lat = location.latitude\n    long = location.longitude \n    print(lat,long)\n\n51.5073219 -0.1276474\n40.4167047 -3.7035825\n52.5170365 13.3888599\n41.8933203 12.4829321\n48.8588897 2.3200410217200766\n24.638916 46.7160104\n25.2856329 51.5264162\n38.8950368 -77.0365427\n50.8465573 4.351697\n19.4326296 -99.1331785\n39.9207886 32.8540482\n52.3727598 4.8936041\n38.7077507 -9.1365919\n37.3361663 -121.890591\n37.5666791 126.9782914\n37.9839412 23.7283052\n55.9533456 -3.1883749\n35.6828387 139.7594549\n45.4208777 -75.6901106\n46.9482713 7.4514512\n35.6892523 51.3896004\n55.6867243 12.5700724\n33.8439408 9.400138\n-35.2975906 149.1012676\n45.84264135 15.962231476593626\n-10.3333333 -53.2\n-34.6075682 -58.4370894\n51.4816546 -3.1791934\n52.2337172 21.071432235636493\n34.022405 -6.834543\n-0.2201641 -78.5123274\n48.2083537 16.3725042\n44.8178131 20.4568974\n-34.9058916 -56.1913095\n29.3796532 47.9734174\n55.7504461 37.6174943\n5.5571096 -0.2012376\n30.0443879 31.2357257\n35.1748976 33.3638568\n39.906217 116.3912757\n3.8689867 11.5213344\n4.6534649 -74.0836453\n24.4538352 54.3774014\n14.693425 -17.447938\n\n\nNow create a list which combines the capitals and their co-ordinates:\n\nco_ordinates = [[\"London\", 51.5073219, -0.1276474],\n['Madrid', 40.4167047, -3.7035825],\n['Berlin', 52.5170365, 13.3888599],\n['Rome',  41.8933203, 12.4829321],\n['Paris', 48.8588897, 2.3200410217200766],\n['Riyadh', 24.638916, 46.7160104],\n['Doha', 25.2856329, 51.5264162],\n['Washington, DC', 38.8950368, -77.0365427],\n['Brussels', 50.8465573, 4.351697],\n['Mexico City', 19.4326296, -99.1331785],\n['Ankara', 39.9207886, 32.8540482],\n['Amsterdam', 52.3727598, 4.8936041],\n['Lisbon', 38.7077507, -9.1365919],\n['San Jose', 37.3361663, -121.890591],\n['Seoul', 37.5666791, 126.9782914],\n['Athens', 37.9839412, 23.7283052],\n['Edinburgh', 55.9533456, -3.1883749],\n['Tokyo', 35.6828387, 139.7594549],\n['Ottawa', 45.4208777, -75.6901106],\n['Berne', 46.9482713, 7.4514512],\n['Tehran', 35.6892523, 51.3896004],\n['Copenhagen', 55.6867243, 12.5700724],\n['Tunis', 33.8439408, 9.400138],\n['Canberra', 35.2975906, 149.1012676],\n['Zagreb', 45.84264135, 15.962231476593626],\n['Brasilia', -10.3333333, -53.2],\n['Buenos Aires', -34.6075682, -58.4370894],\n['Cardiff', 51.4816546, -3.1791934],\n['Warsaw', 52.2337172, 21.071432235636493],\n['Rabat', 34.022405, -6.834543],\n['Quito', -0.2201641, -78.5123274],\n['Vienna', 48.2083537, 16.3725042],\n['Belgrade', 44.8178131, 20.4568974],\n['Montevideo', -34.9058916, -56.1913095],\n['Kuwait City', 29.3796532, 47.9734174],\n['Moscow', 55.7504461, 37.6174943],\n['Accra', 5.5571096, -0.2012376],\n['Cairo', 30.0443879, 31.2357257],\n['Nicosia', 35.1748976, 33.3638568],\n['Beijing', 39.906217, 116.3912757],\n['Yaounde', 3.8689867, 11.5213344],\n['Bogota', 4.6534649, -74.0836453],\n['Abu Dhabi', 24.4538352, 54.3774014],\n['Dakar', 14.693425, -17.447938]]    \n\nAnd now create separate DataFrames for our ‘from’ and ‘to’ destinations:\n\nfrom_co_ord = pd.DataFrame(co_ordinates,columns=['from', 'from_long', 'from_lat'])\nto_co_ord = pd.DataFrame(co_ordinates,columns=['to', 'to_long', 'to_lat'])\n\n\nfrom_co_ord.head()\n\n\n\n\n\n  \n    \n      \n      from\n      from_long\n      from_lat\n    \n  \n  \n    \n      0\n      London\n      51.51\n      -0.13\n    \n    \n      1\n      Madrid\n      40.42\n      -3.70\n    \n    \n      2\n      Berlin\n      52.52\n      13.39\n    \n    \n      3\n      Rome\n      41.89\n      12.48\n    \n    \n      4\n      Paris\n      48.86\n      2.32\n    \n  \n\n\n\n\n\nto_co_ord.head()\n\n\n\n\n\n  \n    \n      \n      to\n      to_long\n      to_lat\n    \n  \n  \n    \n      0\n      London\n      51.51\n      -0.13\n    \n    \n      1\n      Madrid\n      40.42\n      -3.70\n    \n    \n      2\n      Berlin\n      52.52\n      13.39\n    \n    \n      3\n      Rome\n      41.89\n      12.48\n    \n    \n      4\n      Paris\n      48.86\n      2.32\n    \n  \n\n\n\n\n\n\n\nWe need to find all the possible ‘from’ : ‘to’ combinations in order to calculate the distances between the cities. We have 44 cities which gives according to this handy calculator , 946 pairings without repetitions.\nAfter some digging around I found this post on stackoverflow which gave a general overview of how this might be achieved. In order to obtain the pairings and index them, we can use the MultiIndex.from_product pandas class:\n\nidx = pd.MultiIndex.from_product([from_co_ord.index, to_co_ord.index], names=['from', 'to'])\n\n\n# create a combined DataFrame that joins our from and to DataFrames\n# Includes all possible pairings (including duplicates)\n\nfrom_to = pd.DataFrame(index=idx) \\\n        .join(from_co_ord[['from','from_lat', 'from_long']], on='from') \\\n        .join(to_co_ord[['to','to_lat', 'to_long']], on='to')\n\n\nfrom_to\n\n\n\n\n\n  \n    \n      \n      \n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n    \n    \n      from\n      to\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      London\n      -0.13\n      51.51\n      London\n      -0.13\n      51.51\n    \n    \n      1\n      London\n      -0.13\n      51.51\n      Madrid\n      -3.70\n      40.42\n    \n    \n      2\n      London\n      -0.13\n      51.51\n      Berlin\n      13.39\n      52.52\n    \n    \n      3\n      London\n      -0.13\n      51.51\n      Rome\n      12.48\n      41.89\n    \n    \n      4\n      London\n      -0.13\n      51.51\n      Paris\n      2.32\n      48.86\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      39\n      Dakar\n      -17.45\n      14.69\n      Beijing\n      116.39\n      39.91\n    \n    \n      40\n      Dakar\n      -17.45\n      14.69\n      Yaounde\n      11.52\n      3.87\n    \n    \n      41\n      Dakar\n      -17.45\n      14.69\n      Bogota\n      -74.08\n      4.65\n    \n    \n      42\n      Dakar\n      -17.45\n      14.69\n      Abu Dhabi\n      54.38\n      24.45\n    \n    \n      43\n      Dakar\n      -17.45\n      14.69\n      Dakar\n      -17.45\n      14.69\n    \n  \n\n1936 rows × 6 columns\n\n\n\nThat’s not quite as concise as we would like - we have 1936 pairings. That’s because MultiIndex.from_prodcut has returned values for London to London, Dakar to Dakar, etc. We also have London to Madrid, and Madrid to London which is also duplication. Let’s move forward.\n\n\n\n\n\n\nproclaimers.jpg\n\n\nWe can calculate the distance between two locations using Haversine. Let’s create a function that allows us to return values for all our pairings:\n\ndef haversine_np(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees)\n\n    All args must be of equal length.    \n\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    miles = km * 0.621371\n    return miles\n\n# Add a new column to our from_to DataFrame to include the distance calculations\nfrom_to['Distance_miles'] = haversine_np(*from_to[['from_lat', 'from_long', 'to_lat', 'to_long']].values.T)\n\nLet’s take a look at our completed distances DataFrame:\n\nfrom_to\n\n\n\n\n\n  \n    \n      \n      \n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n      Distance_miles\n    \n    \n      from\n      to\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      London\n      -0.13\n      51.51\n      London\n      -0.13\n      51.51\n      0.00\n    \n    \n      1\n      London\n      -0.13\n      51.51\n      Madrid\n      -3.70\n      40.42\n      784.56\n    \n    \n      2\n      London\n      -0.13\n      51.51\n      Berlin\n      13.39\n      52.52\n      577.80\n    \n    \n      3\n      London\n      -0.13\n      51.51\n      Rome\n      12.48\n      41.89\n      890.44\n    \n    \n      4\n      London\n      -0.13\n      51.51\n      Paris\n      2.32\n      48.86\n      212.47\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      39\n      Dakar\n      -17.45\n      14.69\n      Beijing\n      116.39\n      39.91\n      7634.29\n    \n    \n      40\n      Dakar\n      -17.45\n      14.69\n      Yaounde\n      11.52\n      3.87\n      2107.38\n    \n    \n      41\n      Dakar\n      -17.45\n      14.69\n      Bogota\n      -74.08\n      4.65\n      3906.72\n    \n    \n      42\n      Dakar\n      -17.45\n      14.69\n      Abu Dhabi\n      54.38\n      24.45\n      4673.86\n    \n    \n      43\n      Dakar\n      -17.45\n      14.69\n      Dakar\n      -17.45\n      14.69\n      0.00\n    \n  \n\n1936 rows × 7 columns\n\n\n\n\n\n\n\nWe now want to include the distance figures in our orginal world_cup DataFrame to allow us to calculate the total distance per squad, which will give us some sort of comaprison of the ‘closeness’ of the 32 teams. I am familiar with this feature in Excel but you can do the same thing in Python. Here is a useful article on how to do it.\nLet’s first get our DataFrames tee’d up by adding a common column for joining on:\n\nfrom_to[\"from_to\"] = from_to['from'] + \" to \" + from_to['to']\nfrom_to\n\n\n\n\n\n  \n    \n      \n      \n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n      Distance_miles\n      from_to\n    \n    \n      from\n      to\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      London\n      -0.13\n      51.51\n      London\n      -0.13\n      51.51\n      0.00\n      London to London\n    \n    \n      1\n      London\n      -0.13\n      51.51\n      Madrid\n      -3.70\n      40.42\n      784.56\n      London to Madrid\n    \n    \n      2\n      London\n      -0.13\n      51.51\n      Berlin\n      13.39\n      52.52\n      577.80\n      London to Berlin\n    \n    \n      3\n      London\n      -0.13\n      51.51\n      Rome\n      12.48\n      41.89\n      890.44\n      London to Rome\n    \n    \n      4\n      London\n      -0.13\n      51.51\n      Paris\n      2.32\n      48.86\n      212.47\n      London to Paris\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43\n      39\n      Dakar\n      -17.45\n      14.69\n      Beijing\n      116.39\n      39.91\n      7634.29\n      Dakar to Beijing\n    \n    \n      40\n      Dakar\n      -17.45\n      14.69\n      Yaounde\n      11.52\n      3.87\n      2107.38\n      Dakar to Yaounde\n    \n    \n      41\n      Dakar\n      -17.45\n      14.69\n      Bogota\n      -74.08\n      4.65\n      3906.72\n      Dakar to Bogota\n    \n    \n      42\n      Dakar\n      -17.45\n      14.69\n      Abu Dhabi\n      54.38\n      24.45\n      4673.86\n      Dakar to Abu Dhabi\n    \n    \n      43\n      Dakar\n      -17.45\n      14.69\n      Dakar\n      -17.45\n      14.69\n      0.00\n      Dakar to Dakar\n    \n  \n\n1936 rows × 8 columns\n\n\n\n\nworld_cup[\"from_to\"] = world_cup['Ctry_cap'] + \" to \" + world_cup['Club_ctry_cap']\nworld_cup\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n      from_to\n    \n  \n  \n    \n      0\n      Saad Al-Sheeb\n      GK\n      32.00\n      Qatar\n      Doha\n      80\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      1\n      Meshaal Barsham\n      GK\n      24.00\n      Qatar\n      Doha\n      15\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      2\n      Yousuf Hassan\n      GK\n      26.00\n      Qatar\n      Doha\n      9\n      A\n      Al-Gharafa\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      3\n      Pedro Miguel\n      DEF\n      32.00\n      Qatar\n      Doha\n      78\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      4\n      Musaab Khidir\n      DEF\n      29.00\n      Qatar\n      Doha\n      29\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      825\n      Kamaldeen Sulemana\n      MID\n      20.00\n      Ghana\n      Accra\n      11\n      H\n      Stade Rennes\n      France\n      Paris\n      Away\n      Accra to Paris\n    \n    \n      826\n      Antoine Semenyo\n      FWD\n      22.00\n      Ghana\n      Accra\n      1\n      H\n      Bristol City\n      England\n      London\n      Away\n      Accra to London\n    \n    \n      827\n      Andre Ayew\n      FWD\n      32.00\n      Ghana\n      Accra\n      107\n      H\n      Al Sadd\n      Qatar\n      Doha\n      Away\n      Accra to Doha\n    \n    \n      828\n      Jordan Ayew\n      FWD\n      31.00\n      Ghana\n      Accra\n      82\n      H\n      Crystal Palace\n      England\n      London\n      Away\n      Accra to London\n    \n    \n      829\n      Inaki Williams\n      FWD\n      28.00\n      Ghana\n      Accra\n      1\n      H\n      Athletic Club\n      Spain\n      Madrid\n      Away\n      Accra to Madrid\n    \n  \n\n830 rows × 12 columns\n\n\n\nFinally we can join our two DataFrames together which will give us a distance column and value for each player in the tournament:\n\ninner_join = pd.merge(world_cup, from_to, on='from_to',how='inner')\n\n\ninner_join\n\n\n\n\n\n  \n    \n      \n      Name\n      Position\n      Age\n      Country\n      Ctry_cap\n      Caps\n      Group\n      Club\n      Club_ctry\n      Club_ctry_cap\n      Home_Away\n      from_to\n      from\n      from_lat\n      from_long\n      to\n      to_lat\n      to_long\n      Distance_miles\n    \n  \n  \n    \n      0\n      Saad Al-Sheeb\n      GK\n      32.00\n      Qatar\n      Doha\n      80\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      1\n      Meshaal Barsham\n      GK\n      24.00\n      Qatar\n      Doha\n      15\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      2\n      Yousuf Hassan\n      GK\n      26.00\n      Qatar\n      Doha\n      9\n      A\n      Al-Gharafa\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      3\n      Pedro Miguel\n      DEF\n      32.00\n      Qatar\n      Doha\n      78\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      4\n      Musaab Khidir\n      DEF\n      29.00\n      Qatar\n      Doha\n      29\n      A\n      Al-Sadd\n      Qatar\n      Doha\n      Home\n      Doha to Doha\n      Doha\n      51.53\n      25.29\n      Doha\n      51.53\n      25.29\n      0.00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      805\n      Mohammed Kudus\n      MID\n      22.00\n      Ghana\n      Accra\n      16\n      H\n      Ajax\n      Netherlands\n      Amsterdam\n      Away\n      Accra to Amsterdam\n      Accra\n      -0.20\n      5.56\n      Amsterdam\n      4.89\n      52.37\n      3245.62\n    \n    \n      806\n      Daniel Kofi-Kyereh\n      MID\n      26.00\n      Ghana\n      Accra\n      12\n      H\n      Freiburg\n      Germany\n      Berlin\n      Away\n      Accra to Berlin\n      Accra\n      -0.20\n      5.56\n      Berlin\n      13.39\n      52.52\n      3333.41\n    \n    \n      807\n      Fatawu Issahaku\n      MID\n      18.00\n      Ghana\n      Accra\n      11\n      H\n      Sporting\n      Portugal\n      Lisbon\n      Away\n      Accra to Lisbon\n      Accra\n      -0.20\n      5.56\n      Lisbon\n      -9.14\n      38.71\n      2356.37\n    \n    \n      808\n      Osman Bukari\n      MID\n      23.00\n      Ghana\n      Accra\n      5\n      H\n      Red Star Belgrade\n      Serbia\n      Belgrade\n      Away\n      Accra to Belgrade\n      Accra\n      -0.20\n      5.56\n      Belgrade\n      20.46\n      44.82\n      2983.47\n    \n    \n      809\n      Andre Ayew\n      FWD\n      32.00\n      Ghana\n      Accra\n      107\n      H\n      Al Sadd\n      Qatar\n      Doha\n      Away\n      Accra to Doha\n      Accra\n      -0.20\n      5.56\n      Doha\n      51.53\n      25.29\n      3674.63\n    \n  \n\n810 rows × 19 columns\n\n\n\n\ntitle_string = \"FIFA World Cup - Qatar 2022\"\nsubtitle_string = \"Total miles travelled by squad - from their club country capital to their nation capital\"\n\nx = inner_join.groupby('Country')['Distance_miles'].sum().sort_values(ascending=False)\nplt.figure()\nx.plot(kind='barh')\nplt.suptitle(title_string, y=1.05, fontsize=18)\nplt.title(subtitle_string, fontsize=10)\n\nText(0.5, 1.0, 'Total miles travelled by squad - from their club country capital to their nation capital')\n\n\n\n\n\n\ninner_join.groupby(\"Country\")[\"Distance_miles\"].sum().sort_values(ascending=True)\n\nCountry\nSaudi Arabia        0.00\nQatar               0.00\nEngland           577.80\nGermany          4631.29\nSpain            6389.33\nNetherlands      6658.89\nWales            7674.01\nBelgium          7824.11\nFrance           9663.44\nSwitzerland     12648.83\nCroatia         13322.72\nDenmark         15194.21\nPortugal        18366.13\nSerbia          19948.33\nPoland          23557.92\nTunisia         25922.17\nIran            26752.28\nMorocco         27042.30\nCosta Rica      37651.80\nCanada          47922.17\nSouth Korea     53858.03\nMexico          55687.73\nUSA             61448.54\nSenegal         66198.44\nGhana           73052.50\nCameroon        78656.09\nEcuador         80700.85\nUruguay         93702.58\nAustralia       98441.33\nJapan          112062.53\nBrazil         118045.02\nArgentina      162925.56\nName: Distance_miles, dtype: float64\n\n\n\n\n\n\n\n\nHome advantage often counts. If you recall South Korea reached the semi final when they hosted the tournament back in 2002, and if you look back even further to 1966 then… Let’s end that there.\nWill this togetherness give Qatar an advantage? Time will tell. They also have the ‘tightest’ squad (along with Saudi Arabia) in terms of the fact that all of their players play their club football at home. Outside of those two, England are the ‘closest’ squad, with only Jude Bellingham (Borussia Dortmund) playing his club football outside of England.\nThe Argentina squad are the most scattered, followed by Brazil, Argentina, and Japan. This makes sense, as most of their squads play in Europe, which is a long way from home!\n\n\n\nThis has been a rewarding project overall for me. I achieved what I set out to do, which began with a vague idea of the ‘tightness’ of the World Cup squads, and how I might quantify this, perhaps by looking at where the players play their club football, and how far they would have to travel to begin preparations back in their home nation.\nThere were many obstacles along the way, the main one was working out how to calculate the distance between two points, and this gave me a first introduction to working with geospatial data, including the Geopy library and Haversine. I also managed to create one or two functions to automate the extraction of co-ordinate data and distance calculations and found about one of the more advanced panda classes, MultiIndex.from_product.\nI was familiar with the VLOOKUP fuction in Excel, and the various join clauses in SQL, but I now know how to achieve the same end result using pandas merge.\nI’m looking forward to seeing how this Tournament unfolds - may the best team win!"
  },
  {
    "objectID": "posts/DE_Zoomcamp_Week_1/DE_Zoomcamp_Week_1.html",
    "href": "posts/DE_Zoomcamp_Week_1/DE_Zoomcamp_Week_1.html",
    "title": "Data Engineering Zoomcamp - Week 1",
    "section": "",
    "text": "This course will cover a number of technologies, including Google Cloud Platform (GCP): Cloud-based auto-scaling platform by Google, Google Cloud Storage (GCS): Data Lake, BigQuery: Data Warehouse, Terraform: Infrastructure-as-Code (IaC), Docker: Containerization, SQL: Data Analysis & Exploration, Prefect: Workflow Orchestration, dbt: Data Transformation, Spark: Distributed Processing, Kafka: Streaming.\nAn overview of the course architecture is included below:\n\n\n\nArchitecture.png\n\n\nThe first thing to do is to navigate to the course github page and clone the course repo to your local machine :\n\n\n\nclone_repository.PNG\n\n\nby running the following command in your terminal. I have a Windows machine but have a Linux environment installed, and use the Ubuntu on Windows terminal.\ngit clone https://github.com/DataTalksClub/data-engineering-zoomcamp.git\nI have a nifty terminal splitter called tmux which allows me to have multiple terminals running - which comes in very handy sometimes!!! There’s a handy cheatsheet here but basically CTRL+B is the way in to the magic :)\n\n\n\ntmux.PNG\n\n\nOther things we can do from the command line:\nls (lists all files in the directory)\nexit (self explanatory)\nrm -rf / (remove all files from directory)\nWe can use sudo to execute commands where we don’t have the necessary permissions.\n\n\n\n\n\nWhat_is_GCP.PNG\n\n\n\n\n\nGCP.PNG\n\n\n\n\n\nGCP_dashboard.PNG\n\n\n\n\n\nGCP_cloud_storage.PNG\n\n\n\n\n\nGCP_search_bar.PNG\n\n\n\n\n\nWhat is Docker? Why do we need it?\nDocker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.\nYou can find out more from the source documentation.\n\nLocal experiments\nIntegration tests (Continuous Integraton (CI) / Continuous Development (CD)) - Github Actions, Jenkins\nReproducibility (isolated CONTAINER) ensures environment on local machine can be directly replicated ANYWHERE\nRunning pipelines on the cloud (AWS Batch, Kubernetes jobs)\nSpark\nServerless (AWS Lambda, [Google] Cloud functions)\n\nLet’s illustrate how Docker works with a simple example. First let’s create a Dockerfile within Visual Studio Code (VSC). VSC can be accessed from the command line using :\ncode .\n\n\n\ndockerfile.JPG\n\n\nLet’s first build an image - we’ll name it test:pandas by running the following command from within the terminal:\ndocker build -t test:pandas . # this searches in the current directory for a dockerfile and creates an image\nand run the image using :\ndocker run -it test:pandas \nThis takes us to a bash command prompt, as this is our entrypoint as defined in our Dockerfile. We can then open up Python, import pandas and check which version we have:\npython\nimport pandas as pd\npd.__version__    \nThe key point about images is that they are as the name suggests a snapshot of all file dependencies at the specific point in time that they are created.\nWe can automate things further by creating a data pipeline.py file and configuring our Dockerfile to include this :\n\n\n\npipeline.JPG\n\n\n\n\n\ndockerfile_2.JPG\n\n\nLet’s run the following commands in the terminal :\ndocker build -t test:pandas .\ndocker run -it test:pandas\n\n\nroot@f009333fb3e5:/app# pwd (`pwd` takes us to the CURRENT directory)\n/app   \nWe can see that this is /app as specified in WORKDIR in our Dockefile above. Finally, if we run our pipeline.py file:\npython pipeline.py\nwe get the following output:\njob finished successfully\nThis was the final item in our pipeline.py file :)\nFine tuning our Docker container\nLet’s fine tune the configuration of our pipleline a bit more prior to scheduling a run:\n\n\n\npipeline_2.JPG\n\n\nWe first rebuild using our usual command:\ndocker build -t test:pandas .\nSay we want to schedule the run for a particular today - for illustrative purposes let’s use today’s date. We define day as system argument number 1 (argument 0 is the file name). We then pass in that argument (today’s date - 2023-03-05) in our command line prompt:\ndocker run -it test:pandas 2023-03-05\nAnd we get the following output:\n[‘pipeline.py’, ‘2023-03-05’]\njob finished successfully for day = 2023-03-05\nThe items inside [ ] are the arguments - number 0 is the file name pipeline.py, number 1 is the date as configured in our pipepline. We can include further arguments within our command line prompt e.g :\ndocker run -it test:pandas 2023-02-09 Incoming_137_new_album!\nThis returns the following output:\n[‘pipeline.py’, ‘2023-02-09’, ‘Incoming.’, ‘137’, ‘new’, ‘album!’] job finished successfully for day = 2023-02-09\nThe additonal arguments specified are listed as we included\nprint(sys.arg)\nin our pipeline.py file\n\n\n\nDownloading our datasets\nLet’s now go ahead and download the datasets that we will be working with over the next few weeks. We can do this from the command line using :\nwget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz \n\nwget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\nWe can unzip the .gz file and retain the original using :\ngzip -dk <file_name.csv.gz>\nWe can count number of lines using wc (word count) -l (lines) :\nwc -l <file_name>     \nWe can look at say the first 100 rows:\nhead -n 100 <file_name>\nWe can then save this subset to csv using :\nhead -n 100 yellow_tripdata_2021-01.csv > yellow_head.csv\n \nWe can copy a file to the current directory using :\ncp ~ </existing/file/path> .     \nWe can look at a text data file from the command line using:\nless <file_name>  \nand exit the terminal using CTRL + Z\nExplore our dataset\nLet’s now take a look at our data within Jupter Notebooks using pandas. We will only carry out limited pre-processing at this stage - the focus is to demonstrate how to take a csv file and ingest it to a database.\n\nimport pandas as pd\n\n\npd.__version__\n\n'1.5.2'\n\n\n\ndf = pd.read_csv('Data/yellow_tripdata_2021-01.csv')\ndf\n\n/tmp/ipykernel_194/843851997.py:1: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('Data/yellow_tripdata_2021-01.csv')\n\n\n\n\n\n\n  \n    \n      \n      VendorID\n      tpep_pickup_datetime\n      tpep_dropoff_datetime\n      passenger_count\n      trip_distance\n      RatecodeID\n      store_and_fwd_flag\n      PULocationID\n      DOLocationID\n      payment_type\n      fare_amount\n      extra\n      mta_tax\n      tip_amount\n      tolls_amount\n      improvement_surcharge\n      total_amount\n      congestion_surcharge\n    \n  \n  \n    \n      0\n      1.0\n      2021-01-01 00:30:10\n      2021-01-01 00:36:12\n      1.0\n      2.10\n      1.0\n      N\n      142\n      43\n      2.0\n      8.00\n      3.00\n      0.5\n      0.00\n      0.0\n      0.3\n      11.80\n      2.5\n    \n    \n      1\n      1.0\n      2021-01-01 00:51:20\n      2021-01-01 00:52:19\n      1.0\n      0.20\n      1.0\n      N\n      238\n      151\n      2.0\n      3.00\n      0.50\n      0.5\n      0.00\n      0.0\n      0.3\n      4.30\n      0.0\n    \n    \n      2\n      1.0\n      2021-01-01 00:43:30\n      2021-01-01 01:11:06\n      1.0\n      14.70\n      1.0\n      N\n      132\n      165\n      1.0\n      42.00\n      0.50\n      0.5\n      8.65\n      0.0\n      0.3\n      51.95\n      0.0\n    \n    \n      3\n      1.0\n      2021-01-01 00:15:48\n      2021-01-01 00:31:01\n      0.0\n      10.60\n      1.0\n      N\n      138\n      132\n      1.0\n      29.00\n      0.50\n      0.5\n      6.05\n      0.0\n      0.3\n      36.35\n      0.0\n    \n    \n      4\n      2.0\n      2021-01-01 00:31:49\n      2021-01-01 00:48:21\n      1.0\n      4.94\n      1.0\n      N\n      68\n      33\n      1.0\n      16.50\n      0.50\n      0.5\n      4.06\n      0.0\n      0.3\n      24.36\n      2.5\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1369760\n      NaN\n      2021-01-25 08:32:04\n      2021-01-25 08:49:32\n      NaN\n      8.80\n      NaN\n      NaN\n      135\n      82\n      NaN\n      21.84\n      2.75\n      0.5\n      0.00\n      0.0\n      0.3\n      25.39\n      0.0\n    \n    \n      1369761\n      NaN\n      2021-01-25 08:34:00\n      2021-01-25 09:04:00\n      NaN\n      5.86\n      NaN\n      NaN\n      42\n      161\n      NaN\n      26.67\n      2.75\n      0.5\n      0.00\n      0.0\n      0.3\n      30.22\n      0.0\n    \n    \n      1369762\n      NaN\n      2021-01-25 08:37:00\n      2021-01-25 08:53:00\n      NaN\n      4.45\n      NaN\n      NaN\n      14\n      106\n      NaN\n      25.29\n      2.75\n      0.5\n      0.00\n      0.0\n      0.3\n      28.84\n      0.0\n    \n    \n      1369763\n      NaN\n      2021-01-25 08:28:00\n      2021-01-25 08:50:00\n      NaN\n      10.04\n      NaN\n      NaN\n      175\n      216\n      NaN\n      28.24\n      2.75\n      0.5\n      0.00\n      0.0\n      0.3\n      31.79\n      0.0\n    \n    \n      1369764\n      NaN\n      2021-01-25 08:38:00\n      2021-01-25 08:50:00\n      NaN\n      4.93\n      NaN\n      NaN\n      248\n      168\n      NaN\n      20.76\n      2.75\n      0.5\n      0.00\n      0.0\n      0.3\n      24.31\n      0.0\n    \n  \n\n1369765 rows × 18 columns\n\n\n\n\ndf.dtypes\n\nVendorID                 float64\ntpep_pickup_datetime      object\ntpep_dropoff_datetime     object\npassenger_count          float64\ntrip_distance            float64\nRatecodeID               float64\nstore_and_fwd_flag        object\nPULocationID               int64\nDOLocationID               int64\npayment_type             float64\nfare_amount              float64\nextra                    float64\nmta_tax                  float64\ntip_amount               float64\ntolls_amount             float64\nimprovement_surcharge    float64\ntotal_amount             float64\ncongestion_surcharge     float64\ndtype: object\n\n\nGenerate table Schema\nTo generate a schema for use within postgreSQL there is a module within pandas named io to convert to Data Definition Language (DDL) :\n\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data')) # name of Table\n\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" REAL,\n  \"tpep_pickup_datetime\" TEXT,\n  \"tpep_dropoff_datetime\" TEXT,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" REAL,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL\n)\n\n\nWe can see immmediately that pick up and drop off datatype is TEXT but needs to be converted (parsed) to datetime. We can do this using pandas to_datetime:\n\npd.to_datetime(df.tpep_pickup_datetime)\n\n0         2021-01-01 00:30:10\n1         2021-01-01 00:51:20\n2         2021-01-01 00:43:30\n3         2021-01-01 00:15:48\n4         2021-01-01 00:31:49\n                  ...        \n1369760   2021-01-25 08:32:04\n1369761   2021-01-25 08:34:00\n1369762   2021-01-25 08:37:00\n1369763   2021-01-25 08:28:00\n1369764   2021-01-25 08:38:00\nName: tpep_pickup_datetime, Length: 1369765, dtype: datetime64[ns]\n\n\n\npd.to_datetime(df.tpep_dropoff_datetime)\n\n0         2021-01-01 00:36:12\n1         2021-01-01 00:52:19\n2         2021-01-01 01:11:06\n3         2021-01-01 00:31:01\n4         2021-01-01 00:48:21\n                  ...        \n1369760   2021-01-25 08:49:32\n1369761   2021-01-25 09:04:00\n1369762   2021-01-25 08:53:00\n1369763   2021-01-25 08:50:00\n1369764   2021-01-25 08:50:00\nName: tpep_dropoff_datetime, Length: 1369765, dtype: datetime64[ns]\n\n\nWe now need to update our dataframe:\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\n\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data')) # name of Table\n\nCREATE TABLE \"yellow_taxi_data\" (\n\"VendorID\" REAL,\n  \"tpep_pickup_datetime\" TIMESTAMP,\n  \"tpep_dropoff_datetime\" TIMESTAMP,\n  \"passenger_count\" REAL,\n  \"trip_distance\" REAL,\n  \"RatecodeID\" REAL,\n  \"store_and_fwd_flag\" TEXT,\n  \"PULocationID\" INTEGER,\n  \"DOLocationID\" INTEGER,\n  \"payment_type\" REAL,\n  \"fare_amount\" REAL,\n  \"extra\" REAL,\n  \"mta_tax\" REAL,\n  \"tip_amount\" REAL,\n  \"tolls_amount\" REAL,\n  \"improvement_surcharge\" REAL,\n  \"total_amount\" REAL,\n  \"congestion_surcharge\" REAL\n)\n\n\nsqlalchemy\nNote that we have successfully updated our pick up and drop off to Timestamp. Simply copying and pasting the above might work but we need to create the above statement in a way that postgreSQL will understand for sure. For that we need to tell pandas that we want to put this into postgres. For this we can use sqlalchemy.\n\nfrom sqlalchemy import create_engine\n\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n\n\nprint(pd.io.sql.get_schema(df, name='yellow_taxi_data',con=engine)) \n\nOur dataframe has 1.3m + rows, so it is prudent to break this down into batches for passing into postgreSQL :\n\ndf_iter = pd.read_csv('Data/yellow_tripdata_2021-01.csv', iterator=True, chunksize=100000)\n\n\ndf = next(df_iter)\nlen(df)\n\nWe want to set up the data headers first and then insert the data in chunks later :\n\n# To get our column names\ndf.head(n=0)\n\nUpload column headers to postgres :\n\ndf.head(n=0).to_sql(name='yellow_taxi_data', \n                    con=engine, \n                    if_exists='replace')         #if a table name with yellow_taxi_data exists then replace\n\nUpload first chunk of 100000 rows to postgres :\n\n%time\ndf.to_sql(name='yellow_taxi_data', \n                    con=engine, \n                    if_exists='append')  \n\n\nfrom time import time\n\nUpload the rest of the dataframe:\n\nwhile True:\n    t_start = time() # returns current timestamp in seconds\n    \n    df = next(df_iter) \n    \n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n    \n    df.to_sql(name='yellow_taxi_data', \n                    con=engine, \n                    if_exists='append')  \n    \n    t_end = time()\n    \n    print ('inserted another chunk, took %.3f second' % (t_end - t_start)) # .3f means to 3 decimal places - the % in the text is a variable defined by % outside the text\n\nThe above error can be ignored - it just means there are no more chunks to add our dataframe has been successfully uploaded in full to postgres.\n\n\n\nPostgreSQL\nPostgreSQL, often simply “Postgres”, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards-compliance. As a database server, its primary function is to store data, securely and supporting best practices, and retrieve it later, as requested by other software applications, be it those on the same computer or those running on another computer across a network (including the Internet). It can handle workloads ranging from small single-machine applications to large Internet-facing applications with many concurrent users. Recent versions also provide replication of the database itself for security and scalability.\nPostgreSQL implements the majority of the SQL:2011 standard, is ACID-compliant and transactional (including most DDL statements) avoiding locking issues using multiversion concurrency control (MVCC), provides immunity to dirty reads and full serializability; handles complex SQL queries using many indexing methods that are not available in other databases; has updateable views and materialized views, triggers, foreign keys; supports functions and stored procedures, and other expandability, and has a large number of extensions written by third parties. In addition to the possibility of working with the major proprietary and open source databases, PostgreSQL supports migration from them, by its extensive standard SQL support and available migration tools. And if proprietary extensions had been used, by its extensibility that can emulate many through some built-in and third-party open source compatibility extensions, such as for Oracle.\nWe can run postgreSQL from the terminal :\n  docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --network=pg-network \\\n  --name pg-database \\\n  postgres:13\nLet’s break this down and explain the configuration :\ndocker run -it    # -it means interactive terminal - allows us to stop it\npostgres:13       # this is our IMAGE   \nConfigure our environment using -e :\n -e POSTGRES_USER=\"root\" \\              # user name\n -e POSTGRES_PASSWORD=\"root\" \\          # password\n -e POSTGRES_DB=\"ny_taxi\" \\             # database name\nConfigure our VOLUME using -v\nNote that because I am using Ununtu I need to map full path of existing directory using $(pwd) :\n -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\nMap a port on our host machine to a port on our CONTAINER using -p :\n -p 5432:5432 \\\nIf you get an error:\n\ninitdb: error: directory “/var/lib/postgresql/data” exists but is not empty\n\nRemove the ny_taxi_postgres_data directory and run the command again.\nIn Visual Studio Code it looks like there are no files in the directory despite a succesful connection to postgres. But the files are actually just hidden - and can be accessed using the sudo command in Ununtu.\nWe can then initiate an interface with PostgreSQL via the command line using :\npgcli -h localhost -p 5432 -u root -d ny_taxi\n-h = host\n-p = port\n-u = user\n-d = database\nand check that our data has been successfully loaded to postgreSQL\n\n\n\npostgres_dataload.JPG\n\n\nWe can then run queries from there:\n\n\n\npostgres_query.JPG\n\n\nhowever we would be better to make use of a GUI tool which provides an improved visualization.\npgAdmin\npgAdmin is a web-based GUI tool used to interact with the Postgres database sessions, both locally and remote servers as well. It can be used to perform any sort of database administration required for a Postgres database. Although this is a GUI and can be installed, we don’t need to - we have Docker!\nFirst we create a network to ensure that pgAdmin can talk to postgreSQL.\n\n\n\ndocker_network.JPG\n\n\nWhere's that confounded bridge\nThe bridge network works as a private network internal to the host so containers on it can communicate. External access is granted by exposing ports to containers. Bridge networks are used when your applications run in standalone containers that need to communicate.\nIn the picture above db and web can communicate with each other on a user created bridge network called mybridge.\nWe can view the current networks running on our machine using:\ndocker network ls\n\n\n\ndocker_network_ls.PNG\n\n\nDocker inspect is a great way to retrieve low-level information on Docker objects. We can pick out any field from the returned JSON in a fairly straightforward manner.\nSo let’s use it to get the IP Address from the 2_docker_sql_pgadmin-1 container using :\n       docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <network ID>\n       \n\n\n\ncontainer_IP.JPG\n\n\nWe can then just pull pgAdmin by running the folllowing from the command line :\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\nWe then go to our browser and type:\nlocalhost8080\nwhich takes us to the pgAdmin loginpage\n\n\n\npgadmin.JPG\n\n\n\n\n\npgadmin_2.JPG\n\n\n\n\n\npgadmin_connection_2.JPG\n\n\n\n\n\npg_admin_connected.JPG\n\n\nAs we can see that has successfully loaded the data to postgres : \n\n\n\n\n\n\nBy default, the container is assigned an IP address for every Docker network it connects to. And each network is created with a default subnet mask, using it as a pool later on to give away the IP addresses. Usually Docker uses the default 172.17. 0.0/16 subnet for container networking.\nNow to better understand it, we will execute a real use case.\nTo illustrate this, we will use a postgreSQL and pgADmin environment, containing 2 Docker Containers, configured in the yaml file below:\nservices:\n  pgdatabase:\n    image: postgres:13\n    environment:\n      - POSTGRES_USER=root\n      - POSTGRES_PASSWORD=root\n      - POSTGRES_DB=ny_taxi\n    volumes:\n      - \"./data/ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\n    ports:\n      - \"5432:5432\"\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    ports:\n      - \"8080:80\"\nNow let’s start up those containers using:\ndocker-compose up -d\nand see the two containers by running the command:\ndocker ps --format \"table {{.ID}}\\t{{.Status}}\\t{{.Names}}\"\n\n\n\ncontainers.JPG\n\n\nNext let’s check our Docker network using:\ndocker network ls\n\n\n\nnew_network.JPG\n\n\nThere’s a new network called 2_docker_sql_default. By default docker compose sets up a single network for your app. And your app’s network is given a name based on the “project name”, originated from the name of the directory it lives in. So since our directory is named 2_docker_sql, this explains the new network.\nNext some examples on how to get the Docker IP Address.\nHow to Get A Docker Container IP Address\ndocker network inspect -f '{{range .IPAM.Config}}{{.Subnet}}{{end}}' b7be6c0c20e1\n\n\n\nnetwork_inspect.JPG\n\n\nWe don’t need to look up each Container’s IP individually:\ndocker network inspect -f '{{json .Containers}}' b7be6c0c20e1 | jq '.[] | .Name + \":\" + .IPv4Address'\nNote that we used jq help to parse the Containers map object which you may need to install.\n\n\n\nnetworked_containers_IP.JPG\n\n\nSo we can see the IP addresses of our containers :\n\npgdatabase-1 172.20.0.3 clear\npgadmin-1 172.20.0.2\n\nThis could prove useful when mapping our database container to pgadmin.\n\n\n\nWe can introduce further automation by creating a python data ingest script which:\n\ndownloads the data\ndoes some basic pre-processing\nuploads the data in batches to postgresql\n\ningest_data.py\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport argparse\n\nfrom time import time\n\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\n\ndef main(params):\n    user = params.user\n    password = params.password\n    host = params.host \n    port = params.port \n    db = params.db\n    table_name = params.table_name\n    url = params.url\n\n    # the backup files are gzipped, and it's important to keep the correct extension\n    # for pandas to be able to open the file\n    if url.endswith('.csv.gz'):\n        csv_name = 'output.csv.gz'\n    else:\n        csv_name = 'output.csv'\n\nos.system(f\"wget {url} -O {csv_name}\")\n\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n\ndf_iter = pd.read_csv(csv_name, iterator=True, chunksize=100000)\n\ndf = next(df_iter)\n\ndf.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\ndf.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\ndf.head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n\ndf.to_sql(name=table_name, con=engine, if_exists='append')\n\n\nwhile True: \n\n    try:\n        t_start = time()\n        \n        df = next(df_iter)\n\n        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\n        df.to_sql(name=table_name, con=engine, if_exists='append')\n\n        t_end = time()\n\n        print('inserted another chunk, took %.3f second' % (t_end - t_start))\n\n    except StopIteration:\n        print(\"Finished ingesting data into the postgres database\")\n        break\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Ingest CSV data to Postgres')\n\n    parser.add_argument('--user', required=True, help='user name for postgres')\n    parser.add_argument('--password', required=True, help='password for postgres')\n    parser.add_argument('--host', required=True, help='host for postgres')\n    parser.add_argument('--port', required=True, help='port for postgres')\n    parser.add_argument('--db', required=True, help='database name for postgres')\n    parser.add_argument('--table_name', required=True, help='name of the table where we will write the results to')\n    parser.add_argument('--url', required=True, help='url of the csv file')\n\n    args = parser.parse_args()\n\n    main(args)\nThis command line prompt runs the python data ingest file :\nCreate our network :\ndocker network create pg-network     \nRun Postgres (change the path) :\n  docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --network=pg-network \\\n  --name pg-database \\\n  postgres:13\nRun pgAdmin :\ndocker run -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -p 8080:80 \\\n  dpage/pgadmin4\n\nURL=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\npython ingest_data.py \\\n  --user=root \\\n  --password=root \\\n  --host=localhost \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=yellow_taxi_trips \\\n  --url=${URL}\nUsing docker\nCreate the following Dockerfile :\nFROM python:3.9.1\n\nRUN apt-get install wget    \nRUN pip install pandas sqlalchemy psycopg2  \n\nWORKDIR /app\nCOPY ingest_data.py ingest_data.py\n\nENTRYPOINT [ \"python\", \"ingest_data.py\" ]\nThen run the following command line prompts :\ndocker build -t taxi_ingest:v001 .\nRunning this throws up the following error:\nDocker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.\nThis happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again. A folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this command :\nsudo chmod -R 755 ny_taxi_postgres_data\nNow we can run the ingest_data.py script from the command line :\nURL=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n\ndocker run -it \\\n  --network=pg-network \\\n  taxi_ingest:v001 \\\n    --user=root \\\n    --password=root \\\n    --host=pg-database \\\n    --port=5432 \\\n    --db=ny_taxi \\\n    --table_name=yellow_taxi_trips \\\n\n    --url=${URL}\nTo get all the files on a localhost directory we can run the following command:\npython -m http.server\nTo get the IP address of your computer you can run :\nifconfig\n\n\n\nIn the previous section we:\n\nran postgres\nran pgAdmin\n\nin one network using two docker commands.\nThis works fine but there is a lot of configuration required. We can streamline the process by pooling everything together in one yaml file where we can configure multiple CONTAINERS. We can then run from the command line using docker-compose :\nLet’s try docker-compose from the command line :\n\n\n\ndocker_compose.JPG\n\n\nDocker Compose comes as part of Windows Docker Desktop, but if like me, you are running things in Linux from the Ubuntu command line, then you need to activate the WSL integration:\n\n\n\ndocker_compose_ubuntu.JPG\n\n\nRunning docker-compose now works:\n\n\n\ndocker_compose_wsl_integration.JPG\n\n\nWe can now create our yaml file named docker-compose.yaml :\nservices:\n  pgdatabase:\n    image: postgres:13\n    environment:\n      - POSTGRES_USER=root\n      - POSTGRES_PASSWORD=root\n      - POSTGRES_DB=ny_taxi\n    volumes:\n      - \"./ny_taxi_postgres_data:/var/lib/postgres/data:rw\"\n    ports:\n      - \"5432:5432\"\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    ports:\n      - \"8080:80\"\nEnsure all existing containers, volumes and images are cleared and run using :\ndocker-compose up\n\n\n\ndocker_compose_up.JPG\n\n\nThen we go to localhost 8080 and use the pgAdmin login details configured in the yaml file. Unfortunately the yaml file is not configured to ensure persistent state for pgAdmin, so we have to register a server again.\n\n\n\ndocker_localhost.JPG\n\n\n\n\n\npgdatabase.JPG\n\n\nWe can close the terminal using CTRL + C but then we should also run docker-compose down.\nA better way is to run with docker-compose up -d runs in detached mode which then allows us to bypass CTRL + C and go straight to docker-compose down.\n\n\n\n\ndf_zones = pd.read_csv('Data/taxi_zone_lookup.csv')\n\nTake a look at the first 5 rows :\n\ndf_zones.head(140)\n\n\n\n\n\n  \n    \n      \n      LocationID\n      Borough\n      Zone\n      service_zone\n    \n  \n  \n    \n      0\n      1\n      EWR\n      Newark Airport\n      EWR\n    \n    \n      1\n      2\n      Queens\n      Jamaica Bay\n      Boro Zone\n    \n    \n      2\n      3\n      Bronx\n      Allerton/Pelham Gardens\n      Boro Zone\n    \n    \n      3\n      4\n      Manhattan\n      Alphabet City\n      Yellow Zone\n    \n    \n      4\n      5\n      Staten Island\n      Arden Heights\n      Boro Zone\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      135\n      136\n      Bronx\n      Kingsbridge Heights\n      Boro Zone\n    \n    \n      136\n      137\n      Manhattan\n      Kips Bay\n      Yellow Zone\n    \n    \n      137\n      138\n      Queens\n      LaGuardia Airport\n      Airports\n    \n    \n      138\n      139\n      Queens\n      Laurelton\n      Boro Zone\n    \n    \n      139\n      140\n      Manhattan\n      Lenox Hill East\n      Yellow Zone\n    \n  \n\n140 rows × 4 columns\n\n\n\nUpload to postgres :\n\nfrom sqlalchemy import create_engine\n\n\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n\n\ndf_zones.to_sql(name='zones', con=engine, if_exists='replace')\n\n265\n\n\n\n\n\ntaxi_zones.JPG\n\n\nLet’s now carry out some SQL queries on our tables:\n\nyellow_taxi_trips\nzones\n\n\n\nIt will be useful to join these tables. There are different ways to do this. First let’s look at query which returns specified columns which combine certain information common to both tables - in this case LocationID :\n\n\n\nSQL_join_1.JPG\n\n\nAnother way to construct the query is to explicitly use the JOIN command:\n\n\n\nSQL_join_2.JPG\n\n\nBoth queries are equivalent.\nSay, we wanted to check for pick up or drop off locations which are in one table but not the other:\n\n\n\npick_up.JPG\n\n\n\n\n\ndrop_off.JPG\n\n\nBoth queries return no records so that means that all the records have pick up and drop off locations and all the IDs in the zones table are present in the taxis table. In some cases there might not be fully matching records. In this case we can use other join methods :\nFor illustration purposes let’s remove a LocationID record from our zones table:\n\n\n\ndelete.JPG\n\n\nAnd now when we query records that don’t match :\n\n\n\npulocation_unmatched.JPG\n\n\nWe can use LEFT JOIN which will still return a record even where LocationID is not available :\n\n\n\nleft_join.JPG\n\n\nThere are also RIGHT JOIN and OUTER JOIN statements but these will be covered further in Week 4 .\n\n\n\n\n\n\nday.JPG\n\n\n\n\n\ndate.JPG\n\n\n\n\n\nSay we wanted to find how many records there were for each day. We can build on our date parsing above and use a GROUP BY and ORDER BY query :\n\n\n\ngroup_order.JPG\n\n\nIf we wanted to see the day with the largest number of records we coud order by count:\n\n\n\norder_count.JPG\n\n\nWe can use a variety of aggregation methods. Note that we can use numbers to reference the ordering of GROUP BY :\n\n\n\ngroup_by_number_ref.JPG\n\n\nWe can also include multiple conditions in our ORDER BY clause :\n\n\n\norder_by_multiple.JPG\n\n\n\n\n\nGCP_navigate.JPG\n\n\n\n\n\n\nTerraform is an open-source infrastructure-as-code software tool created by HashiCorp. Users define and provide data center infrastructure using a declarative configuration language known as HashiCorp Configuration Language, or optionally JSON.\nBuild, change, and destroy Google Cloud Platform (GCP) infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time.\nhttps://learn.hashicorp.com/collections/terraform/gcp-get-started\nTerraform can be installed using the following command line prompts in Ubuntu:\nwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg \\\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee\\ /etc/apt/sources.list.d/hashicorp.list\nsudo apt update && sudo apt install terraform\n\n\n\ngcp_service_accounts.JPG\n\n\n\n\n\ngcp_manage_keys.JPG\n\n\n\n\n\ngcp_add_key.JPG\n\n\nThis will download a key in json format\n\n\n\nCloud SDK provides language-specific Cloud Client Libraries supporting each language’s natural conventions and styles. This makes it easier for you to interact with Google Cloud APIs in your language of choice. Client libraries also handle authentication, reduce the amount of necessary boilerplate code, and provide helper functions for pagination of large datasets and asynchronous handling of long-running operations.\nTo check if we have it installed we can run the following prompt at the command line :\ngcloud -v\nI did not have it so need to install:\nhttps://cloud.google.com/sdk/docs/install-sdk#deb\n\n\n\n\nAdd the gcloud CLI distribution URI as a package source. If your distribution supports the signed-by option, run the following command:\necho “deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main” | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\nImport the Google Cloud public key. If your distribution’s apt-key command supports the –keyring argument, run the following command:\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key –keyring /usr/share/keyrings/cloud.google.gpg add -\nUpdate and install the gcloud CLI:\nsudo apt-get update && sudo apt-get install google-cloud-cli\n(Optional) Install any of the additional components.\nRun gcloud init to get started:\ngcloud init\n\n\n\n\ngcloud_init.JPG\n\n\n\nSay yes to the above, and then in your browser, log in to your Google user account when prompted and click Allow to grant permission to access Google Cloud resources. Copy the verification code to the awaiting command line prompt\nAt the command prompt, select a Google Cloud project from the list of projects where you have Owner, Editor or Viewer permissions:\nIf you have the Compute Engine API enabled, gcloud init allows you to choose a default Compute Engine zone:\n\n\n\n\ncompute_engine_api.JPG\n\n\n\n(Optional) To improve the screen reader experience, enable the accessibility/screen_reader property:\ngcloud config set accessibility/screen_reader true\n\nNow we need to export our key credentials from the json file at the command line:\nexport GOOGLE_APPLICATION_CREDENTIALS=$(pwd)/<json_file_name>.json\nFinally, refresh token/session, and verify authentication:\ngcloud auth application-default login\nThen need to login from the browser to Google account once again and Allow and then copy verification code to terminal:\nWe are now going to set up the following infrastructures within Google Cloud Platform (GCP):\n- Google Cloud Storage (GCS): (a bucket in GCP environment where you can store files) Data Lake - raw data in organised fashion \n- Big Query: Data Warehouse\nWe need to grant two additional service permissions:\n\nStorage Admin (the bucket itself) and Storage Object Admin (the objects within the bucket)\nBigQuery Admin\n\n\n\n\ngcp_additional_permissions.JPG\n\n\n\nIn production there would be custom created access parameters, restricting access by certain people to certain files.\n\nWe still require to enable the APIs:\nhttps://console.cloud.google.com/apis/library/iam.googleapis.com\nhttps://console.cloud.google.com/apis/library/iamcredentials.googleapis.com\n\n\n\nNow that we have everything set up within GCP let’s get started with the Terraform config. We need two files :\n\nmain.tf (which references the variables.tf file)\nvariables.tf\n\nmain.tf\nterraform {\n  required_version = \">= 1.0\"\n  backend \"local\" {}  # Can change from \"local\" to \"gcs\" (for google) or \"s3\" (for aws), if you would like to preserve your tf-state online\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n    }\n  }\n}\n\nprovider \"google\" {\n  project = var.project\n  region = var.region\n  // credentials = file(var.credentials)  # Use this if you do not want to set env-var GOOGLE_APPLICATION_CREDENTIALS\n}\n\n# Data Lake Bucket\n# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_bucket\nresource \"google_storage_bucket\" \"data-lake-bucket\" {\n  name          = \"${local.data_lake_bucket}_${var.project}\" # Concatenating DL bucket & Project name for unique naming\n  location      = var.region\n\n  # Optional, but recommended settings:\n  storage_class = var.storage_class\n  uniform_bucket_level_access = true\n\n  versioning {\n    enabled     = true\n  }\n\n      lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      age = 30  // days\n    }\n  }\n\n  force_destroy = true\n}\n\n# DWH\n# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset\nresource \"google_bigquery_dataset\" \"dataset\" {\n  dataset_id = var.BQ_DATASET\n  project    = var.project\n  location   = var.region\nvariables.tf\nlocals {\n  data_lake_bucket = \"dtc_data_lake\"\n}\n\nvariable \"project\" {\n  description = \"data-engineering-377711\"\n}\n\nvariable \"region\" {\n  description = \"Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations\"\n  default = \"europe-west6\"\n  type = string\n}\n\nvariable \"storage_class\" {\n  description = \"Storage class type for your bucket. Check official docs for more info.\"\n  default = \"STANDARD\"\n}\n\nvariable \"BQ_DATASET\" {\n  description = \"BigQuery Dataset that raw data (from GCS) will be written to\"\n  type = string\n  default = \"trips_data_all\"\nOnce we have configured the above Terraform files, there are only a few execution commands which makes it very convenient to work with.\nterraform init: - Initializes & configures the backend, installs plugins/providers, & checks out an existing configuration from a version control\nterraform plan: - Matches/previews local changes against a remote state, and proposes an Execution Plan.\nterraform apply: - Asks for approval to the proposed plan, and applies changes to cloud\n!!!terraform destroy!!!: - Removes your stack from the Cloud\nLet’s initialize state file (.tfstate) from the command line using\nterraform init\n\n\n\nterraform_init.JPG\n\n\nNext, propose an execution plan using\nterraform plan\n\n\n\nterraform_plan.JPG\n\n\nWe need to enter our GCP Project ID at the command prompt to progress.\nNow let’s ask for approval to the proposed plan, and apply the changes to cloud using\nterraform apply\n\n\n\nterraform apply.JPG\n\n\nOnce again, we need to enter our GCP Project ID at the command prompt to progress:\n\n\n\nterraform apply_2.JPG\n\n\n\n\n\nterraform_apply_complete.JPG\n\n\nWe’re all set! Let’s return to our GCP account and confirm that we do now have a data lake bucket :\n\n\n\ngcp_bucket.JPG\n\n\nAnd also check that we have our Big Query :\n\n\n\nbig_query.PNG\n\n\n\n\n\nThe intro video to the course did stress that the set up would take more than a week! I started the set up on my own machine using Ubuntu on Windows 10 but ran into some dependency issues. I then went through the process for setting up a Virtual Machine on Google Cloud, but wasn’t completely comfortable working within that environment so in the end I went full circle and managed to resolve the previous issues I was having. Onwards and upwards! Looking forward to “Week 2” Workflow Orchestration."
  },
  {
    "objectID": "posts/DE_Zoomcamp_Week_4/DE_Zoomcamp_Week_4.html",
    "href": "posts/DE_Zoomcamp_Week_4/DE_Zoomcamp_Week_4.html",
    "title": "Data Engineering Zoomcamp - Week 4",
    "section": "",
    "text": "Goal: Transforming the data previously loaded in to our data warehouse (in my case BigQuery) by building models using a dbt project, testing and deploying those models in a production environment, before finally visualizing the data in Looker.\nNote that I have already ingested the prerequisite datasets to Google Cloud Storage using the Prefect work orchestration solution, and have created the following external tables in BigQuery:\n\nYellow trip data (109,047,518 rows) \nfhv trip data (43,244,696) \nGreen trip data (7,778,101 rows) \n\n\n\n\nweek_4.JPG\n\n\n\n\n\n\n\n\n\ndata_team_roles.PNG\n\n\n\n\n\ntooling.PNG\n\n\n\n\n\n\n\n\nelt_etl.PNG\n\n\n\n\n\nkimball.PNG\n\n\n\n\n\nelements_dimensional_modelling.PNG\n\n\n\n\n\narchitect_dimension_modeling.PNG\n\n\n\n\n\n\n\n\n\nwhat_is_dbt.PNG\n\n\n\n\n\nhow_does_dbt_work.PNG\n\n\n\n\n\nhow_to_use_dbt.PNG\n\n\n\n\n\nhow_are_we_going_to_use_dbt.PNG\n\n\n\n\n\nOfficial documentation\n\n\nIn order to connect we need the service account JSON file generated from bigquery:\n\nOpen the BigQuery credential wizard to create a service account in your taxi project\n\n\n\n\ndbt_bq_1.PNG\n\n\n\n\n\ndbt_bq_2.PNG\n\n\n\nYou can either grant the specific roles the account will need or simply use bq admin, as you’ll be the sole user of both accounts and data.\n\nNote: if you decide to use specific roles instead of BQ Admin, some users reported that they needed to add also viewer role to avoid encountering denied access errors\n\n\n\ndbt_bq_3.PNG\n\n\n\nNow that the service account has been created we need to add and download a JSON key, go to the keys section, navigate to ADD KEY then select Create new key from the dropdown. Select key type JSON and once you click on create it will get inmediately downloaded for you to use.\n\n\n\n\ndbt_bq_4.PNG\n\n\n\n\n\n\n\nCreate a dbt cloud account from their website (free for solo developers)\nOnce you have logged in into dbt cloud you will be prompted to create a new project\n\nWe are going to need:\n\naccess to our data warehouse (BigQuery - set up in weeks 2 and 3)\nadmin access to our repo, where we will have the dbt project\n\n\n\n\nproject_setup.PNG\n\n\n\nName our project\nChoose BigQuery as our data warehouse:\n\n\n\n\ndbase_connection.PNG\n\n\n\nUpload the key you downloaded from BQ on the create from file option. This will fill out most fields related to the production credentials. Scroll down to the end of the page and set up your development credentials.\n\n\n\n\ndbt_bq_5.PNG\n\n\n\n\n\ndbt_bq_6.PNG\n\n\n\n\n\ndbt_test.PNG\n\n\n\n\n\nNote: This step could be skipped by using a managed repository if you don’t have your own GitHub repo for the course.\n\nSelect git clone and paste the SSH key from your repo.\n\n\n\n\ngithub_repo.PNG\n\n\n\nYou will get a deploy key, head to your GH repo and go to the settings tab. Under security you’ll find the menu deploy keys. Click on Add deploy key and paste the deploy key provided by dbt cloud. Make sure to tick on “write access”.\n\n\n\n\nAt the end, if you go to your projects it should look some like this:\n\n\n\ndbt_project.PNG\n\n\n\n\n\nYou’ll notice after initializing your project that the main branch will be read-only. This is because any changes to code cannot and should not be made directly in the base branch. A new branch must be created in the dbt Cloud IDE in order to make any changes to your project.\nYou can perform git tasks (such as create a new branch) with the Commit and Sync button. Once you commit the initial “changes” even though you haven’t made any changes because it’s read only! you can then create a new branch from the same place, and make edits within that branch.\nOnce you have created the new branch - edit the dbt_project_yaml to as per below and save the file:\n\n\n\ndbt_yaml.PNG\n\n\n\n\n\n\n\n\n\n\ndbt_model_anatomy.PNG\n\n\n\n\n\n\n\n\ndbt_model_FROM_clause.PNG\n\n\n\n\n\ndbt_model_FROM_clause_ref.PNG\n\n\nLet’s now go back to our dbt IDE and create two new directories for the models we will be creating:\n\n\n\nmodels_directories.PNG\n\n\nThe first directory Staging will be used for our raw data. Let’s create a schema.yml file to use the source macro to resolve the correct schema, build the correct dependencies, and generate the lineage automatically:\n\n\n\nstaging_schema_yml.PNG\n\n\nNow let’s create a sql file :\n\n\n\nstaging_green_sql.PNG\n\n\nWe can run this model using one of the following commands within the dbt terminal:\ndbt run  # Builds models in your target database.\ndbt run -m stg_green_tripdata.sql\ndbt run --select stg_green_tripdata  # Builds a specific model.\ndbt run --select stg_green_tripdata+  # Builds a specific model and its children.\ndbt run --select +stg_green_tripdata  # Builds a specific model and its ancestors.\ndbt run --select +stg_green_tripdata+  # Builds a specific model and its children and ancestors.\nLet’s go with :\ndbt run -m stg_green_tripdata.sql\n\n\n\nmodel_run_error_region.PNG\n\n\nI ran into an error because of inconsistent regions. The data buckets and BigQuery tables have to be configured to the same region. I reconfigured my staging table to Central Europe 2 to be consistent with my data bucket and the model ran successfully!\n\n\n\ndbt_green_tripdata_modelrun.PNG\n\n\nIf we now head to BigQuery we can see that our sql table has been created :\n\n\n\nBQ_stg_green_tripdata.PNG\n\n\n\n\n\nMacros in Jinja are pieces of code that can be reused multiple times – they are analogous to “functions” in other programming languages, and are extremely useful if you find yourself repeating code across multiple models. Macros are defined in .sql files, typically in your macros directory (docs).\n\n\n\nmacros.PNG\n\n\nLet’s now create a macros file for our project :\n\n\n\nget_payment_type_macro.PNG\n\n\nAnd let’s add that macro to our stg_green_tripdata.sql file :\n\n\n\nstaging_green_sql_updated.PNG\n\n\nAnd run the model once again using the following command within the dbt terminal:\ndbt run --select stg_green_tripdata    \n\n09:09:43  Began running node model.taxi_rides_ny.stg_green_tripdata\n09:09:43  1 of 1 START sql view model dbt_stephen_barrie.stg_green_tripdata .............. [RUN]\n09:09:43  Acquiring new bigquery connection 'model.taxi_rides_ny.stg_green_tripdata'\n09:09:43  Began compiling node model.taxi_rides_ny.stg_green_tripdata\n09:09:43  Writing injected SQL for node \"model.taxi_rides_ny.stg_green_tripdata\"\n09:09:43  Timing info for model.taxi_rides_ny.stg_green_tripdata (compile): 2023-03-27 09:09:43.922045 => 2023-03-27 09:09:43.940627\n09:09:43  Began executing node model.taxi_rides_ny.stg_green_tripdata\n09:09:43  Writing runtime sql for node \"model.taxi_rides_ny.stg_green_tripdata\"\n09:09:43  Opening a new connection, currently in state closed\n09:09:43  On model.taxi_rides_ny.stg_green_tripdata: /* {\"app\": \"dbt\", \"dbt_version\": \"1.4.5\", \"profile_name\": \"user\", \"target_name\": \"default\", \"node_id\": \"model.taxi_rides_ny.stg_green_tripdata\"} */\n\n\n  create or replace view `taxi-rides-ny-137`.`dbt_stephen_barrie`.`stg_green_tripdata`\n  OPTIONS()\n  as \n\nselect\n    -- identifiers\n    cast(vendorid as integer) as vendorid,\n    cast(ratecodeid as integer) as ratecodeid,\n    cast(pulocationid as integer) as  pickup_locationid,\n    cast(dolocationid as integer) as dropoff_locationid,\n\n    -- timestamps\n    cast(lpep_pickup_datetime as timestamp) as pickup_datetime,\n    cast(lpep_dropoff_datetime as timestamp) as dropoff_datetime,\n\n    -- trip info\n    store_and_fwd_flag,\n    cast(passenger_count as integer) as passenger_count,\n    cast(trip_distance as numeric) as trip_distance,\n    cast(trip_type as integer) as trip_type,\n\n    -- payment info\n    cast(fare_amount as numeric) as fare_amount,\n    cast(extra as numeric) as extra,\n    cast(mta_tax as numeric) as mta_tax,\n    cast(tip_amount as numeric) as tip_amount,\n    cast(tolls_amount as numeric) as tolls_amount,\n    cast(ehail_fee as numeric) as ehail_fee,\n    cast(improvement_surcharge as numeric) as improvement_surcharge,\n    cast(total_amount as numeric) as total_amount,\n    cast(payment_type as integer) as payment_type,\n    case payment_type\n        when 1 then 'Credit card'\n        when 2 then 'Cash'\n        when 3 then 'No charge'\n        when 4 then 'Dispute'\n        when 5 then 'Unknown'\n        when 6 then 'Voided trip'\n    end as payment_type_description,\n    cast(congestion_surcharge as numeric) as congestion_surcharge\n\nfrom `taxi-rides-ny-137`.`trips_data_all`.`green_tripdata`\nlimit 100;\n\n\n09:09:44  BigQuery adapter: https://console.cloud.google.com/bigquery?project=taxi-rides-ny-137&j=bq:europe-central2:8ca3c78e-ccc7-4992-95bd-bcdb0df40f4e&page=queryresults\n09:09:45  Timing info for model.taxi_rides_ny.stg_green_tripdata (execute): 2023-03-27 09:09:43.941082 => 2023-03-27 09:09:45.000530\n09:09:45  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '40e82f39-51cb-4467-afa1-3237172c4c5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6833835e0>]}\n09:09:45  1 of 1 OK created sql view model dbt_stephen_barrie.stg_green_tripdata ......... [\u001b[32mCREATE VIEW (0 processed)\u001b[0m in 1.08s]\n09:09:45  Finished running node model.taxi_rides_ny.stg_green_tripdata\n\nWe can also see the compiled code:\n\n\n\ncompiled_code_stg_green.PNG\n\n\n\n\n\ndbt packages are essentially standalone dbt projects, with models and macros that tackle a specific problem area. As a dbt user, by adding a package to our project, the package’s models and macros will become part of our own project.\n\n\n\npackages.PNG\n\n\nLet’s create a packages.yaml file to include the following :\n\n\n\npackages_yml.PNG\n\n\nTo install this package and download the dependencies run the following command in the dbt terminal:\ndbt deps   \n\n\n\ndbt_deps.PNG\n\n\nA number of folders and files have now been created:\n\n\n\ndbt_packages.PNG\n\n\nLet’s return to our model and update our stg_green_tripdata.sql file to make use of a macro:\n\n\n\nmacro_surrogate.PNG\n\n\nNow let’s run the command from the dbt IDE :\ndbt run --select stg_green_tripdata\nAnd we can see the compiled code extract:\n\n\n\nsurrogate_compiled.PNG\n\n\n\n\n\ndbt provides a mechanism, variables, to provide data to models for compilation. Variables can be used to configure timezones, avoid hardcoding table names or otherwise provide data to models to configure how they are compiled.\nSee Project variables for further details.\n\n\n\nvariables.PNG\n\n\nLet’s now introduce the above is_test_run variable into our stg_green_tripdata.sql file which will allow us to control from the command line whether we execute a test_run with a limit of 100 rows, or a full run, with no limit:\n\n\n\nlimit_variable.PNG\n\n\nAnd run the model from the IDE first, without specifying our variable function, and so the run will default to True and the limit of 100 rows will be applied :\ndbt run --select stg_green_tripdata\n\n\n\nlimit_100.PNG\n\n\nBut if we run again, this time specifying our variable as False then the 100 row limit will not be applied :\ndbt run --select stg_green_tripdata --var 'is_test_run: false'\nWe can see from our Big Query table that we now have 6,835,902 rows :\n\n\n\ngreen_tripdata_BQ_no_limit.PNG\n\n\nOK, now that we have built our green_tripdata model let’s replicate these steps for our yellow_tripdata :\n\n\n\nstaging_yellow.PNG\n\n\nNow use dbt run because we have two models:\ndbt run --var 'is_test_run: false'\n\n\n\ngreen_and_yellow.PNG\n\n\nAnd if we check Big Query we can see our stg_yellow_tripdata table has been added with `107,991,349 rows:\n\n\n\nyellow_trip_data_BQ.PNG\n\n\n\n\n\nSeeds are CSV files in your dbt project (typically in your seeds directory), that dbt can load into your data warehouse using the dbt seed command. Typically these are small files that contain data that will not change that often. See Seeds for further detail.\nLet’s add the taxi zone look up csv file by creating a new file within the seeds directory and simply copying and pasitng the contents of the raw csv file:\n\n\n\ntaxi_zone_lookup.PNG\n\n\nand then run the following command from the dbt terminal:\ndbt seed\n\n\n\ndbt_seed.PNG\n\n\nAnd head back to BigQuery and we find our table created with 265 rows :\n\n\n\ntaxi_zone_lookup_BQ.PNG\n\n\nWe can specify the data types of the csv fill, by modifying our dbt_project.yml file - otherwise default types will be applied :\n\n\n\ndbt_seed_project_yml.PNG\n\n\nIf we slightly modify data(for example, change 1,“EWR”,“Newark Airport”,“EWR” to 1,“NEWR”,“Newark Airport”,“NEWR”) in the csv file, and then run the following command:\ndbt seed --full-refresh\n\n\n\nupdated_seed.PNG\n\n\nLet’s now create a model from this seed. First, create a new file within the core directory called dim_zones.sql :\n\n\n\ndim_zones.PNG\n\n\nIdeally, we want everything in the directory to be tables to have efficient queries.\nNow, create the model fact_trips.sql within the core directory :\n\n\n\nfact_trips.PNG\n\n\nAnd we can have this nice visualization of the lineage :\n\n\n\nfact_trips_lineage.PNG\n\n\nThe dbt run command will create everything, except the seeds:\n\n\n\nfact_trips_error.PNG\n\n\nI ran into an error and after quite a bit of head scratching, it arose because the order of my columns was not consistent between my yellow and green tables. The following are basic rules for combining the result sets of two queries by using UNION:\n\nThe number and the order of the columns must be the same in all queries.\nThe data types must be compatible.\n\nAfter synchronizing the column order it ran successfully:\n\n\n\ndbt_run.PNG\n\n\nAnd I now have my fact_trips table in BQ:\n\n\n\nfact_trips_BQ.PNG\n\n\nHowever we also want to run the seeds, so we will use the command dbt build --select fact_trips to run only the fact_trips model :\n\n\n\nselect_fact_trips.PNG\n\n\nThe command dbt build --select +fact_trips --var 'is_test_run: false' will run everything that fact_trips needs. dbt already knows the dependencies :\n\n\n\nselect_plus_fact_trips.PNG\n\n\nAnd we can see in BQ that our fact_trips table now has 113,324,889 rows :\n\n\n\nfact_trips_BQ_updated.PNG\n\n\n\n\n\n\nTests are assertions you make about your models and other resources in your dbt project (e.g. sources, seeds and snapshots). When you run dbt test, dbt will tell you if each test in your project passes or fails.\n\n\n\ntests.PNG\n\n\nSee Tests for further details.\n\n\n\n\n\ndocumentation.PNG\n\n\nFor further detail about documentation see here.\n\n\n\n\nLet’s create a dm_monthly_zone_revenue.sql file :\n\n\n\ndm_monthly_zone_revenue.PNG\n\n\nThen build the table and its dependencies using:\ndbt build --select +dm_monthly_zone_revenue --var 'is_test_run: false'\n\n\n\ndm_monthly_zone_revenue_build.PNG\n\n\nAnd we can see that we now have the table in BQ :\n\n\n\ndm_monthly_zone_revenue_BQ.PNG\n\n\nThen, we update the schema.yml file in models/staging to define the model. This section is used in particular to document the model and to add tests:\nmodels/staging/schema.yml\n\nversion: 2\n\nsources:\n    - name: staging\n      #For bigquery:\n      database: taxi-rides-ny-137\n\n      # For postgres:\n      # database: production\n\n      schema: trips_data_all\n\n      # loaded_at_field: record_loaded_at\n      tables:\n        - name: green_tripdata\n        - name: yellow_tripdata\n         # freshness:\n           # error_after: {count: 6, period: hour}\n\nmodels:\n    - name: stg_green_tripdata\n      description: >\n        Trip made by green taxis, also known as boro taxis and street-hail liveries.\n        Green taxis may respond to street hails,but only in the areas indicated in green on the\n        map (i.e. above W 110 St/E 96th St in Manhattan and in the boroughs).\n        The records were collected and provided to the NYC Taxi and Limousine Commission (TLC) by\n        technology service providers. \n      columns:\n          - name: tripid\n            description: Primary key for this table, generated with a concatenation of vendorid+pickup_datetime\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n                - unique:\n                    severity: warn\n                - not_null:\n                    severity: warn\n          - name: VendorID \n            description: > \n                A code indicating the TPEP provider that provided the record.\n                1= Creative Mobile Technologies, LLC; \n                2= VeriFone Inc.\n          - name: pickup_datetime \n            description: The date and time when the meter was engaged.\n          - name: dropoff_datetime \n            description: The date and time when the meter was disengaged.\n          - name: Passenger_count \n            description: The number of passengers in the vehicle. This is a driver-entered value.\n          - name: Trip_distance \n            description: The elapsed trip distance in miles reported by the taximeter.\n          - name: Pickup_locationid\n            description: locationid where the meter was engaged.\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n              - relationships:\n                  to: ref('taxi_zone_lookup')\n                  field: locationid\n                  severity: warn\n          - name: dropoff_locationid \n            description: locationid where the meter was engaged.\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n              - relationships:\n                  to: ref('taxi_zone_lookup')\n                  field: locationid\n          - name: RateCodeID \n            description: >\n                The final rate code in effect at the end of the trip.\n                  1= Standard rate\n                  2=JFK\n                  3=Newark\n                  4=Nassau or Westchester\n                  5=Negotiated fare\n                  6=Group ride\n          - name: Store_and_fwd_flag \n            description: > \n              This flag indicates whether the trip record was held in vehicle\n              memory before sending to the vendor, aka “store and forward,”\n              because the vehicle did not have a connection to the server.\n                Y= store and forward trip\n                N= not a store and forward trip\n          - name: Dropoff_longitude \n            description: Longitude where the meter was disengaged.\n          - name: Dropoff_latitude \n            description: Latitude where the meter was disengaged.\n          - name: Payment_type \n            description: >\n              A numeric code signifying how the passenger paid for the trip.\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n              - accepted_values:\n                  values: \"{{ var('payment_type_values') }}\"\n                  severity: warn\n                  quote: false\n          - name: payment_type_description\n            description: Description of the payment_type code\n          - name: Fare_amount \n            description: > \n              The time-and-distance fare calculated by the meter.\n              Extra Miscellaneous extras and surcharges. Currently, this only includes\n              the $0.50 and $1 rush hour and overnight charges.\n              MTA_tax $0.50 MTA tax that is automatically triggered based on the metered\n              rate in use.\n          - name: Improvement_surcharge \n            description: > \n              $0.30 improvement surcharge assessed trips at the flag drop. The\n              improvement surcharge began being levied in 2015.\n          - name: Tip_amount \n            description: > \n              Tip amount. This field is automatically populated for credit card\n              tips. Cash tips are not included.\n          - name: Tolls_amount \n            description: Total amount of all tolls paid in trip.\n          - name: Total_amount \n            description: The total amount charged to passengers. Does not include cash tips.\n\n    - name: stg_yellow_tripdata\n      description: > \n        Trips made by New York City's iconic yellow taxis. \n        Yellow taxis are the only vehicles permitted to respond to a street hail from a passenger in all five\n        boroughs. They may also be hailed using an e-hail app like Curb or Arro.\n        The records were collected and provided to the NYC Taxi and Limousine Commission (TLC) by\n        technology service providers. \n      columns:\n          - name: tripid\n            description: Primary key for this table, generated with a concatenation of vendorid+pickup_datetime\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n                - unique:\n                    severity: warn\n                - not_null:\n                    severity: warn\n          - name: VendorID \n            description: > \n                A code indicating the TPEP provider that provided the record.\n                1= Creative Mobile Technologies, LLC; \n                2= VeriFone Inc.\n          - name: pickup_datetime \n            description: The date and time when the meter was engaged.\n          - name: dropoff_datetime \n            description: The date and time when the meter was disengaged.\n          - name: Passenger_count \n            description: The number of passengers in the vehicle. This is a driver-entered value.\n          - name: Trip_distance \n            description: The elapsed trip distance in miles reported by the taximeter.\n          - name: Pickup_locationid\n            description: locationid where the meter was engaged.\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n              - relationships:\n                  to: ref('taxi_zone_lookup')\n                  field: locationid\n                  severity: warn\n          - name: dropoff_locationid \n            description: locationid where the meter was engaged.\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n              - relationships:\n                  to: ref('taxi_zone_lookup')\n                  field: locationid\n                  severity: warn\n          - name: RateCodeID \n            description: >\n                The final rate code in effect at the end of the trip.\n                  1= Standard rate\n                  2=JFK\n                  3=Newark\n                  4=Nassau or Westchester\n                  5=Negotiated fare\n                  6=Group ride\n          - name: Store_and_fwd_flag \n            description: > \n              This flag indicates whether the trip record was held in vehicle\n              memory before sending to the vendor, aka “store and forward,”\n              because the vehicle did not have a connection to the server.\n                Y= store and forward trip\n                N= not a store and forward trip\n          - name: Dropoff_longitude \n            description: Longitude where the meter was disengaged.\n          - name: Dropoff_latitude \n            description: Latitude where the meter was disengaged.\n          - name: Payment_type \n            description: >\n              A numeric code signifying how the passenger paid for the trip.\n            tests: # severity = warn = warning given but program run continues | severity = never = IMMEDIATE stop\n              - accepted_values:\n                  values: \"{{ var('payment_type_values') }}\"\n                  severity: warn\n                  quote: false\n          - name: payment_type_description\n            description: Description of the payment_type code\n          - name: Fare_amount \n            description: > \n              The time-and-distance fare calculated by the meter.\n              Extra Miscellaneous extras and surcharges. Currently, this only includes\n              the $0.50 and $1 rush hour and overnight charges.\n              MTA_tax $0.50 MTA tax that is automatically triggered based on the metered\n              rate in use.\n          - name: Improvement_surcharge \n            description: > \n              $0.30 improvement surcharge assessed trips at the flag drop. The\n              improvement surcharge began being levied in 2015.\n          - name: Tip_amount \n            description: > \n              Tip amount. This field is automatically populated for credit card\n              tips. Cash tips are not included.\n          - name: Tolls_amount \n            description: Total amount of all tolls paid in trip.\n          - name: Total_amount \n            description: The total amount charged to passengers. Does not include cash tips.                \n\nNext, we modify the dbt_project.yml to define the payment_type_values variable :\ndbt.project/yml\n\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'taxi_rides_ny'\nversion: '1.0.0'\nconfig-version: 2\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'pg-dbt-workshop'\n\n# These configurations specify where dbt should look for different types of files.\n# The `source-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analysis\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\ntarget-path: \"target\"  # directory which will store compiled SQL files\nclean-targets:         # directories to be removed by `dbt clean`\n    - \"target\"\n    - \"dbt_packages\"\n    - \"dbt_modules\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/ directory\n# as tables. These settings can be overridden in the individual model files\n# using the `{{ config(...) }}` macro.\nmodels:\n  taxi_rides_ny:\n      # Applies to all files under models/.../\n      staging:\n          materialized: view\n      core:\n          materialized: table\nvars:\n  payment_type_values: [1, 2, 3, 4, 5, 6]\n\nseeds: \n    taxi_rides_ny:\n        taxi_zone_lookup:\n            +column_types:\n                locationid: numeric\n\nThen, we can run dbt test or one of the following commands:\ndbt test                               # Run tests on data in deployed models.\ndbt test --select stg_green_tripdata   # Run tests on data in specified model.\ndbt test --select stg_green_tripdata+  # Run tests on data in specified model and its children.\ndbt test --select +stg_green_tripdata  # Run tests on data in specified model and its ancestors.\ndbt build  # Run the seeds, run the tests and run the models.\nLet’s use dbt test :\n\n\n\ndbt_test_warn.PNG\n\n\nWe see that the column tripid from the stg_green_tripdata table is not unique after all. So let’s update our sql files - to keep only the first row of duplicates, with the condition where rn = 1 - rn means row number\n\n\n\ngreen_trip_data_updated.PNG\n\n\n\n\n\nyellow_trip_data_updated.PNG\n\n\nFor further detail on duplicates, see 4 Ways to Check for Duplicate Rows in SQL Server.\nLet’s now run everything we have built so far with dbt build :\n\n\n\ndbt_build_all_pass.PNG\n\n\nAs we can see, this time, all tests have passed.\nThe final thing to do is to complete our project is to add a schema.yml file for our core models :\n\nversion: 2\n\nmodels:\n  - name: dim_zones\n    description: >\n      List of unique zones idefied by locationid.\n      Includes the service zone they correspond to (Green or yellow).\n  - name: fact_trips\n    description: >\n      Taxi trips corresponding to both service zones (Green and yellow).\n      The table contains records where both pickup and dropoff locations are valid and known zones.\n      Each record corresponds to a trip uniquely identified by tripid.\n\n  - name: dm_monthly_zone_revenue\n    description: >\n      Aggregated table of all taxi trips corresponding to both service zones (Green and yellow) per pickup zone, month and service.\n      The table contains monthly sums of the fare elements used to calculate the monthly revenue.\n      The table contains also monthly indicators like number of trips, and average trip distance.\n    columns:\n      - name: revenue_monthly_total_amount\n        description: Monthly sum of the the total_amount of the fare charged for the trip per pickup zone, month and service.\n        tests:\n            - not_null:\n                severity: error\n\nand a macros_properties.yml file for completeness :\n\nversion: 2\n\nmacros:\n  - name: get_payment_type_description\n    description: >\n      This macro receives a payment_type and returns the corresponding description.\n    arguments:\n      - name: payment_type\n        type: int\n        description: > \n          payment_type value.\n          Must be one of the accepted values, otherwise the macro will return null\n\nSee Tests and Configuring test severity for more information about tests.\n\n\n\nNow that we have created a complete project, and tested it, and know that everything runs error-free it is time to deploy our project using version control and CI/CD in a production environment.\n\n\n\ndeployment.PNG\n\n\n\n\n\ndbt_production.PNG\n\n\n\n\n\n\n\ncontinuous_integration.PNG\n\n\nFor further detail on enabling CI in dbt Cloud see the documentation.\n\n\n\nIn dbt cloud, commit my-new-branch and pull request. Then in our GitHub repository taxi_rides_ny, merge this pull request to our main branch.\nNow head back to the dbt IDE. We already have a development environment :\n\n\n\ndevelopment.PNG\n\n\nBut let’s now create a production environment :\n\n\n\nproduction.PNG\n\n\nAnd now let’s create a Job :\n\n\n\ncreate_job_1.PNG\n\n\n\n\n\ncreate_job_2.PNG\n\n\n\n\n\ncreate_job_3.PNG\n\n\nNote that in order to have access to Webhooks you need to have either a Team or Enterprise subscription.\nLet’s now run the job :\n\n\n\ndbt_build_run.PNG\n\n\n\n\n\ndbt_run_overview.PNG\n\n\nUnder Artifacts we can see the compiled code together with the manifest :\n\n\n\nartefacts.PNG\n\n\nand the documentation :\n\n\n\ndocs.PNG\n\n\nWe can also see the Lineage Graph for our models :\n\n\n\nproduction_lineage_graph.PNG\n\n\n\n\n\nlineage_graph_2.PNG\n\n\nLet’s head back to the dbt cloud IDE and account settings and within Artifacts link the documentation to our dbt build :\n\n\n\nartifact_link.PNG\n\n\nNow that we have now successfully transformed our data using dbt models and tested and deployed those models, the final step is to visualize our data.\n\n\n\n\nNow that we have successfully transformed our data using dbt, it is time to visualise that data. Let’s first take a look at Looker formerly known as Google Data Studio, an online tool for converting data into customizable informative reports and dashboards introduced by Google on March 15, 2016 as part of the enterprise Google Analytics 360 suite.\nIn May 2016, Google announced a free version of Data Studio for individuals and small teams. See https://en.wikipedia.org/wiki/Looker_Studio.\nGo to Looker Studio and click USE IT FOR FREE. Make sure you are signed in to the correct Google account. The first thing we need to do is create a data source. There are 23 different connectors at the time of writing. We will be using BigQuery :\n\n\n\ngoogle_connectors.PNG\n\n\nWe will be working with the table fact_trips table of our taxi-rides-ny project created in our dbt production environment :\n\n\n\nlooker_BQ.PNG\n\n\nAfter connecting we are taken to the main dashboard. Although dimensions and facts should be familiar terms to those with knowledge of dimensional modeling the name fact_trips can be changed to something that is more palatable for other stakeholders.\nWe can see all 27 columns and their data types. Default aggregation is applied on some, but these can and should be removed where this is not appropriate, e.g. categorical data. Columns can be added from here. Descriptions can also be added to provide useful information for other users of the data.\nNote at the top there is a Data Freshness tab - the default setting is 12 hours but can be changed.\nAt the bottom there is a default Metric named Record Count which is useful - others can be added from here.\nAfter modifying the default aggregations go ahead and create a report :\n\n\n\nlooker_create_report.PNG\n\n\nLet’s choose a time series chart to begin with :\n\n\n\nlooker_time_series.PNG\n\n\nWe can see right away that the date range is 2001 to 2082! This is because we didn’t filter the data and we have some outliers. We can define our date range by using Add a control from the dropdown, and dragging the filter above our chart.\nThere is a wide variety of visualizations available to us. For illustrative purposes we will :\n\ncreate a scorecard for Total trips recorded\ncreate a pie chart for Service type distribution\ncreate a table with a heatmap for Trips per pickup zone\ncreate a stacked column chart for Trips per month and year. We need to create a new field to allow us to filter by month\n\n\n\n\nnew_field_formula.PNG\n\n\nThe final result is an aesthetic visualization ready for sharing. There is an email delivery scheduler which is very useful for providing for example weekly KPIs for key stakeholders :\n\n\n\ndashboard.PNG\n\n\n\n\n\nWe have seen what Looker can do, but there is also another visualization package called Metabase. There is a cloud version but also a free open source version where you can grab the Docker image for local hosting.\nTo learn more about Metabase check out the Metabase docs and tutorials.\n\n\n\n\nMake a model Incremental\nUse of tags\nHooks\nAnalysis\nSnapshots\nExposure\nMetrics\n\n\n\n\nMaximizing Confidence in Your Data Model Changes with dbt and PipeRider\nTo learn how to use PipeRider together with dbt for detecting changes in model and data, sign up for a workshop here."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html",
    "href": "posts/Huggy Bear/huggy-bear.html",
    "title": "Huggy Bear",
    "section": "",
    "text": "This is my follow up to Lesson 2: Practical Deep Learning for Coders 2022 in which Jeremy created a dog | cat classifier model and deployed to Hugging Face. During this project I will try to replicate on an image classification model, which discriminates between three types of bear: grizzly, black, and teddy bears. Once we’ve done this we will proceed to deploy the model as a working app on Hugging Face!"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#install-the-required-packages",
    "href": "posts/Huggy Bear/huggy-bear.html#install-the-required-packages",
    "title": "Huggy Bear",
    "section": "Install the required packages",
    "text": "Install the required packages\n::: {.cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ outputId=‘ba21b811-767c-459a-ccdf-044758720a55’ papermill=‘{“duration”:23.212506,“end_time”:“2022-10-10T06:59:22.512822”,“exception”:false,“start_time”:“2022-10-10T06:58:59.300316”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\n! pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n:::"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#grab-our-images",
    "href": "posts/Huggy Bear/huggy-bear.html#grab-our-images",
    "title": "Huggy Bear",
    "section": "Grab our images",
    "text": "Grab our images\nNow, it’s time to get hold of our bear images. There are many images on the internet of each type of bear that we can use. We just need a way to find them and download them. One method is to use fastai’s search_images_ddg which grabs images from DuckDuckGo. Note that the number of images is restricted by default to a maximum of 200:\n\nsearch_images_ddg\nims = search_images_ddg('grizzly bear')\nlen(ims)\n\n200\n\n\nNB: there’s no way to be sure exactly what images a search like this will find. The results can change over time. We’ve heard of at least one case of a community member who found some unpleasant pictures of dead bears in their search results. You’ll receive whatever images are found by the web search engine. If you’re running this at work, or with kids, etc, then be cautious before you display the downloaded images.\nLet’s take a look at an image:\n\ndest = 'images/grizzly.jpg'\ndownload_url(ims[0], dest)\n\n\n\n\n\n\n    \n      \n      102.83% [262144/254920 00:00<00:00]\n    \n    \n\n\nPath('images/grizzly.jpg')\n\n\n\nim = Image.open(dest)\nim.to_thumb(128,128)\n\n\n\n\n\nbear_types = 'grizzly','black','teddy'\npath = Path('bears')\n\nThis seems to have worked nicely. Let’s use fastai’s download_images to download all the URLs for each of our search terms. We’ll put each in a separate folder:\n\nif not path.exists():\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o)\n        dest.mkdir(exist_ok=True)\n        results = search_images_ddg(f'{o} bear')\n        download_images(dest, urls=results)\n\nOur folder has image files, as we’d expect:\n\nfns = get_image_files(path)\nfns\n\n(#295) [Path('bears/grizzly/bd6ab4a5-5126-492d-b69e-bd15e8f8db86.jpg'),Path('bears/grizzly/54c2968d-f1eb-473f-8979-abd2c6a1e9b9.jpg'),Path('bears/grizzly/06c6eeac-94a8-4634-a979-5de67ced9b3e.jpg'),Path('bears/grizzly/6d6c394c-596b-4953-b98c-f90df597d7c1.jpg'),Path('bears/grizzly/d0fb653a-1328-4f3d-95ce-1b5bc588ef04.jpg'),Path('bears/grizzly/b5179606-5957-4a29-8623-fa05ff910ccc.jpg'),Path('bears/grizzly/3e5bb95f-76bf-4847-9c86-a000077fc371.jpg'),Path('bears/grizzly/484483b9-7453-4e08-8487-9066da336bbb.jpg'),Path('bears/grizzly/d39dd49d-669a-4f15-96e7-9476875d859f.jpg'),Path('bears/grizzly/d3f5a27e-a2bc-474a-8acb-afd0708dee20.jpg')...]"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#cleaning-our-data",
    "href": "posts/Huggy Bear/huggy-bear.html#cleaning-our-data",
    "title": "Huggy Bear",
    "section": "Cleaning our data",
    "text": "Cleaning our data\nOften when we download files from the internet, there are a few that are corrupt. Let’s check:\n\nfailed = verify_images(fns)\nfailed\n\n(#5) [Path('bears/grizzly/8ce9dba9-9dc8-4bf3-b392-097b28d86baf.jpg'),Path('bears/grizzly/13127ddd-1639-4b89-b81a-439c43a56fc6.jpg'),Path('bears/grizzly/e537fa0c-c6bf-43f3-acda-024af91b1745.jpg'),Path('bears/black/946c1956-6068-4468-bd3a-10af1fc2ae6a.jpg'),Path('bears/black/001594ba-562a-4ff0-8e3a-d84c654146ea.jpg')]\n\n\nTo remove all the failed images, you can use unlink on each of them. Note that, like most fastai functions that return a collection, verify_images returns an object of type L, which includes the map method. This calls the passed function on each element of the collection:\n\nfailed.map(Path.unlink);"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#from-data-to-dataloaders",
    "href": "posts/Huggy Bear/huggy-bear.html#from-data-to-dataloaders",
    "title": "Huggy Bear",
    "section": "From Data To DataLoaders",
    "text": "From Data To DataLoaders\nNow that we have downloaded some data, we need to assemble it in a format suitable for model training. In fastai, that means creating an object called DataLoaders.\nTo turn our downloaded data into a DataLoaders object we need to tell fastai at least four things:\n\nWhat kinds of data we are working with\nHow to get the list of items\nHow to label these items\nHow to create the validation set\n\nFastai has an extremely flexible system called the data block API. With this API you can fully customize every stage of the creation of your DataLoaders. Here is what we need to create a DataLoaders for the dataset that we just downloaded:\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nThis command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data — in this case, the path where the images can be found:\n\ndls = bears.dataloaders(path)\n\nA DataLoaders includes validation and training DataLoaders. DataLoader is a class that provides batches of a few items at a time to the GPU. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader:\n\ndls.valid.show_batch(max_n=4, nrows=1)"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#data-augmentation",
    "href": "posts/Huggy Bear/huggy-bear.html#data-augmentation",
    "title": "Huggy Bear",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData augmentation refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. By default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them:\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\n\nbears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = bears.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n\n\n\n\nAll of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use.\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn’t even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\nHere’s another example where we replace Resize with RandomResizedCrop, which is the transform that provides the behavior we just described. The most important parameter to pass in is min_scale, which determines how much of the image to select at minimum each time:\n\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\n\nWe used unique=True to have the same image repeated with different versions of this RandomResizedCrop transform.\nFor natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the aug_transforms function. Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the batch_tfms parameter (note that we’re not using RandomResizedCrop in this example, so you can see the differences more clearly; we’re also using double the amount of augmentation compared to the default, for the same reason):\n\nbears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nNow that we have assembled our data in a format fit for model training, let’s actually train an image classifier using it."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#training-our-model-and-using-it-to-clean-our-data",
    "href": "posts/Huggy Bear/huggy-bear.html#training-our-model-and-using-it-to-clean-our-data",
    "title": "Huggy Bear",
    "section": "Training our model and using it to clean our data",
    "text": "Training our model and using it to clean our data\nWe don’t have a lot of data for our problem (200 pictures of each sort of bear at most), so to train our model, we’ll use RandomResizedCrop with an image size of 224 px, which is fairly standard for image classification, and default aug_transforms:\n\nbears = bears.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = bears.dataloaders(path)\n\nWe can now create our Learner and fine-tune it in the usual way:\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/stephen137/mambaforge/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/stephen137/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.192118\n      0.813263\n      0.327586\n      00:11\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.407396\n      0.375259\n      0.155172\n      00:13\n    \n    \n      1\n      0.308869\n      0.153977\n      0.051724\n      00:14\n    \n    \n      2\n      0.247075\n      0.101119\n      0.051724\n      00:13\n    \n    \n      3\n      0.217112\n      0.085641\n      0.051724\n      00:13"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#confusion-matrix",
    "href": "posts/Huggy Bear/huggy-bear.html#confusion-matrix",
    "title": "Huggy Bear",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nNow let’s see whether the mistakes the model is making are mainly thinking that grizzlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or something else. To visualize this, we can create a confusion matrix:\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe rows represent all the black, grizzly, and teddy bears in our dataset, respectively. The columns represent the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the off-diagonal cells represent those which were classified incorrectly. This is one of the many ways that fastai allows you to view the results of your model. It is (of course!) calculated using the validation set. With the color-coding, the goal is to have white everywhere except the diagonal, where we want dark blue. Our bear classifier isn’t making many mistakes!"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#losses",
    "href": "posts/Huggy Bear/huggy-bear.html#losses",
    "title": "Huggy Bear",
    "section": "Losses",
    "text": "Losses\nIt’s helpful to see where exactly our errors are occurring, to see whether they’re due to a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn’t handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their loss.\nThe loss is a number that is higher if the model is incorrect (especially if it’s also confident of its incorrect answer), or if it’s correct, but not confident of its correct answer. In a couple of chapters we’ll learn in depth how loss is calculated and used in the training process. For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction:\n\ninterp.plot_top_losses(5, nrows=1)\n\n\n\n\n\n\n\n\n\n\n\nThis output shows that the images with the highest losses are ones where the prediction matches the label, however with a low degree of confidence.\nThe intuitive approach to doing data cleaning is to do it before you train a model. But as you’ve seen in this case, a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#image-classifier-cleaner",
    "href": "posts/Huggy Bear/huggy-bear.html#image-classifier-cleaner",
    "title": "Huggy Bear",
    "section": "Image Classifier Cleaner",
    "text": "Image Classifier Cleaner\nfastai includes a handy GUI for data cleaning called ImageClassifierCleaner that allows you to choose a category and the training versus validation set and view the highest-loss images (in order), along with menus to allow images to be selected for removal or relabeling:\n\n#hide_output\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that amongst our “black bears” is an image that contains two bears: one grizzly, one black. So, we should choose ‘Delete’ in the menu under this image\nImageClassifierCleaner doesn’t actually do the deleting or changing of labels for you; it just returns the indices of items to change. So, for instance, to delete i.e. (unlink) all images selected for deletion, we would run:\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\nTo recatogrize i.e.(move) images for which we’ve selected a different category, we would run:\n\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#saving-our-model",
    "href": "posts/Huggy Bear/huggy-bear.html#saving-our-model",
    "title": "Huggy Bear",
    "section": "Saving our model",
    "text": "Saving our model\nOnce you’ve got a model you’re happy with, you need to save it, so that you can then copy it over to a server where you’ll use it in production. Remember that a model consists of two parts: the architecture and the trained parameters. The easiest way to save the model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the export method.\nThis method even saves the definition of how to create your DataLoaders. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set DataLoader for inference by default, so your data augmentation will not be applied, which is generally what you want.\nWhen you call export, fastai will save a file called “export.pkl”:\n\nlearn.export('export.pkl')\n\nAfter a few seconds, your model will be downloaded to your computer, where you can then create your app that uses the model."
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#hugging-face",
    "href": "posts/Huggy Bear/huggy-bear.html#hugging-face",
    "title": "Huggy Bear",
    "section": "Hugging Face",
    "text": "Hugging Face\nNow that we have a saved model that we are happy with, we can go ahead and deploy it as a working app on Hugging Face.\nYou can view the app here:\nI did run into some issues which required quite a bit of troubleshooting. My problem was that my Hugging Face environment was missing a requirements.txt file - which Hugging Face needs to recognise the fastai library. The text file should include the following text: fastai>=2.0.0"
  },
  {
    "objectID": "posts/Huggy Bear/huggy-bear.html#key-takeaways",
    "href": "posts/Huggy Bear/huggy-bear.html#key-takeaways",
    "title": "Huggy Bear",
    "section": "Key takeaways",
    "text": "Key takeaways\nThis project involved the following:\n\ndownloading an image set for training our model\ncleaning the data and employing various data augmentation techniques\ntraining our model\nevaluating our model using a Confusion Matrix\nsaving our model and ‘pickling’ it\n\n\n\ndeployment of our image classification app via Hugging Face"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html",
    "href": "posts/Pandas/Effective_Pandas.html",
    "title": "Efficient Pandas",
    "section": "",
    "text": "Having code that is clean, readable and has a logical flow is invaluable. I discovered Structured Query Language (SQL) before Python, and as the name suggests, this already pushes you down this structured, logical road. I have only recently started to explore Python, but my experience so far is that the code can quickly become scattered and difficult to follow, particulary during the exploratory data analysis (EDA) phase.\nI have just finished actively watching Efficient Pandas by Matt Harrison and decided to share the content via this blog. The video feels like a bit of a breakthrough for me, someone who is just starting out in the world of data science, and hopefully others will also benefit from reading this. Adopting the chaining method covered in this blog, whenever possible, should ensure that your code is cleaner, and reads like a recipe of ordered steps, reducing any potential ambiguities."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#initial-set-up",
    "href": "posts/Pandas/Effective_Pandas.html#initial-set-up",
    "title": "Efficient Pandas",
    "section": "Initial set up",
    "text": "Initial set up\n\n# bring in the pandas!\nimport pandas as pd\n\n\n# check which version of pandas we're on\npd.__version__\n\n'1.5.0'\n\n\n\n# control the pandas display features\npd.options.display.min_rows = 20"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#dataset",
    "href": "posts/Pandas/Effective_Pandas.html#dataset",
    "title": "Efficient Pandas",
    "section": "Dataset",
    "text": "Dataset\nThe dataset we will be exploring is from https://www.fueleconomy.gov/feg/download.shtml which is the official U.S. government source for fuel economy information. The zipped csv file can be downloaded from here but we can just read in the file using pandas:\n\n# read in our dataset\nautos = pd.read_csv('https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip')\n\n/tmp/ipykernel_2753/3884461791.py:2: DtypeWarning: Columns (68,70,71,72,73,74,76,79) have mixed types. Specify dtype option on import or set low_memory=False.\n  autos = pd.read_csv('https://github.com/mattharrison/datasets/raw/master/data/vehicles.csv.zip')\n\n\n\n# Let's take a look\nautos\n\n\n\n\n\n  \n    \n      \n      barrels08\n      barrelsA08\n      charge120\n      charge240\n      city08\n      city08U\n      cityA08\n      cityA08U\n      cityCD\n      cityE\n      ...\n      mfrCode\n      c240Dscr\n      charge240b\n      c240bDscr\n      createdOn\n      modifiedOn\n      startStop\n      phevCity\n      phevHwy\n      phevComb\n    \n  \n  \n    \n      0\n      15.695714\n      0.0\n      0.0\n      0.0\n      19\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      1\n      29.964545\n      0.0\n      0.0\n      0.0\n      9\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      2\n      12.207778\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      3\n      29.964545\n      0.0\n      0.0\n      0.0\n      10\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      4\n      17.347895\n      0.0\n      0.0\n      0.0\n      17\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      5\n      14.982273\n      0.0\n      0.0\n      0.0\n      21\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      6\n      13.184400\n      0.0\n      0.0\n      0.0\n      22\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      7\n      13.733750\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      8\n      12.677308\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      9\n      13.184400\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41134\n      16.480500\n      0.0\n      0.0\n      0.0\n      18\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41135\n      12.677308\n      0.0\n      0.0\n      0.0\n      23\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41136\n      13.733750\n      0.0\n      0.0\n      0.0\n      21\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41137\n      11.771786\n      0.0\n      0.0\n      0.0\n      24\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41138\n      13.184400\n      0.0\n      0.0\n      0.0\n      21\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41139\n      14.982273\n      0.0\n      0.0\n      0.0\n      19\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41140\n      14.330870\n      0.0\n      0.0\n      0.0\n      20\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41141\n      15.695714\n      0.0\n      0.0\n      0.0\n      18\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41142\n      15.695714\n      0.0\n      0.0\n      0.0\n      18\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n    \n      41143\n      18.311667\n      0.0\n      0.0\n      0.0\n      16\n      0.0\n      0\n      0.0\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      0.0\n      NaN\n      Tue Jan 01 00:00:00 EST 2013\n      Tue Jan 01 00:00:00 EST 2013\n      NaN\n      0\n      0\n      0\n    \n  \n\n41144 rows × 83 columns\n\n\n\nSo our dataset includes 41,144 rows and 83 columns - that’s a lot of data! Let’s have a look at the columns:\n\nautos.columns\n\nIndex(['barrels08', 'barrelsA08', 'charge120', 'charge240', 'city08',\n       'city08U', 'cityA08', 'cityA08U', 'cityCD', 'cityE', 'cityUF', 'co2',\n       'co2A', 'co2TailpipeAGpm', 'co2TailpipeGpm', 'comb08', 'comb08U',\n       'combA08', 'combA08U', 'combE', 'combinedCD', 'combinedUF', 'cylinders',\n       'displ', 'drive', 'engId', 'eng_dscr', 'feScore', 'fuelCost08',\n       'fuelCostA08', 'fuelType', 'fuelType1', 'ghgScore', 'ghgScoreA',\n       'highway08', 'highway08U', 'highwayA08', 'highwayA08U', 'highwayCD',\n       'highwayE', 'highwayUF', 'hlv', 'hpv', 'id', 'lv2', 'lv4', 'make',\n       'model', 'mpgData', 'phevBlended', 'pv2', 'pv4', 'range', 'rangeCity',\n       'rangeCityA', 'rangeHwy', 'rangeHwyA', 'trany', 'UCity', 'UCityA',\n       'UHighway', 'UHighwayA', 'VClass', 'year', 'youSaveSpend', 'guzzler',\n       'trans_dscr', 'tCharger', 'sCharger', 'atvType', 'fuelType2', 'rangeA',\n       'evMotor', 'mfrCode', 'c240Dscr', 'charge240b', 'c240bDscr',\n       'createdOn', 'modifiedOn', 'startStop', 'phevCity', 'phevHwy',\n       'phevComb'],\n      dtype='object')"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#data-types",
    "href": "posts/Pandas/Effective_Pandas.html#data-types",
    "title": "Efficient Pandas",
    "section": "Data Types",
    "text": "Data Types\nLet’s concentrate our focus on a subset of the data. Let’s look at 14 of the 83 columns and also find out about the types of data included. Getting the right types will enable analysis and correctness.\n\n# Let's drill down and focus on just 14 of the 83 columns\ncols = ['city08','comb08', 'highway08','cylinders', 'displ', 'drive', 'eng_dscr', 'fuelCost08',\n      'make', 'model', 'trany', 'range', 'createdOn', 'year']       \n\n\n# Let's see the data types for each column\nautos[cols].dtypes\n\ncity08          int64\ncomb08          int64\nhighway08       int64\ncylinders     float64\ndispl         float64\ndrive          object\neng_dscr       object\nfuelCost08      int64\nmake           object\nmodel          object\ntrany          object\nrange           int64\ncreatedOn      object\nyear            int64\ndtype: object\n\n\n\n# Let's see how much memory is being used by column\nautos[cols].memory_usage(deep=True)\n\nIndex             128\ncity08         329152\ncomb08         329152\nhighway08      329152\ncylinders      329152\ndispl          329152\ndrive         3028369\neng_dscr      2135693\nfuelCost08     329152\nmake          2606267\nmodel         2813134\ntrany         2933276\nrange          329152\ncreatedOn     3497240\nyear           329152\ndtype: int64\n\n\n\n# Let's see how much memory is being used in total\nautos[cols].memory_usage(deep=True).sum()\n\n19647323"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#integers-int",
    "href": "posts/Pandas/Effective_Pandas.html#integers-int",
    "title": "Efficient Pandas",
    "section": "Integers (int)",
    "text": "Integers (int)\nIntegers(int) are numbers without a decimal point. Let’s grab some summary statistics for our integer columns:\n\n# summary stats for integer columns\n(autos\n [cols]\n .select_dtypes(int)\n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      7400.000000\n      370.000000\n      2020.000000"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#chaining",
    "href": "posts/Pandas/Effective_Pandas.html#chaining",
    "title": "Efficient Pandas",
    "section": "Chaining",
    "text": "Chaining\nThe above code is fine but it can quickly become cluttered and unreadable. A better way is to lean on SQL coding best practice which means that our code reads more like a recipe of ordered steps:\n\n# use chaining to grab summary stats for integer columns\n(autos\n [cols]\n .select_dtypes(int)\n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      7400.000000\n      370.000000\n      2020.000000\n    \n  \n\n\n\n\nSame result, much more readable code! Chaining is also known as ‘flow programming’. Rather than creating intermediate variables, leverage the fact that most operations return a new object which can be worked on.\nNote, if you can’t find a way to chain we can use pandas .pipe. We’ll see how this works later."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#saving-space",
    "href": "posts/Pandas/Effective_Pandas.html#saving-space",
    "title": "Efficient Pandas",
    "section": "Saving Space",
    "text": "Saving Space\n\nimport numpy as np\n\nThe int columns are currently in int64 format. Let’s try to free up some space by representing our data more memory efficiently. We can use Numpy to help with this:\n\n# Can comb08 column be int8?\nnp.iinfo(np.int8)\n\niinfo(min=-128, max=127, dtype=int8)\n\n\nThe range -128 to 127 can be represented as 8 bits. (There are 256 values which in binary form can be represented by 11111111 i.e. 8 bits). We have a maximum value of 136 for the comb08 column so we can’t convert to int8 without losing info. We can however convert the highway08 column to int8. Let’s try int16:\n\n# Can comb08 column be int16?\nnp.iinfo(np.int16)\n\niinfo(min=-32768, max=32767, dtype=int16)\n\n\nAll of our data is within this range, so we can go ahead and convert all int64 columns to int16 (and int8 for highway08):\n\n# convert from int64 to int16 and int8 in order to free up some memory\n# also obtain summary statistics for integer columns\n(autos\n [cols]\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .select_dtypes(['integer'])\n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      7400.000000\n      370.000000\n      2020.000000\n    \n  \n\n\n\n\nLet’s see if we have saved any space by converting:\n\n# check memory usage\n(autos\n [cols]\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n18124995\n\n\nSo a saving, but not substantial - just under 8%. Let’s see if we can improve on this:"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#floats",
    "href": "posts/Pandas/Effective_Pandas.html#floats",
    "title": "Efficient Pandas",
    "section": "Floats",
    "text": "Floats\nA floating point (known as a float) number has decimal points even if that decimal point value is 0. For example: 1.13, 2.0, 1234.345. If we have a column that contains both integers and floating point numbers, Pandas will assign the entire column to the float data type so the decimal points are not lost.\n\n# Let's take a look at the columns with a float data type\n(autos\n [cols]\n .select_dtypes('float')\n)\n\n\n\n\n\n  \n    \n      \n      cylinders\n      displ\n    \n  \n  \n    \n      0\n      4.0\n      2.0\n    \n    \n      1\n      12.0\n      4.9\n    \n    \n      2\n      4.0\n      2.2\n    \n    \n      3\n      8.0\n      5.2\n    \n    \n      4\n      4.0\n      2.2\n    \n    \n      5\n      4.0\n      1.8\n    \n    \n      6\n      4.0\n      1.8\n    \n    \n      7\n      4.0\n      1.6\n    \n    \n      8\n      4.0\n      1.6\n    \n    \n      9\n      4.0\n      1.8\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      41134\n      4.0\n      2.1\n    \n    \n      41135\n      4.0\n      1.9\n    \n    \n      41136\n      4.0\n      1.9\n    \n    \n      41137\n      4.0\n      1.9\n    \n    \n      41138\n      4.0\n      1.9\n    \n    \n      41139\n      4.0\n      2.2\n    \n    \n      41140\n      4.0\n      2.2\n    \n    \n      41141\n      4.0\n      2.2\n    \n    \n      41142\n      4.0\n      2.2\n    \n    \n      41143\n      4.0\n      2.2\n    \n  \n\n41144 rows × 2 columns\n\n\n\nCylinders look int like - we would expect the number of cylinders to be an integer, and not a float (decimal).\n\n# summary stats for cylinders\n(autos\n .cylinders\n .describe()\n)\n\ncount    40938.000000\nmean         5.717084\nstd          1.755517\nmin          2.000000\n25%          4.000000\n50%          6.000000\n75%          6.000000\nmax         16.000000\nName: cylinders, dtype: float64\n\n\nOops, we have missing values - count = 40,938 but we have 41,144 rows.\n\n# Let's count the various values for cylinders\n(autos\n .cylinders\n .value_counts(dropna=False)\n)\n\n4.0     15938\n6.0     14284\n8.0      8801\n5.0       771\n12.0      626\n3.0       279\nNaN       206\n10.0      170\n2.0        59\n16.0       10\nName: cylinders, dtype: int64\n\n\nAs anticipated, we have missing values (206) represented by NaN\n\n## where are they missing? We can use .query\n(autos\n [cols]\n .query('cylinders.isna()')\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      eng_dscr\n      fuelCost08\n      make\n      model\n      trany\n      range\n      createdOn\n      year\n    \n  \n  \n    \n      7138\n      81\n      85\n      91\n      NaN\n      NaN\n      NaN\n      NaN\n      800\n      Nissan\n      Altra EV\n      NaN\n      90\n      Tue Jan 01 00:00:00 EST 2013\n      2000\n    \n    \n      7139\n      81\n      72\n      64\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      900\n      Toyota\n      RAV4 EV\n      NaN\n      88\n      Tue Jan 01 00:00:00 EST 2013\n      2000\n    \n    \n      8143\n      81\n      72\n      64\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      900\n      Toyota\n      RAV4 EV\n      NaN\n      88\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8144\n      74\n      65\n      58\n      NaN\n      NaN\n      NaN\n      NaN\n      1000\n      Ford\n      Th!nk\n      NaN\n      29\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8146\n      45\n      39\n      33\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      1700\n      Ford\n      Explorer USPS Electric\n      NaN\n      38\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8147\n      84\n      75\n      66\n      NaN\n      NaN\n      NaN\n      NaN\n      900\n      Nissan\n      Hyper-Mini\n      NaN\n      33\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      9212\n      87\n      78\n      69\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      850\n      Toyota\n      RAV4 EV\n      NaN\n      95\n      Tue Jan 01 00:00:00 EST 2013\n      2002\n    \n    \n      9213\n      45\n      39\n      33\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      1700\n      Ford\n      Explorer USPS Electric\n      NaN\n      38\n      Tue Jan 01 00:00:00 EST 2013\n      2002\n    \n    \n      10329\n      87\n      78\n      69\n      NaN\n      NaN\n      2-Wheel Drive\n      NaN\n      850\n      Toyota\n      RAV4 EV\n      NaN\n      95\n      Tue Jan 01 00:00:00 EST 2013\n      2003\n    \n    \n      21413\n      22\n      24\n      28\n      NaN\n      NaN\n      4-Wheel Drive\n      NaN\n      1750\n      Subaru\n      RX Turbo\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      34407\n      73\n      72\n      71\n      NaN\n      NaN\n      Front-Wheel Drive\n      NaN\n      900\n      BYD\n      e6\n      Automatic (A1)\n      187\n      Wed Mar 13 00:00:00 EDT 2019\n      2019\n    \n    \n      34408\n      118\n      108\n      97\n      NaN\n      NaN\n      Front-Wheel Drive\n      NaN\n      600\n      Nissan\n      Leaf (62 kW-hr battery pack)\n      Automatic (A1)\n      226\n      Wed Mar 13 00:00:00 EDT 2019\n      2019\n    \n    \n      34409\n      114\n      104\n      94\n      NaN\n      NaN\n      Front-Wheel Drive\n      NaN\n      650\n      Nissan\n      Leaf SV/SL (62 kW-hr battery pack)\n      Automatic (A1)\n      215\n      Wed Mar 13 00:00:00 EDT 2019\n      2019\n    \n    \n      34538\n      74\n      74\n      73\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      900\n      Audi\n      e-tron\n      Automatic (A1)\n      204\n      Tue Apr 16 00:00:00 EDT 2019\n      2019\n    \n    \n      34561\n      80\n      76\n      72\n      NaN\n      NaN\n      4-Wheel Drive\n      NaN\n      850\n      Jaguar\n      I-Pace\n      Automatic (A1)\n      234\n      Thu May 02 00:00:00 EDT 2019\n      2020\n    \n    \n      34563\n      138\n      131\n      124\n      NaN\n      NaN\n      Rear-Wheel Drive\n      NaN\n      500\n      Tesla\n      Model 3 Standard Range\n      Automatic (A1)\n      220\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34564\n      140\n      133\n      124\n      NaN\n      NaN\n      Rear-Wheel Drive\n      NaN\n      500\n      Tesla\n      Model 3 Standard Range Plus\n      Automatic (A1)\n      240\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34565\n      115\n      111\n      107\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      600\n      Tesla\n      Model S Long Range\n      Automatic (A1)\n      370\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34566\n      104\n      104\n      104\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      650\n      Tesla\n      Model S Performance (19\" Wheels)\n      Automatic (A1)\n      345\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n    \n      34567\n      98\n      97\n      96\n      NaN\n      NaN\n      All-Wheel Drive\n      NaN\n      700\n      Tesla\n      Model S Performance (21\" Wheels)\n      Automatic (A1)\n      325\n      Thu May 02 00:00:00 EDT 2019\n      2019\n    \n  \n\n206 rows × 14 columns\n\n\n\n\n## chaining - add cylinders and displ columns replacing NaN with 0\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .describe()\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      year\n    \n  \n  \n    \n      count\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n      41144.000000\n    \n    \n      mean\n      18.369045\n      20.616396\n      24.504667\n      5.688460\n      3.277904\n      2362.335942\n      0.793506\n      2001.535266\n    \n    \n      std\n      7.905886\n      7.674535\n      7.730364\n      1.797009\n      1.373415\n      654.981925\n      13.041592\n      11.142414\n    \n    \n      min\n      6.000000\n      7.000000\n      9.000000\n      0.000000\n      0.000000\n      500.000000\n      0.000000\n      1984.000000\n    \n    \n      25%\n      15.000000\n      17.000000\n      20.000000\n      4.000000\n      2.200000\n      1900.000000\n      0.000000\n      1991.000000\n    \n    \n      50%\n      17.000000\n      20.000000\n      24.000000\n      6.000000\n      3.000000\n      2350.000000\n      0.000000\n      2002.000000\n    \n    \n      75%\n      20.000000\n      23.000000\n      28.000000\n      6.000000\n      4.300000\n      2700.000000\n      0.000000\n      2011.000000\n    \n    \n      max\n      150.000000\n      136.000000\n      124.000000\n      16.000000\n      8.400000\n      7400.000000\n      370.000000\n      2020.000000\n    \n  \n\n\n\n\n\n# use this to inspect float sizes\nnp.finfo(np.float16)\n\nfinfo(resolution=0.001, min=-6.55040e+04, max=6.55040e+04, dtype=float16)\n\n\n\n## chaining - add cylinders and displ columns replacing NaN with 0\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n )\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      eng_dscr\n      fuelCost08\n      make\n      model\n      trany\n      range\n      createdOn\n      year\n    \n  \n  \n    \n      0\n      19\n      21\n      25\n      4\n      2.000000\n      Rear-Wheel Drive\n      (FFS)\n      2000\n      Alfa Romeo\n      Spider Veloce 2000\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      1\n      9\n      11\n      14\n      12\n      4.898438\n      Rear-Wheel Drive\n      (GUZZLER)\n      3850\n      Ferrari\n      Testarossa\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      2\n      23\n      27\n      33\n      4\n      2.199219\n      Front-Wheel Drive\n      (FFS)\n      1550\n      Dodge\n      Charger\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      3\n      10\n      11\n      12\n      8\n      5.199219\n      Rear-Wheel Drive\n      NaN\n      3850\n      Dodge\n      B150/B250 Wagon 2WD\n      Automatic 3-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1985\n    \n    \n      4\n      17\n      19\n      23\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      2700\n      Subaru\n      Legacy AWD Turbo\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      5\n      21\n      22\n      24\n      4\n      1.799805\n      Front-Wheel Drive\n      (FFS)\n      1900\n      Subaru\n      Loyale\n      Automatic 3-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      6\n      22\n      25\n      29\n      4\n      1.799805\n      Front-Wheel Drive\n      (FFS)\n      1700\n      Subaru\n      Loyale\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      7\n      23\n      24\n      26\n      4\n      1.599609\n      Front-Wheel Drive\n      (FFS)\n      1750\n      Toyota\n      Corolla\n      Automatic 3-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      8\n      23\n      26\n      31\n      4\n      1.599609\n      Front-Wheel Drive\n      (FFS)\n      1600\n      Toyota\n      Corolla\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      9\n      23\n      25\n      30\n      4\n      1.799805\n      Front-Wheel Drive\n      (FFS)\n      1700\n      Toyota\n      Corolla\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41134\n      18\n      20\n      24\n      4\n      2.099609\n      Front-Wheel Drive\n      (FFS)\n      2100\n      Saab\n      900\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41135\n      23\n      26\n      33\n      4\n      1.900391\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      1600\n      Saturn\n      SL\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41136\n      21\n      24\n      30\n      4\n      1.900391\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      1750\n      Saturn\n      SL\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41137\n      24\n      28\n      33\n      4\n      1.900391\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      1500\n      Saturn\n      SL\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41138\n      21\n      25\n      32\n      4\n      1.900391\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      1700\n      Saturn\n      SL\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41139\n      19\n      22\n      26\n      4\n      2.199219\n      Front-Wheel Drive\n      (FFS)\n      1900\n      Subaru\n      Legacy\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41140\n      20\n      23\n      28\n      4\n      2.199219\n      Front-Wheel Drive\n      (FFS)\n      1850\n      Subaru\n      Legacy\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41141\n      18\n      21\n      24\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      2000\n      Subaru\n      Legacy AWD\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41142\n      18\n      21\n      24\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      2000\n      Subaru\n      Legacy AWD\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n    \n      41143\n      16\n      18\n      21\n      4\n      2.199219\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      2900\n      Subaru\n      Legacy AWD Turbo\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1993\n    \n  \n\n41144 rows × 14 columns\n\n\n\n\n# new memory usage\n(autos\n #[cols]\n .loc[:,cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16'}) \n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n17590123\n\n\nA further reduction."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#objects",
    "href": "posts/Pandas/Effective_Pandas.html#objects",
    "title": "Efficient Pandas",
    "section": "Objects",
    "text": "Objects\nAt the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices.\n\n# let's take a look at our object columns\n(autos\n [cols]\n .select_dtypes(object)\n)\n\n\n\n\n\n  \n    \n      \n      drive\n      eng_dscr\n      make\n      model\n      trany\n      createdOn\n    \n  \n  \n    \n      0\n      Rear-Wheel Drive\n      (FFS)\n      Alfa Romeo\n      Spider Veloce 2000\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      1\n      Rear-Wheel Drive\n      (GUZZLER)\n      Ferrari\n      Testarossa\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      2\n      Front-Wheel Drive\n      (FFS)\n      Dodge\n      Charger\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      3\n      Rear-Wheel Drive\n      NaN\n      Dodge\n      B150/B250 Wagon 2WD\n      Automatic 3-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      4\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      Subaru\n      Legacy AWD Turbo\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      5\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Loyale\n      Automatic 3-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      6\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Loyale\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      7\n      Front-Wheel Drive\n      (FFS)\n      Toyota\n      Corolla\n      Automatic 3-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      8\n      Front-Wheel Drive\n      (FFS)\n      Toyota\n      Corolla\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      9\n      Front-Wheel Drive\n      (FFS)\n      Toyota\n      Corolla\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      41134\n      Front-Wheel Drive\n      (FFS)\n      Saab\n      900\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41135\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      Saturn\n      SL\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41136\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      Saturn\n      SL\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41137\n      Front-Wheel Drive\n      (TBI)      (FFS)\n      Saturn\n      SL\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41138\n      Front-Wheel Drive\n      (MFI)      (FFS)\n      Saturn\n      SL\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41139\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41140\n      Front-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41141\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy AWD\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41142\n      4-Wheel or All-Wheel Drive\n      (FFS)\n      Subaru\n      Legacy AWD\n      Manual 5-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n    \n      41143\n      4-Wheel or All-Wheel Drive\n      (FFS,TRBO)\n      Subaru\n      Legacy AWD Turbo\n      Automatic 4-spd\n      Tue Jan 01 00:00:00 EST 2013\n    \n  \n\n41144 rows × 6 columns\n\n\n\n\n## drive looks categorical\n(autos\n .drive\n .value_counts(dropna=False)\n)\n\nFront-Wheel Drive             14236\nRear-Wheel Drive              13831\n4-Wheel or All-Wheel Drive     6648\nAll-Wheel Drive                3015\n4-Wheel Drive                  1460\nNaN                            1189\n2-Wheel Drive                   507\nPart-time 4-Wheel Drive         258\nName: drive, dtype: int64\n\n\n\n# Where are the missing values NaN ?\n(autos\n [cols]\n .query('drive.isna()')\n)\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      eng_dscr\n      fuelCost08\n      make\n      model\n      trany\n      range\n      createdOn\n      year\n    \n  \n  \n    \n      7138\n      81\n      85\n      91\n      NaN\n      NaN\n      NaN\n      NaN\n      800\n      Nissan\n      Altra EV\n      NaN\n      90\n      Tue Jan 01 00:00:00 EST 2013\n      2000\n    \n    \n      8144\n      74\n      65\n      58\n      NaN\n      NaN\n      NaN\n      NaN\n      1000\n      Ford\n      Th!nk\n      NaN\n      29\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      8147\n      84\n      75\n      66\n      NaN\n      NaN\n      NaN\n      NaN\n      900\n      Nissan\n      Hyper-Mini\n      NaN\n      33\n      Tue Jan 01 00:00:00 EST 2013\n      2001\n    \n    \n      18217\n      18\n      21\n      25\n      4.0\n      2.0\n      NaN\n      (FFS)\n      2000\n      Alfa Romeo\n      Spider Veloce 2000\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18218\n      20\n      22\n      26\n      4.0\n      1.5\n      NaN\n      (FFS)\n      1900\n      Bertone\n      X1/9\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18219\n      13\n      15\n      20\n      8.0\n      5.7\n      NaN\n      (350 V8) (FFS)\n      2800\n      Chevrolet\n      Corvette\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18220\n      13\n      15\n      20\n      8.0\n      5.7\n      NaN\n      (350 V8) (FFS)\n      2800\n      Chevrolet\n      Corvette\n      Manual 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18221\n      15\n      17\n      20\n      6.0\n      3.0\n      NaN\n      (FFS,TRBO)\n      2500\n      Nissan\n      300ZX\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18222\n      16\n      18\n      20\n      6.0\n      3.0\n      NaN\n      (FFS)\n      2350\n      Nissan\n      300ZX\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      18223\n      16\n      18\n      22\n      6.0\n      3.0\n      NaN\n      (FFS,TRBO)\n      2350\n      Nissan\n      300ZX\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      20063\n      13\n      15\n      19\n      8.0\n      5.0\n      NaN\n      (FFS) CA model\n      2800\n      Mercury\n      Grand Marquis Wagon\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      20064\n      13\n      15\n      20\n      8.0\n      5.0\n      NaN\n      (GM-OLDS) CA model\n      2800\n      Oldsmobile\n      Custom Cruiser Wagon\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      20065\n      14\n      16\n      19\n      8.0\n      5.0\n      NaN\n      (GM-CHEV) CA model\n      2650\n      Pontiac\n      Parisienne Wagon\n      Automatic 4-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      20387\n      14\n      14\n      15\n      4.0\n      2.4\n      NaN\n      (FFS) CA model\n      3000\n      Nissan\n      Pickup Cab Chassis\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      1984\n    \n    \n      21129\n      14\n      16\n      21\n      8.0\n      3.5\n      NaN\n      GUZZLER  FFS,TURBO\n      3250\n      Lotus\n      Esprit V8\n      Manual 5-spd\n      0\n      Tue Jan 01 00:00:00 EST 2013\n      2002\n    \n    \n      23029\n      79\n      85\n      94\n      NaN\n      NaN\n      NaN\n      Lead Acid\n      800\n      GMC\n      EV1\n      Automatic (A1)\n      55\n      Tue Jan 01 00:00:00 EST 2013\n      1999\n    \n    \n      23030\n      35\n      37\n      39\n      NaN\n      NaN\n      NaN\n      NiMH\n      1750\n      GMC\n      EV1\n      Automatic (A1)\n      105\n      Tue Jan 01 00:00:00 EST 2013\n      1999\n    \n    \n      23032\n      49\n      48\n      46\n      NaN\n      NaN\n      NaN\n      NaN\n      1400\n      Honda\n      EV Plus\n      Automatic (A1)\n      81\n      Tue Jan 01 00:00:00 EST 2013\n      1999\n    \n    \n      23037\n      49\n      48\n      46\n      NaN\n      NaN\n      NaN\n      NaN\n      1400\n      Honda\n      EV Plus\n      Automatic (A1)\n      81\n      Tue Jan 01 00:00:00 EST 2013\n      1998\n    \n    \n      23040\n      102\n      98\n      94\n      NaN\n      NaN\n      NaN\n      NaN\n      650\n      MINI\n      MiniE\n      Automatic (A1)\n      100\n      Tue Jan 01 00:00:00 EST 2013\n      2008\n    \n  \n\n1189 rows × 14 columns\n\n\n\n\n# let's look at the drive column, grouped by year\n(autos\n [cols]\n .groupby('year')\n .drive\n .nunique()\n) \n\nyear\n1984    3\n1985    4\n1986    4\n1987    3\n1988    3\n1989    3\n1990    3\n1991    3\n1992    3\n1993    3\n1994    3\n1995    4\n1996    3\n1997    4\n1998    4\n1999    4\n2000    4\n2001    4\n2002    4\n2003    4\n2004    4\n2005    4\n2006    4\n2007    4\n2008    3\n2009    4\n2010    6\n2011    5\n2012    5\n2013    5\n2014    5\n2015    5\n2016    5\n2017    5\n2018    5\n2019    5\n2020    5\nName: drive, dtype: int64\n\n\n\n# let's convert drive to category, replacing NaN with 'Other using .assign .astype\n# and convert make to category, updating .astype dictionary \n# and check our memory usage\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'))\n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n12093275\n\n\nAs we can see, converting to category has freed up a lot of space, a reduction from 17590123 - just over 30%\n\n# Let's inspect trany \n# looks like it has two pices of info embedded in the column\n\n(autos\n .trany\n .value_counts(dropna=False)\n)\n\nAutomatic 4-spd                     11047\nManual 5-spd                         8361\nAutomatic 3-spd                      3151\nAutomatic (S6)                       3106\nManual 6-spd                         2757\nAutomatic 5-spd                      2203\nAutomatic (S8)                       1665\nAutomatic 6-spd                      1619\nManual 4-spd                         1483\nAutomatic (S5)                        833\nAutomatic (variable gear ratios)      826\nAutomatic 7-spd                       724\nAutomatic 8-spd                       433\nAutomatic (AM-S7)                     424\nAutomatic (S7)                        327\nAutomatic 9-spd                       293\nAutomatic (AM7)                       245\nAutomatic (S4)                        233\nAutomatic (AV-S6)                     208\nAutomatic (A1)                        201\nAutomatic (AM6)                       151\nAutomatic (AV-S7)                     139\nAutomatic (S10)                       124\nAutomatic (AM-S6)                     116\nManual 7-spd                          114\nAutomatic (S9)                         86\nManual 3-spd                           77\nAutomatic (AM-S8)                      60\nAutomatic (AV-S8)                      47\nAutomatic 10-spd                       25\nManual 4-spd Doubled                   17\nAutomatic (AM5)                        14\nNaN                                    11\nAutomatic (AV-S10)                     11\nAutomatic (AM8)                         6\nAutomatic (AM-S9)                       3\nAutomatic (L3)                          2\nAutomatic (L4)                          2\nName: trany, dtype: int64\n\n\n11 NaN values\nThe information from the trany column seems to have two components:\n\nAutomatic v Manual\nSpeed\n\n\n# add new columns for automatic using .str.contains\n# add new column for speeds using .str.extract\n# drop exisitng trany column\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8')\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany'])\n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n10631047\n\n\nGreat, another reduction - we have almost halved our original memory usage."
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#dates",
    "href": "posts/Pandas/Effective_Pandas.html#dates",
    "title": "Efficient Pandas",
    "section": "Dates",
    "text": "Dates\npandas contains extensive capabilities and features for working with time series data for all domains. Check out the documentation for more info.\nWe can convert the CreatedOn column from an object to datetime using.\n\n# add createdOn using pd.to_datetime .dt.tz_localize\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn).dt.tz_localize('America/New_York')\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany'])\n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n/home/stephen137/mambaforge/lib/python3.10/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n\n\n7462959\n\n\nExcellent, we have successfully reduced our memory usage by 62%!\n\n# Pythom doesn't like EST/EDT\n(autos\n [cols]\n .createdOn\n)\n\n0        Tue Jan 01 00:00:00 EST 2013\n1        Tue Jan 01 00:00:00 EST 2013\n2        Tue Jan 01 00:00:00 EST 2013\n3        Tue Jan 01 00:00:00 EST 2013\n4        Tue Jan 01 00:00:00 EST 2013\n5        Tue Jan 01 00:00:00 EST 2013\n6        Tue Jan 01 00:00:00 EST 2013\n7        Tue Jan 01 00:00:00 EST 2013\n8        Tue Jan 01 00:00:00 EST 2013\n9        Tue Jan 01 00:00:00 EST 2013\n                     ...             \n41134    Tue Jan 01 00:00:00 EST 2013\n41135    Tue Jan 01 00:00:00 EST 2013\n41136    Tue Jan 01 00:00:00 EST 2013\n41137    Tue Jan 01 00:00:00 EST 2013\n41138    Tue Jan 01 00:00:00 EST 2013\n41139    Tue Jan 01 00:00:00 EST 2013\n41140    Tue Jan 01 00:00:00 EST 2013\n41141    Tue Jan 01 00:00:00 EST 2013\n41142    Tue Jan 01 00:00:00 EST 2013\n41143    Tue Jan 01 00:00:00 EST 2013\nName: createdOn, Length: 41144, dtype: object\n\n\n\n# Fix date warnings - move on to eng_dscr\n# https://www.fueleconomy.gov/feg/findacarhelp.shtml\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True))\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany'])\n .eng_dscr\n .value_counts(dropna=False)\n)\n\nNaN                                 16153\n(FFS)                                8827\nSIDI                                 5526\n(FFS) CA model                        926\n(FFS)      (MPFI)                     734\nFFV                                   701\n(FFS,TRBO)                            666\n(350 V8) (FFS)                        411\n(GUZZLER)  (FFS)                      366\nSOHC                                  354\n                                    ...  \nB234L/R4 (FFS,TRBO)                     1\nGUZZLER V8 FFS,TURBO                    1\n4.6M FFS MPFI                           1\nCNG FFS                                 1\nPOLICE FFS MPFI                         1\nB308E5 FFS,TURBO                        1\n5.4E-R FFS MPFI                         1\nV-6 FFS                                 1\n(GUZZLER)  (FFS)      (S-CHARGE)        1\nR-ENG (FFS,TRBO)                        1\nName: eng_dscr, Length: 558, dtype: int64\n\n\nAs we can see the majority of values within the eng_dscr column are NaN and the other values are very messy. How should we deal with this?\n\n# drop eng_dscr column, and bring in an FFS column (feedback fuel system)\n# check update to memory usage\n\n(autos\n [cols]\n .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True)),\n         ffs=autos.eng_dscr.str.contains('FFS')\n        ) \n .astype({'city08': 'int16', 'comb08': 'int16', 'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n .drop(columns=['trany','eng_dscr'])\n .memory_usage(deep=True)\n .sum() # was 19,647,323\n)\n\n8676214"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#functions---.apply",
    "href": "posts/Pandas/Effective_Pandas.html#functions---.apply",
    "title": "Efficient Pandas",
    "section": "Functions - .apply",
    "text": "Functions - .apply\nLet’s now create a function which brings together all the exploratory data analysis we have performed in one place:\n\ndef autos_tweak(autos):\n    cols = ['city08','comb08', 'highway08','cylinders', 'displ', 'drive', 'eng_dscr', 'fuelCost08',\n      'make', 'model', 'trany', 'range', 'createdOn', 'year']\n    return (autos\n    [cols]\n    .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True)),\n         ffs=autos.eng_dscr.str.contains('FFS')\n        ) \n    .astype({'city08': 'int16', 'comb08': 'int16',  'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n    .drop(columns=['trany','eng_dscr'])\n    )\n\nLook how neat and tidy the above code is compared to the following alternative approach:\n\na1 = autos[cols]\ncyls = autos.cylinders.fillna(0)\ncyls2 = cyls.astype('int8')\na1['cylinders'] = cyls2\ndispl = a1.displ\ndispl2 = displ.fillna(0)\ndispl3=displ2.astype('float16')\na1.displ=displ3\na1.drive=autos.drive.fillna('Other').astype('category')\na1['automatic'] = autos.trany.str.contains('Auto')\nspeed=autos.trany.str.extract(r'(\\d)+')\nspeedfill = speed.fillna('20')\nspeedint = speedfill.astype('int8')\na1['speeds'] = speedint\na1.createdOn=pd.to_datetime(autos.createdOn).dt.tz_localize('America/New_York')\na1.ffs=autos.eng_dscr.str.contains('FFS')\na1['highway08'] = autos.highway08.astype('int8')\na1['city08'] = autos.city08.astype('int8')\na1['comb08'] = autos.comb08.astype('int16')\na1['fuelCost08'] =autos.fuelCost08.astype('int16')\na1['range'] = autos.range.astype('int16')\na1['make'] = autos.make.astype('category')\na3 = a1.drop(columns=['trany','eng_dscr'])"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#dont-mutate",
    "href": "posts/Pandas/Effective_Pandas.html#dont-mutate",
    "title": "Efficient Pandas",
    "section": "Don’t Mutate",
    "text": "Don’t Mutate\n“you are missing the point, inplace rarely actually does something inplace. you are thinking that you are saving memory but you are not.”\njreback - Pandas core dev\nhttps://github.com/pandas-dev/pandas/issues/16529#issuecomment-676518136\n\nin general, no performance benefits\nprohibits chaining\nSettingWithCopyWarning fun"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#try-to-avoid-using-.apply-where-possible",
    "href": "posts/Pandas/Effective_Pandas.html#try-to-avoid-using-.apply-where-possible",
    "title": "Efficient Pandas",
    "section": "Try to avoid using .apply (where possible)",
    "text": "Try to avoid using .apply (where possible)\n\ndef autos_tweak(autos):\n    cols = ['city08','comb08', 'highway08','cylinders', 'displ', 'drive', 'eng_dscr', 'fuelCost08',\n      'make', 'model', 'trany', 'range', 'createdOn', 'year']\n    return (autos\n    [cols]\n    .assign(cylinders=autos.cylinders.fillna(0).astype('int8'),\n         displ=autos.displ.fillna(0).astype('float16'),\n         drive=autos.drive.fillna('Other').astype('category'),\n         automatic=autos.trany.str.contains('Auto'),\n         speeds=autos.trany.str.extract(r'(\\d)+').fillna('20').astype('int8'),\n         createdOn=pd.to_datetime(autos.createdOn.replace({'EDT': '-04:00', 'EST':'-05:00'}, regex=True)),\n         ffs=autos.eng_dscr.str.contains('FFS')\n        ) \n    .astype({'city08': 'int16', 'comb08': 'int16',  'highway08': 'int8','fuelCost08':'int16', 'range':'int16', 'year':'int16', 'make': 'category'}) \n    .drop(columns=['trany','eng_dscr'])\n    )\n\nautos2 = autos_tweak(autos)\n\n\n# try to be more Euro-centric\ndef to_lper100km(val):\n    return 235.215 / val\n\n\n%%timeit\nautos2.city08.apply(to_lper100km)\n\n5.3 ms ± 390 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n4.95 ms (milliseconds) is equivalent to 4,950 μs (microseconds)\nThere is a lot of computational overhead using this method - the function pulls out each individual entry from the Series, convert it to a Python object, pass the individual entry into the function, and then convert back to a pandas object.\n\n%%timeit\n# note that you can't run %%timeit with a leading #comment\n# this gives the same results\n235.215 / autos2.city08\n\n84.5 µs ± 3.29 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n90.7 μs (microseconds)\nThis gives the same answer but is more than 50 x faster than using the .apply method, because it is leveraging modern CPU single instruction multiple data (SIMD) architecture - here’s a block of data - do the division on it.\n\n# create a function to return whether the make of the car is US\ndef is_american(val):\n    return val in {'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}\n\n\n%%timeit\n\n# use .apply\nautos2.make.apply(is_american)\n\n233 µs ± 6.36 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n245 μs (microseconds)\n\n%%timeit\n\n# use .isin \nautos2.make.isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'})\n\n448 µs ± 9.32 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n465 μs (microseconds)\n\n%%timeit\n\n# use .astype(str) and then .isin\nautos2.make.astype(str).isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'})\n\n4.91 ms ± 65.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n5.35 ms (milliseconds) is equivalent to 5,350 μs (microseconds)\n\n%%timeit\n\n# use .astype(str) and then .apply\nautos2.make.astype(str).apply(is_american)\n\n8.23 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n8.93 ms (milliseconds) is equivalent to 8,930 μs (microseconds)\nIn the case of the categorical column - make:\n\nthe .apply method on the function was fastest\nthe .isin method was next fastest (~ 2 x slower)\nthird fastest was (~ 22 x slower)\nfinally the .astype(str).apply method (~36 x slower)\n\n\ndef country(val):\n    if val in {'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}:\n        return 'US'\n    # else\n    return 'Other'\n\n\n%%timeit\n\n# use .apply\n# Might be OK for strings, since they are not vectorized...\n(autos2\n .assign(country=autos2.make.apply(country))\n)\n\n2.14 ms ± 66.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit \n\n# use .assign\nvalues = {'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}\n(autos2\n .assign(country='US')\n .assign(country=lambda df_:df_.country.where(df_.make.isin(values), 'Other'))\n)\n\n4.31 ms ± 83 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit \n\n# using MumPy .select - allows you to specify a list of Booleans, and wherever they hold true [i.e make is Chevrolet, Ford, Dodge...Tesla, you specify the value ['US'] to put into placeholder \n# this method is not available within pandas\n(autos2\n .assign(country=np.select([autos2.make.isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'})],\n                           ['US'], 'Other'))\n)                          \n\n3.36 ms ± 145 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit \n\n# using MumPy .where - allows you to specify a list of Booleans, and wherever they hold true [i.e make is Chevrolet, Ford, Dodge...Tesla, you specify the value ['US'] to put into placeholder \n# this method is not available within pandas\n(autos2\n .assign(country=np.where(autos2.make.isin({'Chevrolet', 'Ford', 'Dodge', 'GMC', 'Tesla'}),\n                           ['US'], 'Other'))\n)  \n\n3.38 ms ± 35.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nKey takeaways\n\nif you find yourself using a for loop alarm bells should be ringing!\nyou could use .apply but still slow\napply where or np.select\nthe same result can be obtained much faster using list comprehension"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#aggregation",
    "href": "posts/Pandas/Effective_Pandas.html#aggregation",
    "title": "Efficient Pandas",
    "section": "Aggregation",
    "text": "Aggregation\nIt is important as a data science to work with the raw data and get to know the finer details, but ultimately, providing higher level insights are our main goal. This can be obtained by aggregating data. Let’s compare mileage by country by year…\n\n# start off with auto\n# group by year\n# then grab the mean values\n(autos2\n # Year will therefore be our index\n .groupby('year')\n .mean()\n)\n\n/tmp/ipykernel_2753/262800323.py:7: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      speeds\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      17.982688\n      19.881874\n      23.075356\n      5.385438\n      3.165017\n      2313.543788\n      0.000000\n      3.928208\n    \n    \n      1985\n      17.878307\n      19.808348\n      23.042328\n      5.375661\n      3.164080\n      2334.509112\n      0.000000\n      3.924750\n    \n    \n      1986\n      17.665289\n      19.550413\n      22.699174\n      5.425620\n      3.183762\n      2354.049587\n      0.000000\n      3.984298\n    \n    \n      1987\n      17.310345\n      19.228549\n      22.445068\n      5.412189\n      3.173949\n      2403.648757\n      0.000000\n      4.037690\n    \n    \n      1988\n      17.333628\n      19.328319\n      22.702655\n      5.461947\n      3.194899\n      2387.035398\n      0.000000\n      4.129204\n    \n    \n      1989\n      17.143972\n      19.125759\n      22.465742\n      5.488291\n      3.209926\n      2433.434519\n      0.000000\n      4.166522\n    \n    \n      1990\n      17.033395\n      19.000928\n      22.337662\n      5.496289\n      3.217369\n      2436.178108\n      0.000000\n      4.238404\n    \n    \n      1991\n      16.848940\n      18.825972\n      22.253534\n      5.598940\n      3.266809\n      2490.856890\n      0.000000\n      4.301237\n    \n    \n      1992\n      16.805531\n      18.862623\n      22.439786\n      5.623550\n      3.276159\n      2494.736842\n      0.000000\n      4.318466\n    \n    \n      1993\n      16.998170\n      19.104300\n      22.780421\n      5.602928\n      3.248540\n      2454.620311\n      0.000000\n      4.339433\n    \n    \n      1994\n      16.918534\n      19.012220\n      22.725051\n      5.704684\n      3.333190\n      2461.507128\n      0.000000\n      4.332994\n    \n    \n      1995\n      16.569804\n      18.797311\n      22.671148\n      5.892451\n      3.471776\n      2497.828335\n      0.000000\n      4.356774\n    \n    \n      1996\n      17.289780\n      19.584735\n      23.569211\n      5.627426\n      3.234789\n      2375.032342\n      0.000000\n      4.364812\n    \n    \n      1997\n      17.135171\n      19.429134\n      23.451444\n      5.666667\n      3.226933\n      2405.511811\n      0.000000\n      4.402887\n    \n    \n      1998\n      17.113300\n      19.518473\n      23.546798\n      5.633005\n      3.201979\n      2382.635468\n      0.229064\n      4.419951\n    \n    \n      1999\n      17.272300\n      19.611502\n      23.552817\n      5.667840\n      3.188794\n      2392.194836\n      0.570423\n      4.421362\n    \n    \n      2000\n      17.221429\n      19.526190\n      23.414286\n      5.713095\n      3.200517\n      2429.702381\n      0.348810\n      4.508333\n    \n    \n      2001\n      17.275521\n      19.479693\n      23.328211\n      5.720088\n      3.192452\n      2448.463227\n      0.261251\n      4.660812\n    \n    \n      2002\n      16.893333\n      19.168205\n      23.030769\n      5.827692\n      3.264525\n      2479.794872\n      0.136410\n      4.757949\n    \n    \n      2003\n      16.780651\n      19.000958\n      22.836207\n      5.942529\n      3.358259\n      2525.574713\n      0.090996\n      4.911877\n    \n    \n      2004\n      16.740642\n      19.067736\n      23.064171\n      5.957219\n      3.393626\n      2512.566845\n      0.000000\n      4.976827\n    \n    \n      2005\n      16.851630\n      19.193825\n      23.297599\n      5.944254\n      3.399485\n      2518.610635\n      0.000000\n      5.192110\n    \n    \n      2006\n      16.626812\n      18.959239\n      23.048913\n      6.100543\n      3.549294\n      2539.175725\n      0.000000\n      5.315217\n    \n    \n      2007\n      16.605684\n      18.978686\n      23.083481\n      6.166075\n      3.628539\n      2535.923623\n      0.000000\n      5.610124\n    \n    \n      2008\n      16.900590\n      19.276327\n      23.455771\n      6.192923\n      3.637796\n      2536.436394\n      0.084246\n      5.773378\n    \n    \n      2009\n      17.334459\n      19.735642\n      24.017736\n      6.122466\n      3.624839\n      2427.027027\n      0.000000\n      6.043074\n    \n    \n      2010\n      18.105500\n      20.588819\n      24.947701\n      5.965735\n      3.502548\n      2351.082056\n      0.000000\n      6.271416\n    \n    \n      2011\n      18.669027\n      21.011504\n      25.169912\n      5.980531\n      3.521903\n      2333.982301\n      0.259292\n      6.560177\n    \n    \n      2012\n      19.362847\n      21.819444\n      26.105035\n      5.910590\n      3.460015\n      2289.973958\n      0.782118\n      6.706597\n    \n    \n      2013\n      20.661318\n      23.125000\n      27.504223\n      5.762669\n      3.327529\n      2210.768581\n      1.255068\n      6.896959\n    \n    \n      2014\n      21.033469\n      23.531429\n      27.978776\n      5.745306\n      3.289703\n      2198.040816\n      1.405714\n      6.985306\n    \n    \n      2015\n      21.445830\n      24.038971\n      28.586906\n      5.635230\n      3.205085\n      2148.869836\n      2.208106\n      7.035853\n    \n    \n      2016\n      22.591918\n      25.150555\n      29.606973\n      5.463550\n      3.054415\n      2091.204437\n      4.546751\n      7.080032\n    \n    \n      2017\n      22.761021\n      25.249033\n      29.554524\n      5.453210\n      3.026032\n      2096.558391\n      4.336427\n      7.225058\n    \n    \n      2018\n      22.564732\n      25.019345\n      29.273065\n      5.438988\n      2.992239\n      2103.980655\n      3.519345\n      7.017113\n    \n    \n      2019\n      23.318147\n      25.627942\n      29.664389\n      5.368261\n      2.964679\n      2093.545938\n      5.565680\n      7.136674\n    \n    \n      2020\n      22.679426\n      25.267943\n      29.617225\n      5.071770\n      2.644994\n      2023.444976\n      2.282297\n      7.746411\n    \n  \n\n\n\n\n\n# let's focus on just the comb08 and speeds columns\n(autos2\n .groupby('year')\n [['comb08','speeds']]\n .mean()\n)\n\n\n\n\n\n  \n    \n      \n      comb08\n      speeds\n    \n    \n      year\n      \n      \n    \n  \n  \n    \n      1984\n      19.881874\n      3.928208\n    \n    \n      1985\n      19.808348\n      3.924750\n    \n    \n      1986\n      19.550413\n      3.984298\n    \n    \n      1987\n      19.228549\n      4.037690\n    \n    \n      1988\n      19.328319\n      4.129204\n    \n    \n      1989\n      19.125759\n      4.166522\n    \n    \n      1990\n      19.000928\n      4.238404\n    \n    \n      1991\n      18.825972\n      4.301237\n    \n    \n      1992\n      18.862623\n      4.318466\n    \n    \n      1993\n      19.104300\n      4.339433\n    \n    \n      1994\n      19.012220\n      4.332994\n    \n    \n      1995\n      18.797311\n      4.356774\n    \n    \n      1996\n      19.584735\n      4.364812\n    \n    \n      1997\n      19.429134\n      4.402887\n    \n    \n      1998\n      19.518473\n      4.419951\n    \n    \n      1999\n      19.611502\n      4.421362\n    \n    \n      2000\n      19.526190\n      4.508333\n    \n    \n      2001\n      19.479693\n      4.660812\n    \n    \n      2002\n      19.168205\n      4.757949\n    \n    \n      2003\n      19.000958\n      4.911877\n    \n    \n      2004\n      19.067736\n      4.976827\n    \n    \n      2005\n      19.193825\n      5.192110\n    \n    \n      2006\n      18.959239\n      5.315217\n    \n    \n      2007\n      18.978686\n      5.610124\n    \n    \n      2008\n      19.276327\n      5.773378\n    \n    \n      2009\n      19.735642\n      6.043074\n    \n    \n      2010\n      20.588819\n      6.271416\n    \n    \n      2011\n      21.011504\n      6.560177\n    \n    \n      2012\n      21.819444\n      6.706597\n    \n    \n      2013\n      23.125000\n      6.896959\n    \n    \n      2014\n      23.531429\n      6.985306\n    \n    \n      2015\n      24.038971\n      7.035853\n    \n    \n      2016\n      25.150555\n      7.080032\n    \n    \n      2017\n      25.249033\n      7.225058\n    \n    \n      2018\n      25.019345\n      7.017113\n    \n    \n      2019\n      25.627942\n      7.136674\n    \n    \n      2020\n      25.267943\n      7.746411\n    \n  \n\n\n\n\n\n%%timeit\n\n# Watch out for the ordering!!!\n# here we are grouping by year\n# but then we are taking average of all columns - computationally expensive\n# we are only interested in comb08 and speeds\n(autos2\n .groupby('year')\n .mean()\n #.median()\n #.quantile(.1)\n #.std()\n [['comb08','speeds']]\n)"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#visualizations",
    "href": "posts/Pandas/Effective_Pandas.html#visualizations",
    "title": "Efficient Pandas",
    "section": "Visualizations",
    "text": "Visualizations\nThe pandas library includes a variety of visualization tools which allow us to communicate our findings visually. Note that is very easy to show a variety of different plots quickly, simply by commenting out (#) to leave the desired plot:\n\n# in pandas default plot is a line plot\n# with index as the x axis and the selected grouped columns as the lines\n(autos2\n .groupby('year')\n  [['comb08','speeds']]\n #.mean()\n #.median()\n #.quantile(.1)\n .std()\n #.var()\n .plot()\n)\n\n<AxesSubplot: xlabel='year'>\n\n\n\n\n\n\n# add country\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n)\n\n/tmp/ipykernel_2753/361744348.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n\n\n\n\n  \n    \n      \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      speeds\n    \n    \n      year\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      Other\n      19.384615\n      21.417330\n      24.847038\n      4.908046\n      2.690516\n      2118.125553\n      0.000000\n      3.969054\n    \n    \n      US\n      16.079232\n      17.797119\n      20.669868\n      6.033613\n      3.809268\n      2578.871549\n      0.000000\n      3.872749\n    \n    \n      1985\n      Other\n      19.284768\n      21.373068\n      24.816777\n      4.871965\n      2.636070\n      2141.997792\n      0.000000\n      3.958057\n    \n    \n      US\n      16.275472\n      18.025157\n      21.020126\n      5.949686\n      3.765813\n      2553.899371\n      0.000000\n      3.886792\n    \n    \n      1986\n      Other\n      19.167183\n      21.213622\n      24.650155\n      4.804954\n      2.536234\n      2149.148607\n      0.000000\n      4.069659\n    \n    \n      US\n      15.945035\n      17.645390\n      20.464539\n      6.136525\n      3.925433\n      2588.741135\n      0.000000\n      3.886525\n    \n    \n      1987\n      Other\n      18.633381\n      20.710414\n      24.186876\n      4.825963\n      2.583168\n      2227.318117\n      0.000000\n      4.142653\n    \n    \n      US\n      15.611722\n      17.326007\n      20.208791\n      6.164835\n      3.932442\n      2630.036630\n      0.000000\n      3.902930\n    \n    \n      1988\n      Other\n      18.668224\n      20.814642\n      24.437695\n      4.819315\n      2.531434\n      2207.476636\n      0.000000\n      4.205607\n    \n    \n      US\n      15.577869\n      17.372951\n      20.420082\n      6.307377\n      4.067735\n      2623.258197\n      0.000000\n      4.028689\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2016\n      Other\n      21.903749\n      24.439716\n      28.866261\n      5.493414\n      2.992272\n      2127.608916\n      1.017224\n      7.296859\n    \n    \n      US\n      25.061818\n      27.701818\n      32.265455\n      5.356364\n      3.277454\n      1960.545455\n      17.214545\n      6.301818\n    \n    \n      2017\n      Other\n      22.423795\n      24.910521\n      29.208456\n      5.431662\n      2.919041\n      2114.110128\n      1.243854\n      7.474926\n    \n    \n      US\n      24.003623\n      26.496377\n      30.829710\n      5.532609\n      3.420272\n      2031.884058\n      15.731884\n      6.304348\n    \n    \n      2018\n      Other\n      22.310442\n      24.779868\n      29.042333\n      5.396990\n      2.886801\n      2121.448730\n      1.135466\n      7.391345\n    \n    \n      US\n      23.526690\n      25.925267\n      30.145907\n      5.597865\n      3.391101\n      2037.900356\n      12.537367\n      5.601423\n    \n    \n      2019\n      Other\n      23.084221\n      25.456922\n      29.560503\n      5.315586\n      2.839671\n      2093.659245\n      2.581801\n      7.545983\n    \n    \n      US\n      24.169014\n      26.250000\n      30.042254\n      5.559859\n      3.419375\n      2093.133803\n      16.419014\n      5.647887\n    \n    \n      2020\n      Other\n      22.579487\n      25.174359\n      29.543590\n      5.148718\n      2.692823\n      2050.256410\n      2.446154\n      7.743590\n    \n    \n      US\n      24.071429\n      26.571429\n      30.642857\n      4.000000\n      1.978795\n      1650.000000\n      0.000000\n      7.785714\n    \n  \n\n74 rows × 8 columns\n\n\n\n\n# we can go deeper and apply multiple aggregates\n# this is loosely equivalent to the sort of thing that a pivot table in Excel might provide\n\n# penultimate row\ndef second_to_last(ser):\n    return ser.iloc[-2]\n\n(autos2\n .assign(country=autos2.make.apply(country))\n .groupby(['year', 'country'])\n# we can use .agg to include a list of different aggregation types - we can even call a function\n .agg(['min', 'mean', second_to_last])\n)\n\n/tmp/ipykernel_2753/2706922386.py:12: FutureWarning: ['drive', 'make', 'model', 'createdOn'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n  .agg(['min', 'mean', second_to_last])\n\n\n\n\n\n\n  \n    \n      \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      ...\n      range\n      automatic\n      speeds\n      ffs\n    \n    \n      \n      \n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      ...\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n      min\n      mean\n      second_to_last\n    \n    \n      year\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      Other\n      7\n      19.384615\n      14\n      8\n      21.417330\n      14\n      9\n      24.847038\n      15\n      2\n      ...\n      0\n      False\n      0.550840\n      False\n      3\n      3.969054\n      5\n      False\n      0.714554\n      True\n    \n    \n      US\n      8\n      16.079232\n      15\n      9\n      17.797119\n      17\n      10\n      20.669868\n      19\n      4\n      ...\n      0\n      False\n      0.521059\n      False\n      3\n      3.872749\n      4\n      False\n      0.638801\n      NaN\n    \n    \n      1985\n      Other\n      7\n      19.284768\n      19\n      8\n      21.373068\n      20\n      9\n      24.816777\n      22\n      0\n      ...\n      0\n      False\n      0.554084\n      True\n      3\n      3.958057\n      4\n      False\n      0.889160\n      True\n    \n    \n      US\n      8\n      16.275472\n      14\n      10\n      18.025157\n      15\n      10\n      21.020126\n      17\n      3\n      ...\n      0\n      False\n      0.520755\n      False\n      3\n      3.886792\n      4\n      False\n      0.851351\n      NaN\n    \n    \n      1986\n      Other\n      6\n      19.167183\n      10\n      7\n      21.213622\n      11\n      9\n      24.650155\n      12\n      0\n      ...\n      0\n      False\n      0.520124\n      False\n      3\n      4.069659\n      4\n      False\n      0.934211\n      NaN\n    \n    \n      US\n      9\n      15.945035\n      16\n      10\n      17.645390\n      17\n      11\n      20.464539\n      19\n      3\n      ...\n      0\n      False\n      0.533688\n      False\n      3\n      3.886525\n      4\n      False\n      0.795699\n      NaN\n    \n    \n      1987\n      Other\n      6\n      18.633381\n      12\n      7\n      20.710414\n      12\n      9\n      24.186876\n      12\n      2\n      ...\n      0\n      False\n      0.516405\n      True\n      3\n      4.142653\n      4\n      False\n      0.949778\n      True\n    \n    \n      US\n      8\n      15.611722\n      12\n      9\n      17.326007\n      13\n      10\n      20.208791\n      14\n      3\n      ...\n      0\n      False\n      0.549451\n      True\n      3\n      3.902930\n      4\n      False\n      0.909457\n      True\n    \n    \n      1988\n      Other\n      6\n      18.668224\n      12\n      7\n      20.814642\n      12\n      10\n      24.437695\n      12\n      2\n      ...\n      0\n      False\n      0.521807\n      True\n      3\n      4.205607\n      4\n      False\n      0.993681\n      True\n    \n    \n      US\n      8\n      15.577869\n      14\n      9\n      17.372951\n      14\n      10\n      20.420082\n      15\n      3\n      ...\n      0\n      False\n      0.569672\n      True\n      3\n      4.028689\n      4\n      False\n      0.936306\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2016\n      Other\n      10\n      21.903749\n      28\n      12\n      24.439716\n      30\n      13\n      28.866261\n      32\n      0\n      ...\n      0\n      False\n      0.837893\n      True\n      1\n      7.296859\n      7\n      False\n      0.000000\n      False\n    \n    \n      US\n      11\n      25.061818\n      91\n      12\n      27.701818\n      93\n      16\n      32.265455\n      94\n      0\n      ...\n      200\n      False\n      0.850909\n      True\n      1\n      6.301818\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2017\n      Other\n      10\n      22.423795\n      21\n      11\n      24.910521\n      24\n      11\n      29.208456\n      28\n      0\n      ...\n      0\n      False\n      0.848574\n      True\n      1\n      7.474926\n      7\n      False\n      0.000000\n      False\n    \n    \n      US\n      11\n      24.003623\n      131\n      12\n      26.496377\n      126\n      15\n      30.829710\n      120\n      0\n      ...\n      310\n      False\n      0.858696\n      True\n      0\n      6.304348\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2018\n      Other\n      9\n      22.310442\n      11\n      11\n      24.779868\n      12\n      11\n      29.042333\n      15\n      0\n      ...\n      0\n      False\n      0.863594\n      True\n      0\n      7.391345\n      0\n      False\n      0.000000\n      NaN\n    \n    \n      US\n      11\n      23.526690\n      120\n      14\n      25.925267\n      116\n      15\n      30.145907\n      112\n      0\n      ...\n      310\n      False\n      0.882562\n      True\n      0\n      5.601423\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2019\n      Other\n      9\n      23.084221\n      19\n      11\n      25.456922\n      22\n      14\n      29.560503\n      27\n      0\n      ...\n      0\n      False\n      0.879961\n      True\n      0\n      7.545983\n      8\n      False\n      0.000000\n      NaN\n    \n    \n      US\n      11\n      24.169014\n      104\n      14\n      26.250000\n      104\n      15\n      30.042254\n      104\n      0\n      ...\n      345\n      False\n      0.915493\n      True\n      0\n      5.647887\n      1\n      False\n      0.000000\n      NaN\n    \n    \n      2020\n      Other\n      13\n      22.579487\n      17\n      15\n      25.174359\n      20\n      18\n      29.543590\n      24\n      0\n      ...\n      0\n      False\n      0.871795\n      True\n      0\n      7.743590\n      0\n      False\n      0.000000\n      False\n    \n    \n      US\n      20\n      24.071429\n      21\n      22\n      26.571429\n      24\n      26\n      30.642857\n      28\n      4\n      ...\n      0\n      False\n      0.857143\n      True\n      0\n      7.785714\n      0\n      False\n      0.000000\n      False\n    \n  \n\n74 rows × 30 columns\n\n\n\n\n# back to simpler example, adding plots\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n .plot()\n)\n\n/tmp/ipykernel_2753/1855101905.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n<AxesSubplot: xlabel='year,country'>\n\n\n\n\n\nThis doesn’t really work as we can see. Let’s see if we can resolve this:\n\n# back to simpler example, adding plots\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n # rotates or unstacks the innermost index, country\n .unstack()\n)\n\n/tmp/ipykernel_2753/1937283994.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n\n\n\n\n  \n    \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      fuelCost08\n      range\n      speeds\n    \n    \n      country\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n      Other\n      US\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      19.384615\n      16.079232\n      21.417330\n      17.797119\n      24.847038\n      20.669868\n      4.908046\n      6.033613\n      2.690516\n      3.809268\n      2118.125553\n      2578.871549\n      0.000000\n      0.000000\n      3.969054\n      3.872749\n    \n    \n      1985\n      19.284768\n      16.275472\n      21.373068\n      18.025157\n      24.816777\n      21.020126\n      4.871965\n      5.949686\n      2.636070\n      3.765813\n      2141.997792\n      2553.899371\n      0.000000\n      0.000000\n      3.958057\n      3.886792\n    \n    \n      1986\n      19.167183\n      15.945035\n      21.213622\n      17.645390\n      24.650155\n      20.464539\n      4.804954\n      6.136525\n      2.536234\n      3.925433\n      2149.148607\n      2588.741135\n      0.000000\n      0.000000\n      4.069659\n      3.886525\n    \n    \n      1987\n      18.633381\n      15.611722\n      20.710414\n      17.326007\n      24.186876\n      20.208791\n      4.825963\n      6.164835\n      2.583168\n      3.932442\n      2227.318117\n      2630.036630\n      0.000000\n      0.000000\n      4.142653\n      3.902930\n    \n    \n      1988\n      18.668224\n      15.577869\n      20.814642\n      17.372951\n      24.437695\n      20.420082\n      4.819315\n      6.307377\n      2.531434\n      4.067735\n      2207.476636\n      2623.258197\n      0.000000\n      0.000000\n      4.205607\n      4.028689\n    \n    \n      1989\n      18.533040\n      15.139831\n      20.662261\n      16.908898\n      24.252570\n      19.887712\n      4.879589\n      6.366525\n      2.542154\n      4.173385\n      2250.000000\n      2698.093220\n      0.000000\n      0.000000\n      4.264317\n      4.025424\n    \n    \n      1990\n      18.510109\n      14.850575\n      20.640747\n      16.577011\n      24.267496\n      19.485057\n      4.839813\n      6.466667\n      2.508090\n      4.265798\n      2238.258165\n      2728.735632\n      0.000000\n      0.000000\n      4.328149\n      4.105747\n    \n    \n      1991\n      18.087943\n      14.803279\n      20.174468\n      16.599532\n      23.809929\n      19.683841\n      5.029787\n      6.538642\n      2.610219\n      4.350876\n      2348.581560\n      2725.761124\n      0.000000\n      0.000000\n      4.341844\n      4.234192\n    \n    \n      1992\n      17.915374\n      14.895631\n      20.098731\n      16.735437\n      23.820874\n      20.063107\n      5.145275\n      6.446602\n      2.709618\n      4.251104\n      2373.272214\n      2703.762136\n      0.000000\n      0.000000\n      4.356841\n      4.252427\n    \n    \n      1993\n      18.084866\n      15.007772\n      20.309760\n      16.896373\n      24.172560\n      20.230570\n      5.114569\n      6.497409\n      2.683870\n      4.282793\n      2333.097595\n      2677.202073\n      0.000000\n      0.000000\n      4.371994\n      4.279793\n    \n    \n      1994\n      18.046474\n      14.952514\n      20.264423\n      16.829609\n      24.173077\n      20.201117\n      5.185897\n      6.608939\n      2.713631\n      4.413091\n      2326.041667\n      2697.625698\n      0.000000\n      0.000000\n      4.355769\n      4.293296\n    \n    \n      1995\n      17.678914\n      14.533724\n      20.091054\n      16.422287\n      24.263578\n      19.747801\n      5.444089\n      6.715543\n      2.908022\n      4.506701\n      2355.191693\n      2759.677419\n      0.000000\n      0.000000\n      4.380192\n      4.313783\n    \n    \n      1996\n      18.480545\n      14.926641\n      20.906615\n      16.961390\n      25.093385\n      20.544402\n      5.147860\n      6.579151\n      2.708768\n      4.278708\n      2250.291829\n      2622.586873\n      0.000000\n      0.000000\n      4.416342\n      4.262548\n    \n    \n      1997\n      18.090909\n      14.978632\n      20.509470\n      16.991453\n      24.678030\n      20.683761\n      5.261364\n      6.581197\n      2.786582\n      4.220544\n      2319.128788\n      2600.427350\n      0.000000\n      0.000000\n      4.452652\n      4.290598\n    \n    \n      1998\n      17.925267\n      15.288000\n      20.457295\n      17.408000\n      24.704626\n      20.944000\n      5.275801\n      6.436000\n      2.800378\n      4.104777\n      2295.373665\n      2578.800000\n      0.144128\n      0.420000\n      4.485765\n      4.272000\n    \n    \n      1999\n      17.925125\n      15.709163\n      20.386023\n      17.756972\n      24.577371\n      21.099602\n      5.377704\n      6.362550\n      2.832181\n      4.042677\n      2312.728785\n      2582.470120\n      0.251248\n      1.334661\n      4.507488\n      4.215139\n    \n    \n      2000\n      17.881849\n      15.714844\n      20.301370\n      17.757812\n      24.416096\n      21.128906\n      5.441781\n      6.332031\n      2.859111\n      3.979351\n      2385.958904\n      2529.492188\n      0.304795\n      0.449219\n      4.619863\n      4.253906\n    \n    \n      2001\n      17.941267\n      15.643939\n      20.289026\n      17.496212\n      24.372488\n      20.768939\n      5.479134\n      6.310606\n      2.872808\n      3.975823\n      2399.536321\n      2568.371212\n      0.187017\n      0.443182\n      4.761978\n      4.412879\n    \n    \n      2002\n      17.644412\n      15.083916\n      20.076923\n      16.979021\n      24.207547\n      20.195804\n      5.576197\n      6.433566\n      2.935398\n      4.057423\n      2425.689405\n      2610.139860\n      0.137881\n      0.132867\n      4.920174\n      4.367133\n    \n    \n      2003\n      17.565101\n      14.826087\n      19.953020\n      16.628763\n      24.052349\n      19.806020\n      5.683221\n      6.588629\n      3.032161\n      4.170778\n      2480.604027\n      2637.625418\n      0.127517\n      0.000000\n      5.154362\n      4.307692\n    \n    \n      2004\n      17.426290\n      14.928571\n      19.923833\n      16.805195\n      24.160934\n      20.165584\n      5.729730\n      6.558442\n      3.088666\n      4.199593\n      2476.719902\n      2607.305195\n      0.000000\n      0.000000\n      5.229730\n      4.308442\n    \n    \n      2005\n      17.412170\n      15.196610\n      19.892078\n      17.132203\n      24.189437\n      20.664407\n      5.773823\n      6.447458\n      3.151592\n      4.131402\n      2493.455798\n      2592.881356\n      0.000000\n      0.000000\n      5.362801\n      4.688136\n    \n    \n      2006\n      17.062575\n      15.300366\n      19.509025\n      17.285714\n      23.762936\n      20.875458\n      5.977136\n      6.476190\n      3.345220\n      4.170487\n      2527.496992\n      2574.725275\n      0.000000\n      0.000000\n      5.492178\n      4.776557\n    \n    \n      2007\n      16.996403\n      15.489726\n      19.452038\n      17.626712\n      23.742206\n      21.202055\n      6.044365\n      6.513699\n      3.423963\n      4.212841\n      2544.664269\n      2510.958904\n      0.000000\n      0.000000\n      5.864508\n      4.883562\n    \n    \n      2008\n      17.239869\n      15.770073\n      19.677985\n      17.937956\n      23.983571\n      21.697080\n      6.095290\n      6.518248\n      3.462049\n      4.223408\n      2551.369113\n      2486.678832\n      0.109529\n      0.000000\n      5.969332\n      5.120438\n    \n    \n      2009\n      17.696803\n      16.148014\n      20.186329\n      18.259928\n      24.590959\n      22.140794\n      5.970232\n      6.620939\n      3.402613\n      4.352489\n      2433.076075\n      2407.220217\n      0.000000\n      0.000000\n      6.189636\n      5.563177\n    \n    \n      2010\n      18.325342\n      17.278970\n      20.851598\n      19.600858\n      25.256849\n      23.785408\n      5.897260\n      6.223176\n      3.357208\n      4.048979\n      2374.429224\n      2263.304721\n      0.000000\n      0.000000\n      6.378995\n      5.866953\n    \n    \n      2011\n      19.247387\n      16.817844\n      21.635308\n      19.014870\n      25.855981\n      22.973978\n      5.851336\n      6.394052\n      3.319702\n      4.169094\n      2326.248548\n      2358.736059\n      0.340302\n      0.000000\n      6.714286\n      6.066914\n    \n    \n      2012\n      19.838052\n      17.802974\n      22.339751\n      20.111524\n      26.695357\n      24.167286\n      5.792752\n      6.297398\n      3.268908\n      4.087332\n      2282.502831\n      2314.498141\n      0.634202\n      1.267658\n      6.834655\n      6.286245\n    \n    \n      2013\n      20.982888\n      19.453815\n      23.471658\n      21.823293\n      27.860963\n      26.164659\n      5.658824\n      6.152610\n      3.179253\n      3.884311\n      2208.288770\n      2220.080321\n      0.853476\n      2.763052\n      7.033155\n      6.385542\n    \n    \n      2014\n      21.159919\n      20.506329\n      23.655870\n      23.012658\n      28.088057\n      27.523207\n      5.719636\n      5.852321\n      3.211738\n      3.614723\n      2212.196356\n      2139.029536\n      0.859312\n      3.683544\n      7.210526\n      6.046414\n    \n    \n      2015\n      21.350000\n      21.817490\n      23.935294\n      24.441065\n      28.481373\n      28.996198\n      5.604902\n      5.752852\n      3.101696\n      3.606063\n      2164.215686\n      2089.353612\n      0.638235\n      8.296578\n      7.211765\n      6.353612\n    \n    \n      2016\n      21.903749\n      25.061818\n      24.439716\n      27.701818\n      28.866261\n      32.265455\n      5.493414\n      5.356364\n      2.992272\n      3.277454\n      2127.608916\n      1960.545455\n      1.017224\n      17.214545\n      7.296859\n      6.301818\n    \n    \n      2017\n      22.423795\n      24.003623\n      24.910521\n      26.496377\n      29.208456\n      30.829710\n      5.431662\n      5.532609\n      2.919041\n      3.420272\n      2114.110128\n      2031.884058\n      1.243854\n      15.731884\n      7.474926\n      6.304348\n    \n    \n      2018\n      22.310442\n      23.526690\n      24.779868\n      25.925267\n      29.042333\n      30.145907\n      5.396990\n      5.597865\n      2.886801\n      3.391101\n      2121.448730\n      2037.900356\n      1.135466\n      12.537367\n      7.391345\n      5.601423\n    \n    \n      2019\n      23.084221\n      24.169014\n      25.456922\n      26.250000\n      29.560503\n      30.042254\n      5.315586\n      5.559859\n      2.839671\n      3.419375\n      2093.659245\n      2093.133803\n      2.581801\n      16.419014\n      7.545983\n      5.647887\n    \n    \n      2020\n      22.579487\n      24.071429\n      25.174359\n      26.571429\n      29.543590\n      30.642857\n      5.148718\n      4.000000\n      2.692823\n      1.978795\n      2050.256410\n      1650.000000\n      2.446154\n      0.000000\n      7.743590\n      7.785714\n    \n  \n\n\n\n\n\n# back to simpler example, adding plots\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n # .std()\n # rotates or unstacks the innermost index, country\n .unstack()\n # focus on city08 column\n .city08\n .plot()\n .legend(bbox_to_anchor=(1,1))\n)\n\n/tmp/ipykernel_2753/2308684326.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n<matplotlib.legend.Legend at 0x7f2091b75e10>\n\n\n\n\n\n\n# time series smoothing - rolling average(2)\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .mean()\n # .std()\n # rotates or unstacks the innermost index, country\n .unstack()\n # focus on city08 column\n .city08\n # rolling average to smooth time series\n .rolling(2)\n .mean()\n .plot()\n .legend(bbox_to_anchor=(1,1))\n)\n\n/tmp/ipykernel_2753/2271732648.py:6: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  .mean()\n\n\n<matplotlib.legend.Legend at 0x7f2091a62ad0>\n\n\n\n\n\n\n# can we emulate a SQL GROUP BY - HAVING query?\n# let's only show grouped values over say 750\ndef vals_gt(df_, num):\n    return df_[df_.gt(num)].dropna()\n\n(autos2\n .assign(country=autos2.make.apply(country))\n # can group by more than one column\n .groupby(['year','country'])\n .count()\n .pipe(vals_gt, 750)\n)\n\n\n\n\n\n  \n    \n      \n      \n      city08\n      comb08\n      highway08\n      cylinders\n      displ\n      drive\n      fuelCost08\n      make\n      model\n      range\n      createdOn\n      automatic\n      speeds\n      ffs\n    \n    \n      year\n      country\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1984\n      Other\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1131.0\n      1065.0\n    \n    \n      1985\n      Other\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      906.0\n      821.0\n    \n    \n      2017\n      Other\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      1017.0\n      766.0\n    \n    \n      2018\n      Other\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      1063.0\n      844.0\n    \n    \n      2019\n      Other\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      1033.0\n      855.0"
  },
  {
    "objectID": "posts/Pandas/Effective_Pandas.html#key-takeaways",
    "href": "posts/Pandas/Effective_Pandas.html#key-takeaways",
    "title": "Efficient Pandas",
    "section": "Key takeaways",
    "text": "Key takeaways\nHaving code that is clean, readable and has a logical flow is invaluable. Structured Query Language (SQL), as the name suggests, already pushes you down this road. Python, although often referred to as the swiss army knife of programming languages, can quickly beocme scattered and difficult to follow particulary during the exploratory data analysis (EDA) phase. However by adopting the chaining method covered in this blog, your code should ensure that others (as well as yourself!) can follow exactly what is going on.\nAlthough computer memory continues to get cheaper, I think it is good practice to keep track of how much data is being used, and there are some interesting ways covered in this blog as to how we might go about achieving this. e.g. numpy.iinfo. In the same breath, speed is also of the essence, and the handy Python cell magic %%timeit provides an easy way to quantify this. With pandas there is generally a variety of different ways to arrive at the same outcome, but as this blog covers there can be substantial time savings if you know which method to use and when, and which to avoid if possible, in particular the use of .apply.\nThis blog has been produced in response to the Effective Pandas video presented by Matt Harrison which you can view below. Thanks also to Santiago for the heads up on Twitter.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zgbUk90aQ6A?start=482\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n\n\nIf the above %%HTML cell magic is unfamiliar to you, then I recommend you watch the excellent video below posted by Corey Shafer. An excellent introduction for anyone starting out with Jupyter NoteBooks.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/HW29067qVWk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html",
    "title": "Credit Risk Modeling in Python",
    "section": "",
    "text": "If you’ve ever applied for a credit card or loan, you know that financial firms process your information before making a decision. This is because giving you a loan can have a serious financial impact on their business. But how do they make a decision? In this blog, we will learn how to prepare credit application data. After that, we will apply machine learning and business rules to reduce risk and ensure profitability. We will use two data sets that emulate real credit applications while focusing on business value."
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#exploring-and-preparing-loan-data",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#exploring-and-preparing-loan-data",
    "title": "Credit Risk Modeling in Python",
    "section": "1. Exploring and Preparing Loan Data",
    "text": "1. Exploring and Preparing Loan Data\nIn this first section, we will discuss the concept of credit risk and define how it is calculated. Using cross tables and plots, we will explore a real-world data set. Before applying machine learning, we will process this data by finding and resolving problems.\n\n1.1 Explore the credit data\nWell begin by loading in the dataset cr_loan.\n\n# import required packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\n\n# load in our dataset\ncr_loan = pd.read_csv('Data/cr_loan2.csv')\ncr_loan\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_home_ownership\n      person_emp_length\n      loan_intent\n      loan_grade\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_default_on_file\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      0\n      22\n      59000\n      RENT\n      123.0\n      PERSONAL\n      D\n      35000\n      16.02\n      1\n      0.59\n      Y\n      3\n    \n    \n      1\n      21\n      9600\n      OWN\n      5.0\n      EDUCATION\n      B\n      1000\n      11.14\n      0\n      0.10\n      N\n      2\n    \n    \n      2\n      25\n      9600\n      MORTGAGE\n      1.0\n      MEDICAL\n      C\n      5500\n      12.87\n      1\n      0.57\n      N\n      3\n    \n    \n      3\n      23\n      65500\n      RENT\n      4.0\n      MEDICAL\n      C\n      35000\n      15.23\n      1\n      0.53\n      N\n      2\n    \n    \n      4\n      24\n      54400\n      RENT\n      8.0\n      MEDICAL\n      C\n      35000\n      14.27\n      1\n      0.55\n      Y\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32576\n      57\n      53000\n      MORTGAGE\n      1.0\n      PERSONAL\n      C\n      5800\n      13.16\n      0\n      0.11\n      N\n      30\n    \n    \n      32577\n      54\n      120000\n      MORTGAGE\n      4.0\n      PERSONAL\n      A\n      17625\n      7.49\n      0\n      0.15\n      N\n      19\n    \n    \n      32578\n      65\n      76000\n      RENT\n      3.0\n      HOMEIMPROVEMENT\n      B\n      35000\n      10.99\n      1\n      0.46\n      N\n      28\n    \n    \n      32579\n      56\n      150000\n      MORTGAGE\n      5.0\n      PERSONAL\n      B\n      15000\n      11.48\n      0\n      0.10\n      N\n      26\n    \n    \n      32580\n      66\n      42000\n      RENT\n      2.0\n      MEDICAL\n      B\n      6475\n      9.99\n      0\n      0.15\n      N\n      30\n    \n  \n\n32581 rows × 12 columns\n\n\n\nIn this data set, loan_status shows whether the loan is currently in default with 1 being default and 0 being non-default.\nWe have eleven other columns within the data, and many could have a relationship with the values in loan_status. We will explore the data and these relationships more with further analysis to understand the impact of the data on credit loan defaults.\nChecking the structure of the data as well as seeing a snapshot helps us better understand what’s inside the set. Let’s check what type of data we are dealing with here and then look at the first five rows:\n\n# Check the structure of the data\ncr_loan.dtypes\n\nperson_age                      int64\nperson_income                   int64\nperson_home_ownership          object\nperson_emp_length             float64\nloan_intent                    object\nloan_grade                     object\nloan_amnt                       int64\nloan_int_rate                 float64\nloan_status                     int64\nloan_percent_income           float64\ncb_person_default_on_file      object\ncb_person_cred_hist_length      int64\ndtype: object\n\n\n\n# Check the first five rows of the data\ncr_loan.head()\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_home_ownership\n      person_emp_length\n      loan_intent\n      loan_grade\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_default_on_file\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      0\n      22\n      59000\n      RENT\n      123.0\n      PERSONAL\n      D\n      35000\n      16.02\n      1\n      0.59\n      Y\n      3\n    \n    \n      1\n      21\n      9600\n      OWN\n      5.0\n      EDUCATION\n      B\n      1000\n      11.14\n      0\n      0.10\n      N\n      2\n    \n    \n      2\n      25\n      9600\n      MORTGAGE\n      1.0\n      MEDICAL\n      C\n      5500\n      12.87\n      1\n      0.57\n      N\n      3\n    \n    \n      3\n      23\n      65500\n      RENT\n      4.0\n      MEDICAL\n      C\n      35000\n      15.23\n      1\n      0.53\n      N\n      2\n    \n    \n      4\n      24\n      54400\n      RENT\n      8.0\n      MEDICAL\n      C\n      35000\n      14.27\n      1\n      0.55\n      Y\n      4\n    \n  \n\n\n\n\n\n# get an overview of the numeric columns\ncr_loan.describe()\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_emp_length\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      count\n      32581.000000\n      3.258100e+04\n      31686.000000\n      32581.000000\n      29465.000000\n      32581.000000\n      32581.000000\n      32581.000000\n    \n    \n      mean\n      27.734600\n      6.607485e+04\n      4.789686\n      9589.371106\n      11.011695\n      0.218164\n      0.170203\n      5.804211\n    \n    \n      std\n      6.348078\n      6.198312e+04\n      4.142630\n      6322.086646\n      3.240459\n      0.413006\n      0.106782\n      4.055001\n    \n    \n      min\n      20.000000\n      4.000000e+03\n      0.000000\n      500.000000\n      5.420000\n      0.000000\n      0.000000\n      2.000000\n    \n    \n      25%\n      23.000000\n      3.850000e+04\n      2.000000\n      5000.000000\n      7.900000\n      0.000000\n      0.090000\n      3.000000\n    \n    \n      50%\n      26.000000\n      5.500000e+04\n      4.000000\n      8000.000000\n      10.990000\n      0.000000\n      0.150000\n      4.000000\n    \n    \n      75%\n      30.000000\n      7.920000e+04\n      7.000000\n      12200.000000\n      13.470000\n      0.000000\n      0.230000\n      8.000000\n    \n    \n      max\n      144.000000\n      6.000000e+06\n      123.000000\n      35000.000000\n      23.220000\n      1.000000\n      0.830000\n      30.000000\n    \n  \n\n\n\n\n\n# get an overview of the numeric columns\ncr_loan.describe(include='object')\n\n\n\n\n\n  \n    \n      \n      person_home_ownership\n      loan_intent\n      loan_grade\n      cb_person_default_on_file\n    \n  \n  \n    \n      count\n      32581\n      32581\n      32581\n      32581\n    \n    \n      unique\n      4\n      6\n      7\n      2\n    \n    \n      top\n      RENT\n      EDUCATION\n      A\n      N\n    \n    \n      freq\n      16446\n      6453\n      10777\n      26836\n    \n  \n\n\n\n\nSimilarly, visualizations provide a high level view of the data in addition to important trends and patterns. Let’s plot a histogram of loan_amt which will provide us with a visual of the distribution of loan amounts.\n\n# Look at the distribution of loan amounts with a histogram\nn, bins, patches = plt.hist(x=cr_loan['loan_amnt'], bins='auto', color='blue',alpha=0.7, rwidth=0.85)\nplt.xlabel(\"Loan Amount\")\nplt.show()\n\n\n\n\nLet’s investigate the relationship between income and age, by creating a scatter plot. In this case, income is the independent variable and age is the dependent variable.\n\nprint(\"There are over 32 000 rows of data so the scatter plot may take a little while to plot.\")\n\n# Plot a scatter plot of income against age\nplt.scatter(cr_loan['person_income'], cr_loan['person_age'],c='blue', alpha=0.5)\nplt.xlabel('Personal Income')\nplt.ylabel('Person Age')\nplt.show()\n\nThere are over 32 000 rows of data so the scatter plot may take a little while to plot.\n\n\n\n\n\nStarting with data exploration helps us keep from getting a.head() of ourselves! We can already see a positive correlation with age and income, which could mean these older recipients are further along in their career and therefore earn higher salaries. There also appears to be an outlier in the data.\n\n\n1.2 Crosstab and pivot tables\nOften, financial data is viewed as a pivot table in spreadsheets like Excel. With cross tables, we can get a high level view of selected columns and even aggregation like a count or average. For most credit risk models, especially for probability of default, columns like person_emp_length and person_home_ownership are common to begin investigating.\nWe will be able to see how the values are populated throughout the data, and visualize them. For now, we need to check how loan_status is affected by factors like home ownership status, loan grade, and loan percentage of income.\nLet’s dive in, and create a cross table of loan_intent and loan_status :\n\n# Create a cross table of the loan intent and loan status\npd.crosstab(cr_loan['loan_intent'], cr_loan['loan_status'], margins = True)\n\n\n\n\n\n  \n    \n      loan_status\n      0\n      1\n      All\n    \n    \n      loan_intent\n      \n      \n      \n    \n  \n  \n    \n      DEBTCONSOLIDATION\n      3722\n      1490\n      5212\n    \n    \n      EDUCATION\n      5342\n      1111\n      6453\n    \n    \n      HOMEIMPROVEMENT\n      2664\n      941\n      3605\n    \n    \n      MEDICAL\n      4450\n      1621\n      6071\n    \n    \n      PERSONAL\n      4423\n      1098\n      5521\n    \n    \n      VENTURE\n      4872\n      847\n      5719\n    \n    \n      All\n      25473\n      7108\n      32581\n    \n  \n\n\n\n\nSo, the largest number of loan defaults 1,621 happen when the reason for the loan was to cover medical expenses. That is perhaps not surprising - in some cases the medical conditon might mean that the loan customer is unable to work and therefore might struggle to keep up with the loan repayments.\nLet’s now look at home ownership grouped by loan_status and loan_grade :\n\n# Create a cross table of home ownership, loan status, and grade\npd.crosstab(cr_loan['person_home_ownership'],[cr_loan['loan_status'],cr_loan['loan_grade']])\n\n\n\n\n\n  \n    \n      loan_status\n      0\n      1\n    \n    \n      loan_grade\n      A\n      B\n      C\n      D\n      E\n      F\n      G\n      A\n      B\n      C\n      D\n      E\n      F\n      G\n    \n    \n      person_home_ownership\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      MORTGAGE\n      5219\n      3729\n      1934\n      658\n      178\n      36\n      0\n      239\n      324\n      321\n      553\n      161\n      61\n      31\n    \n    \n      OTHER\n      23\n      29\n      11\n      9\n      2\n      0\n      0\n      3\n      5\n      6\n      11\n      6\n      2\n      0\n    \n    \n      OWN\n      860\n      770\n      464\n      264\n      26\n      7\n      0\n      66\n      34\n      31\n      18\n      31\n      8\n      5\n    \n    \n      RENT\n      3602\n      4222\n      2710\n      554\n      137\n      28\n      1\n      765\n      1338\n      981\n      1559\n      423\n      99\n      27\n    \n  \n\n\n\n\nSo the largest amount of loan defaults 1,559 happen where the customer is a renter, and has taken out a loan grade D. We don’t know what these gradings mean and should find out more to aid our understanding.\nLet’s now look at home ownership grouped by loan_status and average loan_percent_income :\n\n# Create a cross table of home ownership, loan status, and average percent income\npd.crosstab(cr_loan['person_home_ownership'], cr_loan['loan_status'],\n              values=cr_loan['loan_percent_income'], aggfunc='mean')\n\n\n\n\n\n  \n    \n      loan_status\n      0\n      1\n    \n    \n      person_home_ownership\n      \n      \n    \n  \n  \n    \n      MORTGAGE\n      0.146504\n      0.184882\n    \n    \n      OTHER\n      0.143784\n      0.300000\n    \n    \n      OWN\n      0.180013\n      0.297358\n    \n    \n      RENT\n      0.144611\n      0.264859\n    \n  \n\n\n\n\nLet’s now create a boxplot of the loan’s percent of the person’s income grouped by loan_status :\n\n# Create a box plot of percentage income by loan status\ncr_loan.boxplot(column = ['loan_percent_income'], by = 'loan_status')\nplt.title('Average Percent Income by Loan Status')\nplt.suptitle('')\nplt.show()\n\n\n\n\nIt looks like the average percentage of income for defaults is higher. This could indicate those recipients have a debt-to-income ratio that’s already too high.\n\n\n1.3 Finding outliers with cross tables\nNow we need to find and remove outliers we suspect might be in the data. For this exercise, we can use cross tables and aggregate functions.\nHave a look at the person_emp_length column. We used the aggfunc = mean argument to see the average of a numeric column before, but to detect outliers we can use other functions like min and max.\nIt may not be possible for a person to have an employment length of less than 0 or greater than 60. We can use cross tables to check the data and see if there are any instances of this!\nLet’s print the cross table of loan_status and person_home_ownership with the max person_emp_length :\n\n# Create the cross table for loan status, home ownership, and the max employment length\npd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n        values=cr_loan['person_emp_length'], aggfunc='max')\n\n\n\n\n\n  \n    \n      person_home_ownership\n      MORTGAGE\n      OTHER\n      OWN\n      RENT\n    \n    \n      loan_status\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      123.0\n      24.0\n      31.0\n      41.0\n    \n    \n      1\n      34.0\n      11.0\n      17.0\n      123.0\n    \n  \n\n\n\n\nLet’s now create an array of indices for records with an employment length greater than 60. Store it as indices.\n\n# Create an array of indices where employment length is greater than 60\nindices = cr_loan[cr_loan['person_emp_length'] > 60].index\n\nAnd then drop those records from the data :\n\n# Drop the records from the data based on the indices and create a new dataframe\ncr_loan = cr_loan.drop(indices)\ncr_loan\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_home_ownership\n      person_emp_length\n      loan_intent\n      loan_grade\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_default_on_file\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      1\n      21\n      9600\n      OWN\n      5.0\n      EDUCATION\n      B\n      1000\n      11.14\n      0\n      0.10\n      N\n      2\n    \n    \n      2\n      25\n      9600\n      MORTGAGE\n      1.0\n      MEDICAL\n      C\n      5500\n      12.87\n      1\n      0.57\n      N\n      3\n    \n    \n      3\n      23\n      65500\n      RENT\n      4.0\n      MEDICAL\n      C\n      35000\n      15.23\n      1\n      0.53\n      N\n      2\n    \n    \n      4\n      24\n      54400\n      RENT\n      8.0\n      MEDICAL\n      C\n      35000\n      14.27\n      1\n      0.55\n      Y\n      4\n    \n    \n      5\n      21\n      9900\n      OWN\n      2.0\n      VENTURE\n      A\n      2500\n      7.14\n      1\n      0.25\n      N\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32576\n      57\n      53000\n      MORTGAGE\n      1.0\n      PERSONAL\n      C\n      5800\n      13.16\n      0\n      0.11\n      N\n      30\n    \n    \n      32577\n      54\n      120000\n      MORTGAGE\n      4.0\n      PERSONAL\n      A\n      17625\n      7.49\n      0\n      0.15\n      N\n      19\n    \n    \n      32578\n      65\n      76000\n      RENT\n      3.0\n      HOMEIMPROVEMENT\n      B\n      35000\n      10.99\n      1\n      0.46\n      N\n      28\n    \n    \n      32579\n      56\n      150000\n      MORTGAGE\n      5.0\n      PERSONAL\n      B\n      15000\n      11.48\n      0\n      0.10\n      N\n      26\n    \n    \n      32580\n      66\n      42000\n      RENT\n      2.0\n      MEDICAL\n      B\n      6475\n      9.99\n      0\n      0.15\n      N\n      30\n    \n  \n\n32579 rows × 12 columns\n\n\n\nWe now have 32,759 rows - two have been removed following our removal of records with an employment length greater than 60.\n\n# Create the cross table from earlier and include minimum employment length\npd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n            values=cr_loan['person_emp_length'], aggfunc=['min','max'])\n\n\n\n\n\n  \n    \n      \n      min\n      max\n    \n    \n      person_home_ownership\n      MORTGAGE\n      OTHER\n      OWN\n      RENT\n      MORTGAGE\n      OTHER\n      OWN\n      RENT\n    \n    \n      loan_status\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0.0\n      0.0\n      0.0\n      0.0\n      38.0\n      24.0\n      31.0\n      41.0\n    \n    \n      1\n      0.0\n      0.0\n      0.0\n      0.0\n      34.0\n      11.0\n      17.0\n      27.0\n    \n  \n\n\n\n\nGenerally with credit data, key columns like person_emp_length are of high quality, but there is always room for error. With this in mind, we build our intuition for detecting outliers!\n\n\n1.4 Visualizing credit outliers\nWe discovered outliers in person_emp_length where values greater than 60 were far above the norm. person_age is another column in which a person can use a common sense approach to say it is very unlikely that a person applying for a loan will be over 100 years old.\nVisualizing the data here can be another easy way to detect outliers. We can use other numeric columns like loan_amnt and loan_int_rate to create plots with person_age to search for outliers.\nLet’s create a scatter plot of person_age on the x-axis and loan_amnt on the y-axis :\n\n# Create the scatter plot for age and amount\nplt.scatter(cr_loan['person_age'], cr_loan['loan_amnt'], c='blue', alpha=0.5)\nplt.xlabel(\"Person Age\")\nplt.ylabel(\"Loan Amount\")\nplt.show()\n\n\n\n\nLet’s use the .drop() method from Pandas to remove the outliers :\n\n# Use Pandas to drop the record from the data frame and create a new one\ncr_loan = cr_loan.drop(cr_loan[cr_loan['person_age'] > 100].index)\ncr_loan\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_home_ownership\n      person_emp_length\n      loan_intent\n      loan_grade\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_default_on_file\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      1\n      21\n      9600\n      OWN\n      5.0\n      EDUCATION\n      B\n      1000\n      11.14\n      0\n      0.10\n      N\n      2\n    \n    \n      2\n      25\n      9600\n      MORTGAGE\n      1.0\n      MEDICAL\n      C\n      5500\n      12.87\n      1\n      0.57\n      N\n      3\n    \n    \n      3\n      23\n      65500\n      RENT\n      4.0\n      MEDICAL\n      C\n      35000\n      15.23\n      1\n      0.53\n      N\n      2\n    \n    \n      4\n      24\n      54400\n      RENT\n      8.0\n      MEDICAL\n      C\n      35000\n      14.27\n      1\n      0.55\n      Y\n      4\n    \n    \n      5\n      21\n      9900\n      OWN\n      2.0\n      VENTURE\n      A\n      2500\n      7.14\n      1\n      0.25\n      N\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32576\n      57\n      53000\n      MORTGAGE\n      1.0\n      PERSONAL\n      C\n      5800\n      13.16\n      0\n      0.11\n      N\n      30\n    \n    \n      32577\n      54\n      120000\n      MORTGAGE\n      4.0\n      PERSONAL\n      A\n      17625\n      7.49\n      0\n      0.15\n      N\n      19\n    \n    \n      32578\n      65\n      76000\n      RENT\n      3.0\n      HOMEIMPROVEMENT\n      B\n      35000\n      10.99\n      1\n      0.46\n      N\n      28\n    \n    \n      32579\n      56\n      150000\n      MORTGAGE\n      5.0\n      PERSONAL\n      B\n      15000\n      11.48\n      0\n      0.10\n      N\n      26\n    \n    \n      32580\n      66\n      42000\n      RENT\n      2.0\n      MEDICAL\n      B\n      6475\n      9.99\n      0\n      0.15\n      N\n      30\n    \n  \n\n32574 rows × 12 columns\n\n\n\nWe now have 32,574 rows - a further five have been removed following our removal of records with an age greater than 100.\nThen, let’s create a scatter plot of person_age on the x-axis and loan_int_rate on the y-axis with a label for loan_status :\n\n# Create a scatter plot of age and interest rate\ncolors = [\"blue\",\"red\"] # default is red\nplt.scatter(cr_loan['person_age'], cr_loan['loan_int_rate'],\n            c = cr_loan['loan_status'],\n            cmap = matplotlib.colors.ListedColormap(colors),\n            alpha=0.5)\nplt.xlabel(\"Person Age\")\nplt.ylabel(\"Loan Interest Rate\")\nplt.show()\n\n\n\n\nNotice that in the last plot we have loan_status as a label for colors. This shows a different color depending on the class (red for default). In this case, it’s loan default and non-default, and it looks like there are more defaults with high interest rates.\n\n\n1.5 Replacing missing credit data\n\n\n\nmissing_data.JPG\n\n\n\n\n\nmissing_data_perf.JPG\n\n\nNow, we should check for missing data. If we find missing data within loan_status, we would not be able to use the data for predicting probability of default because we wouldn’t know if the loan was a default or not. Missing data within person_emp_length would not be as damaging, but would still cause training errors.\nSo, let’s check for missing data in the person_emp_length column and replace any missing values with the median.\n\n# Print a null value column array\ncr_loan.columns[cr_loan.isnull().any()]\n\nIndex(['person_emp_length', 'loan_int_rate'], dtype='object')\n\n\n\n# Print the top five rows with nulls for employment length\ncr_loan[cr_loan['person_emp_length'].isnull()].head()\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_home_ownership\n      person_emp_length\n      loan_intent\n      loan_grade\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_default_on_file\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      105\n      22\n      12600\n      MORTGAGE\n      NaN\n      PERSONAL\n      A\n      2000\n      5.42\n      1\n      0.16\n      N\n      4\n    \n    \n      222\n      24\n      185000\n      MORTGAGE\n      NaN\n      EDUCATION\n      B\n      35000\n      12.42\n      0\n      0.19\n      N\n      2\n    \n    \n      379\n      24\n      16800\n      MORTGAGE\n      NaN\n      DEBTCONSOLIDATION\n      A\n      3900\n      NaN\n      1\n      0.23\n      N\n      3\n    \n    \n      407\n      25\n      52000\n      RENT\n      NaN\n      PERSONAL\n      B\n      24000\n      10.74\n      1\n      0.46\n      N\n      2\n    \n    \n      408\n      22\n      17352\n      MORTGAGE\n      NaN\n      EDUCATION\n      C\n      2250\n      15.27\n      0\n      0.13\n      Y\n      3\n    \n  \n\n\n\n\n\n# Impute the null values with the median value for all employment lengths\ncr_loan['person_emp_length'].fillna((cr_loan['person_emp_length'].median()), inplace=True)\n\n# Create a histogram of employment length\nn, bins, patches = plt.hist(cr_loan['person_emp_length'], bins='auto', color='blue')\nplt.xlabel(\"Person Employment Length\")\nplt.show()\n\n\n\n\nWe can use several different functions like mean() and median() to replace missing data. The goal here is to keep as much of our data as we can! It’s also important to check the distribution of that feature to see if it changed.\n\n\n1.6 Removing missing data\nWe replaced missing data in person_emp_length, but in the previous section we saw that loan_int_rate has missing data as well.\nSimilar to having missing data within loan_status, having missing data within loan_int_rate will make predictions difficult.\nBecause interest rates are set by our company, having missing data in this column is very strange. It’s possible that data ingestion issues created errors, but we cannot know for sure. For now, it’s best to .drop() these records before moving forward.\n\n# Print the number of nulls\nprint(cr_loan['loan_int_rate'].isnull().sum())\n\n# Store the array on indices\nindices = cr_loan[cr_loan['loan_int_rate'].isnull()].index\n                  \n# Save the new data without missing data\ncr_loan_clean = cr_loan.drop(indices)\n\n3115\n\n\n\ncr_loan_clean\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_home_ownership\n      person_emp_length\n      loan_intent\n      loan_grade\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_default_on_file\n      cb_person_cred_hist_length\n    \n  \n  \n    \n      1\n      21\n      9600\n      OWN\n      5.0\n      EDUCATION\n      B\n      1000\n      11.14\n      0\n      0.10\n      N\n      2\n    \n    \n      2\n      25\n      9600\n      MORTGAGE\n      1.0\n      MEDICAL\n      C\n      5500\n      12.87\n      1\n      0.57\n      N\n      3\n    \n    \n      3\n      23\n      65500\n      RENT\n      4.0\n      MEDICAL\n      C\n      35000\n      15.23\n      1\n      0.53\n      N\n      2\n    \n    \n      4\n      24\n      54400\n      RENT\n      8.0\n      MEDICAL\n      C\n      35000\n      14.27\n      1\n      0.55\n      Y\n      4\n    \n    \n      5\n      21\n      9900\n      OWN\n      2.0\n      VENTURE\n      A\n      2500\n      7.14\n      1\n      0.25\n      N\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32576\n      57\n      53000\n      MORTGAGE\n      1.0\n      PERSONAL\n      C\n      5800\n      13.16\n      0\n      0.11\n      N\n      30\n    \n    \n      32577\n      54\n      120000\n      MORTGAGE\n      4.0\n      PERSONAL\n      A\n      17625\n      7.49\n      0\n      0.15\n      N\n      19\n    \n    \n      32578\n      65\n      76000\n      RENT\n      3.0\n      HOMEIMPROVEMENT\n      B\n      35000\n      10.99\n      1\n      0.46\n      N\n      28\n    \n    \n      32579\n      56\n      150000\n      MORTGAGE\n      5.0\n      PERSONAL\n      B\n      15000\n      11.48\n      0\n      0.10\n      N\n      26\n    \n    \n      32580\n      66\n      42000\n      RENT\n      2.0\n      MEDICAL\n      B\n      6475\n      9.99\n      0\n      0.15\n      N\n      30\n    \n  \n\n29459 rows × 12 columns\n\n\n\nOur clean dataset now has 29,459 - 3,115 have been removed as these contained nulls.\nNow that the missing data and outliers have been processed, the data is ready for modeling! More often than not, financial data is fairly tidy, but it’s always good to practice preparing data for analytical work."
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#logistic-regression-for-defaults",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#logistic-regression-for-defaults",
    "title": "Credit Risk Modeling in Python",
    "section": "2. Logistic Regression for Defaults",
    "text": "2. Logistic Regression for Defaults\nWith the loan data fully prepared, we will discuss the logistic regression model which is a standard in risk modeling. We will understand the components of this model as well as how to score its performance. Once we’ve created predictions, we can explore the financial impact of utilizing this model.\n\n\n\ndeafult_prob.JPG\n\n\n\n\n\nprobabilities.JPG\n\n\n\n2.1 Logistic regression basics\n\n\n\nlogistic_regression.JPG\n\n\n\n\n\nscikit_learn.JPG\n\n\nYWe have now cleaned up the data and created the new data set cr_loan_clean.\nThink back to the final scatter plot from section 1 which showed more defaults with high loan_int_rate. Interest rates are easy to understand, but how useful are they for predicting the probability of default?\nSince we haven’t tried predicting the probability of default yet, let’s test out creating and training a logistic regression model with just loan_int_rate. Also check the model’s internal parameters, which are like settings, to see the structure of the model with this one column.\n\n## import required packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\n\nimport numpy as np\n\n\n# Create the X and y data sets\nX = cr_loan_clean[['loan_int_rate']]\ny = cr_loan_clean[['loan_status']]\n\n# Create and fit a logistic regression model\nclf_logistic_single = LogisticRegression(solver='lbfgs') # solver is the optimizer like we have in Excel for gradient descent\nclf_logistic_single.fit(X, np.ravel(y))\n\n# Print the parameters of the model\nprint(clf_logistic_single.get_params())\n\n# Print the intercept of the model\nprint(clf_logistic_single.intercept_)\n\n{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n[-4.45785901]\n\n\nNote the solver included within the Logistic Regression model. This is like the solver function within Excel which is used to optimize the randomized initial weightings, a bit like Stochastic Gradient Descent (SGD). The particular solver or algorithm used here is lbfgs which stands for Limited-memory - Broyden–Fletcher–Goldfarb–Shanno.\nNote that we use Numpy’s np.ravel to make our labels a one-dimensional array instead of a DataFrame as this is the format our model requires.\nNotice that the model was able to fit to the data, and establish some parameters internally. It’s even produced a y .intercept_ value [-4.45785901] which represents the overall log-odds of non-default. What if we use more than one column?\n\n\n2.2 Multivariate logistic regression\nGenerally, we won’t use only loan_int_rate to predict the probability of default. We will want to use all the data we have to make predictions.\nWith this in mind, let’s try training a new model with different columns, called features, from the cr_loan_clean data. Will this model differ from the first one? For this, we can easily check the .intercept_ of the logistic regression. Remember that this is the y-intercept of the function and the overall log-odds of non-default.\nLet’s add employment length to our features:\n\n# Create X data for the model\nX_multi = cr_loan_clean[['loan_int_rate','person_emp_length']]\n\n# Create a set of y data for training\ny = cr_loan_clean[['loan_status']]\n\n# Create and train a new logistic regression\nclf_logistic_multi = LogisticRegression(solver='lbfgs').fit(X_multi, np.ravel(y))\n\n# Print the intercept of the model\nprint(clf_logistic_multi.intercept_)\n\n[-4.21645549]\n\n\nTake a closer look at each model’s .intercept_ value. The values have changed! The new clf_logistic_multi model has an .intercept_ value closer to zero. This means the log odds of a non-default is approaching zero.\n\n\n2.3 Creating training and test sets\n\nWe’ve just trained LogisticRegression() models on different columns. we know that the data should be separated into training and test sets. test_train_split() is used to create both at the same time. The training set is used to make predictions, while the test set is used for evaluation. Without evaluating the model, we have no way to tell how well it will perform on new loan data.\nIn addition to the intercept_, which is an attribute of the model, LogisticRegression() models also have the .coef_ attribute. This shows how important each training column is for predicting the probability of default.\nLet’s create a data set X using interest rate, employment length, and income. Create the y set as always using our target variable (label) loan status :\n\n# Create the X and y data sets\nX = cr_loan_clean[['loan_int_rate','person_emp_length','person_income']]\ny = cr_loan_clean[['loan_status']]\n\n# Use test_train_split to create the training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n\n# Create and fit the logistic regression model to our training set\nclf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n\n# Print the model coefficients\nprint(clf_logistic .coef_)\n\n# Print the model intercept\nprint(clf_logistic.intercept_)\n\n[[ 1.28517496e-09 -2.27622202e-09 -2.17211991e-05]]\n[-3.30582292e-10]\n\n\nWe can see that three columns were used for training and there are three values in .coef_. This tells us how important each column, or feature, was for predicting. The more positive the value, the more it predicts defaults, for example look at the value for loan_int_rate - 1.28517496e-09. This makes sense, the higher the interest rate the higher the loan repayments and therefore an increased risk of default. On the other hand we have negative values for both employment length and income. This also makes sense. A long time in employment suggests stability and the higher a person’s income is the less likely they are to default, as they are more likely to meet the loan repayments.\nWe can plug these 4 values into our Logistic Regression formula to arrive at the overall prediciton for loan default :\n\n\n\nco_efficients.JPG\n\n\n\n\n2.4 Changing coefficients\nWith this understanding of the coefficients of a LogisticRegression() model, let’s have a closer look at them to see how they change depending on what columns are used for training. Will the column coefficients change from model to model?\nWe should .fit() two different LogisticRegression() models on different groups of columns to check. We should also consider what the potential impact on the probability of default might be.\n\n# create X1, X2 and y datasets\nX1 = cr_loan_clean[['person_income','person_emp_length','loan_amnt']]\nX2 = cr_loan_clean[['person_income','loan_percent_income','cb_person_cred_hist_length']]\ny = cr_loan_clean[['loan_status']]\n\n\n# train, test, split\nX1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=.4, random_state=123)\nX2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=.4, random_state=123)\n\n\n# Print the first five rows of each training set\nprint(X1_train.head())\nprint(X2_train.head())\n\n# Create and train a model on the first training data\nclf_logistic1 = LogisticRegression(solver='lbfgs').fit(X1_train, np.ravel(y_train))\n\n# Create and train a model on the second training data\nclf_logistic2 = LogisticRegression(solver='lbfgs').fit(X2_train, np.ravel(y_train))\n\n# Print the coefficients of each model\nprint(clf_logistic1.coef_)\nprint(clf_logistic2.coef_)\n\n       person_income  person_emp_length  loan_amnt\n24407          72000                3.0       9000\n2528           21000                2.0       3000\n15961           4800                0.0       1200\n6966           92000                0.0       6000\n13832          96000                1.0       7000\n       person_income  loan_percent_income  cb_person_cred_hist_length\n24407          72000                 0.13                           6\n2528           21000                 0.14                           2\n15961           4800                 0.25                           2\n6966           92000                 0.07                           3\n13832          96000                 0.07                           4\n[[-4.02643517e-05 -3.06659219e-08  1.06277246e-04]]\n[[-2.17213449e-05  5.29012401e-10 -2.80735543e-09]]\n\n\nNotice that the coefficient for the person_income changed when we changed the data from X1 -4.02643517e-05 to X2 -2.17213449e-05. This is a reason to keep most of the data like we did in section 1, because the models will learn differently depending on what data they’re given!\n\n\n2.5 One-hot encoding credit data\nPython does not know how to deal with non-numeric columns, and so we have to use one-hot encoding to convert categorical data to a number - either 0 or 1.\n\n\n\none_hot_encoding.JPG\n\n\nWe can use get_dummies() from the pandas library to do this.\n\n\n\nget_dummies.JPG\n\n\nIt’s time to prepare the non-numeric columns so they can be added to our LogisticRegression() model. Once the new columns have been created using one-hot encoding, we can concatenate them with the numeric columns to create a new data frame which will be used throughout the rest of the course for predicting probability of default.\nRemember to only one-hot encode the non-numeric columns. Doing this to the numeric columns would create an incredibly wide data set!\n\n# Create two data sets for numeric and non-numeric data\ncred_num = cr_loan_clean.select_dtypes(exclude=['object'])\ncred_str = cr_loan_clean.select_dtypes(include=['object'])\n\n# One-hot encode the non-numeric columns\ncred_str_onehot = pd.get_dummies(cred_str)\n\n# Union the one-hot encoded columns to the numeric ones\ncr_loan_prep = pd.concat([cred_num, cred_str_onehot], axis=1)\n\n# Print the columns in the new data set\nprint(cr_loan_prep.columns)\n\nIndex(['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n       'loan_int_rate', 'loan_status', 'loan_percent_income',\n       'cb_person_cred_hist_length', 'person_home_ownership_MORTGAGE',\n       'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n       'person_home_ownership_RENT', 'loan_intent_DEBTCONSOLIDATION',\n       'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT',\n       'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE',\n       'loan_grade_A', 'loan_grade_B', 'loan_grade_C', 'loan_grade_D',\n       'loan_grade_E', 'loan_grade_F', 'loan_grade_G',\n       'cb_person_default_on_file_N', 'cb_person_default_on_file_Y'],\n      dtype='object')\n\n\nLook at all those columns! If you’ve ever seen a credit scorecard, the column_name_value format should look familiar. If you haven’t seen one, look up some pictures during your next break!\n\n\n2.6 Predicting probability of default\n\n\n\npredict.JPG\n\n\nAll of the data processing is complete and it’s time to begin creating predictions for probability of default. We want to train a LogisticRegression() model on the data, and examine how it predicts the probability of default.\nSo that we can better grasp what the model produces with predict_proba, you should look at an example record alongside the predicted probability of default. How do the first five predictions look against the actual values of loan_status?\n\ncr_loan_prep\n\n\n\n\n\n  \n    \n      \n      person_age\n      person_income\n      person_emp_length\n      loan_amnt\n      loan_int_rate\n      loan_status\n      loan_percent_income\n      cb_person_cred_hist_length\n      person_home_ownership_MORTGAGE\n      person_home_ownership_OTHER\n      ...\n      loan_intent_VENTURE\n      loan_grade_A\n      loan_grade_B\n      loan_grade_C\n      loan_grade_D\n      loan_grade_E\n      loan_grade_F\n      loan_grade_G\n      cb_person_default_on_file_N\n      cb_person_default_on_file_Y\n    \n  \n  \n    \n      1\n      21\n      9600\n      5.0\n      1000\n      11.14\n      0\n      0.10\n      2\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      25\n      9600\n      1.0\n      5500\n      12.87\n      1\n      0.57\n      3\n      1\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      3\n      23\n      65500\n      4.0\n      35000\n      15.23\n      1\n      0.53\n      2\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      4\n      24\n      54400\n      8.0\n      35000\n      14.27\n      1\n      0.55\n      4\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      5\n      21\n      9900\n      2.0\n      2500\n      7.14\n      1\n      0.25\n      2\n      0\n      0\n      ...\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      32576\n      57\n      53000\n      1.0\n      5800\n      13.16\n      0\n      0.11\n      30\n      1\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      32577\n      54\n      120000\n      4.0\n      17625\n      7.49\n      0\n      0.15\n      19\n      1\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      32578\n      65\n      76000\n      3.0\n      35000\n      10.99\n      1\n      0.46\n      28\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      32579\n      56\n      150000\n      5.0\n      15000\n      11.48\n      0\n      0.10\n      26\n      1\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      32580\n      66\n      42000\n      2.0\n      6475\n      9.99\n      0\n      0.15\n      30\n      0\n      0\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n29459 rows × 27 columns\n\n\n\n\n# set features and target variable\nX = cr_loan_prep.drop('loan_status', axis=1)\ny = cr_loan_prep[['loan_status']]\n\n\n# train, test, split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n\n\n# Train the logistic regression model on the training data\nclf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n\n# Create predictions of probability for loan status using test data\npreds = clf_logistic.predict_proba(X_test)\n\n# Create dataframes of first five predictions\npreds_df = pd.DataFrame(preds[:,1][0:5], columns = ['prob_default'])\n\n# Create dataframe of first five true labels\ntrue_df = y_test.head()\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))\n\n   loan_status  prob_default\n0            1      0.445779\n1            1      0.223447\n2            0      0.288558\n3            0      0.169358\n4            1      0.114182\n\n\nWe have some predictions now, but they don’t look very accurate do they? It looks like most of the rows with loan_status at 1 have a low probability of default. How good are the rest of the predictions? Next, let’s see if we can determine how accurate the entire model is.\n\n\n2.7 Default classification reporting\n\n\n\naccuracy.JPG\n\n\nIt’s time to take a closer look at the evaluation of our model. Here is where setting the threshold for probability of default will help us analyze the model’s performance through classification reporting.\nCreating a data frame of the probabilities makes them easier to work with, because we can use all the power of pandas. Apply the threshold to the data and check the value counts for both classes of loan_status to see how many predictions of each are being created. This will help with insight into the scores from the classification report.\n\npreds_df\n\n\n\n\n\n  \n    \n      \n      prob_default\n    \n  \n  \n    \n      0\n      0.445779\n    \n    \n      1\n      0.223447\n    \n    \n      2\n      0.288558\n    \n    \n      3\n      0.169358\n    \n    \n      4\n      0.114182\n    \n  \n\n\n\n\n\n# Create a dataframe for the probabilities of default\npreds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n\n# Reassign loan status based on the threshold\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0) # lambda alllows us to use a function without defining it using def:\n\n# Print the row counts for each loan status\nprint(preds_df['loan_status'].value_counts())\n\n# Print the classification report\ntarget_names = ['Non-Default', 'Default'] # our labels\nprint(classification_report(y_test, preds_df['loan_status'], target_names=target_names))\n\n0    11175\n1      609\nName: loan_status, dtype: int64\n              precision    recall  f1-score   support\n\n Non-Default       0.81      0.98      0.89      9198\n     Default       0.71      0.17      0.27      2586\n\n    accuracy                           0.80     11784\n   macro avg       0.76      0.57      0.58     11784\nweighted avg       0.79      0.80      0.75     11784\n\n\n\nWell isn’t this a surprise! It looks like almost all of our test set was predicted to be non-default. The recall for defaults is 0.17 meaning 17% of our true defaults were predicted correctly.\n\n\n2.8 Selecting report metrics\nThe classification_report() has many different metrics within it, but you may not always want to print out the full report. Sometimes you just want specific values to compare models or use for other purposes.\nThere is a function within scikit-learn that pulls out the values for you. That function is precision_recall_fscore_support() and it takes in the same parameters as classification_report.\nIt is imported and used like this:\n# Import function\nfrom sklearn.metrics import precision_recall_fscore_support\n# Select all non-averaged values from the report\nprecision_recall_fscore_support(y_true,predicted_values)\n\n# Print the classification report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, preds_df['loan_status'], target_names=target_names))\n\n              precision    recall  f1-score   support\n\n Non-Default       0.81      0.98      0.89      9198\n     Default       0.71      0.17      0.27      2586\n\n    accuracy                           0.80     11784\n   macro avg       0.76      0.57      0.58     11784\nweighted avg       0.79      0.80      0.75     11784\n\n\n\n\n# Print all the non-average values from the report\nprint(precision_recall_fscore_support(y_test,preds_df['loan_status']))\n\n(array([0.80742729, 0.71264368]), array([0.98097412, 0.16782676]), array([0.8857802 , 0.27167449]), array([9198, 2586]))\n\n\n\n# Print the first two numbers from the report\nprint(precision_recall_fscore_support(y_test,preds_df['loan_status'])[0])\n\n[0.80742729 0.71264368]\n\n\nNow we know how to pull out specific values from the report to either store later for comparison, or use to check against portfolio performance. Remember the impact of recall for defaults? This way, we can store that value for later calculations.\n\n\n2.9 Visually scoring credit models\n\n\n\nROC.JPG\n\n\n\n\n\nROC_chart.JPG\n\n\nNow, we want to visualize the performance of the model. In ROC charts, the X and Y axes are two metrics we’ve already looked at: the false positive rate (fall-out), and the true positive rate (sensitivity).\nWe can create a ROC chart of it’s performance with the following code:\nfallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\nplt.plot(fallout, sensitivity)\nTo calculate the AUC score, you use roc_auc_score()\n\n# Create predictions and store them in a variable\npreds= clf_logistic.predict_proba(X_test)\n\n# Print the accuracy score the model\nprint(clf_logistic.score(X_test, y_test))\n\n# Plot the ROC curve of the probabilities of default\nprob_default = preds[:, 1]\nfallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\n\nfig, ax = plt.subplots()\nplt.plot(fallout, sensitivity, color = 'darkorange')\nplt.plot([0, 1], [0, 1], linestyle='--')\nax.set_xlabel('False Positive Rate (Fallout)')\nax.set_ylabel('True Positive Rate (Sensitivity)')\nplt.show()\n\n# Compute the AUC and store it in a variable\nauc = roc_auc_score(y_test, prob_default)\nauc\n\n0.8025288526816021\n\n\n\n\n\n0.7643248801355148\n\n\nSo the accuracy for this model is just over 80.3% and the AUC score is 76.4%. Notice that what the ROC chart above shows us is the tradeoff between all values of our false positive rate (fallout) and true positive rate (sensitivity).\n\n\n2.10 Thresholds and confusion matrices\n\n\n\nthresholds.JPG\n\n\n\n\n\nconfusion_matrix.JPG\n\n\nThe recall score for loan defaults is the number of correctly predicted defaults divided by the total number of defaults. Note that if we were to predict ALL of our loans to default, then our recall score would be 100%!\nThe recall score for non-defaults is the number of correctly predicted non-defaults, divided by the total number of non-defaults.\nThe precision score for loan defaults is the number of correctly predicted defaults divided by the total number of predicted defaults.\nThe precision score for non-loan defaults is the number of correctly predicted non-defaults, divided by the total number of predicted non-defaults.\nWe’ve looked at setting thresholds for defaults, but how does this impact overall performance? To do this, we can start by looking at the effects with confusion matrices. Set different values for the threshold on probability of default, and use a confusion matrix to see how the changing values affect the model’s performance.\n\n# Set the threshold for defaults to 0.5\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n\n# Print the confusion matrix\nprint(confusion_matrix(y_test,preds_df['loan_status']))\n\n[[9023  175]\n [2152  434]]\n\n\n\nprint(recall_score(y_test,preds_df['loan_status']))\n\n0.16782675947409126\n\n\n\n# Set the threshold for defaults to 0.4\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.4 else 0)\n\n# Print the confusion matrix\ny_test,preds_df['loan_status']\n\n(       loan_status\n 31622            1\n 24935            1\n 15342            0\n 3460             0\n 16424            1\n ...            ...\n 25231            0\n 30380            0\n 29849            0\n 1780             0\n 11446            0\n \n [11784 rows x 1 columns],\n 0        1\n 1        0\n 2        0\n 3        0\n 4        0\n         ..\n 11779    0\n 11780    0\n 11781    1\n 11782    0\n 11783    0\n Name: loan_status, Length: 11784, dtype: int64)\n\n\n\nprint(recall_score(y_test,preds_df['loan_status']))\n\n0.46403712296983757\n\n\n\n\n\nrecall_precision_accuracy.JPG\n\n\n\n\n2.11 How thresholds affect performance\nSetting the threshold to 0.4 shows promising results for model evaluation. Now we can assess the financial impact using the default recall which is selected from the classification reporting using the function precision_recall_fscore_support().\nFor this, we will estimate the amount of unexpected loss using the default recall to find what proportion of defaults you did not catch with the new threshold. This will be a dollar amount which tells you how much in losses you would have if all the unfound defaults were to default all at once.\n\n# Reassign the values of loan status based on the new threshold\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.4 else 0)\n\n# Store the number of loan defaults from the prediction data\nnum_defaults = preds_df['loan_status'].value_counts()[1]\n\n# Store the default recall from the classification report\ndefault_recall = precision_recall_fscore_support(y_test,preds_df['loan_status'])[1][1]\n\n# Calculate the estimated impact of the new default recall rate\nprint(cr_loan_prep['loan_amnt'].mean() * num_defaults * (1 - default_recall))\n\n9872265.223119883\n\n\nBy our estimates, this loss would be around $9.8 million. That seems like a lot! Try rerunning this code with threshold values of 0.3 and 0.5. Do you see the estimated losses changing? How do we find a good threshold value based on these metrics alone?\n\n\n2.12 Threshold selection\n\n\n\nthresholds_apply.JPG\n\n\nWe know there is a trade off between metrics like default recall, non-default recall, and model accuracy. One easy way to approximate a good starting threshold value is to look at a plot of all three using matplotlib. With this graph, you can see how each of these metrics look as you change the threshold values and find the point at which the performance of all three is good enough to use for the credit data.\n\n# Instantiate values\nthresh =  [0.2, 0.225, 0.25, 0.275, 0.3, 0.325, 0.35, 0.375, 0.4, 0.425, 0.45, 0.475, 0.5, 0.525, 0.55, 0.575, 0.6, 0.625, 0.65]\ndef_recalls = [0.7981438515081206, 0.7583139984532096, 0.7157772621809745, 0.6759474091260634, 0.6349574632637278, 0.594354215003867, 0.5467904098994586, 0.5054137664346481,\n 0.46403712296983757, 0.39984532095901004, 0.32211910286156226, 0.2354988399071926, 0.16782675947409126, 0.1148491879350348, 0.07733952049497293, 0.05529775715390565,\n 0.03750966744006187, 0.026295436968290797, 0.017788089713843776]\nnondef_recalls = [0.5342465753424658, 0.5973037616873234, 0.6552511415525114, 0.708306153511633, 0.756468797564688, 0.8052837573385518, 0.8482278756251359, 0.8864970645792564,\n 0.9215046749293324, 0.9492280930637095, 0.9646662317895195, 0.9733637747336378, 0.9809741248097412, 0.9857577734290063, 0.9902152641878669, 0.992280930637095, 0.9948901935203305,\n 0.9966297021091541, 0.997499456403566]\naccs = [0.5921588594704684, 0.6326374745417516, 0.6685336048879837, 0.7012050237610319, 0.7298031228784793, 0.7589952477936185, 0.7820773930753564, 0.8028682959945689,\n 0.8211133740665308, 0.8286659877800407, 0.8236591989137814, 0.811439239646979, 0.8025288526816021, 0.7946367956551256, 0.7898845892735913, 0.7866598778004074,\n 0.7847929395790902, 0.7836897488119484, 0.7825016972165648]\nticks = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65]\n\nNote, the above values were pre-loaded within the DataCamp workspace for this lesson, however I have recreated here for reproduceability.\n\nplt.plot(thresh,def_recalls)\nplt.plot(thresh,nondef_recalls)\nplt.plot(thresh,accs)\nplt.xlabel(\"Probability Threshold\")\nplt.xticks(ticks)\nplt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\nplt.show()\n\n\n\n\nHave a closer look at this plot. In fact, expand the window to get a really good look. Think about the threshold values from thresh and how they affect each of these three metrics. Approximately what starting threshold value would maximize these scores evenly?\nThis is the easiest pattern to see on this graph, because it’s the point where all three lines converge. This threshold would make a great starting point, but declaring all loans about 0.275 to be a default is probably not practical."
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#gradient-boosted-trees-using-xgboost",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#gradient-boosted-trees-using-xgboost",
    "title": "Credit Risk Modeling in Python",
    "section": "3. Gradient Boosted Trees Using XGBoost",
    "text": "3. Gradient Boosted Trees Using XGBoost\nDecision trees are another standard credit risk model. We will go beyond decision trees by using the trendy XGBoost package in Python to create gradient boosted trees. After developing sophisticated models, we will stress test their performance and discuss column selection in unbalanced data.\n\n3.1 Trees for defaults\nWe will now train a Gradient Boosted Tree model on the credit data, and see a sample of some of the predictions. Do you remember when we first looked at the predictions of the logistic Regression model? They didn’t look good. Do you think this model will be different?\n\n# Train a model\n!pip install xgboost\nimport xgboost as xgb\nclf_gbt = xgb.XGBClassifier().fit(X_train, np.ravel(y_train))\n\n# Predict with a model\ngbt_preds = clf_gbt.predict_proba(X_test)\n\n# Create dataframes of first five predictions, and first five true labels\npreds_df = pd.DataFrame(gbt_preds[:,1][0:5], columns = ['prob_default'])\ntrue_df = y_test.head()\n\n# Concatenate and print the two data frames for comparison\nprint(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))\n\nRequirement already satisfied: xgboost in /home/stephen137/mambaforge/lib/python3.10/site-packages (1.7.3)\nRequirement already satisfied: numpy in /home/stephen137/mambaforge/lib/python3.10/site-packages (from xgboost) (1.22.4)\nRequirement already satisfied: scipy in /home/stephen137/mambaforge/lib/python3.10/site-packages (from xgboost) (1.9.1)\n   loan_status  prob_default\n0            1      0.990942\n1            1      0.983987\n2            0      0.000807\n3            0      0.001239\n4            1      0.084892\n\n\nThe predictions don’t look the same as with the LogisticRegression(), do they? Notice that this model is already accurately predicting the probability of default for some loans with a true value of 1 in loan_status.\n\n\n3.2 Gradient boosted portfolio performance\nAt this point we’ve looked at predicting probability of default using both a LogisticRegression() and XGBClassifier(). We’ve looked at some scoring and have seen samples of the predictions, but what is the overall affect on portfolio performance? Try using expected loss as a scenario to express the importance of testing different models.\nA data frame called portfolio has been cretaed to combine the probabilities of default for both models, the loss given default (assume 20% for now), and the loan_amnt which will be assumed to be the exposure at default.\n\n# Print the first five rows of the portfolio data frame\nprint(portfolio.head())\n\n\n\n\nportfolio_head.JPG\n\n\n\n# Create expected loss columns for each model using the formula\nportfolio['gbt_expected_loss'] = portfolio['gbt_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\nportfolio['lr_expected_loss'] = portfolio['lr_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n\n# Print the sum of the expected loss for lr\nprint('LR expected loss: ', np.sum(portfolio['lr_expected_loss']))\n\n# Print the sum of the expected loss for gbt\nprint('GBT expected loss: ', np.sum(portfolio['gbt_expected_loss']))\n\nLR expected loss: 5596776.979852879 GBT expected loss: 5383982.809227714\nIt looks like the total expected loss for the XGBClassifier() model is quite a bit lower. When we talk about accuracy and precision, the goal is to generate models which have a low expected loss. Looking at a classification_report() helps as well.\n\n\n3.3 Assessing gradient boosted trees\nSo we’ve now used XGBClassifier() models to predict probability of default. These models can also use the .predict() method for creating predictions that give the actual class for loan_status.\nWe should check the model’s initial performance by looking at the metrics from the classification_report(). Keep in mind that we have not set thresholds for these models yet.\n\n# Predict the labels for loan status\ngbt_preds= clf_gbt.predict(X_test)\n\n# Check the values created by the predict method\nprint(gbt_preds)\n\n# Print the classification report of the model\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, gbt_preds, target_names=target_names))\n\nHave a look at the precision and recall scores! Remember the low default recall values we were getting from the LogisticRegression()? This model already appears to have serious potential.\n\n\n3.4 Column importance and default prediction\n\n\n\ncolumn_importance.JPG\n\n\nWhen using multiple training sets with many different groups of columns, it’s important to keep an eye on which columns matter and which do not. It can be expensive or time-consuming to maintain a set of columns even though they might not have any impact on loan_status.\nThe X data for this exercise was created with the following code:\nX = cr_loan_prep[['person_income','loan_int_rate',\n              'loan_percent_income','loan_amnt',\n              'person_home_ownership_MORTGAGE','loan_grade_F']]\n              \nTrain an XGBClassifier() model on this data, and check the column importance to see how each one performs to predict loan_status.\n\n# redefine our feature columns\nX = cr_loan_prep[['person_income','loan_int_rate','loan_percent_income','loan_amnt','person_home_ownership_MORTGAGE','loan_grade_F']]              \n\n\n# train, test, split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n\n\n# Create and train the model on the training data\ngbt = xgb.XGBClassifier().fit(X_train,np.ravel(y_train))\n\n# Print the column importances from the model\nprint(gbt.get_booster().get_score(importance_type = 'weight'))\n\nSo, the importance for loan_grade_F is only 9 in this case. This could be because there are so few of the F-grade loans. While the F-grade loans don’t add much to predictions here, they might affect the importance of other training columns.\n\n\n3.5 Visualizing column importance\n\n\n\ncolumn_importance_plot.JPG\n\n\nWhen the model is trained on different sets of columns it changes the performance, but does the importance for the same column change depending on which group it’s in?\nThe data sets X2 and X3 have been created with the following code:\nX2 = cr_loan_prep[['loan_int_rate','person_emp_length']] \\\nX3 = cr_loan_prep[['person_income','loan_int_rate','loan_percent_income']]\n\n# Redefine feature columns\nX2 = cr_loan_prep[['loan_int_rate','person_emp_length']] \nX3 = cr_loan_prep[['person_income','loan_int_rate','loan_percent_income']]\n\n\n# train, test, split\nX2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=.4, random_state=123)\nX3_train, X3_test, y_train, y_test = train_test_split(X3, y, test_size=.4, random_state=123)\n\nUnderstanding how different columns are used to arrive at a loan_status prediction is very important for model interpretability.\n\n# Train a model on the X data with 2 columns\ngbt2 = xgb.XGBClassifier().fit(X2_train,np.ravel(y_train))\n\n# Plot the column importance for this model\nxgb.plot_importance(gbt2, importance_type = 'weight')\nplt.show()\n\n\n# Train a model on the X data with 3 columns\ngbt3 = xgb.XGBClassifier().fit(X3_train,np.ravel(y_train))\n\n# Plot the column importance for this model\nxgb.plot_importance(gbt3, importance_type = 'weight')\nplt.show()\n\nTake a closer look at the plots. Did you notice that the importance of loan_int_rate went from 1490 to 1013? Initially, this was the most important column, but person_income ended up taking the top spot here.\n\n\n3.6 Column selection and model performance\n\n\n\ntrain_columns.JPG\n\n\nCreating the training set from different combinations of columns affects the model and the importance values of the columns. Does a different selection of columns also affect the F-1 scores, the combination of the precision and recall, of the model? You can answer this question by training two different models on two different sets of columns, and checking the performance.\nInaccurately predicting defaults as non-default can result in unexpected losses if the probability of default for these loans was very low. You can use the F-1 score for defaults to see how the models will accurately predict the defaults.\n\n# Predict the loan_status using each model\ngbt_preds = gbt.predict(X_test)\ngbt2_preds = gbt2.predict(X2_test)\n\n# Print the classification report of the first model\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test,gbt_preds, target_names=target_names))\n\n# Print the classification report of the second model\nprint(classification_report(y_test, gbt2_preds, target_names=target_names))\n\nOriginally, it looked like the selection of columns affected model accuracy the most, but now we see that the selection of columns also affects recall by quite a bit.\n\n\n3.7 Cross validating credit models\nWe cannot create more loan data to help us to develop our model but we can use cross-validation to simulate how our model will perform on new laon data before it comes in.\n\n\n\ncross_validation.JPG\n\n\n\n\n\nk_fold.JPG\n\n\n\n\n\nxgb_cross_valid_setup.JPG\n\n\n\n\n\nxgb_cross_valid_setup_2.JPG\n\n\n\n# Set the values for number of folds and stopping iterations\nn_folds = 5\nearly_stopping = 10\n\n# define params dictionary\nparams = {'objective': 'binary:logistic', 'seed': 123, 'eval_metric': 'auc'}\n\n# Create the DTrain matrix for XGBoost\nDTrain = xgb.DMatrix(X_train, label = y_train)\n\n# Create the data frame of cross validations\ncv_df = xgb.cv(params, DTrain, num_boost_round = 5, nfold=n_folds,\n            early_stopping_rounds=early_stopping)\n\n# Print the cross validations data frame\nprint(cv_df)\n\nLooks good! Note how the AUC for both train-auc-mean and test-auc-mean improves at each iteration of cross-validation. The improvements suggest that our model has stability, however if we increase iterations will our scores improve until they eventually reach 1.0 ?\n\n\n3.8 Limits to cross-validation testing\nWe can specify very large numbers for both nfold and num_boost_round if we want to perform an extreme amount of cross-validation. The data frame cv_results_big was created with the following code:\ncv = xgb.cv(params, DTrain, num_boost_round = 600, nfold=10,\n        shuffle = True)\n        \nHere, cv() performed 600 iterations of cross-validation! The parameter shuffle tells the function to shuffle the records each time.\nHave a look at this data to see what the AUC are, and check to see if they reach 1.0 using cross validation. We should also plot the test AUC score to see the progression.\n\ncv_results_big = xgb.cv(params, DTrain, num_boost_round = 600, nfold=10, shuffle = True)\n\n\n# Print the first five rows of the CV results data frame\ncv_results_big.head()\n\n\n# Calculate the mean of the test AUC scores\nnp.mean(cv_results_big.head()['test-auc-mean']).round(2)\n\n# Plot the test AUC scores for each iteration\nplt.plot(cv_results_big['test-auc-mean'])\nplt.title('Test AUC Score Over 600 Iterations')\nplt.xlabel('Iteration Number')\nplt.ylabel('Test AUC Score')\nplt.show()\n\nNotice that the test AUC score never quite reaches 1.0 and begins to decrease slightly after 100 iterations. This is because this much cross-validation can actually cause the model to overfit. So, there is a limit to how much cross-validation we should do.\n\n\n3.9 Cross-validation scoring\n\n\n\ncross_val_score.JPG\n\n\nNow, we should use cross-validation scoring with cross_val_score() to check the overall performance.\nThis is exercise presents an excellent opportunity to test out the use of the hyperparameters learning_rate and max_depth. Remember, hyperparameters are like settings which can help create optimum performance.\n\ncr_loan_prep\n\n\nX = cr_loan_prep.drop('loan_status', axis =1)\n# train, test, split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n\n\nX_train\n\n\n# import\nfrom sklearn.model_selection import cross_val_score\n\n# Create a gradient boosted tree model using two hyperparameters\ngbt = xgb.XGBClassifier(learning_rate = 0.1, max_depth = 7)\n\n# Calculate the cross validation scores for 4 folds\ncv_scores = cross_val_score(gbt, X_train, np.ravel(y_train), cv=4)\n\n# Print the cross validation scores\nprint(cv_scores)\n\n# Print the average accuracy and standard deviation of the scores\nprint(\"Average accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(),\n                                              cv_scores.std() * 2))\n\nOur average cv_score for this model is getting higher! With only a couple of hyperparameters and cross-validation, we can get the average accuracy up to 93%. This is a great way to validate how robust the model is.\n\n\n3.10 Undersampling training data\nWe just used cross validation to check the robustness of our model but let’s look at how the data impacts the robustness of our model. In our training dataset there are far more non-defaults than defaults. This is referred to as class imbalance and is a problem.\n\n\n\nimbalance_causes.JPG\n\n\n\ny_train['loan_status'].value_counts()\n\nWe have 13,798 non defaults (78%) and 3,877 defaults (22%).\nOur tree models use a function called log-loss which our model tries to minimise. Take the example below, where we have one default and one non-default. Each of the predictions is equally far away from the true outcome, and so the log-losss value is the same - however, the financial implications of an incorrect default prediction are much more severe than an incorrect non-default prediction.\nOne strategy we can adopt to restore the balance of our training data is undersampling :\n\n\n\nundersampling.JPG\n\n\n\n\n\nundersample_train_test_split_1.JPG\n\n\n\n\n\nundersample_train_test_split_2.JPG\n\n\nIt’s time to undersample the training set with a few lines of code from pandas. Once the undersampling is complete, we can check the value counts for loan_status to verify the results.\n\n# Concat the training sets\nX_y_train = pd.concat([X_train.reset_index(drop = True),\n                       y_train.reset_index(drop = True)], axis = 1)\n\n# Get counts of non default and defaults\ncount_nondefault, count_default = X_y_train['loan_status'].value_counts()\n\n\n# Create data sets for defaults and non-defaults\nnondefaults = X_y_train[X_y_train['loan_status'] == 0]\ndefaults =  X_y_train[ X_y_train['loan_status'] == 1]\n\n# Undersample the non-defaults\nnondefaults_under = nondefaults.sample(count_default)\n\n# Concatenate the undersampled nondefaults with defaults\nX_y_train_under = pd.concat([nondefaults_under.reset_index(drop = True),\n                             defaults.reset_index(drop = True)], axis = 0)\n\n# Print the value counts for loan status\nprint(X_y_train_under['loan_status'].value_counts())\n\nGreat. We now have a training set with an equal number of defaults and non-defaults. Let’s test out some machine learning models on this new undersampled data set and compare their performance to the models trained on the regular data set.\n\n\n3.11 Undersampled tree performance\nWe’ve undersampled the training set and trained a model on the undersampled set.\nThe performance of the model’s predictions not only impact the probability of default on the test set, but also on the scoring of new loan applications as they come in. We also now know that it is even more important that the recall of defaults be high, because a default predicted as non-default is more costly.\nThe next crucial step is to compare the new model’s performance to the original model.\n\n# Check the classification reports\ntarget_names = ['Non-Default', 'Default']\n\n# print classification report for old model\nprint(classification_report(y_test, gbt_preds, target_names=target_names))\n\n             precision    recall  f1-score   support\n\n Non-Default       0.93      0.99      0.96      9198\n     Default       0.95      0.73      0.83      2586\n\n    accuracy                           0.93     11784\n   macro avg       0.94      0.86      0.89     11784\nweighted avg       0.93      0.93      0.93     11784 \n\n# print classification report for new model\nprint(classification_report(y_test, gbt2_preds, target_names=target_names))\n\n             precision    recall  f1-score   support\n\n Non-Default       0.95      0.91      0.93      9198\n     Default       0.72      0.84      0.77      2586\n\n    accuracy                           0.89     11784\n   macro avg       0.83      0.87      0.85     11784\nweighted avg       0.90      0.89      0.89     11784\n\n# Print the confusion matrix for old model\nprint(confusion_matrix(y_test,gbt_preds))\n\n[[9105 93] [ 691 1895]]\n\n# Print the confusion matrix for new model\nprint(confusion_matrix(y_test,gbt2_preds))\n\n[[8338 860] [ 426 2160]]\n\n# Print and compare the AUC scores of the old model\nprint(roc_auc_score(y_test, gbt_preds))\n\n0.8613405315086655\n\n# Print and compare the AUC scores of the new model\nprint(roc_auc_score(y_test,gbt2_preds))\n\n0.870884117348218\nLooks like this is classified as a success! Undersampling the training data results in more false positives, but the recall for defaults and the AUC score are both higher than the original model. This means overall it predicts defaults much more accurately.\n\n\n3.12 Undersampling intuition\nNow we’ve seen the effects of undersampling the training set to improve default prediction. We undersampled the training data set X_train, and it had a positive impact on the new model’s AUC score and recall for defaults. The training data had class imbalance which is normal for most credit loan data.\nWe did not undersample the test data X_test. Why not undersample the test set as well?\nThe test set represents the type of data that will be seen by the model in the real world, so changing it would test the model on unrealistic data."
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#model-evaluation-and-implementation",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#model-evaluation-and-implementation",
    "title": "Credit Risk Modeling in Python",
    "section": "4. Model evaluation and implementation",
    "text": "4. Model evaluation and implementation\nAfter developing and testing two powerful machine learning models, we use key performance metrics to compare them. Using advanced model selection techniques specifically for financial modeling, we will select one model. With that model, we will:\n\ndevelop a business strategy\nestimate portfolio value, and\nminimize expected loss\n\n\n4.1 Comparing model reports\nWe’ve used logistic regression models and gradient boosted trees. It’s time to compare these two to see which model will be used to make the final predictions.\nOne of the easiest first steps for comparing different models’ ability to predict the probability of default is to look at their metrics from the classification_report(). With this, we can see many different scoring metrics side-by-side for each model. Because the data and models are normally unbalanced with few defaults, focus on the metrics for defaults for now.\n\n# Print the logistic regression classification report\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, preds_df_lr['loan_status'], target_names=target_names))\n\n               precision    recall  f1-score   support\n\n Non-Default       0.86      0.92      0.89      9198\n     Default       0.62      0.46      0.53      2586\n\n    accuracy                           0.82     11784\n   macro avg       0.74      0.69      0.71     11784\nweighted avg       0.81      0.82      0.81     11784\n\n# Print the gradient boosted tree classification report\nprint(classification_report(y_test, preds_df_gbt['loan_status'], target_names=target_names))\n\n              precision    recall  f1-score   support\n\n Non-Default       0.93      0.99      0.96      9198\n     Default       0.94      0.73      0.82      2586\n\n    accuracy                           0.93     11784\n   macro avg       0.93      0.86      0.89     11784\nweighted avg       0.93      0.93      0.93     11784\n\n# Print the default F-1 scores for the logistic regression\nprint(precision_recall_fscore_support(y_test,preds_df_lr['loan_status'], average = 'macro')[2])\n\n0.7108943782814463\n\n# Print the default F-1 scores for the gradient boosted tree\nprint(precision_recall_fscore_support(y_test,preds_df_gbt['loan_status'], average = 'macro')[2])\n\n0.8909014142736051\nThere is a noticeable difference between these two models. The scores from the classification_report() are all higher for the gradient boosted tree. This means the tree model is better in all of these aspects. Let’s check the ROC curve.\n\n\n4.2 Comparing with ROCs and AUCs\nWe should use ROC charts and AUC scores to compare the two models. Sometimes, visuals can really help you and potential business users understand the differences between the various models under consideration.\nWith the graph in mind, we will be more equipped to make a decision. The lift is how far the curve is from the random prediction. The AUC is the area between the curve and the random prediction. The model with more lift, and a higher AUC, is the one that’s better at making predictions accurately.\n\n# ROC chart components\nfallout_lr, sensitivity_lr, thresholds_lr = roc_curve(y_test, clf_logistic_preds)\nfallout_gbt, sensitivity_gbt, thresholds_gbt = roc_curve(y_test, clf_gbt_preds)\n\n# ROC Chart with both\nplt.plot(fallout_lr, sensitivity_lr, color = 'blue', label='%s' % 'Logistic Regression')\nplt.plot(fallout_gbt, sensitivity_gbt, color = 'green', label='%s' % 'GBT')\nplt.plot([0, 1], [0, 1], linestyle='--', label='%s' % 'Random Prediction')\nplt.title(\"ROC Chart for LR and GBT on the Probability of Default\")\nplt.xlabel('Fall-out')\nplt.ylabel('Sensitivity')\nplt.legend()\nplt.show()\n\n\n\n# Print the logistic regression AUC with formatting\nprint(\"Logistic Regression AUC Score: %0.2f\" % roc_auc_score(y_test, clf_logistic_preds))\n\n# Print the gradient boosted tree AUC with formatting\nprint(\"Gradient Boosted Tree AUC Score: %0.2f\" % roc_auc_score(y_test, clf_gbt_preds))\n\nLogistic Regression AUC Score: 0.76 Gradient Boosted Tree AUC Score: 0.94\nLook at the ROC curve for the gradient boosted tree. Not only is the lift much higher, the calculated AUC score is also quite a bit higher. It’s beginning to look like the gradient boosted tree is best. Let’s check the calibration to be sure.\n\n\n4.3 Calibration curves\n\n\n\ncalibration.JPG\n\n\n\n\n\ncalibration_calc.JPG\n\n\nWe now know that the gradient boosted tree clf_gbt has the best overall performance. You need to check the calibration of the two models to see how stable the default prediction performance is across probabilities. We can use a chart of each model’s calibration to check this by calling the calibration_curve() function.\nCalibration curves can require many lines of code in python, so we will go through each step slowly to add the different components.\n\n# Set calibration curve outputs\nfrac_of_pos_lr = ([0.07886231, 0.06610942, 0.10835913, 0.13505074, 0.16063348, 0.18333333, 0.21268657, 0.24099099, 0.48036649, 0.72677596,\n       0.73354232, 0.70547945, 0.68 , 0.73913043, 0.55555556, 0.4 ])\n\nmean_pred_val_lr = ([0.02111464, 0.07548788, 0.12582662, 0.17502903, 0.22449499, 0.27491676, 0.32488847, 0.37486698, 0.42302912, 0.47397249,\n       0.52304288, 0.57259508, 0.62200793, 0.67156702, 0.71909209, 0.77024859])\n\n\nfrac_of_pos_gbt = ([0.01916168, 0.06385752, 0.12795793, 0.17460317, 0.21806854, 0.32620321, 0.32653061, 0.33333333, 0.40677966, 0.43181818,\n       0.6, 0.42105263, 0.31578947, 0.6875, 0.78571429, 0.83333333, 0.90697674, 0.95238095, 0.98850575, 1.])\n\nmean_pred_val_gbt = ([0.01937249, 0.07211534, 0.12178284, 0.17298488, 0.22318428, 0.2716055, 0.32285183, 0.369344, 0.42164062, 0.47158214,\n       0.52230485, 0.57041398, 0.62149714, 0.67234764, 0.72826275, 0.77567046, 0.82827961, 0.87636708, 0.92830987, 0.98579916])\n\n\n\n\ncalibration_plot.JPG\n\n\n\n# Create the calibration curve plot with the guideline\nplt.plot([0, 1], [0, 1], 'k:', label= \"Perfectly calibrated\")    \nplt.ylabel('Fraction of positives')\nplt.xlabel('Average Predicted Probability')\nplt.legend()\nplt.title('Calibration Curve')\nplt.show()\n\n\n# Add the calibration curve for the logistic regression to the plot\nplt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')    \nplt.plot(mean_pred_val_lr, frac_of_pos_lr,\n         's-', label='%s' % 'Logistic Regression')\nplt.ylabel('Fraction of positives')\nplt.xlabel('Average Predicted Probability')\nplt.legend()\nplt.title('Calibration Curve')\nplt.show()\n\n\n# Add the calibration curve for the gradient boosted tree\nplt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')    \nplt.plot(mean_pred_val_lr, frac_of_pos_lr,\n         's-', label='%s' % 'Logistic Regression')\nplt.plot(mean_pred_val_gbt, frac_of_pos_gbt,\n         's-', label='%s' % 'Gradient Boosted tree')\nplt.ylabel('Fraction of positives')\nplt.xlabel('Average Predicted Probability')\nplt.legend()\nplt.title('Calibration Curve')\nplt.show()\n\nTake a good look at this. Notice that for the logistic regression, the calibration for probabilities starts off great but then gets more erratic as it the average probability approaches 0.4. Something similar happens to the gradient boosted tree around 0.5, but the model eventually stabilizes. We will be focusing only on the gbt model from now on.\n\n\n4.4 Acceptance rates\nSetting an acceptance rate and calculating the threshold for that rate can be used to set the percentage of new loans we want to accept.\n\n\n\nacceptance_rate.JPG\n\n\nIn the above example, the distribution of our loans is represented in terms of their probability of default. 85% of our loans are to the left of the dashed line, with and 15% are to the right of the dashed line. If our policy is to ensure that 15% of loans are rejected, then the dashed line represents the acceptance threshold. We can see by reading off the graph that we should therefore reject any new loan applications with predicted probability of default of around 78% or above.\nIn our example, the exact acceptance threshold can be calculated using Numpy quantile.\nimport numpy as np\nthreshold = np.quantile(prob_default, 0.85)\nWe would then reassign our loan_status values as we did before arbitrarily, with the calculated threshold:\npreds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.78 else 0)        \nLet’s see how this works in more detail by applying to our loan data. For this exercise, assume the test data is a fresh batch of new loans. We will need to use the quantile() function from numpy to calculate the threshold.\nThe threshold should be used to assign new loan_status values. Does the number of defaults and non-defaults in the data change?\n\n# Check the statistics of the probabilities of default\nprint(test_pred_df['prob_default'].describe())\n\n\n\n\ndefault_stats.JPG\n\n\n\n# Calculate the threshold for a 85% acceptance rate\nthreshold_85= np.quantile(test_pred_df['prob_default'], 0.85)\n\n0.8039963573217376\n\n# Apply acceptance rate threshold\ntest_pred_df['pred_loan_status'] = test_pred_df['prob_default'].apply(lambda x: 1 if x > threshold_85 else 0)\n\n# Print the counts of loan status after the threshold\nprint(test_pred_df['pred_loan_status'].value_counts())\n\n\n\n\nloan_status_counts.JPG\n\n\nIn the results of .describe() we can see how it’s not until 75% that we start to see double-digit numbers. That’s because the majority of our test set is non-default loans. Next let’s look at how the acceptance rate and threshold split up the data.\n\n\n4.5 Visualizing quantiles of acceptance\nWe know how quantile() works to compute a threshold, and we’ve seen an example of what it does to split the loans into accepted and rejected. What does this threshold look like for the test set, and how can you visualize it? To check this, we can create a histogram of the probabilities and add a reference line for the threshold. With this, we can visually show where the threshold exists in the distribution.\n\n# Plot the predicted probabilities of default\nplt.hist(clf_gbt_preds, color = 'blue', bins = 40)\n\n# Calculate the threshold with quantile\nthreshold = np.quantile(clf_gbt_preds, 0.85)\n\n# Add a reference line to the plot for the threshold\nplt.axvline(x = threshold, color = 'red')\nplt.show()\n\n\nHere, we can clearly see where the threshold is on the range of predicted probabilities - 0.804 as calculated in section 4.4 - and indicated by the red line. Not only can we see how many loans will be accepted (left side), but also how many loans will be rejected (right side).\n\n\n4.6 Bad rates\n\n\n\nbad_rate.JPG\n\n\nWith acceptance rate in mind, we can now analyze the bad rate within the accepted loans. This way we will be able to see the percentage of defaults that have been accepted. Think about the impact of the acceptance rate and bad rate. We set an acceptance rate to have fewer defaults in the portfolio because defaults are more costly. Will the bad rate be less than the percentage of defaults in the test data?\n\n# Print the top 5 rows of the new data frame\nprint(test_pred_df.head())\n\n    true_loan_status  prob_default  pred_loan_status\n0                 1         0.982                 1\n1                 1         0.975                 1\n2                 0         0.003                 0\n3                 0         0.005                 0\n4                 1         0.120                 0\n\n# Create a subset of only accepted loans\naccepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n\n# Calculate the bad rate\nprint(np.sum(accepted_loans['true_loan_status']) / accepted_loans['true_loan_status'].count())\n\n0.08256789137380191\nThis bad rate doesn’t look half bad! The bad rate with the threshold set by the 85% quantile() is just over 8%. This means that of all the loans we’ve decided to accept from the test set, only 8% were actual defaults! If we accepted all loans, the percentage of defaults would be around 22%.\n\n\n4.7 Acceptance rate impact\nNow, look at the loan_amnt of each loan to understand the impact on the portfolio for the acceptance rates. We can use cross tables with calculated values, like the average loan amount, of the new set of loans X_test. For this, we will multiply the number of each with an average loan_amnt value.\nWhen printing these values, try formatting them as currency so that the numbers look more realistic. After all, credit risk is all about money. This is accomplished with the following code:\npd.options.display.float_format = '${:,.2f}'.format\n\n# Print the statistics of the loan amount column\nprint(test_pred_df['loan_amnt'].describe())\n\n\n\n\nloan_amount_stats.JPG\n\n\n\n# Store the average loan amount\navg_loan = np.mean(test_pred_df['loan_amnt'])\n\n# Set the formatting for currency, and print the cross tab\npd.options.display.float_format = '${:,.2f}'.format\n\n# print the cross table\nprint(pd.crosstab(test_pred_df['true_loan_status'],\n                 test_pred_df['pred_loan_status_15']).apply(lambda x: x * avg_loan, axis = 0))\n\n\n\n\npred_loan_status.JPG\n\n\nWith this, we can see that our bad rate of about 8% represents an estimated loan value of about 7.9 million dollars. This may seem like a lot at first, but compare it to the total value of non-default loans! With this, we are ready to start talking about our acceptance strategy going forward.\n\n\n4.8 Making the strategy table\nBefore we implement a strategy, we should first create a strategy table containing all the possible acceptance rates we wish to look at along with their associated bad rates and threshold values. This way, we can begin to see each part of our strategy and how it affects our portfolio.\nAutomatically calculating all of these values only requires a for loop, but requires many lines of python code.\n\n\n\nstrategy_table_setup.JPG\n\n\n\n\n\nstrategy_table_calcs.JPG\n\n\n\n# create empty lists for thresholds and bad rates - to be appended\nthresholds = []\nbad_rates = []\n\n\n# let's set the accept rates that we would like to compare calculations for\naccept_rates = [1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]\n\n\n# Populate the arrays for the strategy table with a for loop\nfor rate in accept_rates:\n    \n    # Calculate the threshold for the acceptance rate\n    thresh = np.quantile(preds_df_gbt['prob_default'], rate).round(3)\n    \n    # Add the threshold value to the list of thresholds\n    thresholds.append(np.quantile(preds_df_gbt['prob_default'], rate).round(3))\n    \n    # Reassign the loan_status value using the threshold\n    test_pred_df['pred_loan_status'] = test_pred_df['prob_default'].apply(lambda x: 1 if x > thresh else 0)\n    \n    # Create a set of accepted loans using this acceptance rate\n    accepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n    \n    # Calculate and append the bad rate using the acceptance rate\n    bad_rates.append(np.sum((accepted_loans['true_loan_status']) / len(accepted_loans['true_loan_status'])).round(3))\n\n\n# Instantiate the values for thresholds and bad rates\nthresholds = [1.0, 0.992, 0.976, 0.804, 0.254, 0.178, 0.138, 0.111, 0.093, 0.078, 0.066, 0.055, 0.045, 0.037, 0.03, 0.022, 0.015, 0.008, 0.004, 0.002]\nbad_rates = [0.219, 0.179, 0.132, 0.083, 0.061, 0.052, 0.043, 0.036, 0.03, 0.027, 0.023, 0.02, 0.017, 0.014, 0.01, 0.008, 0.005, 0.001, 0.0, 0.0]\n\n\n# Create a data frame of the strategy table\nstrat_df = pd.DataFrame(zip(accept_rates, thresholds, bad_rates),\n                        columns = ['Acceptance Rate','Threshold','Bad Rate'])\n\n# Print the entire table\nstrat_df\n\n\n\n\n\n  \n    \n      \n      Acceptance Rate\n      Threshold\n      Bad Rate\n    \n  \n  \n    \n      0\n      1.00\n      1.000\n      0.219\n    \n    \n      1\n      0.95\n      0.992\n      0.179\n    \n    \n      2\n      0.90\n      0.976\n      0.132\n    \n    \n      3\n      0.85\n      0.804\n      0.083\n    \n    \n      4\n      0.80\n      0.254\n      0.061\n    \n    \n      5\n      0.75\n      0.178\n      0.052\n    \n    \n      6\n      0.70\n      0.138\n      0.043\n    \n    \n      7\n      0.65\n      0.111\n      0.036\n    \n    \n      8\n      0.60\n      0.093\n      0.030\n    \n    \n      9\n      0.55\n      0.078\n      0.027\n    \n    \n      10\n      0.50\n      0.066\n      0.023\n    \n    \n      11\n      0.45\n      0.055\n      0.020\n    \n    \n      12\n      0.40\n      0.045\n      0.017\n    \n    \n      13\n      0.35\n      0.037\n      0.014\n    \n    \n      14\n      0.30\n      0.030\n      0.010\n    \n    \n      15\n      0.25\n      0.022\n      0.008\n    \n    \n      16\n      0.20\n      0.015\n      0.005\n    \n    \n      17\n      0.15\n      0.008\n      0.001\n    \n    \n      18\n      0.10\n      0.004\n      0.000\n    \n    \n      19\n      0.05\n      0.002\n      0.000\n    \n  \n\n\n\n\nThat for loop was a lot of code, but look at the strategy table we have now. This uses our specific predictions on the credit data, and can be used to see the acceptance rates, bad rates, and financial impact all at once. One of these values has the highest estimated value.\n\n\n4.9 Visualizing the strategy\nNow we have the extended strategy table strat_df. The table is not so big that it’s difficult to analyze, but visuals can help us see the overview all at once.\nWe should check the distribution of each column with a box plot. If the distribution of Acceptance Rate looks the same as the Bad Rate column, that could be a problem. That means that the model’s calibration is likely much worse than we thought.\nWe can also visualize the strategy curve with a line plot. The Acceptance Rate would be the independent variable with the Bad Rate as the dependent variable.\n\n# Visualize the distributions in the strategy table with a boxplot\nstrat_df.boxplot()\nplt.show()\n\n\n\n\nThe boxplots above show us the distribution for each column.\n\n# Plot the strategy curve\nplt.plot(strat_df['Acceptance Rate'], strat_df['Bad Rate']) \nplt.plot()\nplt.xlabel('Acceptance Rate')\nplt.ylabel('Bad Rate')\nplt.title('Acceptance and Bad Rates')\nplt.show()\n\n\n\n\nThe bad rates are very low up until the acceptance rate 0.6 where they suddenly increase. This suggests that many of the accepted defaults may have a prob_default value between 0.6 and 0.8.\n\n\n4.10 Estimated value profiling\nThe strategy table, strat_df, can be used to maximize the estimated portfolio value and minimize expected loss. Extending this table and creating some plots can be very helpful to this end.\nThe strat_df data frame is loaded and has been enhanced already with the following columns:\n\n\n\nstrategy_table_prepop.JPG\n\n\n\n# Create a line plot of estimated value\nplt.plot(strat_df['Acceptance Rate'],strat_df['Estimated Value'])\nplt.title('Estimated Value by Acceptance Rate')\nplt.xlabel('Acceptance Rate')\nplt.ylabel('Estimated Value')\nplt.show()\n\n\n\n# Print the row with the max estimated value\nprint(strat_df.loc[strat_df['Estimated Value'] == np.max(strat_df['Estimated Value'])])\n\n\n\n\nmax_est_value.JPG\n\n\nInteresting! With our credit data and our estimated averag loan value, we clearly see that the acceptance rate 0.85 has the highest potential estimated value. Normally, the allowable bad rate is set, but we can use analyses like this to explore other options.\n\n\n4.11 Total expected loss\n\n\n\ntotal_expected_loss.JPG\n\n\nIt’s time to estimate the total expected loss given all our decisions. The data frame test_pred_df has the probability of default for each loan and that loan’s value. Use these two values to calculate the expected loss for each loan. Then, we can sum those values and get the total expected loss.\nFor this exercise, we will assume that the exposure is the full value of the loan, and the loss given default is 100%. This means that a default on each the loan is a loss of the entire amount.\n\n# Print the first five rows of the data frame\nprint(test_pred_df.head())\n\n\n\n\ntest_pred_df.JPG\n\n\n\n# Calculate the bank's expected loss and assign it to a new column\ntest_pred_df['expected_loss'] = test_pred_df['prob_default'] * test_pred_df['loss_given_default'] * test_pred_df['loan_amnt']\n\n# Calculate the total expected loss to two decimal places\ntot_exp_loss= round(np.sum(test_pred_df['expected_loss']),2)\n\n# Print the total expected loss\nprint('Total expected loss: ', '${:,.2f}'.format(tot_exp_loss))\n\nTotal expected loss: $27,084,153.38\nThis is the total expected loss for the entire portfolio using the gradient boosted tree. 27 million US dollars may seem like a lot, but the total expected loss would have been over 28 million US dollars with the logistic regression. Some losses are unavoidable, but our work here might have saved the company a million dollars!"
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#key-takeaways",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#key-takeaways",
    "title": "Credit Risk Modeling in Python",
    "section": "Key takeaways",
    "text": "Key takeaways\nOur first step was to prepare credit data for machine learning models:\n\nimportant to understand the data\nimproving the data allows for high performing simple models\n\nWe then developed, scored and now understand Logistic Regression and Gradient Boosted Trees:\n\nthese models are simple and explainable\ntheir performance on probabilities is acceptable\n\nWe then analyzed the performance of models by changing the data, and now understand the financial impact of results.\nWe implemented the model with an understanding of strategy. The models and framework covered:\n\ndiscrete-time hazard model (point in time): the probability of default is a point-in-time event\nstructural model framework: the model explains the default even based on other factors\n\nMany financial sectors prefer model interpretability:\n\ncomplex or black-box models are a risk because the business cannot explain their decisions fully. This is important, particularly when considering loan applications. The customer has a right to know on which basis their application was rejected\ndeep neural networks are often too complex"
  },
  {
    "objectID": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#leaving-thoughts-and-acknowledgements",
    "href": "posts/Credit Risk Modelling in Python/Credit_Risk_Modelling.html#leaving-thoughts-and-acknowledgements",
    "title": "Credit Risk Modeling in Python",
    "section": "Leaving thoughts and acknowledgements",
    "text": "Leaving thoughts and acknowledgements\nFocus on the data:\n\ngather as much data as possible\nuse many different techniques to prepare and enhance data\nlearn about the business\nincrease value through data\n\nModel complexity can be a double-edged sword:\n\nreally complex models may perform well, but are seen as a black box\nin many cases, business users will not accept a model they cannot understand\ncomplex models can be very large and difficult to put into production\n\nThanks to Michael Crabtree for creating this course on DataCamp. It explains how to interpret the coefficients and intercepts of a Logistic Regression model particularly well in my view."
  },
  {
    "objectID": "posts/Seaborn/Seaborn.html",
    "href": "posts/Seaborn/Seaborn.html",
    "title": "Seaborn Tutorial",
    "section": "",
    "text": "There is no universally best way to visualize data. Different questions are best answered by different plots. Seaborn makes it easy to switch between different visual representations by using a consistent dataset-oriented API. Seaborn helps you explore and understand your data.\nIts plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\n\n\n\n\nseaborn has a number of built in datases:\n\n# Seaborn provides a playground of built in datasets\nprint(sns.get_dataset_names())\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\n\nFor the purposes of this blog we will be working with the car_crashes dataset.\n\n\n\ncar_crash.jpg\n\n\n\n# Load a built in dataset of US car crash\ncrash_df = sns.load_dataset('car_crashes')\ncrash_df.head()\n\n\n\n\n\n  \n    \n      \n      total\n      speeding\n      alcohol\n      not_distracted\n      no_previous\n      ins_premium\n      ins_losses\n      abbrev\n    \n  \n  \n    \n      0\n      18.8\n      7.332\n      5.640\n      18.048\n      15.040\n      784.55\n      145.08\n      AL\n    \n    \n      1\n      18.1\n      7.421\n      4.525\n      16.290\n      17.014\n      1053.48\n      133.93\n      AK\n    \n    \n      2\n      18.6\n      6.510\n      5.208\n      15.624\n      17.856\n      899.47\n      110.35\n      AZ\n    \n    \n      3\n      22.4\n      4.032\n      5.824\n      21.056\n      21.280\n      827.34\n      142.39\n      AR\n    \n    \n      4\n      12.0\n      4.200\n      3.360\n      10.920\n      10.680\n      878.41\n      165.63\n      CA\n    \n  \n\n\n\n\n\n\n\n\n\nA distribution plot provides a way to look at a univariate distribution. A univeriate distribution provides a distribution for one variable. For more detail check out the seaborn documentation.\n\n# Kernal Density Estimation with a Histogram is provided\nsns.distplot(crash_df['alcohol'])\n\n/tmp/ipykernel_206/3403426496.py:3: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(crash_df['alcohol'])\n\n\n<AxesSubplot: xlabel='alcohol', ylabel='Density'>\n\n\n\n\n\nIf we just want the KDE on its own:\n\n# Get just the KDE plot\nsns.kdeplot(crash_df['alcohol'])\n\n<AxesSubplot: xlabel='alcohol', ylabel='Density'>\n\n\n\n\n\n\n\n\n\n\n\nKernel Density Estimation (KDE)\n\n\n\nThe kernel density estimation is included by default but can be removed by setting kde=False.\n\n\n\n# kde=False removes the KDE\n# Bins define how many buckets to divide the data up into between intervals\n\nsns.distplot(crash_df['alcohol'], \n             kde=False, \n             bins=25)\n\n/tmp/ipykernel_206/1004551440.py:5: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(crash_df['alcohol'], kde=False, bins=25)\n\n\n<AxesSubplot: xlabel='alcohol'>\n\n\n\n\n\n\n\n\nLet’s say we want to investigate the alcohol and speeding variables and also the relationship between the two. A jointplot compares two distributions and plots a scatter plot by default. We can also include a ‘best fit’ regression line by passing in the argument kind=‘reg’.\n\n# joint plot of speeding vs alcohol\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'reg')\n\n<seaborn.axisgrid.JointGrid at 0x7f31f0add2a0>\n\n\n\n\n\nAs we can see there looks to be a clear positive correlation between speeding and drink-driving.\nWe can also create a 2D KDE by passing in the argument kind=‘kde’\n\n# joint plot of speeding vs alcohol\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'kde')\n\n<seaborn.axisgrid.JointGrid at 0x7f31ec15ac80>\n\n\n\n\n\n…and we can create a hexagon distribution with kind=‘hex’\n\n# joint plot of speeding vs alcohol\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'hex')\n\n<seaborn.axisgrid.JointGrid at 0x7f31ecd43c10>\n\n\n\n\n\n\n\n\nPair Plots plot relationships across the entire DataFrame’s numerical values:\n\n# # pair plot for car_crashes DataFrame\nsns.pairplot(crash_df)\n\n<seaborn.axisgrid.PairGrid at 0x7f31cecd4be0>\n\n\n\n\n\nNote that in addition to the relationship scatterplots we also have a histogram for each variable on the leading diagonal.\n\n\n\n\n\n\nUsing hue as a categorical variable\n\n\n\nTWe can pass in hue=‘some_categorical_variable’ to effectively include a THIRD variable in our pair plot!\n\n\nLet’s illustrate this using one of the other inbuilt seaborn datasets tips which includes some categorical variables:\n\n# With hue you can pass in a categorical column and the charts will be colorized\n# You can use color maps (palette) from Matplotlib to define what colors to use\ntips_df = sns.load_dataset('tips')\nsns.pairplot(tips_df, \n             hue = 'sex', \n             palette = 'Blues')\n\n<seaborn.axisgrid.PairGrid at 0x7f31cdf078b0>\n\n\n\n\n\n\n\n\nA Rug Plot plots a single column of datapoints in an array as sticks on an axis. You’ll see a more dense number of lines where the amount is most common. This is like how a histogram is taller where values are more common.\n\n# plot a rugplot of tips\nsns.rugplot(tips_df['tip'])\n\n<AxesSubplot: xlabel='tip'>\n\n\n\n\n\nWe can see that the lines are denser between just below 2 dollars and up towards 3 dollars.\n\n\n\n\nWe can control the overall look of our plots using set_style\n\n# set overall style\nsns.set_style('ticks')\n\n# set size of our plot\nplt.figure(figsize = (8,4))\n\n# labels\nsns.set_context('paper', font_scale = 1.5)\n\n# jointplot of speeding v alcohol from car_crashes DF, include regression line\nsns.jointplot(x = 'speeding', \n              y = 'alcohol', \n              data = crash_df, \n              kind = 'reg') \n\n# we can turn the axis spines on or off\nsns.despine(left = False, \n            bottom = True)\n\n<Figure size 800x400 with 0 Axes>\n\n\n\n\n\n\n\n\n\n\nBar plot or bar chart can be used to aggregate categorical data based on a function. Mean is the default but we can change the reported summary statistic. For more information see the seaborn documentation.\n\n# plot a barplot showing mean total bill by sex\nsns.barplot(x = 'sex', \n            y = 'total_bill', \n            data = tips_df)\n\n<AxesSubplot: xlabel='sex', ylabel='total_bill'>\n\n\n\n\n\nWe can estimate total bill amount based on sex. With estimator we can define functions to use other than the mean like those provided by NumPy : median, std, var, cov or make your own functions\n\n# plot a barplot showing median total bill by sex\nsns.barplot(x = 'sex',\n            y = 'total_bill',\n            data = tips_df, \n            estimator = np.median)\n\n<AxesSubplot: xlabel='sex', ylabel='total_bill'>\n\n\n\n\n\n\n\n\nA count plot is like a bar plot, but the estimator is counting the number of occurences. For more information see the seaborn documentation.\n\n# plot a count of number of males and females\nsns.countplot(x = 'sex',\n              data = tips_df)\n\n<AxesSubplot: xlabel='sex', ylabel='count'>\n\n\n\n\n\n\n\n\n\n\n\nshift and tab\n\n\n\nHit shift and tab after the function to pull up docstring and the list of possible arguments\n\n\n\nshift_tab.JPG\n\n\n\n\n\n\n\nA box plot allows you to compare different variables. The box shows the quartiles of the data. The bar in the middle is the median and the box extends 1 standard deviation from the median. The whiskers extend to all the other data aside from the points that are considered to be outliers. For more information see the seaborn documentation.\n\n# create box plots of total bill per day\n# use hue to allow comparison between male and female\nsns.boxplot(x = 'day',\n            y = 'total_bill', \n            data = tips_df, \n            hue = 'sex')\n\n\n# Sometimes the labelling can get a bit cluttered. We can control this\n# loc=0 moves legend to the best position\nplt.legend(loc=0)\n\n<matplotlib.legend.Legend at 0x7f31c52d92d0>\n\n\n\n\n\nInterestingly, we can see that men spend more than women on a Friday, but less than women on a Saturday.\n\n\n\nA violin plot is a combination of the boxplot and KDE. Whereas a box plot corresponds to data points, the violin plot uses the KDE estimation of the data points. For more information see the seaborn documentation.\n\n# create a violin plot showing distribution of total bill by day and sex\nsns.violinplot(x = 'day', \n               y = 'total_bill', \n               data = tips_df, \n               hue = 'sex')\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n# Split allows you to compare how the categories compare to each other\nsns.violinplot(x = 'day',\n               y = 'total_bill',\n               data = tips_df,\n               hue = 'sex',\n               split = True)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n\n\nThe strip plot or dotplot draws a scatter plot representing all data points where one variable is categorical. It is often used to show all observations with a box plot that represents the average distribution. For more information see the seaborn documentation.\n\nplt.figure(figsize = (8,5))\n\n# Jitter spreads data points out so that they aren't stacked on top of each other\n# Hue breaks data into men and women\nsns.stripplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              jitter = True, \n              hue = 'sex',\n              dodge = False)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\nThis looks great, but the data points for male and female are difficult to distinguish. We can set the argument dode=True to separate them amd show them side by side:\n\nplt.figure(figsize = (8,5))\n\n# Jitter spreads data points out so that they aren't stacked on top of each other\n# Hue breaks data into men and women\nsns.stripplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              jitter = True, \n              hue = 'sex',\n              dodge = True)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n\n\nA swarm plot is like a strip plot, but points are adjusted so they don’t overlap. It looks like a combination of the violin and strip plots. For more information see the seaborn documentation.\n\n# create a raw swarmplot of total bill bu day\nsns.swarmplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df)\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\nWe can then stack a violin plot with a swarm:\n\n# stack a violin plot on top of our swarmplot\nsns.violinplot(x = 'day',\n               y = 'total_bill',\n               data = tips_df)\n\nsns.swarmplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              color = 'white')\n\n<AxesSubplot: xlabel='day', ylabel='total_bill'>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor maps\n\n\n\nYou can use matplotlib’s color maps for styling\n\n\n\nplt.figure(figsize = (8,8))\n\n# let's go for a presentation style using 'talk'\nsns.set_style('ticks')\nsns.set_context('paper')\n\n# customize our colours using palette argument\nsns.stripplot(x = 'day',\n              y = 'total_bill',\n              data = tips_df,\n              hue = 'sex',\n              palette = 'plasma')\n\n# control location of legend \nplt.legend(loc=2)\n\n<matplotlib.legend.Legend at 0x7f31c50bcc40>\n\n\n\n\n\n\n\n\n\n\n\nSetting the legend location\n\n\n\nWe can control the position of the legend by using plt.legend(loc = )\n1 - upper left\n2 - lower left\n3 - lower right\nSee the other options\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo create a heatmap you must have data set up as a matrix where variables are on the columns and rows\n\n\nFor example let’s look at an extract from our car_crashes dataset :\n\n# grab first 5 rows \ncrash_df.head()\n\n\n\n\n\n  \n    \n      \n      total\n      speeding\n      alcohol\n      not_distracted\n      no_previous\n      ins_premium\n      ins_losses\n      abbrev\n    \n  \n  \n    \n      0\n      18.8\n      7.332\n      5.640\n      18.048\n      15.040\n      784.55\n      145.08\n      AL\n    \n    \n      1\n      18.1\n      7.421\n      4.525\n      16.290\n      17.014\n      1053.48\n      133.93\n      AK\n    \n    \n      2\n      18.6\n      6.510\n      5.208\n      15.624\n      17.856\n      899.47\n      110.35\n      AZ\n    \n    \n      3\n      22.4\n      4.032\n      5.824\n      21.056\n      21.280\n      827.34\n      142.39\n      AR\n    \n    \n      4\n      12.0\n      4.200\n      3.360\n      10.920\n      10.680\n      878.41\n      165.63\n      CA\n    \n  \n\n\n\n\nIn its current format (with the variables across the columns but not the rows) we won’t be able to produce a heat map for our car_crashes DataFrame. We can transform our DataFrame into the correct format using a correlation function:\n\nplt.figure(figsize=(8,6))\nsns.set_context('paper',\n                font_scale = 1.4)\n\n# transform our dataset into a correlation matrix\ncrash_mx = crash_df.corr()\ncrash_mx\n\n/tmp/ipykernel_206/2429762792.py:5: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  crash_mx = crash_df.corr()\n\n\n\n\n\n\n  \n    \n      \n      total\n      speeding\n      alcohol\n      not_distracted\n      no_previous\n      ins_premium\n      ins_losses\n    \n  \n  \n    \n      total\n      1.000000\n      0.611548\n      0.852613\n      0.827560\n      0.956179\n      -0.199702\n      -0.036011\n    \n    \n      speeding\n      0.611548\n      1.000000\n      0.669719\n      0.588010\n      0.571976\n      -0.077675\n      -0.065928\n    \n    \n      alcohol\n      0.852613\n      0.669719\n      1.000000\n      0.732816\n      0.783520\n      -0.170612\n      -0.112547\n    \n    \n      not_distracted\n      0.827560\n      0.588010\n      0.732816\n      1.000000\n      0.747307\n      -0.174856\n      -0.075970\n    \n    \n      no_previous\n      0.956179\n      0.571976\n      0.783520\n      0.747307\n      1.000000\n      -0.156895\n      -0.006359\n    \n    \n      ins_premium\n      -0.199702\n      -0.077675\n      -0.170612\n      -0.174856\n      -0.156895\n      1.000000\n      0.623116\n    \n    \n      ins_losses\n      -0.036011\n      -0.065928\n      -0.112547\n      -0.075970\n      -0.006359\n      0.623116\n      1.000000\n    \n  \n\n\n\n\n<Figure size 800x600 with 0 Axes>\n\n\nNow that we have our data in the correct format we can go ahead and plot out heatmap:\n\n# create a heatmap, annotate and customize from colormaps\nsns.heatmap(crash_mx, \n            annot = True, \n            cmap = 'Blues')\n\n<AxesSubplot: >\n\n\n\n\n\nAnother way to prep our data for plotting is to create a pivot table. Let’s use another of seaborn’s built in datasets to illustrate:\n\nflights = sns.load_dataset(\"flights\")\nflights\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      passengers\n    \n  \n  \n    \n      0\n      1949\n      Jan\n      112\n    \n    \n      1\n      1949\n      Feb\n      118\n    \n    \n      2\n      1949\n      Mar\n      132\n    \n    \n      3\n      1949\n      Apr\n      129\n    \n    \n      4\n      1949\n      May\n      121\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      139\n      1960\n      Aug\n      606\n    \n    \n      140\n      1960\n      Sep\n      508\n    \n    \n      141\n      1960\n      Oct\n      461\n    \n    \n      142\n      1960\n      Nov\n      390\n    \n    \n      143\n      1960\n      Dec\n      432\n    \n  \n\n144 rows × 3 columns\n\n\n\nWe can create a matrix with an index of month, columns representing years and the number of passengers for each:\n\nflights = flights.pivot_table(index = 'month',\n                              columns = 'year',\n                              values = 'passengers')\nflights\n\n\n\n\n\n  \n    \n      year\n      1949\n      1950\n      1951\n      1952\n      1953\n      1954\n      1955\n      1956\n      1957\n      1958\n      1959\n      1960\n    \n    \n      month\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Jan\n      112\n      115\n      145\n      171\n      196\n      204\n      242\n      284\n      315\n      340\n      360\n      417\n    \n    \n      Feb\n      118\n      126\n      150\n      180\n      196\n      188\n      233\n      277\n      301\n      318\n      342\n      391\n    \n    \n      Mar\n      132\n      141\n      178\n      193\n      236\n      235\n      267\n      317\n      356\n      362\n      406\n      419\n    \n    \n      Apr\n      129\n      135\n      163\n      181\n      235\n      227\n      269\n      313\n      348\n      348\n      396\n      461\n    \n    \n      May\n      121\n      125\n      172\n      183\n      229\n      234\n      270\n      318\n      355\n      363\n      420\n      472\n    \n    \n      Jun\n      135\n      149\n      178\n      218\n      243\n      264\n      315\n      374\n      422\n      435\n      472\n      535\n    \n    \n      Jul\n      148\n      170\n      199\n      230\n      264\n      302\n      364\n      413\n      465\n      491\n      548\n      622\n    \n    \n      Aug\n      148\n      170\n      199\n      242\n      272\n      293\n      347\n      405\n      467\n      505\n      559\n      606\n    \n    \n      Sep\n      136\n      158\n      184\n      209\n      237\n      259\n      312\n      355\n      404\n      404\n      463\n      508\n    \n    \n      Oct\n      119\n      133\n      162\n      191\n      211\n      229\n      274\n      306\n      347\n      359\n      407\n      461\n    \n    \n      Nov\n      104\n      114\n      146\n      172\n      180\n      203\n      237\n      271\n      305\n      310\n      362\n      390\n    \n    \n      Dec\n      118\n      140\n      166\n      194\n      201\n      229\n      278\n      306\n      336\n      337\n      405\n      432\n    \n  \n\n\n\n\nNow that we have our data in the correct format we can go ahead and plot out heatmap:\n\n# You can separate data with lines\nsns.heatmap(flights,\n            cmap = 'Blues',\n            linecolor = 'white',\n            linewidth = 1)\n\n<AxesSubplot: xlabel='year', ylabel='month'>\n\n\n\n\n\nWe see that flights have increased over time and that most people travel in July and August.\n\n\n\nA Cluster map is a hierarchically clustered heatmap. The distance between points is calculated, the closest are joined, and this continues for the next closest (It compares columns / rows of the heatmap). For more information see the seaborn documentation.\nLet’s illustrate using another of seaborn’s built-in datasets:\n\niris = sns.load_dataset('iris')\niris\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      virginica\n    \n  \n\n150 rows × 5 columns\n\n\n\nIt would be useful to carve up our numerical data based on the different species in order to identify any trends.\n\n# remove the column species using .pop\nspecies = iris.pop('species')\n\nsns.clustermap(iris)\n\n<seaborn.matrix.ClusterGrid at 0x7f31bc001ba0>\n\n\n\n\n\nLet’s try this out on our flight data we looked at earlier:\n\n# standard_scale normalizes the data to focus on the clustering\nsns.clustermap(flights,\n               cmap = \"Blues\",\n               standard_scale = 1)\n\n<seaborn.matrix.ClusterGrid at 0x7f31c2681c00>\n\n\n\n\n\nWith our flights data we can see that years have been reoriented to place like data closer together. You can see clusters of data for July & August for the years 59 & 60.\n\n\n\nPair Grids allow us to create a grid of different plots with complete control over what is displayed. For more information see the seaborn documentation.\n\n# load in iris dataset\niris = sns.load_dataset('iris')\n\n# Create the empty grid system using the provided data\n# Colorize based on species\niris_g = sns.PairGrid(iris,\n                      hue = 'species')\n\n# Put a histogram on the diagonal \niris_g.map_diag(plt.hist)\n\n# And a scatter plot every place else \niris_g.map_offdiag(plt.scatter)\n\n<seaborn.axisgrid.PairGrid at 0x7f31bbc31de0>\n\n\n\n\n\nWe can further tailor by having different plots in the upper, lower and diagonal:\n\n# load in iris dataset\niris = sns.load_dataset('iris')\n\n# Create the empty grid system using the provided data\n# Colorize based on species\niris_g = sns.PairGrid(iris,\n                      hue = 'species')\n\n# Put a histogram on the diagonal \niris_g.map_diag(plt.hist)\n\n# Have different plots in upper, lower and diagonal\niris_g.map_upper(plt.scatter)\niris_g.map_lower(sns.kdeplot)\n\n<seaborn.axisgrid.PairGrid at 0x7f31b6d41960>\n\n\n\n\n\nWe can also define the x and y variables for our custom grids:\n\n# load in iris dataset\niris = sns.load_dataset('iris')\n\n# You can define define variables for x & y for a custom grid\niris_g = sns.PairGrid(iris,\n                      hue = \"species\",\n                      x_vars = [\"sepal_length\", \"sepal_width\"],\n                      y_vars = [\"petal_length\", \"petal_width\"])\n\niris_g.map(plt.scatter)\n\n# Add a legend last\niris_g.add_legend()\n\n<seaborn.axisgrid.PairGrid at 0x7f31b8eecdc0>\n\n\n\n\n\n\n\n\nA facet grid allows us to print multiple plots in a grid where we can define columns & rows. For further information see the seaborn documentation.\nLet’s return to our tips dataset to illustrate what can be done usin facet grids.\n\n# Get histogram for smokers and non with total bill for lunch & dinner\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        row = 'smoker')\n\n# You can pass in attributes for the histogram\ntips_fg.map(plt.hist,\n            'total_bill',\n            bins = 8)\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31bad559f0>\n\n\n\n\n\n\n# Get histogram for smokers and non with total bill for lunch & dinner\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        row = 'smoker')\n\n# Create a scatter plot with data on total bill & tip (You need 2 parameters)\ntips_fg.map(plt.scatter,\n            'total_bill',\n            'tip')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b61160b0>\n\n\n\n\n\n\n# We can assign variables to different colors and increase size of grid\n# Aspect is 1.3 x the size of height\n# You can change the order of the columns\n# Define the palette used\n\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        hue = 'smoker',\n                        height = 4,\n                        aspect = 1.3)\n                 \ntips_fg.map(plt.scatter,\n            \"total_bill\",\n            \"tip\")\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b80d1180>\n\n\n\n\n\nWe can change the column order:\n\n# We can assign variables to different colors and increase size of grid\n# Aspect is 1.3 x the size of height\n# Define the palette used\n\ntips_fg = sns.FacetGrid(tips_df,\n                        col = 'time',\n                        hue = 'smoker',\n                        height = 4,\n                        aspect = 1.3,\n\n# We can change the order of the columns and paletter\n                        col_order = ['Dinner', 'Lunch'], \n                        palette = 'Set1')\n              \n# We can change the edge colour of our dots to white      \ntips_fg.map(plt.scatter,\n            \"total_bill\",\n            \"tip\",\n            edgecolor = 'w')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b82530a0>\n\n\n\n\n\n\n# create a dictionary to define the size, linewidth and edge color of our markers\nkws = dict(s=50, \n           linewidth=.5, \n           edgecolor='w')\n\n# let's go to town on the customizations! note we can reference the dictionary we just created\ntips_fg = sns.FacetGrid(tips_df, \n                        col='sex', \n                        hue='smoker', \n                        height=4, \n                        aspect=1.3, \n                        hue_order=['Yes', 'No'], \n                        hue_kws=dict(marker=['^', 'v'])\n                       )\n\ntips_fg.map(plt.scatter,\n            'total_bill',\n            'tip',\n            **kws\n           )\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31bb0fbb20>\n\n\n\n\n\nHere hue has been used to denote smoker vs non-smoker - blue upward pointing markers represent smokers, and the orange downward facing markers represent non-smokers.\nLet’s look at a seaborn dataset that we haven’t seeen yet. This dataframe provides scores for different students based on the level of attention they could provide during testing:\n\n# load in attention dataset and take a look at first 13 rows\natt_df = sns.load_dataset('attention')\natt_df.head(13)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      subject\n      attention\n      solutions\n      score\n    \n  \n  \n    \n      0\n      0\n      1\n      divided\n      1\n      2.0\n    \n    \n      1\n      1\n      2\n      divided\n      1\n      3.0\n    \n    \n      2\n      2\n      3\n      divided\n      1\n      3.0\n    \n    \n      3\n      3\n      4\n      divided\n      1\n      5.0\n    \n    \n      4\n      4\n      5\n      divided\n      1\n      4.0\n    \n    \n      5\n      5\n      6\n      divided\n      1\n      5.0\n    \n    \n      6\n      6\n      7\n      divided\n      1\n      5.0\n    \n    \n      7\n      7\n      8\n      divided\n      1\n      5.0\n    \n    \n      8\n      8\n      9\n      divided\n      1\n      2.0\n    \n    \n      9\n      9\n      10\n      divided\n      1\n      6.0\n    \n    \n      10\n      10\n      11\n      focused\n      1\n      6.0\n    \n    \n      11\n      11\n      12\n      focused\n      1\n      8.0\n    \n    \n      12\n      12\n      13\n      focused\n      1\n      6.0\n    \n  \n\n\n\n\n\n# Put each student (subject) in their own plot with 5 per line and plot their scores\natt_fg = sns.FacetGrid(att_df, \n                       col = 'subject',\n                       col_wrap = 5,\n                       height = 1.5\n                      )\n\natt_fg.map(plt.plot,\n           'solutions',\n           'score',\n           marker ='.')\n\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n/home/stephen137/mambaforge/lib/python3.10/site-packages/seaborn/axisgrid.py:745: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  plot_args = [v for k, v in plot_data.iteritems()]\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b5546ce0>\n\n\n\n\n\n\n\n\n\nLet’s revisit the tips dataset used earlier. Let’s recall what our dataset looks like:\n\n# grab first 5 rows\ntips_df.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n  \n\n\n\n\nSay we wanted to explore the relationship between tip size and total bill size comparing men and women:\n\nplt.figure(figsize=(8,6))\n\nsns.set_context('paper',\n                font_scale = 1.4)\n\nsns.lmplot(x = 'total_bill',\n           y = 'tip',\n           hue = 'sex',\n           data = tips_df,\n           markers = [ 'o', '^'],\n           scatter_kws = { 's' : 100, 'linewidth': 0.5, 'edgecolor': 'w'})                            \n                                                                                 \n\n<seaborn.axisgrid.FacetGrid at 0x7f31b4cd5720>\n\n\n<Figure size 800x600 with 0 Axes>\n\n\n\n\n\nIt might be useful to see the equivalent broken down by day:\n\n# sns.lmplot(x='total_bill', y='tip', col='sex', row='time', data=tips_df)\ntips_df.head()\n\n# Makes the fonts more readable\nsns.set_context('poster',\n                font_scale=1.4)\n\nsns.lmplot(x='total_bill', \n           y='tip', data=tips_df,\n           col='day',\n           hue='sex',\n          height=8,\n           aspect=0.6)\n\n<seaborn.axisgrid.FacetGrid at 0x7f31b4d48fd0>\n\n\n\n\n\n\n\n\nThanks once again to Santiago for signposting this video posted by Derek Banas. This blog was written after interactively working through it.\n\n%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/6GUZXDef2U0\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
  },
  {
    "objectID": "posts/Biomedical Image Analysis in Python/Biomedical_Image_Analysis_in_Python.html",
    "href": "posts/Biomedical Image Analysis in Python/Biomedical_Image_Analysis_in_Python.html",
    "title": "Biomedical Image Analysis in Python",
    "section": "",
    "text": "Since the first x-ray in 1895, medical imaging technology has advanced clinical care and opened up new fields of scientific investigation. The amount of imaging data is exploding: there was estimated to be more than three and a half billion terabytes of it in the U.S. alone in 2020. This has created amazing opportunities for analysis: measuring organ shape and size; creating detailed reconstructions of anatomy; analyzing tissue composition; predicting pathology, and so much more. The complexity and variety of the data can make it an intimidating field to jump into.\nFortunately, there are fantastic tools and a wealth of resources to support you. In this blog, we’ll focus on several principles underlying biomedical image analysis. We’ll learn how to explore multidimensional arrays, emphasize important features using masks and filters, extract specific measurements from objects, and compare multiple images or patients. While this only scratches the surface of the field, it will give us a firm foundation for advanced concepts like classification and segmentation. Throughout, we’ll rely on packages in Python’s core scientific computing ecosystem: ImageIO, NumPy, SciPy, and matplotlib. Let’s dive in.\n\n\n\noverview.PNG\n\n\n\n\nPrepare to conquer the Nth dimension! In this section, we’ll learn how to load, build and navigate N-dimensional images using a CT image of the human chest. We’ll also leverage the useful ImageIO package and hone our NumPy and matplotlib skills.\n\n\n\n\nTo warm up, let’s load and plot a single image. We can read in images using the ImageIO package. Its imread() function will take a single file and load it as an image object. One useful feature of ImageIO is that it can read DICOM files, the standard format for human medical imaging. The data is read in as an image object, which is a type of NumPy array.\nTo access specific values from our image, we can slice out a single value or a range of index values along each dimension.\n\n\n\nloading_images.PNG\n\n\nIn this section, we’ll work with sections of a computed tomography (CT) scan from The Cancer Imaging Archive. CT uses a rotating X-ray tube to create a 3D image of the target area. The actual content of the image depends on the instrument used: photographs measure visible light, x-ray and CT measure radiation absorbance, and MRI scanners measure magnetic fields.\nTo warm up, use the imageio package to load a single DICOM image from the scan volume and check out a few of its attributes.\nLet’s grab the dataset we’ll be working with :\n\n!wget https://assets.datacamp.com/production/repositories/2085/datasets/f44726fefae841afd24ddf83c58f34722212e67a/tcia-chest-ct-sample.zip\n\n--2023-05-10 09:44:28--  https://assets.datacamp.com/production/repositories/2085/datasets/f44726fefae841afd24ddf83c58f34722212e67a/tcia-chest-ct-sample.zip\nResolving assets.datacamp.com (assets.datacamp.com)... 172.64.153.58, 104.18.34.198\nConnecting to assets.datacamp.com (assets.datacamp.com)|172.64.153.58|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1141220 (1.1M)\nSaving to: ‘tcia-chest-ct-sample.zip’\n\ntcia-chest-ct-sampl 100%[===================>]   1.09M  --.-KB/s    in 0.09s   \n\n2023-05-10 09:44:29 (11.5 MB/s) - ‘tcia-chest-ct-sample.zip’ saved [1141220/1141220]\n\n\n\nand unzip it :\n\n!unzip tcia-chest-ct-sample.zip -d Data\n\nArchive:  tcia-chest-ct-sample.zip\n  inflating: Data/chest-220.dcm      \n  inflating: Data/chest-221.dcm      \n  inflating: Data/chest-222.dcm      \n  inflating: Data/chest-224.dcm      \n  inflating: Data/chest-225.dcm      \n\n\n\n# install imageio\n!pip install imageio\n\n# Import ImageIO\nimport imageio.v2 as imageio\n\n# Load \"chest-220.dcm\"\nim = imageio.imread('Data/chest-220.dcm')\n\n# Print image attributes\nprint('Image type:', type(im))\nprint('Shape of image array:', im.shape)\n\nRequirement already satisfied: imageio in /home/stephen137/mambaforge/lib/python3.10/site-packages (2.22.0)\nRequirement already satisfied: pillow>=8.3.2 in /home/stephen137/mambaforge/lib/python3.10/site-packages (from imageio) (9.2.0)\nRequirement already satisfied: numpy in /home/stephen137/mambaforge/lib/python3.10/site-packages (from imageio) (1.22.4)\nImage type: <class 'imageio.core.util.Array'>\nShape of image array: (512, 512)\n\n\nimageio is a versatile package. It can read in a variety of image data, including JPEG, PNG, and TIFF. But it’s especially useful for its ability to handle DICOM files.\n\n\n\nImages are always acquired in a specific context. This information is often referred to as metadata. ImageIO loads available metadata into a dictionary, accessible through the meta attribute. This is especially important for DICOM files, which contain a lot of patient and acquisition information. Since meta is a Python dictionary, you can access specific information by indexing it with one of the available keys. This file, for example, includes a modality field. You can also call the keys method to list all of the available metadata.\n\n\n\nmetadata.PNG\n\n\nImageIO reads in data as Image objects. These are standard NumPy arrays with a dictionary of metadata.\nMetadata can be quite rich in medical images and can include:\n\nPatient demographics: name, age, sex, clinical information\nAcquisition information: image shape, sampling rates, data type, modality (such as X-Ray, CT or MRI)\n\n\n# Print the available metadata fields\nprint(im.meta.keys())\n\nodict_keys(['TransferSyntaxUID', 'SOPClassUID', 'SOPInstanceUID', 'StudyDate', 'SeriesDate', 'ContentDate', 'StudyTime', 'SeriesTime', 'ContentTime', 'Modality', 'Manufacturer', 'StudyDescription', 'SeriesDescription', 'PatientName', 'PatientID', 'PatientBirthDate', 'PatientSex', 'PatientWeight', 'StudyInstanceUID', 'SeriesInstanceUID', 'SeriesNumber', 'AcquisitionNumber', 'InstanceNumber', 'ImagePositionPatient', 'ImageOrientationPatient', 'SamplesPerPixel', 'Rows', 'Columns', 'PixelSpacing', 'BitsAllocated', 'BitsStored', 'HighBit', 'PixelRepresentation', 'RescaleIntercept', 'RescaleSlope', 'PixelData', 'shape', 'sampling'])\n\n\n\n# Print the available metadata\nprint(im.meta)\n\nDict([('TransferSyntaxUID', '1.2.840.10008.1.2'), ('SOPClassUID', '1.2.840.10008.5.1.4.1.1.2'), ('SOPInstanceUID', '1.3.6.1.4.1.14519.5.2.1.5168.1900.290866807370146801046392918286'), ('StudyDate', '20040529'), ('SeriesDate', '20040515'), ('ContentDate', '20040515'), ('StudyTime', '115208'), ('SeriesTime', '115254'), ('ContentTime', '115325'), ('Modality', 'CT'), ('Manufacturer', 'GE MEDICAL SYSTEMS'), ('StudyDescription', 'PET CT with registered MR'), ('SeriesDescription', 'CT IMAGES - RESEARCH'), ('PatientName', 'STS_007'), ('PatientID', 'STS_007'), ('PatientBirthDate', ''), ('PatientSex', 'F '), ('PatientWeight', 82.0), ('StudyInstanceUID', '1.3.6.1.4.1.14519.5.2.1.5168.1900.381397737790414481604846607090'), ('SeriesInstanceUID', '1.3.6.1.4.1.14519.5.2.1.5168.1900.315477836840324582280843038439'), ('SeriesNumber', 2), ('AcquisitionNumber', 1), ('InstanceNumber', 57), ('ImagePositionPatient', (-250.0, -250.0, -180.62)), ('ImageOrientationPatient', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0)), ('SamplesPerPixel', 1), ('Rows', 512), ('Columns', 512), ('PixelSpacing', (0.976562, 0.976562)), ('BitsAllocated', 16), ('BitsStored', 16), ('HighBit', 15), ('PixelRepresentation', 0), ('RescaleIntercept', -1024.0), ('RescaleSlope', 1.0), ('PixelData', b'Data converted to numpy array, raw data removed to preserve memory'), ('shape', (512, 512)), ('sampling', (0.976562, 0.976562))])\n\n\nDICOM files have rich information related to patient and acquisition information, but other image formats can have helpful information as well.\n\n\n\nVisualization is fundamental to image analysis, and we’ll rely heavily on matplotlib and its imshow() function throughout this course. To draw a simple plot of a 2D image, first, import the PyPlot module from matplotlib. Next, call pyplot.imshow() and pass in the 2D image. If the image does not have color, add cmap=\"gray\" to plot it in grayscale. Next, to reduce clutter, it’s common to turn off the axis ticks, labels, and frame with pyplot. axis off. Finally, call show() to render the image. And, voila: the inside of a human being.\n\n\n\nplotting_images.PNG\n\n\nPerhaps the most critical principle of image analysis is: look at our images!\nMatplotlib’s imshow() function gives us a simple way to do this. Knowing a few simple arguments will help:\n\ncmap controls the color mappings for each value. The “gray” colormap is common, but many others are available.\nvmin and vmax control the color contrast between values. Changing these can reduce the influence of extreme values.\nplt.axis('off') removes axis and tick labels from the image.\n\n\nimport matplotlib.pyplot as plt\n\n\n# Draw the image in grayscale\nplt.imshow(im, cmap=\"gray\")\n\n# Render the image\nplt.show()\n\n\n\n\nWe can set vmin=-200 and vmax=200 to increase the contrast (i.e., the distance between the brightest and darkest colors is smaller than before, and remove the axis ticks and labels.\n\n# Draw the image with greater contrast\nplt.imshow(im,cmap=\"gray\",vmin=-200, vmax=200)\n\n# Remove axis ticks and labels\nplt.axis('off')\n\n# Render the image\nplt.show()\n\n\n\n\n\n\n\n\nNow that we can load and plot two-dimensional images let’s discuss higher dimensional datasets.\nImages come in all shapes and sizes. This makes them versatile, but also complex. A standard grayscale image is the simplest type: it’s an array that can be indexed by rows and columns.\n\n\n\narray_slicing.PNG\n\n\n3D images cover a volume of space rather than just a single plane. This volumetric data is useful because it can better capture the complexities of the human body, but it can be difficult to visualize because it can’t be summarized in a single plot.\nColor images are also three dimensional. RGB images, for example, have three color channels that, when rendered by matplotlib or other image viewers, express a wide range of colors.\nMovies, or time series data, include a temporal dimension, showing how each element changes over time. Like the planar dimension for 3D volumes, the temporal dimension is put first by convention.\n\n\nJust as a 2D image is a stack of 1-dimensional vectors, 3D, 4D and even higher-dimensional images can be thought of as stacks of simpler ones. Let’s illustrate this by creating a 3D volume from a few 2D images. First, we’ll load ImageIO and NumPy. Then, we’ll read in three slices of a chest CT scan. Each of these slices is an array with 512-row elements by 512 column elements. Now, we can feed a list of these three images into NumPy’s stack() function to create a 3D volume. If we look at our new “vol” array, we see that it contains a third dimension with three elements along it, but the row and column dimensions are the same as before\n\n\n\narray_stacks.PNG\n\n\nWe will use NumPy’s stack() function to combine several 2D arrays into a 3D volume. By convention, volumetric data should be stacked along the first dimension (axis=0): vol [plane, row, col].\n\nimport numpy as np\n\n# Read in each 2D image\nim1 = imageio.imread('Data/chest-220.dcm')\nim2 = imageio.imread('Data/chest-221.dcm')\nim3 = imageio.imread('Data/chest-222.dcm')\n\n# Stack images into a volume\nvol = np.stack([im1,im2,im3], axis=0)\nprint('Volume dimensions:', vol.shape)\n\nVolume dimensions: (3, 512, 512)\n\n\nFor large volumes, we can use a for loop to quickly generate our image list.\n\n\n\nImageIO’s volread() function is capable of reading volumes directly from disk, whether your images are stored in their own folder, or if the dataset is already multi-dimensional. In this example, we have a folder named “chest data,” which contains 50 slices of a 3D volume. We simply have to pass the folder name to volread(), and it will assemble the volume for us. Since these are DICOM images, the function actually checks the available metadata to make sure that the images are placed in the correct order. Otherwise, it will default to alphabetical order. Displaying the shape attribute shows us that we have 50 images stacked together.\nFor illustrative purposes let’s read in an entire volume of brain data from the “Data” folder, which contains 5 DICOM images.\n\n# specify directory and file pattern\nfoldername = 'Data'\npattern = '*.dcm'\n\n# Load the \"Data\" directory\nvol = imageio.volread(foldername, pattern)\n\n# Print image attributes\nprint('Available metadata:', vol.meta.keys())\nprint('Shape of image array:', vol.shape)\n\nReading DICOM (examining files): 1/6 files (16.7%6/6 files (100.0%)\n  Found 1 correct series.\nReading DICOM (loading data): 5/5  (100.0%)\nAvailable metadata: odict_keys(['TransferSyntaxUID', 'SOPClassUID', 'SOPInstanceUID', 'StudyDate', 'SeriesDate', 'ContentDate', 'StudyTime', 'SeriesTime', 'ContentTime', 'Modality', 'Manufacturer', 'StudyDescription', 'SeriesDescription', 'PatientName', 'PatientID', 'PatientBirthDate', 'PatientSex', 'PatientWeight', 'StudyInstanceUID', 'SeriesInstanceUID', 'SeriesNumber', 'AcquisitionNumber', 'InstanceNumber', 'ImagePositionPatient', 'ImageOrientationPatient', 'SamplesPerPixel', 'Rows', 'Columns', 'PixelSpacing', 'BitsAllocated', 'BitsStored', 'HighBit', 'PixelRepresentation', 'RescaleIntercept', 'RescaleSlope', 'PixelData', 'shape', 'sampling'])\nShape of image array: (5, 512, 512)\n\n\nUsing volread() to load image volumes can be faster and more reliable than loading them in image-by-image. It also preserves image metadata where possible.\n\n\n\nWhen analyzing images, keep in mind that the data is only a representation of real, physical space. The information in our images is limited to the number of elements in it. This is known as the array shape in NumPy and is always available as an attribute. The amount of space covered by each element is the sampling rate, and it can vary along each dimension. For DICOM images, the sampling rate is usually encoded in the metadata. For other types of image formats, such as JPEG and PNG, you may need to find it elsewhere. The field of view is the total amount of space covered along each axis. It is the product of the shape and sampling rate. Understanding the difference between these concepts is important, and we’ll return to it throughout this blog.\nThe amount of physical space covered by an image is its field of view, which is calculated from two properties:\n\nArray shape, the number of data elements on each axis. Can be accessed with the shape attribute.\nSampling resolution, the amount of physical space covered by each pixel (mm). Sometimes available in metadata (e.g., meta[‘sampling’]).\n\nLet’s have a go at calculating the field of view for our volume.\n\nvol.shape\n\n(5, 512, 512)\n\n\n\nvol.meta['sampling']\n\n(3.269999999999996, 0.976562, 0.976562)\n\n\nMultiply the array shape and sampling resolution along each axis\n(5 x 3.269999999999996, 512 x 0.976562, 512 x 0.976562)\nwhich gives (16.35, 500, 500)\n\n\n\n\nEfficiently and comprehensively visualizing your data is key to successful image analysis.\nTo plot N-dimensional data slice it!\nConsider this loaf of bread - it’s a 3D volume that looks absolutely delicious. But what’s inside the loaf? Is it bananas? Blueberries? Walnuts? This single 2D image cannot answer the question. Instead, you would have to slice it up and look at those pieces individually. The concept for 3D images is exactly the same: to explore your multidimensional array you must visualize many simpler slices of it.\n\n\nIt’s inefficient to plot each of these slices one by one. A better way is to display many at once. To do this, we’ll leverage PyPlot’s subplots() function, which creates a grid of axis objects based on the number of rows and columns you specify. When you call pyplot.subplots(), it returns a figure object and an array of axis handles. In this case, the axes array has a shape of one by three. To draw images on each subplot, we will call the imshow() function directly from the axis object, rather than calling it through PyPlot. Here, we’re plotting the first slice of vol with a gray colormap. After the first slice is drawn, repeat the process for the other slices. Finally, we can clean up and render the figure. Just like before, we want to turn off the ticks and labels by calling the axis() method. However, since we will do the same thing to each subplot, a for loop is more efficient than writing down each command. If you had many images to draw, you could insert the drawing step into a for loop as well. Finally, we render the image using pyplot.show().\n\n\n\nsubplots.PNG\n\n\n\n# Initialize figure and axes grid\nfig, axes = plt.subplots(nrows=2, ncols=1)\n\n# Draw an image on each subplot\naxes[0].imshow(im1, cmap='gray')\naxes[1].imshow(im2, cmap='gray')\n\n# Remove ticks/labels and render\naxes[0].axis('off')\naxes[1].axis('off')\nplt.show()\n\n\n\n\nFor even more rapid visualization, we can use a large number of subplots and loop through our axes and images.\n\n\n\nThe simplest way to plot 3D and 4D images by slicing them into many 2D frames. Plotting many slices sequentially can create a “fly-through” effect that helps you understand the image as a whole.\nTo select a 2D frame, pick a frame for the first axis and select all data from the remaining two: vol[0, :, :]\nFor this illustration, let’s use a for loop to plot every 2nd slice of vol on a separate subplot.\n\n# Print image attributes\nprint('Available metadata:', vol.meta.keys())\nprint('Shape of image array:', vol.shape)\n\n# Plot the images on a subplots array \nfig, axes = plt.subplots(nrows=1, ncols=2)\n\n# Loop through subplots and draw image\nfor ii in range(2):\n    im = vol[ii*2, :, :]\n    axes[ii].imshow(im, cmap='gray')\n    axes[ii].axis('off')\n    \n# Render the figure\nplt.show()\n\nAvailable metadata: odict_keys(['TransferSyntaxUID', 'SOPClassUID', 'SOPInstanceUID', 'StudyDate', 'SeriesDate', 'ContentDate', 'StudyTime', 'SeriesTime', 'ContentTime', 'Modality', 'Manufacturer', 'StudyDescription', 'SeriesDescription', 'PatientName', 'PatientID', 'PatientBirthDate', 'PatientSex', 'PatientWeight', 'StudyInstanceUID', 'SeriesInstanceUID', 'SeriesNumber', 'AcquisitionNumber', 'InstanceNumber', 'ImagePositionPatient', 'ImageOrientationPatient', 'SamplesPerPixel', 'Rows', 'Columns', 'PixelSpacing', 'BitsAllocated', 'BitsStored', 'HighBit', 'PixelRepresentation', 'RescaleIntercept', 'RescaleSlope', 'PixelData', 'shape', 'sampling'])\nShape of image array: (5, 512, 512)\n\n\n\n\n\nWhen selecting frames, any trailing : symbols are implicitly selected. For example, vol[5] is the same as vol[5,:,:]. We will follow this simpler convention moving forward.\n\n\n\nThere are actually multiple ways you can slice a 3D volume into 2D images. The simplest way is to choose a frame along the first dimension and plot the second and third against each other.\nIf you instead selected a slice along the row dimension, you would get a second perspective. In this case, we are plotting head-to-toe versus left-to-right.\nFinally, you could plot the first and second axes against each other, yielding a third view. When looking at human anatomy, these different views or planes have special names: - axial - coronal - sagittal\nKnowing in which plane a dataset is stacked can help you navigate more efficiently.\nAny two dimensions of an array can form an image, and slicing along different axes can provide a useful perspective. However, unequal sampling rates can create distorted images.\n\n\n\ndistorted.PNG\n\n\nMany datasets do not have equal sampling rates across all dimensions. In these cases, you will want to stretch the pixels along one side to account for the differences. The amount of stretching needed is determined by the aspect ratio. Here we’ve decided to plot a slice with data from the first and second dimensions. To determine the aspect ratio, we first get the sampling rates along each dimension from the metadata dictionary. Then, we divide the sampling rate of the first dimension by the sampling rate of the second. When we call imshow(), we pass this ratio to the aspect argument.\nThis results in a properly proportioned image. Failing to adjust the aspect would have resulted in a distorted image.\n\n\n\naspect_ratio.PNG\n\n\nTo illustrate this, let’s plot images that slice along the second and third dimensions of vol, and explicitly set the aspect ratio to generate undistorted images.\n\nvol.shape\n\n(5, 512, 512)\n\n\n\n# Select frame from \"vol\"\nim1 = vol[:, 256, :]\nim2 = vol[:, :, 256]\n\n# Compute aspect ratios\nd0, d1, d2 = vol.meta['sampling']\nasp1 = d0 / d2\nasp2 = d0 / d1\n\n# Plot the images on a subplots array \nfig, axes = plt.subplots(nrows=2, ncols=1)\naxes[0].imshow(im1, cmap='gray', aspect=asp1)\naxes[1].imshow(im2, cmap='gray', aspect=asp2)\nplt.show()\n\n\n\n\nNext, we’ll begin to manipulate images and extract salient features from them.\n\n\n\n\n\nIn this section, we’ll discuss masks and filters, two techniques that emphasize important features in images. To leverage them well, we must have a thorough understanding of your data’s distribution of intensity values.\n\n\nIn this section, we will work with a hand radiograph from a 2017 Radiological Society of North America competition. X-ray absorption is highest in dense tissue such as bone, so the resulting intensities should be high. Consequently, images like this can be used to predict “bone age” in children.\nPixels and voxels\nThe building blocks of medical images are pixels and voxels. Each of these elements has two properties: an intensity value and a location in space. The meaning of the intensity value depends on the imaging modality. For example, pixels in this x-ray image, or radiograph, are brighter in dense tissue such as bone, because it absorbs more radiation than other types.\nData types and image size\nThe range of values allowed in an image is determined by its data type. Generally, lower-bit integers are preferred for images, since memory usage increases dramatically for larger data types. If all values in the image are positive, then unsigned integers can cover the widest range of values while taking up the least amount of memory. You will commonly see images scaled by the value 255, which is the maximum value for 8-bit unsigned integers. You can see the difference in memory usage by calling the size attribute of the array. The foot x-ray we just saw is read by ImageIO as an 8-bit unsigned integer. It takes up about 153 kB. If we convert it to a 64-bit integer, however, the same information now takes up more than a megabyte of space.\nTo start, let’s load the image and check its intensity range.\nThe image datatype determines the range of possible intensities: e.g., 8-bit unsigned integers (uint8) can take values in the range of 0 to 255. A colorbar can be helpful for connecting these values to the visualized image.\n\n# custom function\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    for ax in fig.axes:\n        ax.axis('off')    \n    plt.tight_layout()\n    plt.show()\n\n\n# Load the hand radiograph\nim = imageio.imread('Data/hand.png')\nprint('Data type:', im.dtype)\nprint('Min. value:', im.min())\nprint('Max value:', im.max())\n\n# Set the tick interval\nimport matplotlib.ticker as ticker\nmin_val = 0\nmax_val = np.max(im)\ntick_interval = 50\ntick_vals = np.arange(min_val, max_val + tick_interval, tick_interval)\n\n# Plot the grayscale image\nplt.imshow(im, vmin=0, vmax = 255, cmap='gray')\ncolorbar = plt.colorbar()\ncolorbar.set_ticks(tick_vals)\n\nplt.show()\n\nData type: uint8\nMin. value: 3\nMax value: 224\n\n\n\n\n\nAlthough only a crude descriptor, the range of intensities can help you get a quick feel for your image’s content.\n\n\n\nHistograms summarize the distribution of intensity values in an image. They bin each pixel by its value and then count each bin. SciPy, and especially its Ndimage module, contain some essential tools for image analysis. We’ll dig deeper into SciPy throughout this blog. To generate a histogram for the foot x-ray, we first import SciPy’s Ndimage module as ndi. Then, we call ndimage.histogram() and pass in our array. The histogram() function requires us to specify values for the minimum, maximum, and the number of bins. Since our image is an 8-bit unsigned integer, our range is from 0 to 255, with 256 possible values. This returns a 256 element vector with the count of pixels at each intensity value. Plotting the data as a line plot reveals a highly skewed distribution, with many low values and a wider range of high values.\n\n\n\nhistograms.PNG\n\n\nEqualization\nSkewed distributions are common in medical images: background intensities are usually low and take up a lot of image space. There are ways to modify the intensity distribution. For example, histogram equalization redistributes values based on their abundance in the image. We can perform equalization with the aid of the cumulative distribution function, which shows the proportion of pixels that fall within a given range. Here, we can see that about half the pixels have values less than 32. To equalize the image, we could redistribute these intensity values until they are more evenly represented.\nEqualizing the histogram is actually pretty straightforward. First, we generate the histogram. Then, we calculate the cumulative distribution function by taking the rolling sum of the histogram and dividing it by the total number of pixels. Then, we apply the function to our image and rescale by 255. Plotting the original and equalized image shows that we have increased the pixel intensities for several areas. This has made our foot stand out more clearly, but it has also given extra weight to some background areas. For biomedical applications, global equalization, such as this, should be done with caution, but the principle of redistributing intensity values is a useful one to keep in mind.\n\n\n\nequalization.PNG\n\n\n\n\n\nequalization_result.PNG\n\n\nThe area under a histogram is called the cumulative distribution function. It measures the frequency with which a given range of pixel intensities occurs.\nTo illustrate, let’s describe the intensity distribution in im by calculating the histogram and cumulative distribution function and displaying them together.\n\n# custom functiob\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    for ax in fig.axes:\n        ax.legend(loc='center right')\n    plt.show()\n\n# Import SciPy's \"ndimage\" module\nimport scipy.ndimage as ndi \n\n# Create a histogram, binned at each possible value\nhist = ndi.histogram(im, min=0, max=255, bins=256)\n\n# Create a cumulative distribution function\ncdf = hist.cumsum() / hist.sum()\n\n# Plot the histogram and CDF\nfig, axes = plt.subplots(2, 1, sharex=True)\naxes[0].plot(hist, label='Histogram')\naxes[1].plot(cdf, label='CDF')\nformat_and_render_plot()\n\n\n\n\nWe can see the data is clumped into a few separate distributions, consisting of background noise, skin, bone, and artifacts. Sometimes we can separate these well with global thresholds (foreground/background); other times the distributions overlap quite a bit (skin/bone).\n\n\n\nWe can restrict our analysis to only the most important parts of an image by creating and applying image masks.\nA mask is a Boolean array which serves as a screen to remove undesirable pixels. Masks must retain the same dimensions as the original image so that the two can be overlaid.\nCreating masks One way to create masks is to find all pixels in an image that satisfy a certain condition. For example, let’s create a three by three array of ascending values. If we test for values greater than 5, we will return a three by three array where the values are True when they greater than 5, and False when they are less. Logical operations include comparisons and tests of equivalence. We can also chain operations together to select a specific range of pixels.\nLet’s look at this in action. Recall that the foot x-ray we have been working with has an intensity distribution like this. We see that there is a steep drop-off around 32, so let’s select values greater than this. This seems to do quite a good job highlighting the foot.\nBone is the highest intensity tissue in an x-ray, and if we increase our threshold to 64, we create a rough bone mask. Finally, we can create a mask of non-bone tissue by finding pixels that are in mask 1 and not in mask 2. The selected pixels are in the foot but are not part of the bone. They seem to be related to skin and other tissue.\nApplying masks Masks can be used to screen images, allowing the original values through except where the mask evaluates to False. NumPy’s where() function is useful for this purpose. where() applies a condition on each pixel, and instead of returning a Boolean, it returns x when True and y when False. Each of the arguments can be either arrays or single values, allowing for great flexibility. To see this in action, first import NumPy. Let’s try to filter out pixels that are not part of the bone. We’ll call “where im is greater than 64, return im, otherwise return 0”. Plotting the masked image shows that only the high-intensity values remain, and these are mostly bone.\nTuning masks Data is noisy, so your masks will rarely be perfect. Fortunately, there are simple ways to improve them. To increase the size of your mask, you can add pixels around the edges, a process known as dilation. This can help when the edges are fuzzy or to make sure you don’t accidentally mask out pixels you actually care about. To do this, we call the binary_dilation() function, which converts all background pixels adjacent to the mask into mask pixels.\nThe opposite operation, “binary erosion” can be implemented in the same manner. Use it to cut the mask down to its more central pixels. You can perform these tuning operations many iterations to make your mask much larger or smaller. You can also combine the operations to open or close holes in your mask.\nMasks are the primary method for removing or selecting specific parts of an image. They are binary arrays that indicate whether a value should be included in an analysis. Typically, masks are created by applying one or more logical operations to an image.\nTo illustrate, we will try to use a simple intensity threshold to differentiate between skin and bone in the hand radiograph.\n\n# using loadtxt()\nim_equalized = np.loadtxt(\"Data/hand_equalized.csv\",\n                 delimiter=\",\", dtype=float)\ndisplay(im_equalized)\n\narray([[18., 18., 18., ..., 32., 32., 32.],\n       [18., 18., 18., ..., 32., 32., 32.],\n       [18., 18., 18., ..., 32., 32., 32.],\n       ...,\n       [17., 17., 17., ..., 21., 20., 19.],\n       [16., 16., 16., ..., 21., 20., 19.],\n       [16., 16., 16., ..., 21., 20., 19.]])\n\n\n\n\n\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    for ax in fig.axes:\n        ax.axis('off')    \n    plt.tight_layout()\n    plt.show()\n\n\n# Create skin and bone masks\nmask_skin = (im_equalized >= 45) & (im_equalized < 145)\nmask_bone = im_equalized >=145\n\n# Plot the skin (0) and bone (1) masks\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(mask_skin, cmap='gray')\naxes[1].imshow(mask_bone, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nIt’s possible to chain logic together to make some quite complex masks.\n\n\n\nAlthough masks are binary, they can be applied to images to filter out pixels where the mask is False.\nNumPy’s where() function is a flexible way of applying masks. It takes three arguments:\nnp.where(condition, x, y)\ncondition, x and y can be either arrays or single values. This allows you to pass through original image values while setting masked values to 0.\nLet’s practice applying masks by selecting the bone-like pixels from the hand x-ray.\n\n# custom function\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    fig.axes[0].axis('off')   \n    plt.tight_layout()\n    plt.show()\n\n# Screen out non-bone pixels from \"im\"\nmask_bone = im_equalized >= 145\nim_bone = np.where(mask_bone, im_equalized, 0)\n\n# Get the histogram of bone intensities\nhist = ndi.histogram(im_bone, min=1, max=255, bins=255)\n\n# Plot masked image and histogram\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im_bone, cmap='gray')\naxes[1].plot(hist)\nformat_and_render_plot()\n\n\n\n\nSometimes simpler methods for applying a mask such as multiplication (e.g., im * mask_bone) will meet your needs, but np.where() is an excellent tool to have in your arsenal.\n\n\n\nImperfect masks can be tuned through the addition and subtraction of pixels. SciPy includes several useful methods for accomplishing these ends. These include:\n\nbinary_dilation: Add pixels along edges\nbinary_erosion: Remove pixels along edges\nbinary_opening: Erode then dilate, “opening” areas near edges\nbinary_closing: Dilate then erode, “filling in” holes\n\nTo illustrate, we will create a bone mask then tune it to include additional pixels.\n\n# Create and tune bone mask\nmask_bone = im_equalized >= 145\nmask_dilate = ndi.binary_dilation(mask_bone, iterations=5)\nmask_closed = ndi.binary_closing(mask_bone, iterations=5)\n\n# Plot masked images\nfig, axes = plt.subplots(1,3)\naxes[0].imshow(mask_bone, cmap='gray')\naxes[1].imshow(mask_dilate, cmap='gray')\naxes[2].imshow(mask_closed, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nDilation, erosion, and closing are useful techniques when you want to fine-tune your masks.\n\n\n\n\nSo far, we have only considered the images as a whole. However, we can combine intensity and spatial information by employing convolutional filters.\n\n\n\nfilters.PNG\n\n\nFilters\nTwo common examples of filtering are sharpening and smoothing.\n\nSharpening sharp changes are enhanced, exaggerating the differences between pixels.\nSmoothing emphasizes large intensity patterns in an image by reducing variability between neighboring pixels. Essentially, it suppresses noise by blurring the image.\n\nLet’s look more closely at how this is accomplished.\n\n\nConvolution with a sharpening filter\n\n\n\nsharpening.PNG\n\n\nHere we have a five by five input array, where all the values are one except for the center value of 2. To get a sharpened value for the center pixel, we would first define a set of filter weights, also called a kernel. Then, we would select a window of input data of the same size as our kernel. In this case, the filter will highly weight the center pixel and down-weight the adjacent pixels.\n\n\n\nsharpening_result.PNG\n\n\nTo perform convolution, we multiply these two matrices element-wise, and then we sum them up. In the top-left corner, we have an input value of 1 times a weight of 0, an input value of 1 times a weight of negative 1, and so on. We then sum all these products to get a new, sharpened value for the center pixel of our input image. In this case, it’s been increased from two to six.\nLet’s see how this works on a full-size image.\n\n\n\nCNN.PNG\n\n\nOn the left is a random 2D array; on the right, we have an empty output image that we will create. In between is the sharpening kernel.\nStarting from the top-left corner of the input image, we select the values surrounding the origin pixel. We then multiply each element in this selection by the kernel weights and add them together to get the filtered value. We take one step over to the next element, multiply the input window by the kernel, and repeat the process for each pixel in the image.\n\n\n\nkernel.PNG\n\n\nThis results, in this case, in a sharpened image.\n\n\n\nsharpened.PNG\n\n\nImage convolution\nWe can apply custom filters using the convolve() function. First, we import packages and load the foot x-ray. Next, we create the kernel. In this case, let’s average the center pixel with its neighbors to smooth out variability between pixels. After filtering, the major patterns will remain, but subtle variation between pixels will be dampened. Next, we call ndimage.convolve() and pass in the image and weights. This produces a smoothed output image of the same size as our input.\n\n\n\nimage_convolution.PNG\n\n\nTo illustrate filter convolutions, let’s smooth the hand radiograph. First, specify the weights to be used. (These are called “footprints” and “kernels” as well.) Then, convolve the filter with im and plot the result.\n\nimport imageio.v2 as imageio\nimport scipy.ndimage as ndi \nimport matplotlib.pyplot as plt\n\nim = imageio.imread('Data/hand.png')\n\n\n# Create a three by three array of filter weights\n# Set each element to 0.11 to perform mean filtering (also called \"uniform filtering\")\nweights = [[0.11, 0.11, 0.11],\n           [0.11, 0.11, 0.11], \n           [0.11, 0.11, 0.11]]\n\n# Convolve the image with the filter\nim_filt = ndi.convolve(im, weights)\n\n# Plot the images\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(im, cmap='gray')\naxes[1].imshow(im_filt, cmap='gray')\nplt.show()\n\n\n\n\nThe size and pattern of the filter weights control the effect it will have on our image.\n\n\n\nFiltering can also employ functions other than convolutional kernels, such as the mean, median, and maximum. SciPy has several of these functions built-in. Filter kernels do not have to be 3 x 3; they can be as large as you want. Here, we apply a 10 x 10 median filter to the foot image. You can see it does quite a nice job of smoothing out the variations in intensity.\n\n\n\nfiltering_functions.PNG\n\n\nThe median filter will return the median value of the 3x3 neighborhood. Note that the values on the edges will vary based on the mode setting of your filter.\n\n\n\nGaussian filtering\nThe Gaussian filter is useful for smoothing data across larger areas. It blurs activation based on a Gaussian, or normal, distribution around the filtered pixel. Basically, the filter weights dissipate in a circular pattern as you get further from the center. The width of the distribution is controlled by the sigma parameter. Applying a Gaussian filter can be a great way to reduce noise, but with very large sigma values, you’ll lose a lot of detail in your image.\n\n\n\ngaussian.PNG\n\n\nLet’s illustrate the effects of applying Gaussian filters to the foot x-ray before creating a bone mask.\n\n# Smooth \"im\" with Gaussian filters\nim_s1 = ndi.gaussian_filter(im_equalized, sigma=1)\nim_s3 = ndi.gaussian_filter(im_equalized, sigma=3)\n\n# Draw bone masks of each image\nfig, axes = plt.subplots(1,3)\naxes[0].imshow(im_equalized >= 145, cmap='gray')\naxes[1].imshow(im_s1 >= 145, cmap='gray')\naxes[2].imshow(im_s3 >= 145, cmap='gray')\n\nplt.show()\n\n\n\n\nMany analyses can benefit from an initial smoothing of the data.\n\n\n\n\nFilters aren’t just useful for blurring and smoothing. They can also be used as detectors for features of interest, such as edges.\n\n\n\nedge_detector_kernel.PNG\n\n\nIf we want to construct a filter kernel that will emphasize edges, what should it look like?\nRecall that when we perform convolution, it creates a new image that reflects what the filter looks like: a smoothing filter itself has a smooth gradient, whereas a sharpening filter has a sudden change in intensity. An edge is a change in intensity along an axis. Sharp edges, for example, between the skull and background in this MRI image, have very high contrast. The filter should reflect this.\n\n\nLet’s see if we can make this work. We start by loading the foot x-ray. Next, we construct our kernel: to look for areas that have a change in intensity from top to bottom. We can weight the top row to positive 1 and the bottom row to negative 1.\nEssentially, this filter calculates the difference between the top and bottom rows, returning values far from 0 when there is a sudden change in intensity. Then, we convolve the image with the filter using SciPy.\n\n\n\nedge_detector_convolution.PNG\n\n\nPlotting the image, it’s clear that our detector has done a fine job of highlighting some edges. But note two things:\n\nThis is a horizontal edge detector because it is looking for differences between the top and bottom values at each point. If you look at the vertical edges in the filtered image, you’ll see that they have relatively low values compared to the top and bottom of the foot.\nThere are both positive and negative values. This happens because some edges have high-intensity values on top and low values on bottom, whereas others have the opposite. The direction of this difference determines whether the convolution yields a positive or negative value.\n\nTo illustrate, let’s create a vertical edge detector and see how well it performs on the hand x-ray (im).\n\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    for ax in fig.axes:\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n# Set weights to detect vertical edges\nweights = [[1,0,-1], [1,0,-1], [1,0,-1]]\n\n# Convolve \"im\" with filter weights\nedges = ndi.convolve(im_equalized, weights)\n\n# Draw the image in color\nplt.imshow(edges, cmap='seismic', vmin=-150, vmax=150)\nplt.colorbar()\nformat_and_render_plot()\n\n\n\n\n\n\n\nThere are many possible configurations for edge detectors. A very common one is the Sobel operator, which provides an extra weight to the center pixels of the kernel. The filter can be rotated to make it sensitive to either horizontal or vertical edges.\n\n\n\nsobel_filters.PNG\n\n\nImplementing the Sobel filter is just like implementing other filters: call ndimage.sobel(), then pass in the image and the orientation of the filter.\n\n\n\nsobel_filters_ndi.PNG\n\n\nTo remedy the fact that we have multiple edge maps with positive and negative values, we can create a composite edge map. Recall the Pythagorean Theorem - when you have two perpendicular vectors, you can calculate their distance by taking the root of their squares.\n\n\n\nsobel_filter_magnitude.PNG\n\n\nThis is useful in our situation: if we apply the Sobel filter along the first and second axes, we can then use these as input to the Pythagorean Theorem to get a composite, positively weighted edge image. This nicely highlights intensity changes in our image, and we can use these features for masking or object detection in later analysis steps.\nLet’s improve upon our previous detection effort by merging the results of two Sobel-filtered images into a composite edge map.\n\nhand_sobel = np.loadtxt('Data/hand_sobel.csv', delimiter = ',' , dtype=float)\n\n\n# Apply Sobel filter along both axes\nsobel_ax0 = ndi.sobel(hand_sobel, axis=0)\nsobel_ax1 = ndi.sobel(hand_sobel, axis=1)\n\n# Calculate edge magnitude \nedges = np.sqrt(np.square(sobel_ax0) + np.square(sobel_ax1))\n\n# Plot edge magnitude\nplt.imshow(edges, vmax=75, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nExcellent. In this section we learned how to modify and extract parts of images based on their location and intensity. We are now ready to begin analyzing individual images!\n\n\n\n\n\nIn this chapter, we’ll get to the heart of image analysis: object measurement. Using a 4D cardiac time series, we’ll determine if a patient is likely to have heart disease. Along the way, we’ll learn the fundamentals of image segmentation, object labeling, and morphological measurement.\n\n\nIn this section, we’ll discuss how we can measure one or more component parts of our image. We’ll start by learning how to label objects.\nSegmentation is the process of splitting an image into separate objects. Since whole careers can be spent developing segmentation techniques, we will focus mostly on how to analyze the resulting objects.\n\n\n\nsegmentation.PNG\n\n\n\n\nSunnybrook Cardiac Database\nWe will analyze cardiac magnetic resonance imaging data from the Sunnybrook Cardiac Database. Each Sunnybrook dataset contains a 3D time series of a person’s heart over the course of a single heartbeat. The end goal is to measure the proportion of blood that’s pumped out of the left ventricle, a measure known as the ejection fraction.\nIn this image, the left ventricle is the circular cavity in the center. Abnormal ejection fractions can indicate urgent health issues.\n\n\n\nejection_fraction.PNG\n\n\nOur first step towards calculating the ejection fraction is to segment the left ventricle from the rest of the image. For these MRI data, fluid-filled areas have high-intensity values. So, one approach is to take the original image, filter it to reduce noise and smooth edges, then mask it to select pixels with relatively high values.\n\n\n\nfilter_and_mask.PNG\n\n\nThis does a good job of segmenting the left ventricle, but now we need to remove the pixels that are part of other objects.\nWe can do this using SciPy’s label() function. First, we’ll create the above mask by reading in the file, applying a small Gaussian filter, then masking pixels with intensities lower than 150. Next, we “label” the mask. The labeling algorithm treats 0 values as background pixels, and then it looks for all of the objects that are separated by background. It then returns an array where each object has been indexed, as well as the number of objects detected. It seems we have 14 distinct objects in this image. Plotting the labels with the rainbow colormap shows that the circular left ventricle region in the center has been assigned a unique label value.\n\n\n\nscipy_label.PNG\n\n\nImage data\nLet’s grab some MR imaging data from the Sunnybrook Cardiac Database.\n\n!wget https://assets.datacamp.com/production/repositories/2085/datasets/fabaa1f1675549d624eb8f5d1bc94e0b11e30a8e/sunnybrook-cardiac-mr.zip\n\n\n!unzip sunnybrook-cardiac-mr.zip -d Data\n\n\nimport imageio.v2 as imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.ndimage as ndi \n\nAnd, let’s focus on one particular image :\n\n# load array from csv\nheartbeat= np.loadtxt('Data/heartbeat.csv', delimiter = ',' , dtype=float)\n\n\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    plt.tight_layout()\n    plt.show()\n\n# Set the tick interval\nimport matplotlib.ticker as ticker\nmin_val = 0\nmax_val = 160\ntick_interval = 20\ntick_vals = np.arange(min_val, max_val + tick_interval, tick_interval)\n\n# Plot the grayscale image\nplt.imshow(heartbeat, vmin=min_val, vmax = max_val, cmap='gray')\ncolorbar = plt.colorbar()\ncolorbar.set_ticks(tick_vals)\nformat_and_render_plot()\n\n\n\n\nThe full image is a 3D time series spanning a single heartbeat. These data are used by radiologists to measure the ejection fraction: the proportion of blood ejected from the left ventricle during each stroke.\nIllustration\nTo begin, segment the left ventricle from a single slice of the volume (heartbeat). First, we’ll filter and mask the image; then we’ll label each object with ndi.label().\nApply a median filter to im. Set the size to 3.\n\n# Smooth intensity values\nim_filt = ndi.median_filter(heartbeat, size=3)\n\nCreate a mask of values greater than 60, then use ndi.binary_closing() to fill small holes in it.\n\n# Select high-intensity pixels\nmask_start = np.where(im_filt>60, 1, 0)\nmask = ndi.binary_closing(mask_start)\n\nExtract a labeled array and the number of labels using ndi.label().\n\n# Label the objects in \"mask\"\nlabels, nlabels = ndi.label(mask)\nprint('Num. Labels:', nlabels)\n\nNum. Labels: 26\n\n\nPlot the labels array on top of the original image. To create an overlay, use np.where to convert values of 0 to np.nan. Then, plot the overlay with the rainbow colormap and set alpha=0.75 to make it transparent.\n\n# Create a `labels` overlay\noverlay = np.where(labels!=0, labels, np.nan)\n\n# Use imshow to plot the overlay\nplt.imshow(overlay, cmap='rainbow', alpha=0.75)\nformat_and_render_plot()\n\n\n\n\nRobust image segmentation is an entire research domain, but the simple principle is to leverage intensity and location information to differentiate objects of interest from the background. Once labeled, the objects can be manipulated easily.\n\n\n\nWe can now select individual objects by referencing their index value. To select pixels in the first object, you would use “where labels is 1, return the value from im, else return 0”. Alternatively, you can select a number of labels meeting a condition. Calling “where labels is less than 3, return im, else 0” will select pixels from the first and second objects.\n\n\n\nlabel_selection.PNG\n\n\nLabels are like object “handles” - they give you a way to pick up whole sets of pixels at a time. To select a particular object:\n\nFind the label value associated with the object.\nCreate a mask of matching pixels.\n\nIllustration\nFor this exercise, create a labeled array from the provided mask. Then, find the label value for the centrally-located left ventricle, and create a mask for it.\n\nmask_provided = np.loadtxt('Data/mask_provided.csv', delimiter = ',' , dtype=float)\n\nUse ndi.label() to assign labels to each separate object in mask.\n\n# Label the image \"mask\"\nlabels, nlabels =ndi.label(mask_provided)\n\nFind the index value for the left ventricle label by checking the center pixel (128, 128)\n\n# Select left ventricle pixels\nlv_val = labels[128, 128]\nlv_val\n\n5\n\n\nCreate a mask of pixels matching the left ventricle label. Using np.where, set pixels labeled as lv_val to 1 and other values to np.nan.\n\nlv_mask = np.where(labels==lv_val,1,np.nan )\n\nUse plt.imshow() to overlay the selected label on the current plot.\n\n# Overlay selected label\nplt.imshow(lv_mask, cmap='rainbow')\nplt.show()\n\n\n\n\nWhen running ndi.label(), the image is traversed from top-left to bottom right, so the generated label value could change depending on how many objects are detected. You may need to plot your labeled image to get the appropriate region.\n\n\n\nA bounding boxis the range of indices along each axis which completely enclose an object. You can use the bounding box to extract objects from the larger image. The find_objects() function can create these bounding boxes for you.\n\n\n\nbounding_box.PNG\n\n\nWhen you run find_objects() on a labeled array, it will return a list of bounding boxes. Each item in the returned list is a tuple of index ranges - one slice for each dimension. Indexing the original image using one of these boxes will yield an image cropped to that object. You could then analyze each of these arrays individually.\n\n\n\nndi_find_objects.PNG\n\n\nExtracting objects from the original image eliminates unrelated pixels and provides new images that can be analyzed independently.\nThe key is to crop images so that they only include the object of interest. The range of pixel indices that encompass the object is the bounding box.\nIllustration\nTo illustrate, we will use ndi.find_objects() to create a new image containing only the left ventricle.\nCreate the labels array from mask, then create a mask left ventricle pixels. (Use the coordinates 128, 128 to find the left ventricle label value.)\n\n# Create left ventricle mask\nlabels, nlabels = ndi.label(mask_provided)\nlv_val = labels[128,128]\nlv_mask = np.where(labels==lv_val, 1, 0)\n\nFind the bounding box indices for lv_mask Print the number of objects found and the values for the first box.\n\n# Find bounding box of left ventricle\nbboxes =ndi.find_objects(lv_mask)\nprint('Number of objects:', len(bboxes))\nprint('Indices for first box:', bboxes[0])\n\nNumber of objects: 1\nIndices for first box: (slice(107, 149, None), slice(116, 162, None))\n\n\nSelect the portion of im that is within the left ventricle bounding box, and plot the cropped image.\n\n# Crop to the left ventricle (index 0)\nim_lv = im[bboxes[0]]\n\n# Plot the cropped image\nplt.imshow(im_lv, cmap='gray')\nplt.axis('off')\nformat_and_render_plot()\n\n\n\n\ndi.find_objects() becomes extremely useful when dealing with 3-dimensional objects that are harder to view at a glance.\n\n\n\n\nOnce objects have been segmented from the background, their properties can be efficiently measured using tools within SciPy.\nFor this section, we’ll step up to measuring a full three-dimensional volume.\n\n\n\ncardiac_labels.PNG\n\n\nWe have segmented this volume into two parts:\n\nlabel 1 is the left ventricle, shown here in purple.\nlabel 2 is a circular section from the middle of the image, which will be useful for comparison.\n\n\n\nSciPy has optimized many of the most common descriptive functions for image data, including the mean, median, and standard deviation.\n\n\n\nfunctions.PNG\n\n\nThese functions summarize the array across all dimensions of an image, whether it be 2D, 3D, 4D, or more. They are especially useful when you have a labeled image because you can apply the function to every object independently with a single call. For custom calculations, you can also use the labeled_comprehension() function to summarize your data.\nWhich arguments you specify when you call measurement functions determines the pixels used for the calculation.\n\nHere, we have loaded the MRI volume and its corresponding labels. To get the mean intensity of the entire image, simply call ndimage.mean() with the original volume. If you provide a mask or a labeled array, you will restrict the analysis to all non-zero pixels. However, if you provide a set of labels and an index value, you can get the mean intensity for a single label. On the other hand, if you pass a list of values to the index argument, the function will return a list of mean values - one for each object specified.\nMeasure variance\nSciPy measurement functions allow you to tailor measurements to specific sets of pixels:\n\nSpecifying labels restricts the mask to non-zero pixels.\nSpecifying index value(s) returns a measure for each label value.\n\nIllustration\nFor this exercise, we will calculate the intensity variance of vol with respect to different pixel sets. We have provided the 3D segmented image as labels: label 1 is the left ventricle and label 2 is a circular sample of tissue.\n\nvol = np.asarray(\n\n\n# Variance for all pixels\nvar_all = ndi.variance(vol, labels=None, index=None)\nprint('All pixels:', var_all)\n\n# Variance for labeled pixels\nvar_labels = ndi.variance(vol, labels)\nprint('Labeled pixels:',var_labels)\n\n# Variance for each object\nvar_objects = ndi.variance(vol, labels, index=[1,2])\nprint('Left ventricle:', var_objects[0])\nprint('Other tissue:', var_objects[1])\n\n<script.py> output: All pixels: 840.4457526156154 Labeled pixels: 2166.5887761076724 Left ventricle: 1123.4641972021984 Other tissue: 1972.7151849347783\nIntensity values are based on tissue properties and the imaging modality. Areas that contain many types of tissue will have high variance because there are many different types sampled. Most of the background values have the same value. Even though we don’t care about these values, they make the “global variance” much lower than those encompassing in tissue.\n\n\n\nThis technique can be applied to some other SciPy tools, including the histogram function. In the previous chapter, we simply passed in our image array and then specified the minimum value, maximum value, and the number of bins to use. However, if you also include a label array and indices, ndimage.histogram() will return distributions for each of the selected labels.\n\n\n\nobject_histogram.PNG\n\n\nPlotting these object-level histograms is a great way to evaluate your segmentation. If you see very wide distributions or multiple peaks and valleys in your histogram, your labeled object may include many different tissue types. On the other hand, if the histogram resembles a normal distribution, your segmentation may be doing a good job. This is because the physical properties that influence intensity values should be relatively uniform throughout a tissue. For example, we expect that the voxels in our left ventricle label are filled with blood. Although we expect some variation in their intensity, we also expect them to be centered on some mean value.\n\n\n\nseparate_histograms.PNG\n\n\nIn this case, we can see that the majority of left ventricle intensity values are higher than the other labeled pixels. Although there are some low values, which are likely not part of the ventricle, the segmentation seems to do a good job overall.\nA poor tissue segmentation includes multiple tissue types, leading to a wide distribution of intensity values and more variance.\nOn the other hand, a perfectly segmented left ventricle would contain only blood-related pixels, so the histogram of the segmented values should be roughly bell-shaped.\nIllustration\nFor this exercise, compare the intensity distributions within vol for the listed sets of pixels. Use ndi.histogram, which also accepts labels and index arguments.\nUse the labels and index arguments to extract a histogram for each of the following set of pixels in vol:\n\nAll pixels\nAll labeled pixels\nLeft ventricle pixels (i.e., label 1)\n\n\n# Create histograms for selected pixels\nhist1 = ndi.histogram(vol, min=0, max=255, bins=256)\nhist2 = ndi.histogram(vol, 0, 255, 256, labels=labels)\nhist3 = ndi.histogram(vol, 0, 255, 256, labels=labels, index=1)\n\nPlot each histogram using plt.plot(). For each one, rescale by the total number of pixels to allow comparisons between them.\n\n# Plot the histogram density\nplt.plot(hist1 / hist1.sum(), label='All pixels')\nplt.plot(hist2 / hist2.sum(), label='All labeled pixels')\nplt.plot(hist3 / hist3.sum(), label='Left ventricle')\nformat_and_render_plot()\n\n\n\n\nintensity_value.PNG\n\n\nNotice how the left ventricle segmentation is more normally distributed than the other sets of pixels.\n\n\n\n\nMeasuring object “morphology,” or shape and size, is another principal aim of image analysis. For example, if a patient goes in for an MRI and they find out they have a brain tumor, a natural first question might be: “How big is it?”. Or, if they have been monitoring it for some time, they may want to know: “Has it grown?”\n\n\nTo measure the amount of space occupied by an object, we need two quantities:\n\nthe size of each element in the array; and\nthe number of those elements in the object.\n\nLet’s calculate the volume of the left ventricle in one of our cardiac images.\n\nFirst, we establish the amount of real, physical space taken up by each voxel. Recall that in DICOM images, we can find this in the “sampling” field of the metadata dictionary. Multiplying the lengths of the first, second, and third dimensions will give us the total volume at each voxel. In this case, the measurements are in cubic millimeters.\nNext, we want to count the number of voxels in the left ventricle. We can do this by passing a 1 as input to ndimage.sum() and then providing it with the labeled array and index of our object. The function will weight each left ventricle voxel with a value of 1 and sum them.\nFinally, we multiply the number of voxels by their individual size to get the total volume of the object (in cubic millimeters).\n\n\n\n\nvolume.PNG\n\n\nIllustration\nFor this exercise, measure the volume of the left ventricle in one 3D image (vol).\nFirst, count the number of voxels in the left ventricle (label value of 1). T\n\nnvoxels = ndi.sum(1, labels, index=1)\nnvoxels\n\n6459\nThen, multiply it by the size of each voxel in \\(mm^3\\). (Check vol.meta for the sampling rate.)\n\nd0, d1, d2 = vol.meta['sampling']\ndvoxel = d0 * d1 * d2\ndvoxel\n\n828113.0172042125\n\nvolume = nvoxels * dvoxel\n\n120731.82353614898\nThe volume of the left ventricle is therefore circa 120,731 \\(mm^3\\).\nVolume is a basic but useful measure, and it is a great “reality check” when evaluating your processes.\n\n\n\nAnother useful morphological measure is the distance of each voxel to the nearest background value. This information can help you identify the most embedded points within objects or mask out edge areas. To perform a distance transformation on a mask or label array, use the dist_transform_edt() function. This will return a new array, where each non-zero voxel has been replaced with the distance to the nearest background voxel. The maximum value, in this case, reflects how far from the edge the most embedded point is. If you have access to the sampling rates for each dimension, you can include these to generate values that reflect physical distance. You can see here that the max distance is reduced because the sampling rate is less than one millimeter per pixel.\n\nIllustration\nA distance transformation calculates the distance from each pixel to a given point, usually the nearest background pixel. This allows us to determine which points in the object are more interior and which are closer to edges.\nFor this exercise, we will use the Euclidian distance transform on the left ventricle object in labels.\nCreate a mask of left ventricle pixels (Value of 1 in labels).\n\nlv = np.where(labels==1, 1, 0)\n\nCalculate the distance to background for each pixel using ndi.distance_transform_edt(). Supply pixel dimensions to the sampling argument.\n\ndists = ndi.distance_transform_edt(lv,sampling=vol.meta['sampling'])\n\nPrint out the maximum distance and its coordinates using ndi.maximum and ndi.maximum_position.\n\nprint('Max distance (mm):', ndi.maximum(dists))\nprint('Max location:', ndi.maximum_position(dists))\n\nOverlay a slice of the distance map on the original image.\n\n# Plot overlay of distances\noverlay = np.where(dists[5] > 0, dists[5], np.nan) \nplt.imshow(overlay, cmap='hot')\nformat_and_render_plot()\n\n\n\n\ndistance_left_ventricle.PNG\n\n\nWe can make inferences about the shapes of objects by looking at the distribution of distances. For example, a circle will have a uniform distribution of distances along both dimensions.\n\n\n\nA complementary measure is the center of mass, which you can calculate directly. Mass, in this case, refers to intensity values, with larger values pulling the center towards them. Just like with the intensity measures, the center_of_mass() function accepts “labels” and “index” arguments. The function returns a tuple of coordinates for each object specified. For our cardiac data, the center of mass for the left ventricle corresponds to the center of the volume in all three dimensions.\n\n\n\ncenter_of_mass.PNG\n\n\nIllustration\nThe distance transformation reveals the most embedded portions of an object. On the other hand, ndi.center_of_mass() returns the coordinates for the center of an object.\nThe “mass” corresponds to intensity values, with higher values pulling the center closer to it.\nFor this exercise, we will calculate the center of mass for the two labeled areas, then, plot them on top of the image.\n\n# Extract centers of mass for objects 1 and 2\ncoms = ndi.center_of_mass(vol, labels, index=[1,2])\nprint('Label 1 center:', coms[0])\nprint('Label 2 center:', coms[1])\n\nNote: ndi.center_of_mass() returns [z, x, y] coordinates, rather than [pln, row, col]\n\n# Add marks to plot\nfor c0, c1, c2 in coms:\n    plt.scatter(c2, c1, s=100, marker='o')\nplt.show()\n\n\n\n\ncenter_of_mass_plot.PNG\n\n\nSome shapes, such as those with holes, may have a center of mass that is outside of them.\n\n\n\n\nIf you recall, the proportion of blood that’s pumped out of the left ventricle, is a measure known as the ejection fraction.\nTo calculate the ejection fraction, we have to find the left ventricle’s volume when :\n\nit’s totally relaxed – its maximum; and\nwhen it’s fully squeezed – its minimum.\n\nTaking the difference between these two volumes and dividing by the maximum yields the fraction of blood that is pumped out and into the rest of the circulatory system.\n\n\n\nejection_fraction_calc.PNG\n\n\n\n\nOne way to accomplish this is to:\n\nSegment the left ventricle at each time point.\nCalculate the volume at each time point sequentially, using a for loop.\n\nThis results in a 1D time series from which we can extract our minimum and maximum values. Values in hand, we plug them into the ejection fraction equation.\nLet’s assume that we have access to both the volumetric time series and the segmented left ventricle. The data are 4-dimensional, with time as the first dimension.\n\n\n\ntime.PNG\n\n\n\nFirst, we calculate the volume of each individual voxel. We extract the sampling rate along each dimension, then multiply the spatial dimensions together to get the space occupied by each element.\nNext, we instantiate an empty 1D array to record the volume at each time point.\nWe then loop through each time point, counting the number of voxels in the left ventricle.\nFinally, we multiply the number of voxels by their volume and store the value in the time series array.\n\n\n\n\nvolume_vs_time.PNG\n\n\nThe plot of the data lines up with our expectations: in the first few time points, there is a squeezing action on the ventricle, followed by relaxation, where it fills up again.\n\n\n\nvolume_time_plot.PNG\n\n\nIllustration\nThe ejection fraction is the proportion of blood squeezed out of the left ventricle each heartbeat. To calculate it, radiologists have to identify the maximum volume (systolic volume) and the minimum volume (diastolic volume) of the ventricle.\nInitialize an empty array with 20 elements :\n\n# Create an empty time series\nts = np.zeros(20)\n\nCalculate the volume of each image voxel. (Consult the meta dictionary for sampling rates.)\n\n# Calculate volume at each voxel\nd0, d1, d2, d3 = vol_ts.meta['sampling'] # the first dimension is time\ndvoxel = d1 * d2 * d3\n\nFor each time point, count the pixels in labels, and update the time series array.\n\n# Loop over the labeled arrays\nfor t in range(20):\n    nvoxels = ndi.sum(1, labels[t], index=1)\n    ts[t] = nvoxels * dvoxel\n\nPlot the time series using plt.plot().\n\n# Plot the data\nplt.plot(ts)\nformat_and_render_plot()\n\n\n\n\ntimepoint_volume.PNG\n\n\nWe can see the pumping action of the left ventricle clearly from the time series plot - a sudden decrease followed by a refilling of the chamber.\n\n\n\nNow, it’s simply a matter of selecting the lowest and highest values from the time series and calculating the ejection fraction. Since “ts” is a NumPy array, we can call the min() and max() methods to retrieve these values.\nThen, we find the difference and divide by the maximum volume.\n\n\n\neject_fract_calc.PNG\n\n\nIllustration\nTo close our investigation, plot slices from the maximum and minimum volumes by analyzing the volume time series (ts). Then, calculate the ejection fraction.\n\n# Get index of max volume\ntmax = np.argmax(ts)\n\n18\n\n# Get index of min volume\ntmin = np.argmin(ts)\n\n5\nPlot the extreme volumes (max and min) together. Display the images along the fifth plane, e.g. (vol_ts[t, 4]).\n\n# Plot the largest and smallest volumes\nfig, axes = plt.subplots(2,1)\naxes[0].imshow(vol_ts[tmax, 4], vmax=160)\naxes[1].imshow(vol_ts[tmin, 4], vmax=160)\nformat_and_render_plots()\n\n\n\n\nmax_min.PNG\n\n\nCalculate the ejection volume and fraction using the min() and max() methods of ts. Print these values.\n\n# Calculate ejection fraction\nej_vol = ts.max() - ts.min()\nej_frac = ej_vol / ts.max()\nprint('Est. ejection volume (mm^3):',ej_vol )\nprint('Est. ejection fraction:', ej_frac)\n\n<script.py> output: Est. ejection volume (mm^3): 31268.00536236538 Est. ejection fraction: 0.3202054794520548\nAfter calculating the ejection fraction, review the chart below. Should this patient be concerned?\n\n\n\nejection_fraction_chart.PNG\n\n\nThe patient should be extremely concerned! An ejection fraction of below 0.35 is considered to be severely abnormal and the patient’s score is 0.32.\nThis patient has heart failure with infarction - a serious condition. This case study illustrates a typical image analysis workflow: a single, useful metric is the result of a lot of sophisticated preprocessing, segmentation and measurement techniques.\nAnd that’s it! We’ve put together an estimate of the ejection fraction using SciPy - a process that would normally be done by hand by a radiologist. And we’ve done a pretty good job, even with the simple segmentation method: the expert’s estimate was 0.60, quite close to our value\nOf course, this analysis was for a high-quality image of a single subject. Evaluating data from many subjects and images allows for more interesting insights about health and disease. We’ll discuss techniques and pitfalls of multi-image analysis in the next section.\n\n\n\n\n\nFor the final section, we’ll need to use our brain… and hundreds of others! We’re going to look at brains from the Open Access Series of Imaging Studies. To describe the effects of aging and dementia on the brain, the researchers gathered 3D MRI images of nearly 400 adults. The participants were between the ages of 18 and 80, and many of them had mild to severe Alzheimer’s disease.\n\n\nNow that we’ve seen how we can measure a single image let’s turn our attention to questions that leverage many of them.\nWith a large imaging dataset, there is going to be variability and not just the kind that’s interesting. There will be differences in intensity scales, sampling ratios, object orientation, and object placement within the image window.\n\n\n\nvariability.PNG\n\n\nRegistration\nOne way to address this is to register images to a pre-defined position and coordinate system. For example, you might make all images line up with a template image or atlas. The process of aligning two images together is called “registration.” Registration requires making multiple transformations to an image, such as shifting, rotating, and scaling it.\n\n\n\nregistration.PNG\n\n\n\n\nLet’s see how we can implement some of these transformations in SciPy.\nHere we have an off-center brain that we want to move to the center of the image.\n\n\n\noff_centre_brain.PNG\n\n\nFirst, we’ll load the head slice using ImageIO. It has a shape of 256 by 256, so the center is at 128, 128.\nThen we’ll get the head’s initial center of mass.\nWe next calculate the difference between the head’s current center of mass and the target center, for both the rows and the columns.\nFinally, we call SciPy’s shift() function, passing in the image and the number of pixels we need to move along the first and second axes.\n\n\n\ntranslation.PNG\n\n\nIllustration\n\nimport imageio.v2 as imageio\nimport numpy as np\nimport scipy.ndimage as ndi\nimport matplotlib.pyplot as plt\n\n\n# using loadtxt()\nim = np.loadtxt(\"Data/NumpyData.csv\",\n                 delimiter=\",\", dtype=float)\ndisplay(im)\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\nFind the center-point of im\n\n# Find image center of mass\ncom = ndi.center_of_mass(im)\n\nCalculate the distance from the image center (128, 128), along each axis.\n\n# Calculate amount of shift needed\nd0 = 128 - com[0]\nd1 = 128 - com[1]\n\nUse ndi.shift() to shift the data\n\n# Translate the brain towards the center\nxfm = ndi.shift(im, shift=[d0,d1])\n\nPlot the original and shifted data. First, create an array of subplots with two rows and one column. Then, draw im and xfm on the first and second subplots.\n\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    plt.tight_layout()\n    plt.axis('off')\n    plt.show\n\n# Plot the original and adjusted images\nfig, axes = plt.subplots(2,1)\naxes[0].imshow(im,cmap='gray')\naxes[1].imshow(xfm, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nWe can shift our image in as many directions as there are image dimensions.\n\n\n\nRotations can be performed in a similar manner using the rotate() function.\n\n\n\nrotation.PNG\n\n\nThe angle of rotation is specified in degrees :\n\nwith positive numbers indicating upward from the horizontal; and\nnegative numbers downward.\n\nIn two dimensions, we always rotate along the x-y plane, but in three dimensions, there are three rotational planes we could use.\nOne caveat with the rotate() function is that the default behavior preserves all the original data. This means that your rotated image may actually end up larger than your original. To keep your original image shape, pass \"reshape equals False\" to your function call.\n\n\n\nrotation_preserve.PNG\n\n\nIllustration\nFor this exercise, shift and rotate the brain image (im) so that it is roughly level and “facing” the right side of the image.\nShift im towards the center: 20 pixels left and 20 pixels up.\n\n# Shift the image towards the center\nxfm = ndi.shift(im,shift=[-20,-20]) # [left(-ve), up(-ve)]\n\nUse ndi.rotate to turn xfm 30 degrees downward. Set reshape=False to prevent the image shape from changing.\n\n# Rotate the shifted image\nxfm = ndi.rotate(xfm, angle=-30, reshape=False)\n\nPlot the original and transformed images.\n\n# Plot the original and rotated images\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im, cmap='gray')\naxes[1].imshow(xfm, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nThe order of transformations makes a difference. Rotating the image first will alter the object center, changing the amount of shift needed.\n\n\n\nAffine transformations modify an image while preserving all of the points, straight lines, and planes. Shown here are four examples of affine transformations.\n\n\n\naffine.PNG\n\n\n\nTranslation is the shifting of an image along an axis. It can be used to center an object, for example.\nRotation, on the other hand, will turn the image along a plane.\nScaling increases or decreases the size of the image, and the shearing operation shifts the ends of an axis away from each other.\n\nTransformation matrix\nFor complex registrations, it can be useful to compute a transformation matrix between the original and target space. Essentially, the elements of a transformation matrix encode instructions for the operations we have discussed: translation, rotation, scaling, and shearing. We cannot cover methods for calculating these matrices, but let’s see how they can be used to simplify the registration process.\n\n\n\ntransformation_matrix.PNG\n\n\nApplying a transformation matrix\nFirst, we create the transformation matrix. Let’s first use the identity matrix, which has ones along the diagonal and zeros off it. We can apply it by passing the image and matrix to the affine_transform() function. The resulting image is identical to the original.\nNext, let’s manipulate the matrix values that encode shifting and rescaling. When we apply this new matrix and plot the result, you can see that the image has been centered and made larger.\n\n\n\ntransform_matrix_apply.PNG\n\n\nIllustration\nLet’s use ndi.affine_transform() to explore the impact of applying the following registration matrices to im. Which one does the best job of centering, leveling and enlarging the original image?\n\n\n\n\n# Create identity matrix\nmat_1 = np.identity(3)\nmat_1\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nxfm_1 = ndi.affine_transform(im,mat_1)\n\n\n# Plot the original and rotated images\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im, cmap='gray')\naxes[1].imshow(xfm_1, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nAs expected, this leaves the original image unchanged.\n\n\n\n\nmat_2 = np.array([[1.5,-0.8,60],[0.8,1.5,-140],[0,0,1]])\nmat_2\n\narray([[   1.5,   -0.8,   60. ],\n       [   0.8,    1.5, -140. ],\n       [   0. ,    0. ,    1. ]])\n\n\n\nxfm_2 = ndi.affine_transform(im,mat_2)\n\n\n# Plot the original and rotated images\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im, cmap='gray')\naxes[1].imshow(xfm_2, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nExplanation of transformations :\nTranslation T_x = +60 T_y = -140\nScale S_x = +1.5, S_y = +1.5\nShear Sh_x = -0.8, Sh_y = +0.8\n\n\n\n\nmat_3 = np.array([[1,-0.3,60],[-0.3,1,60],[0,0,1]])\nmat_3\n\narray([[ 1. , -0.3, 60. ],\n       [-0.3,  1. , 60. ],\n       [ 0. ,  0. ,  1. ]])\n\n\n\nxfm_3 = ndi.affine_transform(im,mat_3)\n\n\n# Plot the original and rotated images\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im, cmap='gray')\naxes[1].imshow(xfm_3, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nExplanation of transformations :\nTranslation T_x = +60, T_y = +60\nShear Sh_x = -0.3, Sh_y = -0.3\n\n\n\n\nmat_4 = np.array([[0.8,-0.4,90],[0.4,0.8,-6],[0,0,1]])\nmat_4\n\narray([[ 0.8, -0.4, 90. ],\n       [ 0.4,  0.8, -6. ],\n       [ 0. ,  0. ,  1. ]])\n\n\n\nxfm_4 = ndi.affine_transform(im,mat_4)\n\n\n# Plot the original and rotated images\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im, cmap='gray')\naxes[1].imshow(xfm_4, cmap='gray')\nformat_and_render_plot()\n\n\n\n\nExplanation of transformations :\nTranslation T_x = +90, T_y = -6\nScale S_x = +0.8, S_y = +0.8\nShear Sh_x = -0.4, Sh_y = +0.4\nTo implement matrix transformations in your workflow, you will likely want to use more advanced tools, such as those in scikit-image. The package’s website has some nice tutorials. Also, note that 3D images require different size transformation matrices.\n\n\n\n\nWhen comparing images, differences in array shape and sampling rates can pose hurdles to analysis. Resampling is one way to address this issue.\n\n\nResampling is the process of slicing your data onto a different array. It’s distinct from cropping in that the field of view does not change. Instead, the amount of space sampled by each pixel is increased or decreased, which in turn changes the shape of the array.\n\n\n\nresampling.PNG\n\n\nOne useful application is downsampling in which information is merged across multiple pixels to reduce the image size.\n\n\n\ndownsampling.PNG\n\n\nHere we have loaded a brain volume from the OASIS dataset. Originally, it has 256 elements along each dimension. We’ll halve this grid size using SciPy’s zoom() function. zoom() adjusts the number of elements along each axis by a given factor. We pass in the volume and specify a zoom of 0.5. This reduces the number of elements on each axis by half so that it is 128 by 128 by 128. Plotting the image reveals that it now has less detail than before, but it also takes up half the amount of memory.\nIt’s also possible to upsample onto denser grids. But note that this does not result in a true increase in resolution. Even though you are putting more pixels into your image, you are not adding new information that wasn’t there previously. One useful application of upsampling is to make sampling rates along different dimensions equal, for example, to make voxels cubic.\nTo upsample an image, we call the zoom() function and specify a larger zoom factor. Passing in “2” will double the width of each axis.\n\n\n\nupsampling.PNG\n\n\nIllustration\nImages can be collected in a variety of shapes and sizes. Resampling is a useful tool when these shapes need to be made consistent. For this exercise, we will transform and then resample the brain image (im) to see how it affects image shape.\nShift im 20 pixels left and 20 pixels up, i.e. (-20, -20). Then, rotate it 35 degrees downward. Remember to specify a value for reshape.\n\n# Center and level image\nxfm = ndi.shift(im, shift=[-20,-20]) # +ve direction is right and up\nxfm = ndi.rotate(xfm, angle=-35, reshape=False) # +ve direction is up\n\nUse ndi.zoom() to downsample the image from (256, 256) to (64, 64).\n\n# Resample image - reduce by factor of 4 \nim_dn = ndi.zoom(xfm, zoom=0.25)\n\nUse ndi.zoom() to upsample the image from (256, 256) to (1024, 1024).\n\n# Resample image - increase by factor of 4 \nim_up = ndi.zoom(xfm, zoom=4)\n\nPlot the resampled images.\n\n# custom function\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    plt.tight_layout()\n    plt.axis('off')\n    plt.show\n\n\n# Plot the images\nfig, axes = plt.subplots(2, 1)\naxes[0].imshow(im_dn, cmap='gray')\naxes[1].imshow(im_up, cmap='gray')\nformat_and_render_plot()\n\n\n\n\n\n\n\nResampling actually creates a brand new image that’s based on the old one. And in most cases, filling out this new image requires estimating data that wasn’t originally there. This estimation process is called interpolation.\nFor example, here we have a simple 10 point dataset :\n\n\n\nten_point.PNG\n\n\nIf we want to upsample it to include 100 points, we have to estimate what the values should be between each of these original points.\nOne approach is to use the nearest original value, nearest-neighbour, for each new point. This a zero-order interpolation, because we aren’t modeling any relationship between the original values.\n\n\n\ninterpol_1.PNG\n\n\nFor higher-order estimation, SciPy uses B-spline interpolation, which uses a set of functions to model the space between points. The order controls how complex these functions can be:\n\nan order of 1 is linear\nan order of 2 is quadratic, and so on.\n\nIn the example, we can see that cubic interpolation (order of 3) creates a smooth curve between points :\n\n\n\ncubic_interpol.PNG\n\n\nHere we’ve created a ten by ten toy image of ascending values. If we resample it onto a 100 by 100 grid, changing the order will affect the “smoothness” of the resulting image. With an order of 0, we return essentially the original image on a new grid. At higher orders, though, we can see a smoother gradient of change along each axis.\n\n\n\ninterpol_2.PNG\n\n\nThe principal trade-off with lower and higher-order interpolation is in the computational time: fifth-order interpolation for a 3D volume can take a very long time to calculate!\nIllustration\nFor this exercise, upsample im and investigate the effect of different interpolation orders on the resulting image.\nUse ndi.zoom() to upsample im from a shape of 128, 128 to 512, 512 twice. First, use an interpolation order of 0, then set order to 5.\n\nim.shape\n\n(256, 256)\n\n\n\n# Upsample \"im\" by a factor of 4\nup = ndi.zoom(im, zoom=0.5)\nup0 = ndi.zoom(up, zoom=4, order=0)\nup5 = ndi.zoom(up, zoom=4, order=5)\n\nPrint the array shapes of up and up0.\n\n# Print original and new shape\nprint('Original shape:', up.shape)\nprint('Upsampled shape:', up0.shape)\n\nOriginal shape: (128, 128)\nUpsampled shape: (512, 512)\n\n\nPlot close-ups of the images. Use the index range 128:256 along each axis.\n\n# Plot close-ups of the new images\nfig, axes = plt.subplots(1, 2)\naxes[0].imshow(up0[128:256, 128:256],cmap='gray')\naxes[1].imshow(up5[128:256, 128:256],cmap='gray')\nformat_and_render_plot()\n\n\n\n\nThe key trade-off is that more complex interpolation methods require greater computational resources. This can take a heavy toll when analyzing 3D volumes.\n\n\n\n\nHow do you tell if your images are registered well? How would you compare an automatically segmented image with a hand-labeled one? To directly compare two arrays, we have to generate measures of image similarity.\nHere are two slices of the same person’s brain, taken on separate occasions.\n\n\n\nbrain_compare.PNG\n\n\nYou can see that they are similar objects but not aligned with each other. To quantify exactly how imperfect this alignment is, we need to apply a function that will compare the two images.\nAt the pixel level, there can be thousands and thousands of comparison points between two images. Our need is to summarize all of these comparisons into a single number. As with other areas of data science, such as machine learning, there are many ways to evaluate our data. There are cost functions, such as the mean absolute error or mean squared error, which should be minimized.\nObjective functions, such as the intersection of the union, are supposed to be maximized.\n\n\nLet’s return to our two images, and calculate similarity using a cost function: the mean absolute error.\nFirst, we read in the images. Then, we find the error at each pixel by subtracting im2 from im1. Next, we take the absolute value of the error, because we care about whether the images differ from each other in any way, not whether one is larger than the other. Finally, we take the mean of the entire error image to get a single summary measure of similarity.\n\n\n\nmean_abs_error.PNG\n\n\nThe goal is not to achieve a mean absolute error of zero. In fact, that would mean the images were identical, which is not the case. Instead, you want to minimize the cost function by altering one or both images.\nLet’s shift and rotate the first image, then re-compute the cost. We calculate the mean absolute error in the same way as before, using our new image.\n\n\n\nmean_abs_error_2.PNG\n\n\nThe new cost 13.0376 is lower than the previous one 29.8570, suggesting that the two are better aligned than before.\nIllustration\nThe mean absolute error (MAE), for example, summarizes intensity differences between two images, with higher values indicating greater divergence.\nFor this exercise, calculate the mean absolute error between im1 and im2 step-by-step.\n\nimport imageio.v2 as imageio\nimport numpy as np\nimport scipy.ndimage as ndi\nimport matplotlib.pyplot as plt\n\n\nim1 = np.loadtxt('Data/im1.csv', delimiter = ',' , dtype=float)\n\n\nim2 = np.loadtxt('Data/im2.csv', delimiter = ',' , dtype=float)\n\nCalculate the difference between im1 and im2.\n\n# Calculate image difference\nerr = im1 - im2\n\nPlot err with the seismic colormap. To center the colormap at 0, set vmin=-200 and vmax=200.\n\n# custom function\ndef format_and_render_plot():\n    '''Custom function to simplify common formatting operations for exercises. Operations include: \n    1. Turning off axis grids.\n    2. Calling `plt.tight_layout` to improve subplot spacing.\n    3. Calling `plt.show()` to render plot.'''\n    fig = plt.gcf()\n    plt.tight_layout()\n    plt.axis('off')\n    plt.colorbar()\n    plt.show\n\n# Plot the difference\nplt.imshow(err, cmap='seismic', vmin=-200, vmax=200)\nformat_and_render_plot()\n\n\n\n\nCompute the absolute error of the difference.\n\n# Calculate absolute image difference\nabs_err = np.abs(im1 - im2)\n\nUse np.abs(). Plot the image.\n\n# Plot the difference\nplt.imshow(abs_err, cmap='seismic', vmin=-200, vmax=200)\nformat_and_render_plot()\n\n\n\n\nFind the cost value using np.mean()\n\n# Calculate mean absolute error\nmean_abs_err = np.mean(np.abs(im1 - im2))\nprint('MAE:', mean_abs_err)\n\nMAE: 9.2608642578125\n\n\nThe MAE metric allows for variations in weighting throughout the image, which gives areas with high pixel intensities more influence on the cost calculation than others.\n\n\n\nOne issue with the mean absolute error approach is that tissues with high-intensity values will contribute more towards the error than other types. One remedy is to compare the image masks. The intersection of the union is a measure particularly well-suited to this. It is calculated by dividing the number of shared pixels between two masks by the total number of masked pixels :\n\n\n\nIOU.PNG\n\n\n\nFirst, we create image masks by selecting pixels greater than 0.\nSecond, we take the intersection of the two masks: that is, the pixels that appear in both masks.\nNext, we find the union: the pixels that appear in either mask.\nFinally, we divide the sum of the intersection by the sum of the union.\n\n\n\n\nIOU_score.PNG\n\n\nThe result is a number between 0 and 1, with 0 representing no shared mask pixels, and 1 representing perfect agreement.\nThe principle here is that summarizing image similarity with a single number gives you something to optimize when processing your images.\nIllustration\nFor this exercise, using the following function, determine how best to transform im1 to maximize the IOU cost function with im2\n\ndef intersection_of_union(im1, im2):\n    i = np.logical_and(im1, im2)\n    u = np.logical_or(im1, im2)\n    return i.sum() / u.sum()\n\n\nintersection_of_union(im1, im2)\n\n0.7171385659453746\n\n\nTranformation 1\nShift (-10, 10), rotate -15 deg\n\nim1_transform_1 = ndi.shift(im1,(-10,-10)) # [left(-ve), up(-ve)]\nim1_transform_1 = ndi.rotate(im1_transform_1, -15, reshape=False)\nintersection_of_union(im1_transform_1, im2)\n\n0.8913414634146342\nTranformation 2\nShift (+10, +10), rotate -15 deg\n\nim1_transform_2 = ndi.shift(im1, (10,10)) # [left(-ve), up(-ve)]\nim1_transform_2 = ndi.rotate(im1_transform_2, -15, reshape=False)\nintersection_of_union(im1_transform_2, im2)\n\n0.6069298200114114\nTranformation 3\nShift (+10, +10), rotate +15 deg\n\nim1_transform_3 = ndi.shift(im1,(10,10)) # [left(-ve), up(-ve)]\nim1_transform_3 = ndi.rotate(im1_transform_3, 15, reshape=False)\nintersection_of_union(im1_transform_3, im2)\n\n0.5206319611402777\nTranformation 4\nShift (-10, 10), rotate +15 deg\n\nim1_transform_4 = ndi.shift(im1,shift=(-10,-10)) # [left(-ve), up(-ve)]\nim1_transform_4 = ndi.rotate(im1_transform_4, 15, reshape=False)\nintersection_of_union(im1_transform_4, im2)\n\n0.7081703487411162\nSo, out of the four transformations, Transformation 1, Shift (-10, 10), rotate -15 deg, maximizes the IOU cost function.\nRemember, the core principle is that a cost function must produce a single summary value across all elements in the image. MAE and IOU are just two of the many possible ways you might compare images.\n\n\n\n\nTo round out this blog, let’s discuss the importance of accounting for confounds when comparing images.\nAnalysis workflow\nShown here is a common multi-subject workflow, in which each subject has a raw image that we process, measure, and store in a common group dataset.\n\n\n\nworkflow.PNG\n\n\nIn this model, each image is treated independently, but you are extracting the same metrics from each subject.\nOASIS population\nFor this lesson, let’s assume that we have measured the brain volumes for all 400 images in the OASIS dataset and stored them in a pandas DataFrame. Displaying a few random rows shows that we have information such as age, sex, brain volume, and skull volume available.\n\n\n\noasis_df.PNG\n\n\nHypothesis testing\nWith this data, we want to answer a crucial question: are the brains of men and women different? How would we test this?\nFor this question, we can use a two-sample t-test. We will test the null hypothesis that the mean brain volumes of men and women are the same. The likelihood of the null hypothesis being true is assessed by calculating the t-statistic. If the magnitude of t is very far from 0, then we can reject the null hypothesis that the groups are the same. Fortunately, this function is already implemented in SciPy and is simple to evaluate.\n\n\n\nhypothesis_testing.PNG\n\n\nTo run the t-test, we need the brain volumes for all of the males and females in our sample.\n\n\n\nt_stat_p_value.PNG\n\n\nSelect the male values with df.loc, then specifying the rows where “sex is male” and the column with “brain volume” values. For females, we change the selected sex to “female.”\nTo run the t-test, import the ttest_ind() function from SciPy’s stats module. Then, pass the two vectors as our first and second populations. The results object contains the test statistic, which is quite high, and the p-value. The p-value corresponds to the probability that the null hypothesis is true. In this case, we have a large t-statistic and a low p-value, which suggests that there’s a significant difference in gender brain size! Should we start writing it up?\nCorrelated measurements\nNot so fast. We need to consider the context in which this measurement is acquired. Brains fill up skulls, and skulls are proportional to body size. If we compare brain and skull volumes, we find that they have quite a large correlation.\n\n\n\ncorrelation.PNG\n\n\nAccounting for this shared variation is important if we want to state that there’s something different about the brains of men and women and not just their body size.\nNormalization\nTo account for this potential confound, we can normalize brain volume with respect to skull size by calculating the brain to skull ratio. We can then repeat our t-test using the normalized values for males and females. Reviewing the results leads to disappointment: our amazing finding was likely related simply to the fact that large people have larger brains, and there are more large men than large women.\n\n\n\nnormalization.PNG\n\n\nPotential confounds in imaging\n\n\n\nconfounds.PNG\n\n\nConfounds are omnipresent in data science but can be especially pervasive in image analyses. If you are pooling data across multiple hospitals or sites, you must consider how image acquisition, digital storage, clinical protocols, and subject populations might be biased towards a particular result. In short, you may employ extremely sophisticated image analysis techniques, but if your analytical thinking is faulty, then your conclusions will be too!\n\n\nOnce measures have been extracted, double-check for dependencies within your data. This is especially true if any image parameters (sampling rate, field of view) might differ between subjects, or you pull multiple measures from a single image.\nFor the final exercises, we have combined demographic and brain volume measures into a pandas DataFrame (df).\nFirst, we will explore the table and available variables. Then, we will check for correlations between the data.\n\nimport pandas as pd\ndf = pd.read_csv('Data/oasis_all_volumes.csv')\ndf = df.drop(['gray_matter_vol','white_matter_vol','csf_vol'], axis=1)\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 425 entries, 0 to 424\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   ID          425 non-null    object \n 1   age         425 non-null    int64  \n 2   sex         425 non-null    object \n 3   alzheimers  425 non-null    bool   \n 4   brain_vol   425 non-null    float64\n 5   skull_vol   425 non-null    float64\ndtypes: bool(1), float64(2), int64(1), object(2)\nmemory usage: 17.1+ KB\n\n\n\n# Print 3 random rows\nprint(df.sample(3))\n\n                ID  age sex  alzheimers  brain_vol    skull_vol\n388  OAS1_0441_MR1   81   M        True    971.016  1583.689763\n181  OAS1_0202_MR1   23   F       False   1213.242  1434.512143\n9    OAS1_0011_MR1   52   F       False    920.874  1330.603431\n\n\nPrint the unique number of individuals with Alzheimer’s disease patients.\n\nprint(df.alzheimers.value_counts())\n\nFalse    332\nTrue      93\nName: alzheimers, dtype: int64\n\n\nPrint the correlation table between each variable.\n\nprint(df.corr())\n\n                 age  alzheimers  brain_vol  skull_vol\nage         1.000000    0.542463  -0.719211  -0.141576\nalzheimers  0.542463    1.000000  -0.446771   0.014222\nbrain_vol  -0.719211   -0.446771   1.000000   0.654829\nskull_vol  -0.141576    0.014222   0.654829   1.000000\n\n\n/tmp/ipykernel_94/4212406737.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  print(df.corr())\n\n\nThere is a high correlation - nearly 0.7 - between the brain_vol and skull_vol. We should be wary of this (and other highly correlated variables) when interpreting results.\n\n\n\nLet’s test the hypothesis that Alzheimer’s Disease is characterized by reduced brain volume.\n\n\n\nalzheimers.PNG\n\n\nWe can perform a two-sample t-test between the brain volumes of elderly adults with and without Alzheimer’s Disease. In this case, the two population samples are independent from each other because they are all separate subjects.\n\n# Import independent two-sample t-test\nfrom scipy.stats import ttest_ind\n\nCreate a vector of ‘brain_vol’ values for each of the Alzheimer’s Disease and Typical Elderly groups.\n\n# Select data from \"alzheimers\" and \"typical\" groups\nbrain_alz = df.loc[df.alzheimers == True, 'brain_vol']\nbrain_typ = df.loc[df.alzheimers == False, 'brain_vol']\n\nUse ttest_ind() to test for differences between the two groups’ ‘gray_vol’ metrics. Print the results.\n\n# Perform t-test of \"alz\" > \"typ\"\nresults = ttest_ind(brain_alz, brain_typ)\nprint('t = ', results.statistic)\nprint('p = ', results.pvalue)\n\nt =  -10.27076306169677\np =  3.043550344658516e-22\n\n\nVisualize the ‘brain_vol’ measures using the boxplot() method of df. Group the variables by their disease classification by setting by=‘alzheimers’.\n\n# Show boxplot of brain_vol differences\ndf.boxplot(column='brain_vol', by='alzheimers')\nplt.show()\n\n\n\n\nThere is some evidence for decreased brain volume in individuals with Alzheimer’s Disease. Since the p-value for this t-test is greater than 0.05, we would not reject the null hypothesis that states the two.\n\n\n\nWe previously saw that there was not a significant difference between the brain volumes of elderly individuals with and without Alzheimer’s Disease. But could a correlated measure, such as “skull volume” be masking the differences?\nFor this exercise, calculate a new test statistic for the comparison of brain volume between groups, after adjusting for the subject’s skull size.\nUsing results.statistic and results.pvalue as oour guide, answer the question: Is there strong evidence that Alzheimer’s Disease is marked by smaller brain size, relative to skull size?\n\n# Import independent two-sample t-test\nfrom scipy.stats import ttest_ind\n\n# Divide `df.brain_vol` by `df.skull_vol`\ndf['adj_brain_vol'] = df.brain_vol / df.skull_vol\n\n# Select brain measures by Alzheimers group\nbrain_alz = df.loc[df.alzheimers == True, 'adj_brain_vol']\nbrain_typ = df.loc[df.alzheimers == False, 'adj_brain_vol']\n\n# Evaluate null hypothesis\nresults = ttest_ind(brain_alz, brain_typ)\nprint('t = ', results.statistic)\nprint('p = ', results.pvalue)\n\nt =  -15.311391823926579\np =  2.019286086120867e-42\n\n\nInterpretation of results\nThe “t” value of -15.31 represents the t-statistic calculated from a statistical test, such as a t-test, comparing the mean brain size to skull size ratio between two groups: one with Alzheimer’s Disease and one without. A negative t-value suggests that the group with Alzheimer’s Disease had a smaller mean brain size to skull size ratio than the group without Alzheimer’s Disease.\nThe “p” value of 2.019e-42 (or approximately 0) represents the probability of obtaining such an extreme or more extreme t-value under the null hypothesis that there is no difference in the mean brain size to skull size ratio between the two groups. A p-value of less than 0.05 (or 0.01, depending on the level of significance chosen) is generally considered statistically significant and indicates strong evidence against the null hypothesis. In this case, the extremely low p-value suggests that it is highly unlikely that the observed difference in the mean brain size to skull size ratio between the two groups is due to chance.\nTherefore, there is strong evidence to suggest that Alzheimer’s Disease is marked by smaller brain size, relative to skull size.\nWe’ve worked our way through several levels of biomedical image analysis and are well-prepared for tackling new datasets and problems.\nFor more advanced tools, check out scikit-image, which extends the capabilities of scipy for image processing.\n\n\n\n\nI really enjoyed this course. It was a good NumPy refresher for me, and I was pleased to see Affine Transformations, t-tests, and p-values, outside of the abstraction of my Maths & Statistics degree! I hope to explore image analysis further.\nI sometimes feel like I excelled in mathematics a generation too early! Twenty years or so ago, I don’t recall there being much in the way of commercial or industry application of mathematics. Although I worked in data analytics for over 20 years as a chartered accountant, it never quite satisfied my passion for mathematics. As I transition into data science / machine learning, hopefully I can get my teeth into something challenging but rewarding, at the intersection of business, mathematics and statistics.\nThanks to Stephen Bailey for leading the course in collaboration with DataCamp."
  },
  {
    "objectID": "posts/DE_Zoomcamp_Final_Project/de_zoomcamp_project.html",
    "href": "posts/DE_Zoomcamp_Final_Project/de_zoomcamp_project.html",
    "title": "Data Engineering Zoomcamp - Final Project",
    "section": "",
    "text": "It’s time to pull everything together I’ve learned during this course and complete an end to end data pipeline project. As a musician I decided to chose a dataset of personal interest to me :\n\naudio features of over 1.2 million songs obtained with the Spotify API\n\nAcknowledgements to Rodolfo Figueroa for curating the dataset. Kaggle assigned the dataset a solid Usability score of 8.24 which is a good starting point. Whilst this is primarily a data engineering project I will also be carrying out some data pre-processing.\nData logistics is no different from any other form of logistics, in that it will not be possible to move our data from source, in my case a raw csv file held on Kaggle to destination, in my case Looker Studio without a few bumps along the way. But by carrying out some preliminary data exploration, and harnessing workflow orchestration tools, we can make the journey as smooth as possible.\nSo sit back and enjoy the ride ;)\n\n\n\nOutlined below is an overview of the architecture and technologies that I will be using to unveil some insights from the raw data.\n\n\n\nArchitecture_Technologies.png\n\n\n\n\n\nReproducability\nI use Unbuntu on Windows 20.04.5 and like to run things locally from the command line wherever possible, however this project does make use of cloud applications.\nA pre-requisite for reproducability of this project is having a Google Cloud account. I set mine up at the beginning of this course. You can follow these instructions to get up and running.\nFor the purposes of this specific project, I used Terraform (Infrastructure-as-Code) to automate my cloud resources configuration. I had already set this up locally at the beginning of the course and so only required the following two files to configure my data bucket in Google Cloud Storage, and my Data Warehouse, BigQuery.\nmain.tf\n\nterraform {\n  required_version = \">= 1.0\"\n  backend \"local\" {}  # Can change from \"local\" to \"gcs\" (for google) or \"s3\" (for aws), if you would like to preserve your tf-state online\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n    }\n  }\n}\n\nprovider \"google\" {\n  project = var.project\n  region = var.region\n  // credentials = file(var.credentials)  # Use this if you do not want to set env-var GOOGLE_APPLICATION_CREDENTIALS\n}\n\n# Data Lake Bucket\n# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_bucket\nresource \"google_storage_bucket\" \"data-lake-bucket\" {\n  name          = \"${local.data_lake_bucket}_${var.project}\" # Concatenating DL bucket & Project name for unique naming\n  location      = var.region\n\n  # Optional, but recommended settings:\n  storage_class = var.storage_class\n  uniform_bucket_level_access = true\n\n  versioning {\n    enabled     = true\n  }\n\n  lifecycle_rule {\n    action {\n      type = \"Delete\"\n    }\n    condition {\n      age = 30  // days\n    }\n  }\n\n  force_destroy = true\n}\n\n# DWH\n# Ref: https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset\nresource \"google_bigquery_dataset\" \"dataset\" {\n  dataset_id = var.BQ_DATASET\n  project    = var.project\n  location   = var.region\n}\n\nvariables.tf\n\nlocals {\n  data_lake_bucket = \"spotify\"\n}\n\nvariable \"project\" {\n  description = \"de-zoomcamp-project137\"\n}\n\nvariable \"region\" {\n  description = \"Region for GCP resources. Choose as per your location: https://cloud.google.com/about/locations\"\n  default = \"europe-west6\"\n  type = string\n}\n\nvariable \"storage_class\" {\n  description = \"Storage class type for your bucket. Check official docs for more info.\"\n  default = \"STANDARD\"\n}\n\nvariable \"BQ_DATASET\" {\n  description = \"BigQuery Dataset that raw data (from GCS) will be written to\"\n  type = string\n  default = \"spotify\"\n}\n\nOnce you have configured these files you can simply run the following prompts from the command line :\nRefresh service-account’s auth-token for this session :\ngcloud auth application-default login\nInitialize state file :\nterraform init\nCheck changes to new infra plan :\nterraform plan -var=\"project=<your-gcp-project-id>\"\nAsks for approval to the proposed plan, and applies changes to cloud :\nterraform apply\nFor further assistance refer to this detailed guide on Local Setup for Terraform and GCP.\n\n\n\nI was unable to download the file using the link address to the file on Kaggle :\n\n\n\nkaggle_download.PNG\n\n\nAs a workaround I resorted to clicking on the Download icon to save the file locally. We can access the file size (memory) and number of rows from the command line using the following commands :\ndu -h <file_name>\nwc -l <file_name>  \n\n\n\nspotify_csv.PNG\n\n\nLet’s get to know our data :\n\nimport pandas as pd\ndf = pd.read_csv('Prefect/data/spotify.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      0\n      7lmeHLHBe4nmXzuXc0HDjk\n      Testify\n      The Battle Of Los Angeles\n      2eia0myWFgoHuttJytCxgX\n      ['Rage Against The Machine']\n      ['2d0hyoQ5ynDBnkvAbJKORj']\n      1\n      1\n      False\n      0.470\n      ...\n      0.0727\n      0.02610\n      0.000011\n      0.3560\n      0.503\n      117.906\n      210133\n      4.0\n      1999\n      1999-11-02\n    \n    \n      1\n      1wsRitfRRtWyEapl0q22o8\n      Guerrilla Radio\n      The Battle Of Los Angeles\n      2eia0myWFgoHuttJytCxgX\n      ['Rage Against The Machine']\n      ['2d0hyoQ5ynDBnkvAbJKORj']\n      2\n      1\n      True\n      0.599\n      ...\n      0.1880\n      0.01290\n      0.000071\n      0.1550\n      0.489\n      103.680\n      206200\n      4.0\n      1999\n      1999-11-02\n    \n    \n      2\n      1hR0fIFK2qRG3f3RF70pb7\n      Calm Like a Bomb\n      The Battle Of Los Angeles\n      2eia0myWFgoHuttJytCxgX\n      ['Rage Against The Machine']\n      ['2d0hyoQ5ynDBnkvAbJKORj']\n      3\n      1\n      False\n      0.315\n      ...\n      0.4830\n      0.02340\n      0.000002\n      0.1220\n      0.370\n      149.749\n      298893\n      4.0\n      1999\n      1999-11-02\n    \n    \n      3\n      2lbASgTSoDO7MTuLAXlTW0\n      Mic Check\n      The Battle Of Los Angeles\n      2eia0myWFgoHuttJytCxgX\n      ['Rage Against The Machine']\n      ['2d0hyoQ5ynDBnkvAbJKORj']\n      4\n      1\n      True\n      0.440\n      ...\n      0.2370\n      0.16300\n      0.000004\n      0.1210\n      0.574\n      96.752\n      213640\n      4.0\n      1999\n      1999-11-02\n    \n    \n      4\n      1MQTmpYOZ6fcMQc56Hdo7T\n      Sleep Now In the Fire\n      The Battle Of Los Angeles\n      2eia0myWFgoHuttJytCxgX\n      ['Rage Against The Machine']\n      ['2d0hyoQ5ynDBnkvAbJKORj']\n      5\n      1\n      False\n      0.426\n      ...\n      0.0701\n      0.00162\n      0.105000\n      0.0789\n      0.539\n      127.059\n      205600\n      4.0\n      1999\n      1999-11-02\n    \n  \n\n5 rows × 24 columns\n\n\n\nOn first look the dataset appears to be fairly clean - the artists name are wrapped in [' '] and some of the values for track features are taken to a large number of decimal places. We’ll include these clean up as a flow as part of a data ingestion script using Prefect which will read the csv, convert to parquet format, and upload to Google Cloud Storage.\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1204025 entries, 0 to 1204024\nData columns (total 24 columns):\n #   Column            Non-Null Count    Dtype  \n---  ------            --------------    -----  \n 0   id                1204025 non-null  object \n 1   name              1204025 non-null  object \n 2   album             1204025 non-null  object \n 3   album_id          1204025 non-null  object \n 4   artists           1204025 non-null  object \n 5   artist_ids        1204025 non-null  object \n 6   track_number      1204025 non-null  int64  \n 7   disc_number       1204025 non-null  int64  \n 8   explicit          1204025 non-null  bool   \n 9   danceability      1204025 non-null  float64\n 10  energy            1204025 non-null  float64\n 11  key               1204025 non-null  int64  \n 12  loudness          1204025 non-null  float64\n 13  mode              1204025 non-null  int64  \n 14  speechiness       1204025 non-null  float64\n 15  acousticness      1204025 non-null  float64\n 16  instrumentalness  1204025 non-null  float64\n 17  liveness          1204025 non-null  float64\n 18  valence           1204025 non-null  float64\n 19  tempo             1204025 non-null  float64\n 20  duration_ms       1204025 non-null  int64  \n 21  time_signature    1204025 non-null  float64\n 22  year              1204025 non-null  int64  \n 23  release_date      1204025 non-null  object \ndtypes: bool(1), float64(10), int64(6), object(7)\nmemory usage: 212.4+ MB\n\n\ndf.info gives us the number of entries, in our case 1,204,025, columns 24, number of non-null entries for each column (in our case same as number of entries, so no NULL values), and the datatype for each column.\nIt is vital to ensure that the data is in the correct format for our analytics project. Off the bat, I can see that the year and release_date datatypes will need to be converted to a date type.\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      track_number\n      disc_number\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n    \n  \n  \n    \n      count\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n      1.204025e+06\n    \n    \n      mean\n      7.656352e+00\n      1.055906e+00\n      4.930565e-01\n      5.095363e-01\n      5.194151e+00\n      -1.180870e+01\n      6.714595e-01\n      8.438219e-02\n      4.467511e-01\n      2.828605e-01\n      2.015994e-01\n      4.279866e-01\n      1.176344e+02\n      2.488399e+05\n      3.832494e+00\n      2.007328e+03\n    \n    \n      std\n      5.994977e+00\n      2.953752e-01\n      1.896694e-01\n      2.946839e-01\n      3.536731e+00\n      6.982132e+00\n      4.696827e-01\n      1.159914e-01\n      3.852014e-01\n      3.762844e-01\n      1.804591e-01\n      2.704846e-01\n      3.093705e+01\n      1.622104e+05\n      5.611826e-01\n      1.210117e+01\n    \n    \n      min\n      1.000000e+00\n      1.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      -6.000000e+01\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      1.000000e+03\n      0.000000e+00\n      0.000000e+00\n    \n    \n      25%\n      3.000000e+00\n      1.000000e+00\n      3.560000e-01\n      2.520000e-01\n      2.000000e+00\n      -1.525400e+01\n      0.000000e+00\n      3.510000e-02\n      3.760000e-02\n      7.600000e-06\n      9.680000e-02\n      1.910000e-01\n      9.405400e+01\n      1.740900e+05\n      4.000000e+00\n      2.002000e+03\n    \n    \n      50%\n      7.000000e+00\n      1.000000e+00\n      5.010000e-01\n      5.240000e-01\n      5.000000e+00\n      -9.791000e+00\n      1.000000e+00\n      4.460000e-02\n      3.890000e-01\n      8.080000e-03\n      1.250000e-01\n      4.030000e-01\n      1.167260e+02\n      2.243390e+05\n      4.000000e+00\n      2.009000e+03\n    \n    \n      75%\n      1.000000e+01\n      1.000000e+00\n      6.330000e-01\n      7.660000e-01\n      8.000000e+00\n      -6.717000e+00\n      1.000000e+00\n      7.230000e-02\n      8.610000e-01\n      7.190000e-01\n      2.450000e-01\n      6.440000e-01\n      1.370460e+02\n      2.858400e+05\n      4.000000e+00\n      2.015000e+03\n    \n    \n      max\n      5.000000e+01\n      1.300000e+01\n      1.000000e+00\n      1.000000e+00\n      1.100000e+01\n      7.234000e+00\n      1.000000e+00\n      9.690000e-01\n      9.960000e-01\n      1.000000e+00\n      1.000000e+00\n      1.000000e+00\n      2.489340e+02\n      6.061090e+06\n      5.000000e+00\n      2.020000e+03\n    \n  \n\n\n\n\nThis gives us a very useful overview of our dataset and can highlight anomalies worthy of further investigation. min and max in particular allow us to make a very quick sense check of the range of the data, and might unveil potential outliers.\nTake a look at our year column - the range of values are 0 to 2020. So, it looks like imputed values have been used where information was not available. This may cause problems later when we attempt to convert the datatype for year which is currently int64.\nWe can use .iloc to access a group of rows and columns by index or .loc to access a group of rows and columns by name :\n\ndf.loc[df['year'] == 0 ]\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      815351\n      035h5flqzwF6I5CTfsdHPA\n      Jimmy Neutron\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      1\n      1\n      False\n      0.795\n      ...\n      0.0519\n      0.01560\n      0.439\n      0.0860\n      0.389\n      109.985\n      183000\n      4.0\n      0\n      0000\n    \n    \n      815352\n      49x05fLGDKCsCUA7CG0VpY\n      I Luv You\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      2\n      1\n      False\n      0.762\n      ...\n      0.0950\n      0.88700\n      0.909\n      0.1060\n      0.728\n      92.962\n      145161\n      4.0\n      0\n      0000\n    \n    \n      815353\n      4mNLlSoZOqoPauBAF3bIpx\n      My Heart\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      3\n      1\n      False\n      0.671\n      ...\n      0.0662\n      0.00956\n      0.902\n      0.0455\n      0.893\n      97.865\n      176561\n      4.0\n      0\n      0000\n    \n    \n      815354\n      7w5iwI0wnIiopbCFNe1Txo\n      I Am (Invincible)\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      4\n      1\n      False\n      0.759\n      ...\n      0.1280\n      0.00544\n      0.895\n      0.0538\n      0.537\n      89.989\n      192000\n      4.0\n      0\n      0000\n    \n    \n      815355\n      2Tfy2R2uiWVwxHQUT6oGNp\n      Flower Power\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      5\n      1\n      False\n      0.657\n      ...\n      0.2810\n      0.01800\n      0.245\n      0.2410\n      0.964\n      179.904\n      138666\n      4.0\n      0\n      0000\n    \n    \n      815356\n      05cTbSPQyha6z7opYwH67O\n      Heard It Low\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      6\n      1\n      False\n      0.728\n      ...\n      0.0673\n      0.00785\n      0.275\n      0.0865\n      0.662\n      90.010\n      138667\n      4.0\n      0\n      0000\n    \n    \n      815357\n      1fYK5xB8csOXVEqApkzzm0\n      Hangin On\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      7\n      1\n      False\n      0.822\n      ...\n      0.0758\n      0.11500\n      0.881\n      0.1210\n      0.766\n      119.998\n      142620\n      4.0\n      0\n      0000\n    \n    \n      815358\n      4G51c7cWzB6CLaRq9sYj2w\n      God Loves You\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      8\n      1\n      False\n      0.845\n      ...\n      0.0662\n      0.00274\n      0.548\n      0.0393\n      0.472\n      120.090\n      161000\n      4.0\n      0\n      0000\n    \n    \n      815359\n      45fcUAjXlzDxTwSzoUaO6l\n      You In My Life\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      9\n      1\n      False\n      0.957\n      ...\n      0.0623\n      0.13300\n      0.857\n      0.0968\n      0.258\n      112.987\n      214867\n      4.0\n      0\n      0000\n    \n    \n      815360\n      35TcKSN5hsGcZLrFPkUvIv\n      I Wonder\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      10\n      1\n      False\n      0.659\n      ...\n      0.0581\n      0.00196\n      0.854\n      0.3710\n      0.877\n      146.020\n      180822\n      4.0\n      0\n      0000\n    \n  \n\n10 rows × 24 columns\n\n\n\nSo, the tracks from the album Optimism 2 by the artist iCizzle have been given a year value of 0. A quick internet search and we can see the year should be 2018. Now that we have located our anomalies, we can update these values using .loc :\n\ndf.loc[815351:815360,'year'] = 2018\n\nLet’s check that worked :\n\ndf.loc[815351:815360] \n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      815351\n      035h5flqzwF6I5CTfsdHPA\n      Jimmy Neutron\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      1\n      1\n      False\n      0.795\n      ...\n      0.0519\n      0.01560\n      0.439\n      0.0860\n      0.389\n      109.985\n      183000\n      4.0\n      2018\n      0000\n    \n    \n      815352\n      49x05fLGDKCsCUA7CG0VpY\n      I Luv You\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      2\n      1\n      False\n      0.762\n      ...\n      0.0950\n      0.88700\n      0.909\n      0.1060\n      0.728\n      92.962\n      145161\n      4.0\n      2018\n      0000\n    \n    \n      815353\n      4mNLlSoZOqoPauBAF3bIpx\n      My Heart\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      3\n      1\n      False\n      0.671\n      ...\n      0.0662\n      0.00956\n      0.902\n      0.0455\n      0.893\n      97.865\n      176561\n      4.0\n      2018\n      0000\n    \n    \n      815354\n      7w5iwI0wnIiopbCFNe1Txo\n      I Am (Invincible)\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      4\n      1\n      False\n      0.759\n      ...\n      0.1280\n      0.00544\n      0.895\n      0.0538\n      0.537\n      89.989\n      192000\n      4.0\n      2018\n      0000\n    \n    \n      815355\n      2Tfy2R2uiWVwxHQUT6oGNp\n      Flower Power\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      5\n      1\n      False\n      0.657\n      ...\n      0.2810\n      0.01800\n      0.245\n      0.2410\n      0.964\n      179.904\n      138666\n      4.0\n      2018\n      0000\n    \n    \n      815356\n      05cTbSPQyha6z7opYwH67O\n      Heard It Low\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      6\n      1\n      False\n      0.728\n      ...\n      0.0673\n      0.00785\n      0.275\n      0.0865\n      0.662\n      90.010\n      138667\n      4.0\n      2018\n      0000\n    \n    \n      815357\n      1fYK5xB8csOXVEqApkzzm0\n      Hangin On\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      7\n      1\n      False\n      0.822\n      ...\n      0.0758\n      0.11500\n      0.881\n      0.1210\n      0.766\n      119.998\n      142620\n      4.0\n      2018\n      0000\n    \n    \n      815358\n      4G51c7cWzB6CLaRq9sYj2w\n      God Loves You\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      8\n      1\n      False\n      0.845\n      ...\n      0.0662\n      0.00274\n      0.548\n      0.0393\n      0.472\n      120.090\n      161000\n      4.0\n      2018\n      0000\n    \n    \n      815359\n      45fcUAjXlzDxTwSzoUaO6l\n      You In My Life\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      9\n      1\n      False\n      0.957\n      ...\n      0.0623\n      0.13300\n      0.857\n      0.0968\n      0.258\n      112.987\n      214867\n      4.0\n      2018\n      0000\n    \n    \n      815360\n      35TcKSN5hsGcZLrFPkUvIv\n      I Wonder\n      Optimism 2\n      211vSdhxt58A943r9QWRKo\n      ['iCizzle']\n      ['7arv4matK2uKJrdtPSxU4i']\n      10\n      1\n      False\n      0.659\n      ...\n      0.0581\n      0.00196\n      0.854\n      0.3710\n      0.877\n      146.020\n      180822\n      4.0\n      2018\n      0000\n    \n  \n\n10 rows × 24 columns\n\n\n\nWe have successfully updated the release year to 2018. Let’s check the range of dates once more :\n\ndf.year.describe()\n\ncount    1.204025e+06\nmean     2.007345e+03\nstd      1.062889e+01\nmin      1.900000e+03\n25%      2.002000e+03\n50%      2.009000e+03\n75%      2.015000e+03\nmax      2.020000e+03\nName: year, dtype: float64\n\n\nSo, the minimum year is now 1900. Again, this seems like it might be another imputed value. Let’s check :\n\ndf.loc[df['year'] == 1900 ]\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      450071\n      3xPatAieFSuGIuQfHMDvSw\n      Arabian Waltz\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      1\n      1\n      False\n      0.533\n      ...\n      0.0576\n      0.875\n      0.859\n      0.0887\n      0.8350\n      115.746\n      493867\n      3.0\n      1900\n      1900-01-01\n    \n    \n      450072\n      5vpx0WtYVtKOFu4V65NkUi\n      Dreams Of A Dying City\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      2\n      1\n      False\n      0.476\n      ...\n      0.0334\n      0.843\n      0.893\n      0.1060\n      0.5710\n      92.340\n      730667\n      1.0\n      1900\n      1900-01-01\n    \n    \n      450073\n      0G7vBbeWCcRISsHwcivFgl\n      Ornette Never Sleeps\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      3\n      1\n      False\n      0.605\n      ...\n      0.0457\n      0.912\n      0.693\n      0.1170\n      0.7250\n      139.820\n      421760\n      4.0\n      1900\n      1900-01-01\n    \n    \n      450074\n      6YjrfDT2TPp6pflsCSBHPH\n      Georgina\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      4\n      1\n      False\n      0.406\n      ...\n      0.0433\n      0.849\n      0.866\n      0.1020\n      0.6200\n      93.729\n      672707\n      4.0\n      1900\n      1900-01-01\n    \n    \n      450075\n      2Nq317w5G1gmuhilTCiiqR\n      No Visa\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      5\n      1\n      False\n      0.577\n      ...\n      0.0430\n      0.936\n      0.865\n      0.0999\n      0.5010\n      96.415\n      601027\n      4.0\n      1900\n      1900-01-01\n    \n    \n      450076\n      6PzeE7vvynVguz04STK6RL\n      The Pain After\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      6\n      1\n      False\n      0.291\n      ...\n      0.0477\n      0.956\n      0.939\n      0.1460\n      0.0959\n      71.087\n      566840\n      3.0\n      1900\n      1900-01-01\n    \n    \n      459980\n      4DZ63H1bRMmiTcXiQhERxv\n      Catania\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      1\n      1\n      False\n      0.465\n      ...\n      0.0742\n      0.414\n      0.089\n      0.0936\n      0.3790\n      163.939\n      465000\n      5.0\n      1900\n      1900-01-01\n    \n    \n      459981\n      6QqZn286ICbbhTjNBPlgNY\n      Nashwa\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      2\n      1\n      False\n      0.436\n      ...\n      0.0683\n      0.802\n      0.758\n      0.1070\n      0.1580\n      171.006\n      578000\n      5.0\n      1900\n      1900-01-01\n    \n    \n      459982\n      5Mw5YkQkHuGqYQL5XMrUOI\n      An Evening With Jerry\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      3\n      1\n      False\n      0.511\n      ...\n      0.0350\n      0.211\n      0.550\n      0.1900\n      0.2530\n      144.884\n      423000\n      4.0\n      1900\n      1900-01-01\n    \n    \n      459983\n      4SNCi2xa3dkM0HPTQ1AFBP\n      When The Lights Go Out\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      4\n      1\n      False\n      0.414\n      ...\n      0.0602\n      0.826\n      0.880\n      0.1180\n      0.0783\n      154.842\n      433960\n      4.0\n      1900\n      1900-01-01\n    \n    \n      459984\n      1XEbjKZygiDllK5WpEB73O\n      Story Teller\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      5\n      1\n      False\n      0.595\n      ...\n      0.0458\n      0.537\n      0.658\n      0.3540\n      0.3370\n      109.885\n      532173\n      4.0\n      1900\n      1900-01-01\n    \n    \n      459985\n      5V4pmHLdq0fhEw5DjkaW2w\n      Ornette Never Sleps\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      6\n      1\n      False\n      0.470\n      ...\n      0.0569\n      0.593\n      0.914\n      0.1050\n      0.8140\n      158.412\n      243867\n      3.0\n      1900\n      1900-01-01\n    \n    \n      459986\n      6BeB08lGiB6zd8Fn7BBhb1\n      Nadim\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      7\n      1\n      False\n      0.474\n      ...\n      0.0532\n      0.401\n      0.561\n      0.0785\n      0.3010\n      162.807\n      513000\n      4.0\n      1900\n      1900-01-01\n    \n    \n      459987\n      1WRapjF1HuE2rXUVBGKXXt\n      Wishing Well\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      8\n      1\n      False\n      0.521\n      ...\n      0.0594\n      0.828\n      0.792\n      0.1040\n      0.2420\n      127.288\n      325000\n      4.0\n      1900\n      1900-01-01\n    \n  \n\n14 rows × 24 columns\n\n\n\nSo, the tracks with a year value of 1900 all relate to the artist Rabih Abou-Khalil. Another quick internet search and we can see the actual year for the album Al-Jadida is 1991 and for the album Arabian Waltz is 1996.\nLet’s update these :\n\ndf.loc[450071:450076,'year'] = 1996\ndf.loc[459980:459987,'year'] = 1991\n\nand check that’s worked :\n\ndf.loc[450071:450076]\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      450071\n      3xPatAieFSuGIuQfHMDvSw\n      Arabian Waltz\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      1\n      1\n      False\n      0.533\n      ...\n      0.0576\n      0.875\n      0.859\n      0.0887\n      0.8350\n      115.746\n      493867\n      3.0\n      1996\n      1900-01-01\n    \n    \n      450072\n      5vpx0WtYVtKOFu4V65NkUi\n      Dreams Of A Dying City\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      2\n      1\n      False\n      0.476\n      ...\n      0.0334\n      0.843\n      0.893\n      0.1060\n      0.5710\n      92.340\n      730667\n      1.0\n      1996\n      1900-01-01\n    \n    \n      450073\n      0G7vBbeWCcRISsHwcivFgl\n      Ornette Never Sleeps\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      3\n      1\n      False\n      0.605\n      ...\n      0.0457\n      0.912\n      0.693\n      0.1170\n      0.7250\n      139.820\n      421760\n      4.0\n      1996\n      1900-01-01\n    \n    \n      450074\n      6YjrfDT2TPp6pflsCSBHPH\n      Georgina\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      4\n      1\n      False\n      0.406\n      ...\n      0.0433\n      0.849\n      0.866\n      0.1020\n      0.6200\n      93.729\n      672707\n      4.0\n      1996\n      1900-01-01\n    \n    \n      450075\n      2Nq317w5G1gmuhilTCiiqR\n      No Visa\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      5\n      1\n      False\n      0.577\n      ...\n      0.0430\n      0.936\n      0.865\n      0.0999\n      0.5010\n      96.415\n      601027\n      4.0\n      1996\n      1900-01-01\n    \n    \n      450076\n      6PzeE7vvynVguz04STK6RL\n      The Pain After\n      Arabian Waltz\n      1ggHQJ48NFfYhGu6VznK8K\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      6\n      1\n      False\n      0.291\n      ...\n      0.0477\n      0.956\n      0.939\n      0.1460\n      0.0959\n      71.087\n      566840\n      3.0\n      1996\n      1900-01-01\n    \n  \n\n6 rows × 24 columns\n\n\n\n\ndf.loc[459980:459987]\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      459980\n      4DZ63H1bRMmiTcXiQhERxv\n      Catania\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      1\n      1\n      False\n      0.465\n      ...\n      0.0742\n      0.414\n      0.089\n      0.0936\n      0.3790\n      163.939\n      465000\n      5.0\n      1991\n      1900-01-01\n    \n    \n      459981\n      6QqZn286ICbbhTjNBPlgNY\n      Nashwa\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      2\n      1\n      False\n      0.436\n      ...\n      0.0683\n      0.802\n      0.758\n      0.1070\n      0.1580\n      171.006\n      578000\n      5.0\n      1991\n      1900-01-01\n    \n    \n      459982\n      5Mw5YkQkHuGqYQL5XMrUOI\n      An Evening With Jerry\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      3\n      1\n      False\n      0.511\n      ...\n      0.0350\n      0.211\n      0.550\n      0.1900\n      0.2530\n      144.884\n      423000\n      4.0\n      1991\n      1900-01-01\n    \n    \n      459983\n      4SNCi2xa3dkM0HPTQ1AFBP\n      When The Lights Go Out\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      4\n      1\n      False\n      0.414\n      ...\n      0.0602\n      0.826\n      0.880\n      0.1180\n      0.0783\n      154.842\n      433960\n      4.0\n      1991\n      1900-01-01\n    \n    \n      459984\n      1XEbjKZygiDllK5WpEB73O\n      Story Teller\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      5\n      1\n      False\n      0.595\n      ...\n      0.0458\n      0.537\n      0.658\n      0.3540\n      0.3370\n      109.885\n      532173\n      4.0\n      1991\n      1900-01-01\n    \n    \n      459985\n      5V4pmHLdq0fhEw5DjkaW2w\n      Ornette Never Sleps\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      6\n      1\n      False\n      0.470\n      ...\n      0.0569\n      0.593\n      0.914\n      0.1050\n      0.8140\n      158.412\n      243867\n      3.0\n      1991\n      1900-01-01\n    \n    \n      459986\n      6BeB08lGiB6zd8Fn7BBhb1\n      Nadim\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      7\n      1\n      False\n      0.474\n      ...\n      0.0532\n      0.401\n      0.561\n      0.0785\n      0.3010\n      162.807\n      513000\n      4.0\n      1991\n      1900-01-01\n    \n    \n      459987\n      1WRapjF1HuE2rXUVBGKXXt\n      Wishing Well\n      Al-Jadida\n      2T6FKoeG7EXR0WAsFyXbSq\n      ['Rabih Abou-Khalil']\n      ['7cM9Y2LNnnmmqivaEuH8vT']\n      8\n      1\n      False\n      0.521\n      ...\n      0.0594\n      0.828\n      0.792\n      0.1040\n      0.2420\n      127.288\n      325000\n      4.0\n      1991\n      1900-01-01\n    \n  \n\n8 rows × 24 columns\n\n\n\n\ndf.year.describe()\n\ncount    1.204025e+06\nmean     2.007346e+03\nstd      1.062270e+01\nmin      1.908000e+03\n25%      2.002000e+03\n50%      2.009000e+03\n75%      2.015000e+03\nmax      2.020000e+03\nName: year, dtype: float64\n\n\nThe minimum year is now 1908 which seems plausible :\n\ndf.loc[df['year'] == 1908]\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      album\n      album_id\n      artists\n      artist_ids\n      track_number\n      disc_number\n      explicit\n      danceability\n      ...\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      time_signature\n      year\n      release_date\n    \n  \n  \n    \n      358067\n      2WXXkuoiDuZlyC4vAJUk4U\n      Hard Times\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['A.C. Reed']\n      ['1i31XKLddtEeOIr0nPcxdj']\n      1\n      1\n      False\n      0.708\n      ...\n      0.0589\n      0.6930\n      0.000354\n      0.0700\n      0.774\n      88.159\n      198533\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358068\n      1GUvbwCftGCU9HTeg1DPAW\n      She's Fine\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['A.C. Reed']\n      ['1i31XKLddtEeOIr0nPcxdj']\n      2\n      1\n      False\n      0.501\n      ...\n      0.0372\n      0.2020\n      0.001450\n      0.1070\n      0.868\n      82.489\n      258227\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358069\n      1Z3cZzxa2ulQSnqoPxp9oM\n      Moving Out Of The Ghetto\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['A.C. Reed']\n      ['1i31XKLddtEeOIr0nPcxdj']\n      3\n      1\n      False\n      0.755\n      ...\n      0.0784\n      0.4030\n      0.001180\n      0.1780\n      0.869\n      102.780\n      233733\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358070\n      44Ag9ocysgC0TYZWQ8q2YD\n      Going To New York\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['A.C. Reed']\n      ['1i31XKLddtEeOIr0nPcxdj']\n      4\n      1\n      False\n      0.707\n      ...\n      0.0471\n      0.3480\n      0.000081\n      0.3100\n      0.919\n      110.260\n      219173\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358071\n      3SDq5YWtxDUS05jNM1YDHk\n      Big Leg Woman\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Scotty And The Rib Tips']\n      ['1P2BhUJ1N1bRIF52GZiJFS']\n      5\n      1\n      False\n      0.673\n      ...\n      0.0637\n      0.3690\n      0.001570\n      0.0359\n      0.843\n      94.547\n      221400\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358072\n      0nJeoE8gNObc99KLYjcGSO\n      Careless With Our Love\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Scotty And The Rib Tips']\n      ['1P2BhUJ1N1bRIF52GZiJFS']\n      6\n      1\n      False\n      0.505\n      ...\n      0.0713\n      0.1960\n      0.000005\n      0.0613\n      0.456\n      202.935\n      182733\n      3.0\n      1908\n      1908-08-01\n    \n    \n      358073\n      4iwPEc7B6Jdnm9yBCbRbHi\n      Road Block\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Scotty And The Rib Tips']\n      ['1P2BhUJ1N1bRIF52GZiJFS']\n      7\n      1\n      False\n      0.716\n      ...\n      0.0979\n      0.4400\n      0.002170\n      0.3640\n      0.764\n      113.089\n      169733\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358074\n      27gACliKLkeFZoYwrCzEM0\n      Poison Ivy\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Scotty And The Rib Tips']\n      ['1P2BhUJ1N1bRIF52GZiJFS']\n      8\n      1\n      False\n      0.749\n      ...\n      0.0589\n      0.5290\n      0.000372\n      0.0603\n      0.773\n      105.896\n      197467\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358075\n      2EpGTGT25A1o6p4q4dLOHN\n      I Dare You\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lovie Lee']\n      ['6cOz9TMiL8lfsFoWkxvqKM']\n      9\n      1\n      False\n      0.478\n      ...\n      0.0656\n      0.7300\n      0.000020\n      0.3310\n      0.688\n      155.212\n      168200\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358076\n      1BHivexEpJ8inJqoBZyOQ0\n      Nobody Knows My Troubles\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lovie Lee']\n      ['6cOz9TMiL8lfsFoWkxvqKM']\n      10\n      1\n      False\n      0.433\n      ...\n      0.0493\n      0.7210\n      0.000083\n      0.0646\n      0.366\n      177.106\n      318627\n      3.0\n      1908\n      1908-08-01\n    \n    \n      358077\n      73c2SKi5JPvRf7Exzf3hvz\n      Sweet Little Girl\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lovie Lee']\n      ['6cOz9TMiL8lfsFoWkxvqKM']\n      11\n      1\n      False\n      0.568\n      ...\n      0.0654\n      0.6880\n      0.000000\n      0.1480\n      0.857\n      133.396\n      193933\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358078\n      6XW31kg7cuN17LzgHj1pzM\n      Naptown\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lovie Lee']\n      ['6cOz9TMiL8lfsFoWkxvqKM']\n      12\n      1\n      False\n      0.574\n      ...\n      0.0785\n      0.5130\n      0.000000\n      0.0795\n      0.891\n      136.918\n      185000\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358079\n      4bdbkXLaoccDNp6lLsZWRG\n      Drown In My Own Tears\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lacy Gibson']\n      ['54sySc5ynnkqxkG2dEMLQe']\n      13\n      1\n      False\n      0.639\n      ...\n      0.0753\n      0.4260\n      0.000584\n      0.1240\n      0.488\n      97.814\n      280573\n      1.0\n      1908\n      1908-08-01\n    \n    \n      358080\n      5YuV9oboI6FNhj45w15Bn2\n      Crying For My Baby\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lacy Gibson']\n      ['54sySc5ynnkqxkG2dEMLQe']\n      14\n      1\n      False\n      0.432\n      ...\n      0.1030\n      0.1080\n      0.000752\n      0.3420\n      0.267\n      173.133\n      168893\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358081\n      0FwueIBJWE6EGlv2ipQGpv\n      Feel So Bad\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lacy Gibson']\n      ['54sySc5ynnkqxkG2dEMLQe']\n      15\n      1\n      False\n      0.507\n      ...\n      0.0380\n      0.0637\n      0.014600\n      0.5270\n      0.795\n      158.532\n      234640\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358082\n      5zkdTRxprxh88V9nbNedlf\n      Wish Me Well\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Lacy Gibson']\n      ['54sySc5ynnkqxkG2dEMLQe']\n      16\n      1\n      False\n      0.628\n      ...\n      0.0745\n      0.4820\n      0.086400\n      0.0704\n      0.607\n      137.612\n      177627\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358083\n      4yS6s6PM2i9Q88Jo64gdQf\n      Have You Ever Loved A Woman\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Sons of the Blues']\n      ['5of0zoRrZzeThokJuAPbgq']\n      17\n      1\n      False\n      0.513\n      ...\n      0.0455\n      0.3690\n      0.013000\n      0.5120\n      0.268\n      151.070\n      370067\n      3.0\n      1908\n      1908-08-01\n    \n    \n      358084\n      77CZxGGtBGywPx3Mlak3ji\n      Berlin Wall\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Sons of the Blues']\n      ['5of0zoRrZzeThokJuAPbgq']\n      18\n      1\n      False\n      0.738\n      ...\n      0.1990\n      0.3180\n      0.000289\n      0.0868\n      0.600\n      123.120\n      265640\n      4.0\n      1908\n      1908-08-01\n    \n    \n      358085\n      7nPwFsty5ABGNCsZHuj3b0\n      Prisoner Of The Blues\n      Living Chicago Blues, Vol. 3\n      6l9iPFb3IBKZxrCwphkvH4\n      ['Sons of the Blues']\n      ['5of0zoRrZzeThokJuAPbgq']\n      19\n      1\n      False\n      0.654\n      ...\n      0.1440\n      0.2090\n      0.015400\n      0.0862\n      0.390\n      160.127\n      253667\n      3.0\n      1908\n      1908-08-01\n    \n  \n\n19 rows × 24 columns\n\n\n\nOK, well let’s perform some final tidy up before we bake the data wrangling into a Python script.\n\ndf = df.drop(['id', 'album_id', 'artist_ids', 'track_number', 'disc_number', 'time_signature'], axis=1)\ndf['artists'] = df['artists'].str.strip(\"['']\")\ndf['danceability'] = df['danceability'].round(2)\ndf['energy'] = df['energy'].round(2)\ndf['loudness'] = df['loudness'].round(2)\ndf['speechiness'] = df['speechiness'].round(2)\ndf['acousticness'] = df['acousticness'].round(2)\ndf['instrumentalness'] = df['instrumentalness'].round(2)\ndf['liveness'] = df['liveness'].round(2)\ndf['valence'] = df['valence'].round(2)\ndf[\"tempo\"] = df[\"tempo\"].astype(int)\ndf['year'] = df['year'].astype(str)\ndf[\"duration_s\"] = (df[\"duration_ms\"] / 1000).astype(int).round(0)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      name\n      album\n      artists\n      explicit\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      duration_ms\n      year\n      release_date\n      duration_s\n    \n  \n  \n    \n      0\n      Testify\n      The Battle Of Los Angeles\n      Rage Against The Machine\n      False\n      0.47\n      0.98\n      7\n      -5.40\n      1\n      0.07\n      0.03\n      0.0\n      0.36\n      0.50\n      117\n      210133\n      1999\n      1999-11-02\n      210\n    \n    \n      1\n      Guerrilla Radio\n      The Battle Of Los Angeles\n      Rage Against The Machine\n      True\n      0.60\n      0.96\n      11\n      -5.76\n      1\n      0.19\n      0.01\n      0.0\n      0.16\n      0.49\n      103\n      206200\n      1999\n      1999-11-02\n      206\n    \n    \n      2\n      Calm Like a Bomb\n      The Battle Of Los Angeles\n      Rage Against The Machine\n      False\n      0.32\n      0.97\n      7\n      -5.42\n      1\n      0.48\n      0.02\n      0.0\n      0.12\n      0.37\n      149\n      298893\n      1999\n      1999-11-02\n      298\n    \n    \n      3\n      Mic Check\n      The Battle Of Los Angeles\n      Rage Against The Machine\n      True\n      0.44\n      0.97\n      11\n      -5.83\n      0\n      0.24\n      0.16\n      0.0\n      0.12\n      0.57\n      96\n      213640\n      1999\n      1999-11-02\n      213\n    \n    \n      4\n      Sleep Now In the Fire\n      The Battle Of Los Angeles\n      Rage Against The Machine\n      False\n      0.43\n      0.93\n      2\n      -6.73\n      1\n      0.07\n      0.00\n      0.1\n      0.08\n      0.54\n      127\n      205600\n      1999\n      1999-11-02\n      205\n    \n  \n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1204025 entries, 0 to 1204024\nData columns (total 19 columns):\n #   Column            Non-Null Count    Dtype  \n---  ------            --------------    -----  \n 0   name              1204025 non-null  object \n 1   album             1204025 non-null  object \n 2   artists           1204025 non-null  object \n 3   explicit          1204025 non-null  bool   \n 4   danceability      1204025 non-null  float64\n 5   energy            1204025 non-null  float64\n 6   key               1204025 non-null  int64  \n 7   loudness          1204025 non-null  float64\n 8   mode              1204025 non-null  int64  \n 9   speechiness       1204025 non-null  float64\n 10  acousticness      1204025 non-null  float64\n 11  instrumentalness  1204025 non-null  float64\n 12  liveness          1204025 non-null  float64\n 13  valence           1204025 non-null  float64\n 14  tempo             1204025 non-null  int64  \n 15  duration_ms       1204025 non-null  int64  \n 16  year              1204025 non-null  object \n 17  release_date      1204025 non-null  object \n 18  duration_s        1204025 non-null  int64  \ndtypes: bool(1), float64(8), int64(5), object(5)\nmemory usage: 166.5+ MB\n\n\n\n\n\nI decided to use the Prefect work orchestration tool to streamline my data flows. Prefect is a modern open source dataflow automation platform that will allow us to add observability and orchestration by utilizing python to write tasks and flows decorators to build, run and monitor pipelines at scale. We can also make use of Prefect’s block connectors which allows communication with Google Cloud services.\nAgain, in terms of reproducability of this project, the asssumption is that you have already followed the inital Prefect set up. A brief overview of the process is included below :\nClone the Prefect repo from the command line:\ngit clone https://github.com/discdiver/prefect-zoomcamp.git\nNext, create a python environment :\nconda create -n zoomcamp python=3.9   \nOnce created we need to activate it:\nconda activate zoomcamp\nTo deactivate an environment use:\nconda deactivate  \nNote from the terminal that we are no longer running in base but our newly created zoomcamp environment. Then install all package dependencies with:\npip install -r requirements.txt\nFor more detailed coverage see attached.\nNote, that I had also configured my GCP Credentials and Google Cloud Storage Prefect connector blocks during week 2 of the course :\n\n\n\ngcp_cred_block.PNG\n\n\n\n\n\ngcs_bucket_block.PNG\n\n\nThe basic config template is included below for reference :\n\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n# alternative to creating GCP blocks in the UI\n# copy your own service_account_info dictionary from the json file you downloaded from google\n# IMPORTANT - do not store credentials in a publicly available repository!\n\n\ncredentials_block = GcpCredentials(\n    service_account_info={}  # enter your credentials from the json file\n)\ncredentials_block.save(\"zoom-gcp-creds\", overwrite=True)\n\n\nbucket_block = GcsBucket(\n    gcp_credentials=GcpCredentials.load(\"zoom-gcp-creds\"),\n    bucket=\"prefect-de-zoomcamp\",  # insert your  GCS bucket name\n)\n\nbucket_block.save(\"zoom-gcs\", overwrite=True)\n\nOnce you are ready to run your flows you can make use of the Prefect UI to visualise the flows by entering the following at the command line:\nprefect orion start\nand then navigating to the dashboard at http://127.0.0.1:4200\nI created the following Python script to grab the local csv, clean up using pandas, convert to parquet, and upload to Google Cloud Storage. I ran the file from the command line using :\npython etl_web_to_gcs.py\netl_web_to_gcs.py\n\nfrom pathlib import Path\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect_gcp.cloud_storage import GcsBucket\nimport pyarrow as pa\nfrom random import randint\n\n\n@task(retries=3)\ndef fetch(dataset_url: str) -> pd.DataFrame:\n    \"\"\"Read data into pandas DataFrame\"\"\"\n    df = pd.read_csv(dataset_url)\n    return df\n\n\n@task(log_prints=True)\ndef clean(df: pd.DataFrame) -> pd.DataFrame:\n   \"\"\"Some pandas transforms and print basic info\"\"\"\n   df = df.drop(['id', 'album_id', 'artist_ids', 'track_number', 'disc_number', 'time_signature'], axis=1)\n   df.loc[815351:815360,'year'] = 2018\n   df.loc[450071:450076,'year'] = 1996\n   df.loc[459980:459987,'year'] = 1991\n   df['artists'] = df['artists'].str.strip(\"['']\")\n   df['danceability'] = df['danceability'].round(2)\n   df['energy'] = df['energy'].round(2)\n   df['loudness'] = df['loudness'].round(2)\n   df['speechiness'] = df['speechiness'].round(2)\n   df['acousticness'] = df['acousticness'].round(2)\n   df['instrumentalness'] = df['instrumentalness'].round(2)\n   df['liveness'] = df['liveness'].round(2)\n   df['valence'] = df['valence'].round(2)\n   df[\"tempo\"] = df[\"tempo\"].astype(int)\n   df['year_date'] = pd.to_datetime(df['year'], format='%Y')\n   df[\"duration_s\"] = (df[\"duration_ms\"] / 1000).astype(int).round(0)\n\n   print(df.head(2))\n   print(f\"columns: {df.dtypes}\")\n   print(f\"rows: {len(df)}\")\n   return df\n\n\n@task()\ndef write_local(df: pd.DataFrame, dataset_file: str) -> Path:\n   \"\"\"Write DataFrame out locally as parquet file\"\"\"\n   path = Path(f\"data/{dataset_file}.parquet\")\n   df.to_parquet(path, compression=\"gzip\")\n   return path\n\n\n@task()\ndef write_gcs(path: Path) -> None:\n    \"\"\"Upload local parquet file to GCS\"\"\"\n    gcs_block = GcsBucket.load(\"de-zoomcamp\")\n    gcs_block.upload_from_path(from_path=path, to_path=path)\n    return\n\n\n@flow()\ndef etl_web_to_gcs() -> None:\n    \"\"\"The main ETL function\"\"\"\n    dataset_file = \"spotify\"\n    dataset_url = \"data/spotify.csv\"\n    df = fetch(dataset_url)\n    df_clean = clean(df)\n    path = write_local(df_clean,dataset_file)\n    write_gcs(path)\n    \nif __name__ == \"__main__\":\n    etl_web_to_gcs()\n\n\n\n\nprefect_etl_web_to_gcs_1.PNG\n\n\n\n\n\nprefect_etl_web_to_gcs_2.PNG\n\n\nThat has completed successfully. The parquet file has been uploaded to our data lake :\n\n\n\nbucket.PNG\n\n\nI created the following Python script to take the parquet file from Google Cloud Storage and write to BigQuery as a table, and ran the file from the command line using :\npython etl_web_to_gcs.py\netl_gcs_to_bq.py\n\nfrom pathlib import Path\nimport pandas as pd\nfrom prefect import flow, task\nfrom prefect_gcp.cloud_storage import GcsBucket\nfrom prefect_gcp import GcpCredentials\n\n\n@task(retries=3)\ndef extract_from_gcs() -> Path:\n    \"\"\"Download data from GCS\"\"\"\n    gcs_path = Path(f\"data/spotify.parquet\")\n    gcs_block = GcsBucket.load(\"de-zoomcamp\")\n    gcs_block.get_directory(from_path=gcs_path, local_path=f\"./data/\")\n    return Path(f\"{gcs_path}\")\n\n@task()\ndef transform(path: Path) -> pd.DataFrame:\n    \"\"\"Print some basic info\"\"\"\n    df = pd.read_parquet(path)\n    print(df.head(2))\n    print(f\"columns: {df.dtypes}\")\n    print(f\"rows: {len(df)}\")\n    return df\n \n\n@task()\ndef write_bq(df: pd.DataFrame) -> None:\n    \"\"\"Write DataFrame to BiqQuery\"\"\"\n\n    gcp_credentials_block = GcpCredentials.load(\"de-gcp-creds\")\n\n    df.to_gbq(\n        destination_table=\"spotify.spotify_one_point_two_million\",\n        project_id=\"de-zoomcamp-project137\",\n        credentials=gcp_credentials_block.get_credentials_from_service_account(),\n        chunksize=500_000,\n        if_exists=\"replace\",\n    )\n\n\n@flow()\ndef etl_gcs_to_bq():\n    \"\"\"Main ETL flow to load data into Big Query\"\"\"\n    path = extract_from_gcs()\n    df = transform(path)\n    write_bq(df)\n\n\nif __name__ == \"__main__\":\n    etl_gcs_to_bq()\n\n\n\n\nprefect_etl_gcs_to_bq.PNG\n\n\n\n\n\nprefect_etl_gcs_to_bq_1.PNG\n\n\nThat has also completed successfully. A table has been created in BigQuery from the data held in Google Cloud Storage :\n\n\n\nbig_query.PNG\n\n\n\n\n\n\n\nWe need to create a dedicated service account within Big Query to enable communication with dbt cloud.\n\nOpen the BigQuery credential wizard to create a service account in your project :\n\n\n\n\nbig_query_dbt.PNG\n\n\n\n\n\ndbt_service_account.PNG\n\n\n\nYou can either grant the specific roles the account will need or simply use BigQuery Admin, as you’ll be the sole user of both accounts and data.\n\nNote: if you decide to use specific roles instead of BQ Admin, some users reported that they needed to add also viewer role to avoid encountering denied access errors.\n\n\n\ndbt_service_account_grantaccess.PNG\n\n\n\nNow that the service account has been created we need to add and download a JSON key, go to the keys section, select “create new key”. Select key type JSON and once you click on CREATE it will get inmediately downloaded for you to use.\n\n\n\n\ndbt_service_account_key.PNG\n\n\n\n\n\n\nCreate a dbt cloud account from their website (free for freelance developers)\nOnce you have logged in you will be prompted to Complete Project Setup\nNaming your project - a default name Analytics is given\nChoose BigQuery as your data warehouse:\nUpload the key you downloaded from BigQuery. This will populate most fields related to the production credentials.\n\n\n\n\ndbt_project_setup.PNG\n\n\nScroll down to the end of the page, set up your development credentials, and run the connection test and hit Next:\n\n\n\ndbt_development_credentials.PNG\n\n\n\n\n\n\nSelect git clone and paste the SSH key from your repo. Then hit Import\n\n\n\n\nproject_github_repo.PNG\n\n\n\nYou will get a deploy key :\n\n\n\n\nimage.png\n\n\n\nHead to your GH repo and go to the settings tab. Under security you’ll find the menu deploy keys. Click on Add deploy key and paste the deploy key provided by dbt cloud. Make sure to tick on “write access”.\n\n\n\n\nadd_dbt_deploy_key_to_github.PNG\n\n\nFor a detailed set up guide see here.\nInitialize dbt project\nThis builds out your folder structure with example models.\nMake your initial commit by clicking Commit and sync. Use the commit message “initial commit” and click Commit. Note that the files are read-only and you have to Create branch before you can edit or add new files :\n\n\n\ndbt_initial_commit.PNG\n\n\nOnce you have created a branch you can edit and add new files. Essentially we only need three files to build our model :\ndbt.project.yml\nThe basic config below can be tailored to meet your own needs. The key fields are :\nname <spotify_dbt> This will be the name of the dataset created in BiqQuery on successful run of the transformation model\nmodels:\n<spotify_dbt> This should match the name specified above\n\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\n\nname: 'spotify_dbt' # This will be the name of the dataset dbt will create in BigQuery\nversion: '1.0.0'\nconfig-version: 2\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'default'\n\n# These configurations specify where dbt should look for different types of files.\n# The `source-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\ntarget-path: \"target\"  # directory which will store compiled SQL files\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/ directory\n# as tables. These settings can be overridden in the individual model files\n# using the `{{ config(...) }}` macro.\nmodels:\n  spotify_dbt:\n      # Applies to all files under models/.../\n      staging:\n          materialized: view\n\nschema.yml\nNote that this file also includes the source i.e. the location of the data that the transforms included in our model are to be performed on :\nname : <spotify> Choose a name for this source variable, This will be referenced in the model.sql file\ndatabase: <de-zoomcamp-project137> BigQuery project reference\n\nversion: 2\n\nsources:\n    - name: spotify # Choose a name. This will be the 'source' referred to in the \n      database: de-zoomcamp-project137 # BigQuery project reference\n      tables:\n        - name: spotify_one_point_two_million # Choose a name for the table to be created in BigQuery\n       \nmodels:\n    - name: spotify_one_point_two_million\n      description: >\n        Curated by Rodolfo Gigueroa, over 1.2 million songs downloaded from the MusicBrainz catalog and 24 track features obtained using the Spotify\n        API. \n      columns:\n          - name: id\n            description: >\n              The base-62 identifier found at the end of the Spotify URI for an artist, track, album, playlist, etc. Unlike a Spotify URI, a Spotify ID\n              does not clearly identify the type of resource; that information is provided elsewhere in the call. \n          - name: name \n            description: The name of the track                         \n          - name: album\n            description: The name of the album. In case of an album takedown, the value may be an empty string.\n          - name: album_id\n            description: The Spotify ID for the album.\n          - name: artists\n            description: The name(s) of the artist(s).\n          - name: artist_ids\n            description: The Spotify ID for the artist(s).\n          - name: track_number\n            description: The number of the track. If an album has several discs, the track number is the number on the specified disc.\n          - name: disc_number\n            description: The disc number (usually 1 unless the album consists of more than one disc).\n          - name: explicit\n            description: Whether or not the track has explicit lyrics ( true = yes it does; false = no it does not OR unknown).\n          - name: danceability\n            description: >\n             Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, \n             beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.           \n          - name: energy \n            description: >\n             Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast,\n             loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing\n             to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n          - name: key\n            description: >\n             The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key \n             was detected, the value is -1.\n          - name: loudness\n            description: >\n             The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing \n             relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength \n             (amplitude). Values typically range between -60 and 0 db.\n          - name: mode\n            description: >\n              Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented\n              by 1 and minor is 0.\n          - name: speechiness\n            description: >\n              Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, \n              poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values \n              between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap \n              music. Values below 0.33 most likely represent music and other non-speech-like tracks.         \n          - name: acousticness\n            description: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n          - name: instrumentalness\n            description: >\n             Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks \n             are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above \n             0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n          - name: liveness\n            description: >\n             Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed \n             live. A value above 0.8 provides strong likelihood that the track is live.\n          - name: valence\n            description: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n          - name: tempo\n            description: >\n             The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and \n             derives directly from the average beat duration.\n          - name: duration_ms\n            description: The duration of the track in milliseconds.\n          - name: time_signature\n            description: >\n             An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). \n             The time signature ranges from 3 to 7 indicating time signatures of \"3/4\", to \"7/4\".\n          - name: year\n            description: The year in which the track was released\n          - name: release_date\n            description: Release date of the track in the format 2023-04-17\n\nspotify_one_point_two_million.sql\nAlthough this is a .sql file this is actually our transformation model. Note the first line configuration overrides the materialized setting that I configured in my dbt.project.yml file.\nNote also the following :\nFROM {{ source('spotify', 'spotify_one_point_two_million')}}\nThis specifies that dbt will apply transformations on my dataset in BiqQuery named spotify and specifically, on the table named spotify_one_point_two_million.\n\n{{ config(materialized='table') }}\n\nSELECT \n    -- identifiers\n    name,\n    album,\n    artists,\n    explicit,\n    danceability,\n    energy,\n    {{ get_key_description('key') }} AS key_description, \n    loudness,\n    {{ get_modality_description('mode') }} AS modality_description, \n    speechiness,\n    acousticness,\n    instrumentalness,\n    liveness,\n    valence,\n    tempo,\n    duration_s,\n    year_date,\n     CASE\n        WHEN year_date BETWEEN '1900-01-01 00:00:00 UTC' AND '1909-12-31 00:00:00 UTC' THEN 'Naughts'\n        WHEN year_date BETWEEN '1910-01-01 00:00:00 UTC' AND '1919-12-31 00:00:00 UTC' THEN 'Tens'\n        WHEN year_date BETWEEN '1920-01-01 00:00:00 UTC' AND '1929-12-31 00:00:00 UTC' THEN 'Roaring Twenties'\n        WHEN year_date BETWEEN '1930-01-01 00:00:00 UTC' AND '1939-12-31 00:00:00 UTC' THEN 'Dirty Thirties'\n        WHEN year_date BETWEEN '1940-01-01 00:00:00 UTC' AND '1949-12-31 00:00:00 UTC' THEN 'Forties'\n        WHEN year_date BETWEEN '1950-01-01 00:00:00 UTC' AND '1959-12-31 00:00:00 UTC' THEN 'Fabulous Fifties'\n        WHEN year_date BETWEEN '1960-01-01 00:00:00 UTC' AND '1969-12-31 00:00:00 UTC' THEN 'Swinging Sixties'\n        WHEN year_date BETWEEN '1970-01-01 00:00:00 UTC' AND '1979-12-31 00:00:00 UTC' THEN 'Seventies'\n        WHEN year_date BETWEEN '1980-01-01 00:00:00 UTC' AND '1989-12-31 00:00:00 UTC' THEN 'Eighties'\n        WHEN year_date BETWEEN '1990-01-01 00:00:00 UTC' AND '1999-12-31 00:00:00 UTC' THEN 'Nineties'\n        WHEN year_date BETWEEN '2000-01-01 00:00:00 UTC' AND '2009-12-31 00:00:00 UTC' THEN 'Noughties'\n        WHEN year_date BETWEEN '2010-01-01 00:00:00 UTC' AND '2019-12-31 00:00:00 UTC' THEN 'Teens'\n        WHEN year_date = '2020-01-01 00:00:00 UTC' THEN '2020'\n    END AS Decade,\n    CASE\n        WHEN valence > 0.5 THEN 'Happy'\n        WHEN valence < 0.5 THEN 'Sad'\n        ELSE 'Ambivalent'\n    END AS Happy_Sad\n        \nFROM {{ source('spotify', 'spotify_one_point_two_million')}}\n\nYou might wonder what this syntax is :\n{{ get_key_description('key') }} AS key_description, \nMacros in Jinja are pieces of code that can be reused multiple times – they are analogous to “functions” in other programming languages, and are extremely useful if you find yourself repeating code across multiple models. Macros are defined in .sql files :\nget_key_description\n\n\n {#\n    This macro returns the description of the key\n#}\n\n{% macro get_key_description(key) -%}\n\n    case {{ key }}\n        when 0 then 'C'\n        when 1 then 'C#'\n        when 2 then 'D'\n        when 3 then 'D#'\n        when 4 then 'E'\n        when 5 then 'F'\n        when 6 then 'F#'\n        when 7 then 'G'\n        when 8 then 'G#'\n        when 9 then 'A'\n        when 10 then 'A#'\n        when 11 then 'B'\n\n    end\n\n{%- endmacro %}\n\nget_modality_description\n\n {#\n    This macro returns the description of the modality\n#}\n\n{% macro get_modality_description(mode) -%}\n\n    case {{ mode }}\n        when 0 then 'Minor'\n        when 1 then 'Major'\n       \n    end\n\n{%- endmacro %}\n\nNow that we have our files set up we are ready to run our model. We can see from the lineage that all the connections are complete :\n\n\n\ndbt_lineage.PNG\n\n\nWe can run the model from the dbt console using :\ndbt run -m <model_name.sql>\nAnd we can see from the system log that the run was successful :\n\n\n\ndbt_system_log.PNG\n\n\nAnd we have our table created in Big Query with 1,204,025 rows as expected.\n\n\n\nbiq_query_table.PNG\n\n\n\n\n\n\nWe’ve come a long way since downloading our raw csv file from Kaggle. Our journey is almost over. It’s time now to visualize our data and gather some insights. For this purpose I will be using Looker Studio. This will allow us to connect to our newly created table in BigQuery and create a Dashboard.\nThe first thing we need to do is create a data source. There are 23 different connectors at the time of writing. We will be using BigQuery :\n\n\n\nlooker_connectors.PNG\n\n\nOur recent dataset and table are sitting there ready for connection :\n\n\n\nlooker_dataset.PNG\n\n\nHit CONNECT and we see our fields or columns are there with default settings attached which can be modified if required. Finally hit CREATE REPORT and you are taken to a blank canvass dashboard where the magic begins :)\n\n\n\nblank_canvass.PNG\n\n\nFor a complete guide you can check out the Looker Documentation, but the console is very intuitive, and a few strokes of the brush (or clicks of the keyboard) and I was able to produce this dashboard (screenshot included below if you can’t access the link).\n\n\n\nLooker.PNG"
  }
]